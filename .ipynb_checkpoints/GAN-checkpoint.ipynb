{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "338b5920",
   "metadata": {},
   "source": [
    "## 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31ba8a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d248ca2f",
   "metadata": {},
   "source": [
    "## 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdfb7942",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_composes = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(28),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.5],[0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bbd120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(\"mnist_data\",train=True,transform=trans_composes,download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1508e268",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f384d2",
   "metadata": {},
   "source": [
    "### 查看数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ed56043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1875"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50bed2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89bf5e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28]) tensor([0, 0, 8, 9, 6, 9, 6, 5, 4, 6, 4, 7, 7, 1, 7, 2, 5, 2, 8, 8, 1, 0, 0, 6,\n",
      "        8, 1, 6, 9, 5, 5, 6, 5])\n"
     ]
    }
   ],
   "source": [
    "print(img.shape,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb64c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c2f10e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random noises dim\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb42d1a",
   "metadata": {},
   "source": [
    "## 生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c9840c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim,64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64,128),\n",
    "            nn.BatchNorm1d(128, 0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,256),\n",
    "            nn.BatchNorm1d(256, 0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256,512),\n",
    "            nn.BatchNorm1d(512, 0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512,1024),\n",
    "            nn.BatchNorm1d(1024, 0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024,int(np.prod(image_size))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self,z):\n",
    "        # shape of z: [batchsize, laten_dim]\n",
    "        output = self.model(z)\n",
    "        image = output.reshape(output.shape[0],*image_size)\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b27dda1",
   "metadata": {},
   "source": [
    "## 判别器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb37879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(discriminator,self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(image_size)),1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024,512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512,256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256,128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128,1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    def forward(self,image):\n",
    "        # shape of image: [batchsize, 1, 28, 28]\n",
    "        # output is probably\n",
    "        prob = self.model(image.reshape(image.shape[0],-1))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2178461a",
   "metadata": {},
   "source": [
    "### 使用gpu/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3faf6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12af0c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.cuda.FloatTensor"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tensor = torch.cuda.FloatTensor if device else torch.FloatTensor\n",
    "Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a835ec6",
   "metadata": {},
   "source": [
    "## 构建超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aacc35e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generator\n",
    "G = generator()\n",
    "g_optimizer = torch.optim.Adam(G.parameters(),lr=0.0002,betas=(0.5,0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1854a53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize discriminator\n",
    "D = discriminator()\n",
    "d_optimizer = torch.optim.Adam(D.parameters(),lr=0.0002,betas=(0.5,0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "757a7322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize args\n",
    "# def init_weights(m):\n",
    "#         if type(m) == nn.Linear:\n",
    "#             nn.init.xavier_uniform_(m.weight)\n",
    "# G.apply(init_weights)\n",
    "# D.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b70f9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bef9a6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCELoss()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use device\n",
    "G.to(device)\n",
    "D.to(device)\n",
    "loss_fn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bc2eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 20\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2a88ff",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3e5523c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.760875] [G loss: 1.625079]\n",
      "[D loss: 0.776660] [G loss: 1.735653]\n",
      "[D loss: 0.917835] [G loss: 1.469765]\n",
      "[D loss: 1.071759] [G loss: 1.428819]\n",
      "[D loss: 0.722857] [G loss: 1.517761]\n",
      "[D loss: 0.734444] [G loss: 1.637611]\n",
      "[D loss: 0.938878] [G loss: 1.496275]\n",
      "[D loss: 0.839379] [G loss: 1.403787]\n",
      "[D loss: 0.797666] [G loss: 1.388430]\n",
      "[D loss: 0.857912] [G loss: 1.368140]\n",
      "[D loss: 0.828004] [G loss: 1.514430]\n",
      "[D loss: 0.701043] [G loss: 1.560397]\n",
      "[D loss: 0.969081] [G loss: 1.599490]\n",
      "[D loss: 0.741001] [G loss: 1.583696]\n",
      "[D loss: 0.883016] [G loss: 1.362822]\n",
      "[D loss: 1.016472] [G loss: 1.574749]\n",
      "[D loss: 0.651479] [G loss: 1.417154]\n",
      "[D loss: 0.750450] [G loss: 1.443109]\n",
      "[D loss: 0.785593] [G loss: 1.421734]\n",
      "[D loss: 0.834163] [G loss: 1.443322]\n",
      "[D loss: 0.591033] [G loss: 1.560572]\n",
      "[D loss: 0.692166] [G loss: 1.569144]\n",
      "[D loss: 0.706793] [G loss: 1.408259]\n",
      "[D loss: 1.130300] [G loss: 1.424680]\n",
      "[D loss: 0.863933] [G loss: 1.526588]\n",
      "[D loss: 0.759748] [G loss: 1.651278]\n",
      "[D loss: 0.796627] [G loss: 1.519710]\n",
      "[D loss: 0.841525] [G loss: 1.619871]\n",
      "[D loss: 0.620789] [G loss: 1.595888]\n",
      "[D loss: 0.877628] [G loss: 1.260291]\n",
      "[D loss: 0.686165] [G loss: 1.435445]\n",
      "[D loss: 1.003248] [G loss: 1.563787]\n",
      "[D loss: 0.833613] [G loss: 1.582971]\n",
      "[D loss: 0.646872] [G loss: 1.589228]\n",
      "[D loss: 0.825406] [G loss: 1.539232]\n",
      "[D loss: 0.681161] [G loss: 1.706202]\n",
      "[D loss: 1.065198] [G loss: 1.455337]\n",
      "[D loss: 0.939520] [G loss: 1.357601]\n",
      "[D loss: 0.936222] [G loss: 1.476559]\n",
      "[D loss: 0.644407] [G loss: 1.669554]\n",
      "[D loss: 0.744827] [G loss: 1.651310]\n",
      "[D loss: 0.844586] [G loss: 1.435156]\n",
      "[D loss: 0.964834] [G loss: 1.573378]\n",
      "[D loss: 0.726872] [G loss: 1.562518]\n",
      "[D loss: 0.704620] [G loss: 1.357125]\n",
      "[D loss: 0.865522] [G loss: 1.529156]\n",
      "[D loss: 0.789544] [G loss: 1.632627]\n",
      "[D loss: 0.772999] [G loss: 1.695599]\n",
      "[D loss: 1.009403] [G loss: 1.422203]\n",
      "[D loss: 0.887868] [G loss: 1.497024]\n",
      "[D loss: 0.858940] [G loss: 1.338892]\n",
      "[D loss: 0.852731] [G loss: 1.274126]\n",
      "[D loss: 0.807042] [G loss: 1.495163]\n",
      "[D loss: 0.814198] [G loss: 1.755606]\n",
      "[D loss: 1.020765] [G loss: 1.744734]\n",
      "[D loss: 1.033864] [G loss: 1.442206]\n",
      "[D loss: 0.778185] [G loss: 1.375428]\n",
      "[D loss: 0.681008] [G loss: 1.378915]\n",
      "[D loss: 0.589789] [G loss: 1.523773]\n",
      "[D loss: 1.007239] [G loss: 1.300576]\n",
      "[D loss: 0.660758] [G loss: 1.520252]\n",
      "[D loss: 0.865174] [G loss: 1.635956]\n",
      "[D loss: 0.631271] [G loss: 1.646249]\n",
      "[D loss: 0.751506] [G loss: 1.541427]\n",
      "[D loss: 0.920840] [G loss: 1.295335]\n",
      "[D loss: 0.850751] [G loss: 1.660145]\n",
      "[D loss: 0.871424] [G loss: 1.485690]\n",
      "[D loss: 0.772890] [G loss: 1.470113]\n",
      "[D loss: 0.800006] [G loss: 1.300164]\n",
      "[D loss: 1.081198] [G loss: 1.548558]\n",
      "[D loss: 0.850199] [G loss: 1.438232]\n",
      "[D loss: 0.981209] [G loss: 1.352596]\n",
      "[D loss: 0.766633] [G loss: 1.480657]\n",
      "[D loss: 0.639101] [G loss: 1.578615]\n",
      "[D loss: 0.888413] [G loss: 1.450844]\n",
      "[D loss: 0.918173] [G loss: 1.433755]\n",
      "[D loss: 0.837774] [G loss: 1.427799]\n",
      "[D loss: 0.928100] [G loss: 1.497204]\n",
      "[D loss: 0.701333] [G loss: 1.364829]\n",
      "[D loss: 0.701116] [G loss: 1.373682]\n",
      "[D loss: 1.026849] [G loss: 1.497905]\n",
      "[D loss: 0.871025] [G loss: 1.446894]\n",
      "[D loss: 0.881987] [G loss: 1.249369]\n",
      "[D loss: 0.748273] [G loss: 1.492211]\n",
      "[D loss: 0.844287] [G loss: 1.563174]\n",
      "[D loss: 0.971123] [G loss: 1.457812]\n",
      "[D loss: 0.555763] [G loss: 1.613973]\n",
      "[D loss: 1.105862] [G loss: 1.618929]\n",
      "[D loss: 0.753328] [G loss: 1.627863]\n",
      "[D loss: 0.745322] [G loss: 1.482986]\n",
      "[D loss: 0.785248] [G loss: 1.413769]\n",
      "[D loss: 0.720005] [G loss: 1.148098]\n",
      "[D loss: 0.940771] [G loss: 1.384422]\n",
      "[D loss: 0.901209] [G loss: 1.456465]\n",
      "[D loss: 0.647726] [G loss: 1.383100]\n",
      "[D loss: 0.600073] [G loss: 1.677780]\n",
      "[D loss: 0.789397] [G loss: 1.644643]\n",
      "[D loss: 0.600090] [G loss: 1.629964]\n",
      "[D loss: 0.546900] [G loss: 1.598764]\n",
      "[D loss: 0.712909] [G loss: 1.411145]\n",
      "[D loss: 0.755677] [G loss: 1.469593]\n",
      "[D loss: 0.541259] [G loss: 1.553038]\n",
      "[D loss: 0.627752] [G loss: 1.579434]\n",
      "[D loss: 0.846714] [G loss: 1.543805]\n",
      "[D loss: 0.849375] [G loss: 1.480480]\n",
      "[D loss: 0.819093] [G loss: 1.429746]\n",
      "[D loss: 1.105056] [G loss: 1.539590]\n",
      "[D loss: 0.679441] [G loss: 1.461819]\n",
      "[D loss: 0.594446] [G loss: 1.587126]\n",
      "[D loss: 0.844300] [G loss: 1.501165]\n",
      "[D loss: 1.060046] [G loss: 1.480886]\n",
      "[D loss: 0.768085] [G loss: 1.592377]\n",
      "[D loss: 0.931178] [G loss: 1.558608]\n",
      "[D loss: 0.695383] [G loss: 1.505894]\n",
      "[D loss: 0.936792] [G loss: 1.409074]\n",
      "[D loss: 0.936765] [G loss: 1.428024]\n",
      "[D loss: 0.809171] [G loss: 1.317048]\n",
      "[D loss: 0.801485] [G loss: 1.759758]\n",
      "[D loss: 0.996582] [G loss: 1.583505]\n",
      "[D loss: 0.829386] [G loss: 1.469527]\n",
      "[D loss: 0.687179] [G loss: 1.682036]\n",
      "[D loss: 0.881340] [G loss: 1.294913]\n",
      "[D loss: 0.849681] [G loss: 1.618844]\n",
      "[D loss: 0.846729] [G loss: 1.493684]\n",
      "[D loss: 0.858053] [G loss: 1.419356]\n",
      "[D loss: 0.829447] [G loss: 1.341519]\n",
      "[D loss: 0.946239] [G loss: 1.291097]\n",
      "[D loss: 0.791818] [G loss: 1.527516]\n",
      "[D loss: 0.922204] [G loss: 1.469836]\n",
      "[D loss: 0.558229] [G loss: 1.475822]\n",
      "[D loss: 0.698256] [G loss: 1.444889]\n",
      "[D loss: 0.720103] [G loss: 1.269754]\n",
      "[D loss: 0.736077] [G loss: 1.427257]\n",
      "[D loss: 0.605336] [G loss: 1.949494]\n",
      "[D loss: 1.069376] [G loss: 1.159661]\n",
      "[D loss: 0.803290] [G loss: 1.591392]\n",
      "[D loss: 0.663135] [G loss: 1.518578]\n",
      "[D loss: 0.777978] [G loss: 1.549822]\n",
      "[D loss: 0.872980] [G loss: 1.547000]\n",
      "[D loss: 1.047793] [G loss: 1.645019]\n",
      "[D loss: 0.837443] [G loss: 1.401037]\n",
      "[D loss: 0.846013] [G loss: 1.631557]\n",
      "[D loss: 0.760055] [G loss: 1.540100]\n",
      "[D loss: 0.759434] [G loss: 1.406520]\n",
      "[D loss: 1.048018] [G loss: 1.332721]\n",
      "[D loss: 0.801778] [G loss: 1.572195]\n",
      "[D loss: 0.614387] [G loss: 1.461719]\n",
      "[D loss: 0.511420] [G loss: 1.752741]\n",
      "[D loss: 0.851174] [G loss: 1.794340]\n",
      "[D loss: 0.891000] [G loss: 1.465072]\n",
      "[D loss: 0.703047] [G loss: 1.755022]\n",
      "[D loss: 0.811464] [G loss: 1.379736]\n",
      "[D loss: 0.759270] [G loss: 1.469402]\n",
      "[D loss: 0.730291] [G loss: 1.656988]\n",
      "[D loss: 1.002548] [G loss: 1.571040]\n",
      "[D loss: 0.887594] [G loss: 1.528643]\n",
      "[D loss: 0.952570] [G loss: 1.409055]\n",
      "[D loss: 0.818148] [G loss: 1.240868]\n",
      "[D loss: 0.718641] [G loss: 1.481849]\n",
      "[D loss: 1.137156] [G loss: 1.385285]\n",
      "[D loss: 0.616910] [G loss: 1.642706]\n",
      "[D loss: 0.832115] [G loss: 1.465994]\n",
      "[D loss: 0.763875] [G loss: 1.529352]\n",
      "[D loss: 0.657927] [G loss: 1.549654]\n",
      "[D loss: 0.727914] [G loss: 1.457911]\n",
      "[D loss: 0.699401] [G loss: 1.399196]\n",
      "[D loss: 0.579892] [G loss: 1.584102]\n",
      "[D loss: 0.756111] [G loss: 1.506726]\n",
      "[D loss: 0.610400] [G loss: 1.613494]\n",
      "[D loss: 0.759161] [G loss: 1.562716]\n",
      "[D loss: 0.691741] [G loss: 1.622837]\n",
      "[D loss: 0.861365] [G loss: 1.318778]\n",
      "[D loss: 0.596065] [G loss: 1.529881]\n",
      "[D loss: 0.855291] [G loss: 1.459989]\n",
      "[D loss: 0.511934] [G loss: 1.811462]\n",
      "[D loss: 0.649098] [G loss: 1.532599]\n",
      "[D loss: 0.713799] [G loss: 1.795243]\n",
      "[D loss: 0.804899] [G loss: 1.498342]\n",
      "[D loss: 1.002617] [G loss: 1.495633]\n",
      "[D loss: 0.900993] [G loss: 1.591361]\n",
      "[D loss: 0.823711] [G loss: 1.476015]\n",
      "[D loss: 0.778795] [G loss: 1.487348]\n",
      "[D loss: 0.699528] [G loss: 1.486677]\n",
      "[D loss: 1.046395] [G loss: 1.782073]\n",
      "[D loss: 0.971111] [G loss: 1.568792]\n",
      "[D loss: 0.836497] [G loss: 1.329382]\n",
      "[D loss: 1.127784] [G loss: 1.349879]\n",
      "[D loss: 0.658926] [G loss: 1.489110]\n",
      "[D loss: 0.933471] [G loss: 1.523678]\n",
      "[D loss: 0.765055] [G loss: 1.636991]\n",
      "[D loss: 1.246549] [G loss: 1.265857]\n",
      "[D loss: 0.914397] [G loss: 1.372733]\n",
      "[D loss: 1.096953] [G loss: 1.179438]\n",
      "[D loss: 0.988028] [G loss: 1.253427]\n",
      "[D loss: 0.791660] [G loss: 1.616287]\n",
      "[D loss: 0.749396] [G loss: 1.406975]\n",
      "[D loss: 0.742894] [G loss: 1.452722]\n",
      "[D loss: 0.956725] [G loss: 1.531757]\n",
      "[D loss: 0.751478] [G loss: 1.330149]\n",
      "[D loss: 0.932548] [G loss: 1.226579]\n",
      "[D loss: 0.790790] [G loss: 1.535918]\n",
      "[D loss: 0.806248] [G loss: 1.533340]\n",
      "[D loss: 0.909911] [G loss: 1.475389]\n",
      "[D loss: 0.825074] [G loss: 1.268940]\n",
      "[D loss: 0.860041] [G loss: 1.360795]\n",
      "[D loss: 0.648095] [G loss: 1.469979]\n",
      "[D loss: 0.771229] [G loss: 1.370077]\n",
      "[D loss: 0.982371] [G loss: 1.254138]\n",
      "[D loss: 0.964699] [G loss: 1.366354]\n",
      "[D loss: 0.954611] [G loss: 1.540274]\n",
      "[D loss: 0.830496] [G loss: 1.535398]\n",
      "[D loss: 0.838192] [G loss: 1.519788]\n",
      "[D loss: 0.637701] [G loss: 1.518186]\n",
      "[D loss: 0.951823] [G loss: 1.799383]\n",
      "[D loss: 0.887537] [G loss: 1.586323]\n",
      "[D loss: 0.655784] [G loss: 1.626890]\n",
      "[D loss: 0.842074] [G loss: 1.253294]\n",
      "[D loss: 0.708107] [G loss: 1.261560]\n",
      "[D loss: 0.932736] [G loss: 1.343455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.659533] [G loss: 1.440372]\n",
      "[D loss: 0.846262] [G loss: 1.529022]\n",
      "[D loss: 0.919827] [G loss: 1.481073]\n",
      "[D loss: 0.725185] [G loss: 1.585068]\n",
      "[D loss: 0.851882] [G loss: 1.619965]\n",
      "[D loss: 1.032439] [G loss: 1.214063]\n",
      "[D loss: 0.548891] [G loss: 1.727959]\n",
      "[D loss: 0.735434] [G loss: 1.507069]\n",
      "[D loss: 0.842670] [G loss: 1.529307]\n",
      "[D loss: 0.799185] [G loss: 1.491189]\n",
      "[D loss: 0.853735] [G loss: 1.431814]\n",
      "[D loss: 0.738747] [G loss: 1.554512]\n",
      "[D loss: 0.678845] [G loss: 1.563904]\n",
      "[D loss: 0.799182] [G loss: 1.561727]\n",
      "[D loss: 0.889197] [G loss: 1.438356]\n",
      "[D loss: 0.718956] [G loss: 1.584556]\n",
      "[D loss: 0.754086] [G loss: 1.399008]\n",
      "[D loss: 1.022797] [G loss: 1.348977]\n",
      "[D loss: 0.625731] [G loss: 1.618219]\n",
      "[D loss: 0.671767] [G loss: 1.698970]\n",
      "[D loss: 0.727515] [G loss: 1.527528]\n",
      "[D loss: 0.792094] [G loss: 1.483293]\n",
      "[D loss: 0.704076] [G loss: 1.514463]\n",
      "[D loss: 0.630744] [G loss: 1.794778]\n",
      "[D loss: 0.763910] [G loss: 1.575302]\n",
      "[D loss: 1.034060] [G loss: 1.475943]\n",
      "[D loss: 0.654722] [G loss: 1.551444]\n",
      "[D loss: 0.818845] [G loss: 1.357651]\n",
      "[D loss: 0.686749] [G loss: 1.636798]\n",
      "[D loss: 0.806227] [G loss: 1.648822]\n",
      "[D loss: 1.011533] [G loss: 1.579363]\n",
      "[D loss: 0.739220] [G loss: 1.710396]\n",
      "[D loss: 0.783841] [G loss: 1.403451]\n",
      "[D loss: 0.631059] [G loss: 1.576686]\n",
      "[D loss: 0.972719] [G loss: 1.416765]\n",
      "[D loss: 0.688538] [G loss: 1.775651]\n",
      "[D loss: 0.724629] [G loss: 1.882987]\n",
      "[D loss: 0.848935] [G loss: 1.585743]\n",
      "[D loss: 0.899155] [G loss: 1.595140]\n",
      "[D loss: 0.725950] [G loss: 1.753103]\n",
      "[D loss: 0.927220] [G loss: 1.483071]\n",
      "[D loss: 0.785123] [G loss: 1.332755]\n",
      "[D loss: 1.008595] [G loss: 1.309244]\n",
      "[D loss: 0.698032] [G loss: 1.452151]\n",
      "[D loss: 0.543717] [G loss: 1.889589]\n",
      "[D loss: 0.973220] [G loss: 1.263320]\n",
      "[D loss: 0.797240] [G loss: 1.621203]\n",
      "[D loss: 0.748679] [G loss: 1.437988]\n",
      "[D loss: 0.840475] [G loss: 1.627936]\n",
      "[D loss: 0.770719] [G loss: 1.340222]\n",
      "[D loss: 0.707882] [G loss: 1.235147]\n",
      "[D loss: 0.893097] [G loss: 1.525894]\n",
      "[D loss: 0.701570] [G loss: 1.508885]\n",
      "[D loss: 0.839221] [G loss: 1.656025]\n",
      "[D loss: 0.753449] [G loss: 1.420573]\n",
      "[D loss: 0.882486] [G loss: 1.888006]\n",
      "[D loss: 0.800424] [G loss: 1.806819]\n",
      "[D loss: 0.862513] [G loss: 1.450066]\n",
      "[D loss: 0.891063] [G loss: 1.647519]\n",
      "[D loss: 0.983480] [G loss: 1.519661]\n",
      "[D loss: 0.846235] [G loss: 1.234859]\n",
      "[D loss: 0.656963] [G loss: 1.395277]\n",
      "[D loss: 0.776390] [G loss: 1.675237]\n",
      "[D loss: 0.745445] [G loss: 1.691236]\n",
      "[D loss: 0.865432] [G loss: 1.488416]\n",
      "[D loss: 0.770809] [G loss: 1.400180]\n",
      "[D loss: 0.867675] [G loss: 1.589622]\n",
      "[D loss: 0.610275] [G loss: 1.294243]\n",
      "[D loss: 1.071034] [G loss: 1.671748]\n",
      "[D loss: 0.781972] [G loss: 1.812342]\n",
      "[D loss: 0.648422] [G loss: 1.474859]\n",
      "[D loss: 0.713277] [G loss: 1.720908]\n",
      "[D loss: 0.702873] [G loss: 1.439570]\n",
      "[D loss: 0.673466] [G loss: 1.816004]\n",
      "[D loss: 0.707135] [G loss: 1.795074]\n",
      "[D loss: 0.801785] [G loss: 1.652336]\n",
      "[D loss: 0.685730] [G loss: 1.393551]\n",
      "[D loss: 0.947480] [G loss: 1.333814]\n",
      "[D loss: 0.648233] [G loss: 1.550580]\n",
      "[D loss: 0.821324] [G loss: 1.619715]\n",
      "[D loss: 0.958504] [G loss: 1.651446]\n",
      "[D loss: 0.845555] [G loss: 1.461607]\n",
      "[D loss: 1.053762] [G loss: 1.724971]\n",
      "[D loss: 0.758348] [G loss: 1.496935]\n",
      "[D loss: 0.754160] [G loss: 1.819844]\n",
      "[D loss: 0.689392] [G loss: 1.489977]\n",
      "[D loss: 0.809243] [G loss: 1.460385]\n",
      "[D loss: 0.839377] [G loss: 1.586161]\n",
      "[D loss: 0.979376] [G loss: 1.312407]\n",
      "[D loss: 0.642992] [G loss: 1.582685]\n",
      "[D loss: 0.919537] [G loss: 1.747166]\n",
      "[D loss: 0.691805] [G loss: 1.595991]\n",
      "[D loss: 0.864002] [G loss: 1.533983]\n",
      "[D loss: 0.512703] [G loss: 1.417197]\n",
      "[D loss: 0.680893] [G loss: 1.769452]\n",
      "[D loss: 0.768802] [G loss: 1.840104]\n",
      "[D loss: 0.703105] [G loss: 1.534471]\n",
      "[D loss: 0.896261] [G loss: 1.469789]\n",
      "[D loss: 0.728263] [G loss: 1.960740]\n",
      "[D loss: 0.947199] [G loss: 1.400729]\n",
      "[D loss: 1.002929] [G loss: 1.478470]\n",
      "[D loss: 1.064809] [G loss: 1.736682]\n",
      "[D loss: 0.875107] [G loss: 1.363986]\n",
      "[D loss: 1.080193] [G loss: 1.517103]\n",
      "[D loss: 0.795580] [G loss: 1.498837]\n",
      "[D loss: 0.948316] [G loss: 1.553953]\n",
      "[D loss: 0.787066] [G loss: 1.236630]\n",
      "[D loss: 0.783052] [G loss: 1.401860]\n",
      "[D loss: 0.773925] [G loss: 1.512031]\n",
      "[D loss: 0.993039] [G loss: 1.975835]\n",
      "[D loss: 0.849744] [G loss: 1.524672]\n",
      "[D loss: 0.732318] [G loss: 1.513495]\n",
      "[D loss: 0.702770] [G loss: 1.277459]\n",
      "[D loss: 1.244166] [G loss: 1.198814]\n",
      "[D loss: 0.756660] [G loss: 1.460159]\n",
      "[D loss: 0.892981] [G loss: 2.025781]\n",
      "[D loss: 1.033937] [G loss: 1.730797]\n",
      "[D loss: 0.905476] [G loss: 1.568445]\n",
      "[D loss: 0.636324] [G loss: 1.698304]\n",
      "[D loss: 0.799866] [G loss: 1.375110]\n",
      "[D loss: 0.727234] [G loss: 1.875452]\n",
      "[D loss: 0.713722] [G loss: 1.597220]\n",
      "[D loss: 0.795546] [G loss: 1.412777]\n",
      "[D loss: 0.918010] [G loss: 1.548224]\n",
      "[D loss: 0.690329] [G loss: 1.519062]\n",
      "[D loss: 0.990352] [G loss: 1.689501]\n",
      "[D loss: 0.757226] [G loss: 1.486180]\n",
      "[D loss: 0.815082] [G loss: 1.458444]\n",
      "[D loss: 0.711909] [G loss: 1.724888]\n",
      "[D loss: 0.804064] [G loss: 1.498894]\n",
      "[D loss: 0.697133] [G loss: 1.784890]\n",
      "[D loss: 0.808385] [G loss: 1.781993]\n",
      "[D loss: 0.752196] [G loss: 1.429386]\n",
      "[D loss: 0.727801] [G loss: 1.400768]\n",
      "[D loss: 0.849852] [G loss: 1.532112]\n",
      "[D loss: 0.588122] [G loss: 1.559268]\n",
      "[D loss: 1.122195] [G loss: 1.414021]\n",
      "[D loss: 0.838311] [G loss: 1.360185]\n",
      "[D loss: 0.831161] [G loss: 1.382659]\n",
      "[D loss: 0.919832] [G loss: 1.546152]\n",
      "[D loss: 0.986358] [G loss: 1.173502]\n",
      "[D loss: 0.931850] [G loss: 1.474745]\n",
      "[D loss: 0.681837] [G loss: 1.472323]\n",
      "[D loss: 0.616866] [G loss: 1.494461]\n",
      "[D loss: 0.748674] [G loss: 1.407756]\n",
      "[D loss: 0.793219] [G loss: 1.518947]\n",
      "[D loss: 1.011740] [G loss: 1.527417]\n",
      "[D loss: 0.831364] [G loss: 1.731597]\n",
      "[D loss: 0.740767] [G loss: 1.585261]\n",
      "[D loss: 0.787275] [G loss: 1.370724]\n",
      "[D loss: 0.914808] [G loss: 1.420048]\n",
      "[D loss: 1.008579] [G loss: 1.685397]\n",
      "[D loss: 0.728152] [G loss: 1.646133]\n",
      "[D loss: 0.855094] [G loss: 1.471167]\n",
      "[D loss: 0.777081] [G loss: 1.509832]\n",
      "[D loss: 0.796443] [G loss: 1.523903]\n",
      "[D loss: 0.811163] [G loss: 1.696982]\n",
      "[D loss: 0.649289] [G loss: 1.644276]\n",
      "[D loss: 0.747387] [G loss: 1.464764]\n",
      "[D loss: 0.813228] [G loss: 1.443821]\n",
      "[D loss: 0.814098] [G loss: 1.424130]\n",
      "[D loss: 0.684917] [G loss: 1.624352]\n",
      "[D loss: 0.796534] [G loss: 1.353348]\n",
      "[D loss: 0.467504] [G loss: 1.816072]\n",
      "[D loss: 0.911894] [G loss: 1.553527]\n",
      "[D loss: 0.737806] [G loss: 1.424146]\n",
      "[D loss: 0.803693] [G loss: 1.339816]\n",
      "[D loss: 0.799259] [G loss: 1.370863]\n",
      "[D loss: 0.738967] [G loss: 1.660373]\n",
      "[D loss: 0.832615] [G loss: 1.871176]\n",
      "[D loss: 0.950349] [G loss: 1.404206]\n",
      "[D loss: 0.802940] [G loss: 1.414786]\n",
      "[D loss: 0.951183] [G loss: 1.340945]\n",
      "[D loss: 0.698667] [G loss: 1.332205]\n",
      "[D loss: 0.795059] [G loss: 1.409353]\n",
      "[D loss: 0.600495] [G loss: 1.409287]\n",
      "[D loss: 0.792153] [G loss: 1.707062]\n",
      "[D loss: 0.986050] [G loss: 1.769945]\n",
      "[D loss: 0.903520] [G loss: 1.530811]\n",
      "[D loss: 0.919728] [G loss: 1.369765]\n",
      "[D loss: 0.832681] [G loss: 1.461496]\n",
      "[D loss: 0.922123] [G loss: 1.229405]\n",
      "[D loss: 0.684678] [G loss: 1.400982]\n",
      "[D loss: 0.816242] [G loss: 1.536121]\n",
      "[D loss: 0.759831] [G loss: 1.765850]\n",
      "[D loss: 0.723848] [G loss: 1.645537]\n",
      "[D loss: 0.855060] [G loss: 1.577848]\n",
      "[D loss: 0.702575] [G loss: 1.725711]\n",
      "[D loss: 0.660388] [G loss: 1.438746]\n",
      "[D loss: 0.721669] [G loss: 1.447447]\n",
      "[D loss: 0.879628] [G loss: 1.464165]\n",
      "[D loss: 0.665886] [G loss: 1.530778]\n",
      "[D loss: 0.789911] [G loss: 1.608027]\n",
      "[D loss: 0.803867] [G loss: 1.555604]\n",
      "[D loss: 0.758341] [G loss: 1.753984]\n",
      "[D loss: 0.694512] [G loss: 1.676714]\n",
      "[D loss: 0.827185] [G loss: 1.542328]\n",
      "[D loss: 0.683417] [G loss: 1.705720]\n",
      "[D loss: 1.004480] [G loss: 1.729636]\n",
      "[D loss: 0.959487] [G loss: 1.213181]\n",
      "[D loss: 0.720756] [G loss: 1.451924]\n",
      "[D loss: 1.023099] [G loss: 1.282423]\n",
      "[D loss: 0.954238] [G loss: 1.808280]\n",
      "[D loss: 0.723394] [G loss: 1.894048]\n",
      "[D loss: 0.587716] [G loss: 1.580044]\n",
      "[D loss: 0.690026] [G loss: 1.427359]\n",
      "[D loss: 0.894471] [G loss: 1.626382]\n",
      "[D loss: 0.851806] [G loss: 1.470937]\n",
      "[D loss: 0.760906] [G loss: 1.330140]\n",
      "[D loss: 0.787779] [G loss: 1.660161]\n",
      "[D loss: 1.035610] [G loss: 1.490812]\n",
      "[D loss: 0.751433] [G loss: 1.513955]\n",
      "[D loss: 0.940768] [G loss: 1.588265]\n",
      "[D loss: 0.751229] [G loss: 1.445564]\n",
      "[D loss: 0.853336] [G loss: 1.521677]\n",
      "[D loss: 0.715183] [G loss: 1.607236]\n",
      "[D loss: 0.876258] [G loss: 1.239342]\n",
      "[D loss: 0.829282] [G loss: 1.330316]\n",
      "[D loss: 0.899269] [G loss: 1.533898]\n",
      "[D loss: 0.909091] [G loss: 1.478334]\n",
      "[D loss: 0.688286] [G loss: 1.332917]\n",
      "[D loss: 0.799949] [G loss: 1.576006]\n",
      "[D loss: 0.827607] [G loss: 1.429605]\n",
      "[D loss: 0.735302] [G loss: 1.441716]\n",
      "[D loss: 0.654235] [G loss: 1.424984]\n",
      "[D loss: 0.786666] [G loss: 1.400314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.773046] [G loss: 1.530064]\n",
      "[D loss: 0.834925] [G loss: 1.747313]\n",
      "[D loss: 0.856621] [G loss: 1.334038]\n",
      "[D loss: 0.649806] [G loss: 1.261252]\n",
      "[D loss: 0.838709] [G loss: 1.543360]\n",
      "[D loss: 0.691607] [G loss: 1.695953]\n",
      "[D loss: 0.668939] [G loss: 1.757574]\n",
      "[D loss: 0.758928] [G loss: 1.738251]\n",
      "[D loss: 0.798282] [G loss: 1.299993]\n",
      "[D loss: 0.787334] [G loss: 1.540055]\n",
      "[D loss: 0.956524] [G loss: 1.675674]\n",
      "[D loss: 1.018682] [G loss: 1.783654]\n",
      "[D loss: 0.751777] [G loss: 1.513803]\n",
      "[D loss: 0.906567] [G loss: 1.225568]\n",
      "[D loss: 0.811762] [G loss: 1.408052]\n",
      "[D loss: 0.856737] [G loss: 1.402449]\n",
      "[D loss: 0.559318] [G loss: 1.647343]\n",
      "[D loss: 0.881065] [G loss: 1.529432]\n",
      "[D loss: 0.887951] [G loss: 1.427026]\n",
      "[D loss: 0.939617] [G loss: 1.577906]\n",
      "[D loss: 0.696803] [G loss: 1.559899]\n",
      "[D loss: 1.256428] [G loss: 1.548713]\n",
      "[D loss: 0.757646] [G loss: 1.324991]\n",
      "[D loss: 0.872702] [G loss: 1.541808]\n",
      "[D loss: 0.915174] [G loss: 1.496535]\n",
      "[D loss: 0.927728] [G loss: 1.565568]\n",
      "[D loss: 0.767221] [G loss: 1.671137]\n",
      "[D loss: 0.810748] [G loss: 1.282313]\n",
      "[D loss: 0.707469] [G loss: 1.446164]\n",
      "[D loss: 0.834326] [G loss: 1.462868]\n",
      "[D loss: 0.971602] [G loss: 1.164099]\n",
      "[D loss: 1.014752] [G loss: 1.542006]\n",
      "[D loss: 0.539703] [G loss: 1.673605]\n",
      "[D loss: 0.781241] [G loss: 1.525054]\n",
      "[D loss: 0.730007] [G loss: 1.773681]\n",
      "[D loss: 0.975500] [G loss: 1.594918]\n",
      "[D loss: 0.757077] [G loss: 1.657395]\n",
      "[D loss: 0.767776] [G loss: 1.466325]\n",
      "[D loss: 0.797040] [G loss: 1.502038]\n",
      "[D loss: 0.810244] [G loss: 1.361968]\n",
      "[D loss: 0.729981] [G loss: 1.384224]\n",
      "[D loss: 0.855029] [G loss: 1.450521]\n",
      "[D loss: 0.948908] [G loss: 1.556041]\n",
      "[D loss: 0.792660] [G loss: 1.735150]\n",
      "[D loss: 0.910753] [G loss: 1.500440]\n",
      "[D loss: 0.587610] [G loss: 1.498523]\n",
      "[D loss: 0.956159] [G loss: 1.326481]\n",
      "[D loss: 0.796756] [G loss: 1.784565]\n",
      "[D loss: 0.660106] [G loss: 1.491861]\n",
      "[D loss: 1.015347] [G loss: 1.387265]\n",
      "[D loss: 0.726526] [G loss: 1.619233]\n",
      "[D loss: 0.843894] [G loss: 1.754144]\n",
      "[D loss: 0.602640] [G loss: 1.727368]\n",
      "[D loss: 0.811436] [G loss: 1.684295]\n",
      "[D loss: 0.608845] [G loss: 1.458021]\n",
      "[D loss: 0.817389] [G loss: 1.395476]\n",
      "[D loss: 1.016329] [G loss: 1.079901]\n",
      "[D loss: 0.873955] [G loss: 1.488079]\n",
      "[D loss: 0.929573] [G loss: 1.776999]\n",
      "[D loss: 1.062258] [G loss: 1.347425]\n",
      "[D loss: 0.832399] [G loss: 1.453775]\n",
      "[D loss: 0.797776] [G loss: 1.689675]\n",
      "[D loss: 0.792763] [G loss: 1.353233]\n",
      "[D loss: 0.849617] [G loss: 1.442408]\n",
      "[D loss: 0.894608] [G loss: 1.344970]\n",
      "[D loss: 0.655541] [G loss: 1.442327]\n",
      "[D loss: 0.740960] [G loss: 1.506609]\n",
      "[D loss: 0.983049] [G loss: 1.270988]\n",
      "[D loss: 0.549894] [G loss: 1.565033]\n",
      "[D loss: 0.803055] [G loss: 1.558646]\n",
      "[D loss: 0.597216] [G loss: 1.617148]\n",
      "[D loss: 0.596776] [G loss: 1.830714]\n",
      "[D loss: 0.682762] [G loss: 1.560252]\n",
      "[D loss: 0.847088] [G loss: 1.428137]\n",
      "[D loss: 0.809406] [G loss: 1.719325]\n",
      "[D loss: 0.849650] [G loss: 1.727839]\n",
      "[D loss: 1.041938] [G loss: 1.623665]\n",
      "[D loss: 1.005986] [G loss: 1.396931]\n",
      "[D loss: 0.722342] [G loss: 1.781471]\n",
      "[D loss: 0.834950] [G loss: 1.479564]\n",
      "[D loss: 0.725408] [G loss: 1.549500]\n",
      "[D loss: 1.053633] [G loss: 1.257332]\n",
      "[D loss: 0.731442] [G loss: 1.515735]\n",
      "[D loss: 1.016384] [G loss: 1.409833]\n",
      "[D loss: 0.796808] [G loss: 1.600682]\n",
      "[D loss: 0.841477] [G loss: 1.524357]\n",
      "[D loss: 0.667002] [G loss: 1.492657]\n",
      "[D loss: 0.750745] [G loss: 1.691440]\n",
      "[D loss: 0.706891] [G loss: 1.566737]\n",
      "[D loss: 0.722296] [G loss: 1.501385]\n",
      "[D loss: 0.808637] [G loss: 1.317200]\n",
      "[D loss: 0.690138] [G loss: 1.733875]\n",
      "[D loss: 0.968821] [G loss: 1.588317]\n",
      "[D loss: 0.835900] [G loss: 1.526587]\n",
      "[D loss: 0.989888] [G loss: 1.261607]\n",
      "[D loss: 1.119979] [G loss: 1.415455]\n",
      "[D loss: 0.788942] [G loss: 1.566465]\n",
      "[D loss: 1.076261] [G loss: 1.276547]\n",
      "[D loss: 0.762978] [G loss: 1.379266]\n",
      "[D loss: 0.772610] [G loss: 1.357753]\n",
      "[D loss: 0.926904] [G loss: 1.581282]\n",
      "[D loss: 0.770369] [G loss: 1.578505]\n",
      "[D loss: 0.827316] [G loss: 1.286598]\n",
      "[D loss: 0.786113] [G loss: 1.356947]\n",
      "[D loss: 0.918484] [G loss: 1.346659]\n",
      "[D loss: 0.643291] [G loss: 1.483506]\n",
      "[D loss: 0.697609] [G loss: 1.299914]\n",
      "[D loss: 0.742047] [G loss: 1.437321]\n",
      "[D loss: 1.157457] [G loss: 1.444275]\n",
      "[D loss: 0.668466] [G loss: 1.483648]\n",
      "[D loss: 0.718559] [G loss: 1.578469]\n",
      "[D loss: 0.824349] [G loss: 1.253849]\n",
      "[D loss: 0.951017] [G loss: 1.477616]\n",
      "[D loss: 0.874920] [G loss: 1.672894]\n",
      "[D loss: 0.666872] [G loss: 1.632712]\n",
      "[D loss: 0.873208] [G loss: 1.816595]\n",
      "[D loss: 0.686035] [G loss: 1.529575]\n",
      "[D loss: 1.105003] [G loss: 1.347730]\n",
      "[D loss: 0.686399] [G loss: 1.296302]\n",
      "[D loss: 0.758007] [G loss: 1.800558]\n",
      "[D loss: 0.964495] [G loss: 1.605057]\n",
      "[D loss: 0.770323] [G loss: 1.404520]\n",
      "[D loss: 0.790669] [G loss: 1.408796]\n",
      "[D loss: 0.835364] [G loss: 1.272074]\n",
      "[D loss: 0.896430] [G loss: 1.519962]\n",
      "[D loss: 0.743575] [G loss: 1.508465]\n",
      "[D loss: 0.849297] [G loss: 1.672326]\n",
      "[D loss: 0.640283] [G loss: 1.494967]\n",
      "[D loss: 0.782773] [G loss: 1.225327]\n",
      "[D loss: 0.492937] [G loss: 1.754342]\n",
      "[D loss: 1.125944] [G loss: 1.736125]\n",
      "[D loss: 0.704627] [G loss: 1.490251]\n",
      "[D loss: 0.673984] [G loss: 1.414764]\n",
      "[D loss: 0.658804] [G loss: 1.510821]\n",
      "[D loss: 0.854619] [G loss: 1.583780]\n",
      "[D loss: 0.950200] [G loss: 1.398681]\n",
      "[D loss: 1.048954] [G loss: 1.319303]\n",
      "[D loss: 1.007331] [G loss: 1.741632]\n",
      "[D loss: 1.132902] [G loss: 1.754092]\n",
      "[D loss: 0.901256] [G loss: 1.481513]\n",
      "[D loss: 0.928175] [G loss: 1.190700]\n",
      "[D loss: 0.658905] [G loss: 1.389823]\n",
      "[D loss: 0.619201] [G loss: 1.536930]\n",
      "[D loss: 0.846286] [G loss: 1.464319]\n",
      "[D loss: 0.972250] [G loss: 1.298783]\n",
      "[D loss: 0.740383] [G loss: 1.659466]\n",
      "[D loss: 0.856066] [G loss: 1.444632]\n",
      "[D loss: 0.838121] [G loss: 1.443023]\n",
      "[D loss: 0.665005] [G loss: 1.425411]\n",
      "[D loss: 0.806160] [G loss: 1.392382]\n",
      "[D loss: 0.818308] [G loss: 1.381317]\n",
      "[D loss: 0.953148] [G loss: 1.551871]\n",
      "[D loss: 0.903622] [G loss: 1.572760]\n",
      "[D loss: 0.685812] [G loss: 1.458346]\n",
      "[D loss: 0.598756] [G loss: 1.417233]\n",
      "[D loss: 0.822990] [G loss: 1.602372]\n",
      "[D loss: 0.849240] [G loss: 1.519325]\n",
      "[D loss: 0.932575] [G loss: 1.439121]\n",
      "[D loss: 0.779754] [G loss: 1.468774]\n",
      "[D loss: 0.946995] [G loss: 1.368536]\n",
      "[D loss: 0.956194] [G loss: 1.313822]\n",
      "[D loss: 0.791011] [G loss: 1.646054]\n",
      "[D loss: 0.834360] [G loss: 1.666509]\n",
      "[D loss: 0.740607] [G loss: 1.592488]\n",
      "[D loss: 0.672045] [G loss: 1.549120]\n",
      "[D loss: 0.741229] [G loss: 1.358021]\n",
      "[D loss: 0.676808] [G loss: 1.455311]\n",
      "[D loss: 0.854356] [G loss: 1.341399]\n",
      "[D loss: 0.735006] [G loss: 1.592541]\n",
      "[D loss: 0.649619] [G loss: 1.569615]\n",
      "[D loss: 0.872213] [G loss: 1.601667]\n",
      "[D loss: 0.792077] [G loss: 1.721621]\n",
      "[D loss: 0.703175] [G loss: 1.512362]\n",
      "[D loss: 0.620058] [G loss: 1.601167]\n",
      "[D loss: 0.668107] [G loss: 1.752832]\n",
      "[D loss: 0.896790] [G loss: 1.389261]\n",
      "[D loss: 0.819627] [G loss: 1.452314]\n",
      "[D loss: 1.134050] [G loss: 1.492454]\n",
      "[D loss: 0.721509] [G loss: 1.598617]\n",
      "[D loss: 0.745315] [G loss: 1.549580]\n",
      "[D loss: 0.753780] [G loss: 1.578660]\n",
      "[D loss: 0.905865] [G loss: 1.237851]\n",
      "[D loss: 0.890243] [G loss: 1.303205]\n",
      "[D loss: 0.638352] [G loss: 1.428943]\n",
      "[D loss: 0.806326] [G loss: 1.690611]\n",
      "[D loss: 0.759640] [G loss: 1.583672]\n",
      "[D loss: 0.717563] [G loss: 1.474833]\n",
      "[D loss: 0.989780] [G loss: 1.546843]\n",
      "[D loss: 0.669820] [G loss: 1.577270]\n",
      "[D loss: 0.860075] [G loss: 1.459840]\n",
      "[D loss: 0.704590] [G loss: 1.394233]\n",
      "[D loss: 0.718509] [G loss: 1.601798]\n",
      "[D loss: 0.839628] [G loss: 1.397843]\n",
      "[D loss: 0.783310] [G loss: 1.383422]\n",
      "[D loss: 0.915427] [G loss: 1.620488]\n",
      "[D loss: 0.689672] [G loss: 1.782272]\n",
      "[D loss: 0.812245] [G loss: 1.609990]\n",
      "[D loss: 0.758203] [G loss: 1.737316]\n",
      "[D loss: 0.914667] [G loss: 1.294695]\n",
      "[D loss: 0.780329] [G loss: 1.497808]\n",
      "[D loss: 0.573011] [G loss: 1.920248]\n",
      "[D loss: 0.776402] [G loss: 1.579861]\n",
      "[D loss: 1.138625] [G loss: 1.527523]\n",
      "[D loss: 0.853757] [G loss: 1.442145]\n",
      "[D loss: 0.653479] [G loss: 1.643862]\n",
      "[D loss: 0.877162] [G loss: 1.215121]\n",
      "[D loss: 0.729895] [G loss: 1.531896]\n",
      "[D loss: 0.934240] [G loss: 1.311083]\n",
      "[D loss: 0.696365] [G loss: 1.616773]\n",
      "[D loss: 0.773513] [G loss: 1.950872]\n",
      "[D loss: 0.713682] [G loss: 1.785655]\n",
      "[D loss: 0.826999] [G loss: 1.502148]\n",
      "[D loss: 0.687510] [G loss: 1.516057]\n",
      "[D loss: 0.722632] [G loss: 1.269661]\n",
      "[D loss: 0.857472] [G loss: 1.525941]\n",
      "[D loss: 0.808303] [G loss: 1.745701]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.867533] [G loss: 1.384007]\n",
      "[D loss: 0.697557] [G loss: 1.603045]\n",
      "[D loss: 0.483140] [G loss: 1.664975]\n",
      "[D loss: 1.020409] [G loss: 1.515819]\n",
      "[D loss: 0.750459] [G loss: 1.718884]\n",
      "[D loss: 0.762041] [G loss: 1.752352]\n",
      "[D loss: 0.973368] [G loss: 1.581989]\n",
      "[D loss: 0.663274] [G loss: 1.553560]\n",
      "[D loss: 0.952380] [G loss: 1.479711]\n",
      "[D loss: 0.983918] [G loss: 1.572048]\n",
      "[D loss: 0.973941] [G loss: 1.516973]\n",
      "[D loss: 0.812414] [G loss: 1.340935]\n",
      "[D loss: 0.881857] [G loss: 1.450160]\n",
      "[D loss: 0.997529] [G loss: 1.402948]\n",
      "[D loss: 0.748333] [G loss: 1.410392]\n",
      "[D loss: 0.782754] [G loss: 1.317241]\n",
      "[D loss: 0.800560] [G loss: 1.481839]\n",
      "[D loss: 0.661881] [G loss: 1.518357]\n",
      "[D loss: 1.019906] [G loss: 1.790421]\n",
      "[D loss: 0.944440] [G loss: 1.560969]\n",
      "[D loss: 0.983912] [G loss: 1.566552]\n",
      "[D loss: 0.980111] [G loss: 1.352165]\n",
      "[D loss: 0.793889] [G loss: 1.330423]\n",
      "[D loss: 0.836606] [G loss: 1.282535]\n",
      "[D loss: 0.750434] [G loss: 1.450505]\n",
      "[D loss: 0.624455] [G loss: 1.491000]\n",
      "[D loss: 0.719563] [G loss: 1.597723]\n",
      "[D loss: 0.786142] [G loss: 1.402231]\n",
      "[D loss: 1.019309] [G loss: 1.382273]\n",
      "[D loss: 0.873500] [G loss: 1.471570]\n",
      "[D loss: 0.826162] [G loss: 1.349580]\n",
      "[D loss: 0.886326] [G loss: 1.350865]\n",
      "[D loss: 0.855147] [G loss: 1.329061]\n",
      "[D loss: 0.963100] [G loss: 1.460363]\n",
      "[D loss: 0.836200] [G loss: 1.470529]\n",
      "[D loss: 0.773243] [G loss: 1.599604]\n",
      "[D loss: 0.880722] [G loss: 1.321193]\n",
      "[D loss: 0.688776] [G loss: 1.459917]\n",
      "[D loss: 0.774260] [G loss: 1.506100]\n",
      "[D loss: 0.516414] [G loss: 1.433254]\n",
      "[D loss: 0.619256] [G loss: 1.388125]\n",
      "[D loss: 0.918350] [G loss: 1.508690]\n",
      "[D loss: 0.808268] [G loss: 1.578817]\n",
      "[D loss: 0.763169] [G loss: 1.410382]\n",
      "[D loss: 0.701289] [G loss: 1.745011]\n",
      "[D loss: 0.704809] [G loss: 1.423732]\n",
      "[D loss: 0.681532] [G loss: 1.474627]\n",
      "[D loss: 0.793521] [G loss: 1.589162]\n",
      "[D loss: 0.710879] [G loss: 1.546053]\n",
      "[D loss: 0.785412] [G loss: 1.535253]\n",
      "[D loss: 0.742151] [G loss: 1.616677]\n",
      "[D loss: 0.811108] [G loss: 1.757917]\n",
      "[D loss: 0.623342] [G loss: 1.621423]\n",
      "[D loss: 0.861489] [G loss: 1.616737]\n",
      "[D loss: 0.639934] [G loss: 1.379142]\n",
      "[D loss: 0.962289] [G loss: 1.715236]\n",
      "[D loss: 0.642016] [G loss: 1.468211]\n",
      "[D loss: 0.876831] [G loss: 1.406420]\n",
      "[D loss: 0.691904] [G loss: 1.591589]\n",
      "[D loss: 0.685319] [G loss: 1.457635]\n",
      "[D loss: 0.848908] [G loss: 1.462545]\n",
      "[D loss: 0.940194] [G loss: 1.514686]\n",
      "[D loss: 0.835940] [G loss: 1.443448]\n",
      "[D loss: 0.773052] [G loss: 1.750698]\n",
      "[D loss: 0.999040] [G loss: 1.534047]\n",
      "[D loss: 0.733910] [G loss: 1.657545]\n",
      "[D loss: 1.038399] [G loss: 1.748192]\n",
      "[D loss: 0.884168] [G loss: 1.346265]\n",
      "[D loss: 0.728123] [G loss: 1.486379]\n",
      "[D loss: 0.640809] [G loss: 1.448441]\n",
      "[D loss: 0.804465] [G loss: 1.471284]\n",
      "[D loss: 1.035364] [G loss: 1.344882]\n",
      "[D loss: 0.698149] [G loss: 1.578718]\n",
      "[D loss: 0.839616] [G loss: 1.662948]\n",
      "[D loss: 0.761968] [G loss: 1.452174]\n",
      "[D loss: 0.788683] [G loss: 1.326564]\n",
      "[D loss: 0.741517] [G loss: 1.685315]\n",
      "[D loss: 0.939765] [G loss: 1.338021]\n",
      "[D loss: 0.849906] [G loss: 1.522592]\n",
      "[D loss: 0.674786] [G loss: 1.635714]\n",
      "[D loss: 0.694479] [G loss: 1.360895]\n",
      "[D loss: 0.999873] [G loss: 1.530264]\n",
      "[D loss: 0.740612] [G loss: 1.680216]\n",
      "[D loss: 0.779429] [G loss: 1.486188]\n",
      "[D loss: 0.902031] [G loss: 1.384457]\n",
      "[D loss: 0.832789] [G loss: 1.335544]\n",
      "[D loss: 0.779168] [G loss: 1.604043]\n",
      "[D loss: 0.986515] [G loss: 1.596735]\n",
      "[D loss: 0.693817] [G loss: 1.519433]\n",
      "[D loss: 0.834905] [G loss: 1.525013]\n",
      "[D loss: 0.953703] [G loss: 1.411073]\n",
      "[D loss: 1.122704] [G loss: 1.099666]\n",
      "[D loss: 0.547785] [G loss: 1.576010]\n",
      "[D loss: 0.781614] [G loss: 1.644096]\n",
      "[D loss: 0.678256] [G loss: 1.809442]\n",
      "[D loss: 0.720245] [G loss: 1.624156]\n",
      "[D loss: 0.801575] [G loss: 1.547758]\n",
      "[D loss: 0.603862] [G loss: 1.463035]\n",
      "[D loss: 0.670454] [G loss: 1.497792]\n",
      "[D loss: 0.776584] [G loss: 1.417912]\n",
      "[D loss: 0.713516] [G loss: 1.735075]\n",
      "[D loss: 0.860733] [G loss: 1.511386]\n",
      "[D loss: 1.056642] [G loss: 1.547247]\n",
      "[D loss: 0.871989] [G loss: 1.845315]\n",
      "[D loss: 0.514162] [G loss: 1.618749]\n",
      "[D loss: 0.627865] [G loss: 1.320162]\n",
      "[D loss: 1.054389] [G loss: 1.302836]\n",
      "[D loss: 0.651700] [G loss: 1.709858]\n",
      "[D loss: 1.109922] [G loss: 1.452322]\n",
      "[D loss: 0.933694] [G loss: 1.516406]\n",
      "[D loss: 0.736352] [G loss: 1.626892]\n",
      "[D loss: 0.692849] [G loss: 1.571864]\n",
      "[D loss: 0.855032] [G loss: 1.399510]\n",
      "[D loss: 0.848927] [G loss: 1.290072]\n",
      "[D loss: 0.814239] [G loss: 1.493882]\n",
      "[D loss: 0.982383] [G loss: 1.357295]\n",
      "[D loss: 0.944976] [G loss: 1.416315]\n",
      "[D loss: 0.795959] [G loss: 1.493271]\n",
      "[D loss: 0.866169] [G loss: 1.223827]\n",
      "[D loss: 0.675257] [G loss: 1.581190]\n",
      "[D loss: 0.544238] [G loss: 1.449716]\n",
      "[D loss: 0.691538] [G loss: 1.495725]\n",
      "[D loss: 0.758734] [G loss: 1.471651]\n",
      "[D loss: 0.896507] [G loss: 1.546021]\n",
      "[D loss: 0.628164] [G loss: 1.472213]\n",
      "[D loss: 0.984577] [G loss: 1.413898]\n",
      "[D loss: 1.102774] [G loss: 1.405553]\n",
      "[D loss: 0.601380] [G loss: 1.525250]\n",
      "[D loss: 0.544701] [G loss: 1.664299]\n",
      "[D loss: 0.612692] [G loss: 1.764510]\n",
      "[D loss: 0.881336] [G loss: 1.729019]\n",
      "[D loss: 0.959924] [G loss: 1.671893]\n",
      "[D loss: 0.818843] [G loss: 1.415100]\n",
      "[D loss: 0.735982] [G loss: 1.799300]\n",
      "[D loss: 0.785043] [G loss: 1.602839]\n",
      "[D loss: 0.898509] [G loss: 1.417134]\n",
      "[D loss: 1.080579] [G loss: 1.625689]\n",
      "[D loss: 0.650718] [G loss: 1.559597]\n",
      "[D loss: 1.026608] [G loss: 1.491841]\n",
      "[D loss: 0.990742] [G loss: 1.561445]\n",
      "[D loss: 0.776502] [G loss: 1.291961]\n",
      "[D loss: 0.832735] [G loss: 1.654694]\n",
      "[D loss: 0.776228] [G loss: 1.465893]\n",
      "[D loss: 0.959068] [G loss: 1.287889]\n",
      "[D loss: 0.870817] [G loss: 1.642794]\n",
      "[D loss: 1.006149] [G loss: 1.284094]\n",
      "[D loss: 0.821116] [G loss: 1.401345]\n",
      "[D loss: 0.766756] [G loss: 1.373044]\n",
      "[D loss: 0.913235] [G loss: 1.438367]\n",
      "[D loss: 0.733583] [G loss: 1.513075]\n",
      "[D loss: 1.011397] [G loss: 1.409694]\n",
      "[D loss: 0.704603] [G loss: 1.806982]\n",
      "[D loss: 0.811213] [G loss: 1.552252]\n",
      "[D loss: 0.651451] [G loss: 1.502988]\n",
      "[D loss: 0.581137] [G loss: 1.704187]\n",
      "[D loss: 0.637407] [G loss: 1.556719]\n",
      "[D loss: 0.717295] [G loss: 1.545112]\n",
      "[D loss: 0.887702] [G loss: 1.124694]\n",
      "[D loss: 1.202320] [G loss: 1.311005]\n",
      "[D loss: 0.899135] [G loss: 1.668557]\n",
      "[D loss: 0.804122] [G loss: 1.677425]\n",
      "[D loss: 0.527015] [G loss: 1.845911]\n",
      "[D loss: 1.036069] [G loss: 1.472218]\n",
      "[D loss: 0.964786] [G loss: 1.141642]\n",
      "[D loss: 0.798474] [G loss: 1.560722]\n",
      "[D loss: 1.003003] [G loss: 1.487598]\n",
      "[D loss: 0.840005] [G loss: 1.546506]\n",
      "[D loss: 0.691832] [G loss: 1.600364]\n",
      "[D loss: 0.644239] [G loss: 1.313211]\n",
      "[D loss: 0.797817] [G loss: 1.397891]\n",
      "[D loss: 1.110809] [G loss: 1.611906]\n",
      "[D loss: 0.830298] [G loss: 1.571126]\n",
      "[D loss: 0.826033] [G loss: 1.543833]\n",
      "[D loss: 0.936746] [G loss: 1.501837]\n",
      "[D loss: 0.788441] [G loss: 1.461540]\n",
      "[D loss: 0.719058] [G loss: 1.358482]\n",
      "[D loss: 0.806445] [G loss: 1.367469]\n",
      "[D loss: 0.721144] [G loss: 1.491790]\n",
      "[D loss: 0.914060] [G loss: 1.467648]\n",
      "[D loss: 0.727294] [G loss: 1.459865]\n",
      "[D loss: 0.913479] [G loss: 1.724338]\n",
      "[D loss: 0.807618] [G loss: 1.574231]\n",
      "[D loss: 0.807554] [G loss: 1.430994]\n",
      "[D loss: 1.024081] [G loss: 1.678172]\n",
      "[D loss: 0.694704] [G loss: 1.325733]\n",
      "[D loss: 0.907317] [G loss: 1.336820]\n",
      "[D loss: 0.815709] [G loss: 1.324688]\n",
      "[D loss: 0.789115] [G loss: 1.765731]\n",
      "[D loss: 0.939105] [G loss: 1.464180]\n",
      "[D loss: 0.873220] [G loss: 1.719217]\n",
      "[D loss: 1.187053] [G loss: 1.723351]\n",
      "[D loss: 0.819328] [G loss: 1.300161]\n",
      "[D loss: 0.901618] [G loss: 1.380902]\n",
      "[D loss: 0.884002] [G loss: 1.422663]\n",
      "[D loss: 0.751412] [G loss: 1.534356]\n",
      "[D loss: 0.965510] [G loss: 1.539037]\n",
      "[D loss: 0.612547] [G loss: 1.845517]\n",
      "[D loss: 0.899554] [G loss: 1.494814]\n",
      "[D loss: 0.664203] [G loss: 1.574784]\n",
      "[D loss: 0.747634] [G loss: 1.375170]\n",
      "[D loss: 0.717316] [G loss: 1.442359]\n",
      "[D loss: 0.912333] [G loss: 1.474501]\n",
      "[D loss: 0.726893] [G loss: 1.287189]\n",
      "[D loss: 0.798501] [G loss: 1.320447]\n",
      "[D loss: 0.688478] [G loss: 1.502303]\n",
      "[D loss: 0.887917] [G loss: 1.542570]\n",
      "[D loss: 0.799917] [G loss: 1.685776]\n",
      "[D loss: 0.811974] [G loss: 1.504709]\n",
      "[D loss: 0.725259] [G loss: 1.245399]\n",
      "[D loss: 0.939023] [G loss: 1.100060]\n",
      "[D loss: 0.913620] [G loss: 1.322239]\n",
      "[D loss: 0.706563] [G loss: 1.513064]\n",
      "[D loss: 0.902840] [G loss: 1.580424]\n",
      "[D loss: 1.149261] [G loss: 1.478997]\n",
      "[D loss: 0.781662] [G loss: 1.740243]\n",
      "[D loss: 0.731173] [G loss: 1.486393]\n",
      "[D loss: 0.594938] [G loss: 1.355524]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.625375] [G loss: 1.295236]\n",
      "[D loss: 0.988775] [G loss: 1.477642]\n",
      "[D loss: 0.635123] [G loss: 1.613852]\n",
      "[D loss: 0.528071] [G loss: 1.398475]\n",
      "[D loss: 0.770056] [G loss: 1.605959]\n",
      "[D loss: 0.813327] [G loss: 1.683865]\n",
      "[D loss: 0.669225] [G loss: 1.514287]\n",
      "[D loss: 1.181628] [G loss: 1.342181]\n",
      "[D loss: 0.613840] [G loss: 1.485290]\n",
      "[D loss: 0.822113] [G loss: 1.568959]\n",
      "[D loss: 0.619930] [G loss: 1.431235]\n",
      "[D loss: 0.711028] [G loss: 1.438923]\n",
      "[D loss: 0.874092] [G loss: 1.851031]\n",
      "[D loss: 0.710137] [G loss: 1.634499]\n",
      "[D loss: 0.810618] [G loss: 1.397678]\n",
      "[D loss: 0.776510] [G loss: 1.567293]\n",
      "[D loss: 0.685295] [G loss: 1.665758]\n",
      "[D loss: 0.633749] [G loss: 1.589665]\n",
      "[D loss: 0.805794] [G loss: 1.516067]\n",
      "[D loss: 0.750404] [G loss: 1.765681]\n",
      "[D loss: 0.613333] [G loss: 1.540720]\n",
      "[D loss: 0.783533] [G loss: 1.372576]\n",
      "[D loss: 0.718674] [G loss: 1.678149]\n",
      "[D loss: 0.844730] [G loss: 1.653630]\n",
      "[D loss: 1.065562] [G loss: 1.619960]\n",
      "[D loss: 1.309400] [G loss: 1.197565]\n",
      "[D loss: 0.770209] [G loss: 1.657236]\n",
      "[D loss: 0.776917] [G loss: 1.664738]\n",
      "[D loss: 0.833998] [G loss: 1.560386]\n",
      "[D loss: 0.715869] [G loss: 1.593397]\n",
      "[D loss: 0.943522] [G loss: 1.296617]\n",
      "[D loss: 0.740195] [G loss: 1.499520]\n",
      "[D loss: 0.599970] [G loss: 1.463141]\n",
      "[D loss: 0.918813] [G loss: 1.566015]\n",
      "[D loss: 0.841040] [G loss: 1.617127]\n",
      "[D loss: 0.851575] [G loss: 1.438741]\n",
      "[D loss: 0.854096] [G loss: 1.437993]\n",
      "[D loss: 0.754770] [G loss: 1.521719]\n",
      "[D loss: 0.873915] [G loss: 1.606679]\n",
      "[D loss: 0.726504] [G loss: 1.581127]\n",
      "[D loss: 0.796114] [G loss: 1.512358]\n",
      "[D loss: 0.883536] [G loss: 1.412088]\n",
      "[D loss: 0.744053] [G loss: 1.604048]\n",
      "[D loss: 1.124175] [G loss: 1.289089]\n",
      "[D loss: 0.784420] [G loss: 1.514904]\n",
      "[D loss: 0.968265] [G loss: 1.499905]\n",
      "[D loss: 0.758199] [G loss: 1.413262]\n",
      "[D loss: 0.762756] [G loss: 1.444322]\n",
      "[D loss: 0.796019] [G loss: 1.468208]\n",
      "[D loss: 1.000463] [G loss: 1.598802]\n",
      "[D loss: 0.973354] [G loss: 1.528992]\n",
      "[D loss: 0.758242] [G loss: 1.379038]\n",
      "[D loss: 0.831960] [G loss: 1.381437]\n",
      "[D loss: 0.941173] [G loss: 1.435191]\n",
      "[D loss: 0.669963] [G loss: 1.431908]\n",
      "[D loss: 0.737560] [G loss: 1.462896]\n",
      "[D loss: 0.669864] [G loss: 1.543567]\n",
      "[D loss: 0.886001] [G loss: 1.545159]\n",
      "[D loss: 0.978958] [G loss: 1.450565]\n",
      "[D loss: 0.577707] [G loss: 1.417520]\n",
      "[D loss: 0.837967] [G loss: 1.482922]\n",
      "[D loss: 0.920350] [G loss: 1.412795]\n",
      "[D loss: 0.805904] [G loss: 1.418424]\n",
      "[D loss: 1.013854] [G loss: 1.197296]\n",
      "[D loss: 0.741816] [G loss: 1.426150]\n",
      "[D loss: 0.831729] [G loss: 1.499103]\n",
      "[D loss: 0.655389] [G loss: 1.577663]\n",
      "[D loss: 1.041440] [G loss: 1.492992]\n",
      "[D loss: 0.927557] [G loss: 1.500571]\n",
      "[D loss: 0.685313] [G loss: 1.774530]\n",
      "[D loss: 0.631860] [G loss: 1.593643]\n",
      "[D loss: 0.658574] [G loss: 1.318661]\n",
      "[D loss: 0.858630] [G loss: 1.271294]\n",
      "[D loss: 0.754585] [G loss: 1.359087]\n",
      "[D loss: 0.876381] [G loss: 1.300882]\n",
      "[D loss: 0.623110] [G loss: 1.775402]\n",
      "[D loss: 0.649984] [G loss: 1.551758]\n",
      "[D loss: 0.861090] [G loss: 1.438903]\n",
      "[D loss: 0.738397] [G loss: 1.608128]\n",
      "[D loss: 0.821492] [G loss: 1.568846]\n",
      "[D loss: 0.883928] [G loss: 1.663478]\n",
      "[D loss: 0.938183] [G loss: 1.415827]\n",
      "[D loss: 0.802173] [G loss: 1.467201]\n",
      "[D loss: 0.672454] [G loss: 1.621566]\n",
      "[D loss: 0.734176] [G loss: 1.620404]\n",
      "[D loss: 0.949897] [G loss: 1.447643]\n",
      "[D loss: 0.647748] [G loss: 1.647467]\n",
      "[D loss: 0.732721] [G loss: 1.652400]\n",
      "[D loss: 0.712604] [G loss: 1.535213]\n",
      "[D loss: 0.868125] [G loss: 1.237362]\n",
      "[D loss: 1.003161] [G loss: 1.384765]\n",
      "[D loss: 0.578872] [G loss: 1.811435]\n",
      "[D loss: 0.655097] [G loss: 1.686558]\n",
      "[D loss: 1.195543] [G loss: 1.624096]\n",
      "[D loss: 0.710441] [G loss: 1.627249]\n",
      "[D loss: 0.958024] [G loss: 1.570691]\n",
      "[D loss: 0.677132] [G loss: 1.340552]\n",
      "[D loss: 0.714750] [G loss: 1.608856]\n",
      "[D loss: 0.807042] [G loss: 1.487656]\n",
      "[D loss: 0.983549] [G loss: 1.804413]\n",
      "[D loss: 0.670129] [G loss: 1.784231]\n",
      "[D loss: 0.732283] [G loss: 1.723409]\n",
      "[D loss: 0.937043] [G loss: 1.318391]\n",
      "[D loss: 0.940273] [G loss: 1.308812]\n",
      "[D loss: 0.795160] [G loss: 1.406658]\n",
      "[D loss: 0.684520] [G loss: 1.698126]\n",
      "[D loss: 0.744185] [G loss: 1.826984]\n",
      "[D loss: 0.774039] [G loss: 1.835786]\n",
      "[D loss: 0.734882] [G loss: 1.433300]\n",
      "[D loss: 0.903965] [G loss: 1.458364]\n",
      "[D loss: 0.912464] [G loss: 1.535656]\n",
      "[D loss: 0.604832] [G loss: 1.307009]\n",
      "[D loss: 0.851275] [G loss: 1.537552]\n",
      "[D loss: 0.739074] [G loss: 1.555960]\n",
      "[D loss: 0.773303] [G loss: 1.506247]\n",
      "[D loss: 0.951368] [G loss: 1.368249]\n",
      "[D loss: 1.057725] [G loss: 1.257924]\n",
      "[D loss: 0.806159] [G loss: 1.474207]\n",
      "[D loss: 0.658021] [G loss: 1.496085]\n",
      "[D loss: 0.956917] [G loss: 1.254802]\n",
      "[D loss: 0.910946] [G loss: 1.688651]\n",
      "[D loss: 0.707676] [G loss: 1.542032]\n",
      "[D loss: 0.723299] [G loss: 1.414558]\n",
      "[D loss: 0.897056] [G loss: 1.603595]\n",
      "[D loss: 0.833546] [G loss: 1.333877]\n",
      "[D loss: 0.736378] [G loss: 1.341054]\n",
      "[D loss: 0.826569] [G loss: 1.299291]\n",
      "[D loss: 1.029279] [G loss: 1.332968]\n",
      "[D loss: 0.756047] [G loss: 1.666544]\n",
      "[D loss: 0.940042] [G loss: 1.401735]\n",
      "[D loss: 0.820764] [G loss: 1.350067]\n",
      "[D loss: 0.641094] [G loss: 1.546247]\n",
      "[D loss: 0.858693] [G loss: 1.432824]\n",
      "[D loss: 0.719811] [G loss: 1.500963]\n",
      "[D loss: 0.654285] [G loss: 1.399464]\n",
      "[D loss: 0.957606] [G loss: 1.437591]\n",
      "[D loss: 0.815103] [G loss: 1.647345]\n",
      "[D loss: 1.107202] [G loss: 1.366552]\n",
      "[D loss: 0.831466] [G loss: 1.631888]\n",
      "[D loss: 1.001629] [G loss: 1.380170]\n",
      "[D loss: 0.976321] [G loss: 1.365413]\n",
      "[D loss: 0.753332] [G loss: 1.507115]\n",
      "[D loss: 0.868959] [G loss: 1.288151]\n",
      "[D loss: 0.845330] [G loss: 1.402258]\n",
      "[D loss: 0.908284] [G loss: 1.384752]\n",
      "[D loss: 0.628340] [G loss: 1.649691]\n",
      "[D loss: 0.827333] [G loss: 1.313335]\n",
      "[D loss: 0.616609] [G loss: 1.484565]\n",
      "[D loss: 0.805538] [G loss: 1.662419]\n",
      "[D loss: 0.849416] [G loss: 1.595103]\n",
      "[D loss: 0.698948] [G loss: 1.613185]\n",
      "[D loss: 0.767660] [G loss: 1.675451]\n",
      "[D loss: 0.932219] [G loss: 1.411562]\n",
      "[D loss: 1.011010] [G loss: 1.473490]\n",
      "[D loss: 1.120002] [G loss: 1.491659]\n",
      "[D loss: 0.674246] [G loss: 1.454683]\n",
      "[D loss: 0.810691] [G loss: 1.744906]\n",
      "[D loss: 0.639259] [G loss: 1.562351]\n",
      "[D loss: 0.523054] [G loss: 1.659977]\n",
      "[D loss: 0.734076] [G loss: 1.471241]\n",
      "[D loss: 0.584243] [G loss: 1.588849]\n",
      "[D loss: 0.923384] [G loss: 1.427514]\n",
      "[D loss: 0.589666] [G loss: 1.440166]\n",
      "[D loss: 0.600298] [G loss: 1.420882]\n",
      "[D loss: 0.711087] [G loss: 1.619707]\n",
      "[D loss: 0.690372] [G loss: 1.798747]\n",
      "[D loss: 0.894350] [G loss: 1.385313]\n",
      "[D loss: 0.798726] [G loss: 1.454906]\n",
      "[D loss: 0.595116] [G loss: 1.693542]\n",
      "[D loss: 0.736717] [G loss: 1.732192]\n",
      "[D loss: 0.764272] [G loss: 1.768136]\n",
      "[D loss: 0.972992] [G loss: 1.447153]\n",
      "[D loss: 0.895315] [G loss: 1.724948]\n",
      "[D loss: 0.986357] [G loss: 1.530821]\n",
      "[D loss: 0.652796] [G loss: 1.776095]\n",
      "[D loss: 1.061722] [G loss: 1.461753]\n",
      "[D loss: 0.786272] [G loss: 1.817927]\n",
      "[D loss: 1.124704] [G loss: 1.384464]\n",
      "[D loss: 0.576714] [G loss: 1.542365]\n",
      "[D loss: 0.729921] [G loss: 1.486029]\n",
      "[D loss: 1.031880] [G loss: 1.303575]\n",
      "[D loss: 0.776235] [G loss: 1.472885]\n",
      "[D loss: 0.810792] [G loss: 1.454619]\n",
      "[D loss: 0.684368] [G loss: 1.585208]\n",
      "[D loss: 1.027677] [G loss: 1.364578]\n",
      "[D loss: 0.898919] [G loss: 1.598380]\n",
      "[D loss: 0.891864] [G loss: 1.470393]\n",
      "[D loss: 0.659621] [G loss: 1.576490]\n",
      "[D loss: 0.828118] [G loss: 1.536044]\n",
      "[D loss: 0.708795] [G loss: 1.697580]\n",
      "[D loss: 0.733992] [G loss: 1.403721]\n",
      "[D loss: 0.595716] [G loss: 1.683294]\n",
      "[D loss: 0.452952] [G loss: 1.787305]\n",
      "[D loss: 0.820747] [G loss: 1.493512]\n",
      "[D loss: 0.856889] [G loss: 1.538811]\n",
      "[D loss: 0.676014] [G loss: 1.543190]\n",
      "[D loss: 0.740047] [G loss: 1.428488]\n",
      "[D loss: 0.746328] [G loss: 1.611060]\n",
      "[D loss: 0.716748] [G loss: 1.809056]\n",
      "[D loss: 0.877924] [G loss: 1.443968]\n",
      "[D loss: 0.814382] [G loss: 1.680283]\n",
      "[D loss: 0.771601] [G loss: 1.685261]\n",
      "[D loss: 0.828158] [G loss: 1.444248]\n",
      "[D loss: 0.639917] [G loss: 1.477020]\n",
      "[D loss: 0.731312] [G loss: 1.566782]\n",
      "[D loss: 0.904241] [G loss: 1.660911]\n",
      "[D loss: 0.799181] [G loss: 1.524819]\n",
      "[D loss: 0.569969] [G loss: 1.518168]\n",
      "[D loss: 0.900667] [G loss: 1.672565]\n",
      "[D loss: 1.089259] [G loss: 1.552414]\n",
      "[D loss: 0.793305] [G loss: 1.589578]\n",
      "[D loss: 0.922107] [G loss: 1.555142]\n",
      "[D loss: 0.548121] [G loss: 1.390066]\n",
      "[D loss: 0.930602] [G loss: 1.821006]\n",
      "[D loss: 0.791086] [G loss: 1.819343]\n",
      "[D loss: 0.933383] [G loss: 1.189758]\n",
      "[D loss: 0.738639] [G loss: 1.382171]\n",
      "[D loss: 0.710432] [G loss: 1.867936]\n",
      "[D loss: 0.956280] [G loss: 1.642617]\n",
      "[D loss: 0.595628] [G loss: 1.655191]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.718783] [G loss: 1.672551]\n",
      "[D loss: 0.719437] [G loss: 1.500839]\n",
      "[D loss: 0.685596] [G loss: 1.627570]\n",
      "[D loss: 0.777101] [G loss: 1.591790]\n",
      "[D loss: 0.971930] [G loss: 1.475368]\n",
      "[D loss: 0.598196] [G loss: 1.459786]\n",
      "[D loss: 0.842178] [G loss: 1.246289]\n",
      "[D loss: 0.730603] [G loss: 1.585259]\n",
      "[D loss: 0.848101] [G loss: 1.767810]\n",
      "[D loss: 0.944504] [G loss: 1.539721]\n",
      "[D loss: 0.931614] [G loss: 1.276139]\n",
      "[D loss: 0.608506] [G loss: 1.621469]\n",
      "[D loss: 1.011705] [G loss: 1.518669]\n",
      "[D loss: 1.055518] [G loss: 1.695459]\n",
      "[D loss: 0.870000] [G loss: 1.777879]\n",
      "[D loss: 0.839056] [G loss: 1.374230]\n",
      "[D loss: 0.837041] [G loss: 1.575930]\n",
      "[D loss: 0.777049] [G loss: 1.297634]\n",
      "[D loss: 0.833130] [G loss: 1.592778]\n",
      "[D loss: 0.976304] [G loss: 1.374277]\n",
      "[D loss: 0.909929] [G loss: 1.454154]\n",
      "[D loss: 0.894757] [G loss: 1.546223]\n",
      "[D loss: 0.837104] [G loss: 1.372572]\n",
      "[D loss: 0.950766] [G loss: 1.338917]\n",
      "[D loss: 0.939519] [G loss: 1.368046]\n",
      "[D loss: 0.818571] [G loss: 1.703275]\n",
      "[D loss: 0.732128] [G loss: 1.307421]\n",
      "[D loss: 0.847814] [G loss: 1.580787]\n",
      "[D loss: 0.624361] [G loss: 1.464592]\n",
      "[D loss: 0.788893] [G loss: 1.445040]\n",
      "[D loss: 0.902100] [G loss: 1.228747]\n",
      "[D loss: 0.657745] [G loss: 1.276046]\n",
      "[D loss: 0.849857] [G loss: 1.474658]\n",
      "[D loss: 0.885331] [G loss: 1.624448]\n",
      "[D loss: 0.737189] [G loss: 1.788903]\n",
      "[D loss: 0.910169] [G loss: 1.546057]\n",
      "[D loss: 0.644483] [G loss: 1.705412]\n",
      "[D loss: 0.586400] [G loss: 1.408956]\n",
      "[D loss: 0.591606] [G loss: 1.569623]\n",
      "[D loss: 1.086701] [G loss: 1.505342]\n",
      "[D loss: 0.782992] [G loss: 1.530048]\n",
      "[D loss: 0.776112] [G loss: 1.928140]\n",
      "[D loss: 0.761256] [G loss: 1.541181]\n",
      "[D loss: 0.791894] [G loss: 1.609920]\n",
      "[D loss: 0.691460] [G loss: 1.661268]\n",
      "[D loss: 0.819327] [G loss: 1.468895]\n",
      "[D loss: 0.788797] [G loss: 1.665382]\n",
      "[D loss: 0.763467] [G loss: 1.380140]\n",
      "[D loss: 0.816072] [G loss: 1.534711]\n",
      "[D loss: 0.810827] [G loss: 1.471376]\n",
      "[D loss: 0.876902] [G loss: 1.636986]\n",
      "[D loss: 0.733972] [G loss: 1.717341]\n",
      "[D loss: 0.831343] [G loss: 1.478476]\n",
      "[D loss: 0.723495] [G loss: 1.171518]\n",
      "[D loss: 0.793239] [G loss: 1.726372]\n",
      "[D loss: 0.896531] [G loss: 1.546371]\n",
      "[D loss: 0.755566] [G loss: 1.625003]\n",
      "[D loss: 0.837714] [G loss: 1.393072]\n",
      "[D loss: 0.964965] [G loss: 1.439324]\n",
      "[D loss: 1.041217] [G loss: 1.431002]\n",
      "[D loss: 0.835850] [G loss: 1.429189]\n",
      "[D loss: 0.687345] [G loss: 1.694507]\n",
      "[D loss: 0.703047] [G loss: 1.857303]\n",
      "[D loss: 0.838705] [G loss: 1.387051]\n",
      "[D loss: 0.888452] [G loss: 1.492713]\n",
      "[D loss: 0.730526] [G loss: 1.480114]\n",
      "[D loss: 0.760623] [G loss: 1.524616]\n",
      "[D loss: 0.829346] [G loss: 1.567573]\n",
      "[D loss: 0.701323] [G loss: 1.500337]\n",
      "[D loss: 0.670658] [G loss: 1.645042]\n",
      "[D loss: 0.713189] [G loss: 1.824813]\n",
      "[D loss: 0.627977] [G loss: 1.718821]\n",
      "[D loss: 0.842993] [G loss: 1.519877]\n",
      "[D loss: 0.964161] [G loss: 1.406906]\n",
      "[D loss: 0.661284] [G loss: 1.565206]\n",
      "[D loss: 0.883595] [G loss: 1.653694]\n",
      "[D loss: 0.762323] [G loss: 1.385026]\n",
      "[D loss: 0.943740] [G loss: 1.367528]\n",
      "[D loss: 0.734306] [G loss: 1.756669]\n",
      "[D loss: 0.725798] [G loss: 1.645556]\n",
      "[D loss: 0.736560] [G loss: 1.618269]\n",
      "[D loss: 0.762399] [G loss: 1.588032]\n",
      "[D loss: 0.842226] [G loss: 1.534107]\n",
      "[D loss: 0.902551] [G loss: 1.556721]\n",
      "[D loss: 0.770312] [G loss: 1.622657]\n",
      "[D loss: 0.732847] [G loss: 1.329303]\n",
      "[D loss: 0.808326] [G loss: 1.569842]\n",
      "[D loss: 0.998156] [G loss: 1.499125]\n",
      "[D loss: 1.021683] [G loss: 1.688782]\n",
      "[D loss: 0.613260] [G loss: 1.798713]\n",
      "[D loss: 0.568053] [G loss: 1.444763]\n",
      "[D loss: 0.911702] [G loss: 1.566260]\n",
      "[D loss: 0.839440] [G loss: 1.296338]\n",
      "[D loss: 0.896180] [G loss: 1.559048]\n",
      "[D loss: 0.889616] [G loss: 1.391983]\n",
      "[D loss: 0.806503] [G loss: 1.720951]\n",
      "[D loss: 0.715442] [G loss: 1.925978]\n",
      "[D loss: 0.938883] [G loss: 1.441470]\n",
      "[D loss: 0.760073] [G loss: 1.391556]\n",
      "[D loss: 0.926557] [G loss: 1.368463]\n",
      "[D loss: 0.669992] [G loss: 1.450590]\n",
      "[D loss: 1.010418] [G loss: 1.461789]\n",
      "[D loss: 0.875544] [G loss: 1.687701]\n",
      "[D loss: 0.858747] [G loss: 1.427444]\n",
      "[D loss: 0.605858] [G loss: 1.459511]\n",
      "[D loss: 0.928366] [G loss: 1.724342]\n",
      "[D loss: 0.682024] [G loss: 1.703438]\n",
      "[D loss: 0.912085] [G loss: 1.442553]\n",
      "[D loss: 0.977351] [G loss: 1.340752]\n",
      "[D loss: 0.794407] [G loss: 1.570825]\n",
      "[D loss: 0.723653] [G loss: 1.416792]\n",
      "[D loss: 0.786832] [G loss: 1.362855]\n",
      "[D loss: 0.713737] [G loss: 1.529865]\n",
      "[D loss: 0.820815] [G loss: 1.466846]\n",
      "[D loss: 0.579028] [G loss: 1.554844]\n",
      "[D loss: 0.742870] [G loss: 1.659408]\n",
      "[D loss: 0.877867] [G loss: 1.519406]\n",
      "[D loss: 0.741127] [G loss: 1.329287]\n",
      "[D loss: 0.678012] [G loss: 1.559428]\n",
      "[D loss: 0.730051] [G loss: 1.698280]\n",
      "[D loss: 0.732441] [G loss: 1.630008]\n",
      "[D loss: 0.862561] [G loss: 1.631726]\n",
      "[D loss: 0.748905] [G loss: 1.674899]\n",
      "[D loss: 0.951657] [G loss: 1.711111]\n",
      "[D loss: 0.885013] [G loss: 1.303809]\n",
      "[D loss: 0.934025] [G loss: 1.430048]\n",
      "[D loss: 0.754226] [G loss: 1.613952]\n",
      "[D loss: 0.729548] [G loss: 1.515525]\n",
      "[D loss: 0.726104] [G loss: 1.492034]\n",
      "[D loss: 1.150478] [G loss: 1.575249]\n",
      "[D loss: 0.892682] [G loss: 1.663200]\n",
      "[D loss: 0.713668] [G loss: 1.520890]\n",
      "[D loss: 0.780432] [G loss: 1.260375]\n",
      "[D loss: 0.990506] [G loss: 1.329352]\n",
      "[D loss: 0.902927] [G loss: 1.510953]\n",
      "[D loss: 0.695222] [G loss: 1.533821]\n",
      "[D loss: 0.781150] [G loss: 1.606673]\n",
      "[D loss: 0.774802] [G loss: 1.476026]\n",
      "[D loss: 0.811370] [G loss: 1.383721]\n",
      "[D loss: 0.747281] [G loss: 1.425522]\n",
      "[D loss: 1.020158] [G loss: 1.504081]\n",
      "[D loss: 0.725861] [G loss: 1.646328]\n",
      "[D loss: 0.599784] [G loss: 1.699240]\n",
      "[D loss: 0.568609] [G loss: 1.640100]\n",
      "[D loss: 1.130463] [G loss: 1.409654]\n",
      "[D loss: 0.653194] [G loss: 1.437013]\n",
      "[D loss: 0.813152] [G loss: 1.585876]\n",
      "[D loss: 0.876977] [G loss: 1.560073]\n",
      "[D loss: 0.662436] [G loss: 1.783624]\n",
      "[D loss: 0.621370] [G loss: 1.554482]\n",
      "[D loss: 0.730641] [G loss: 1.500257]\n",
      "[D loss: 0.689597] [G loss: 1.703078]\n",
      "[D loss: 0.794466] [G loss: 1.381932]\n",
      "[D loss: 0.980689] [G loss: 1.301307]\n",
      "[D loss: 0.625601] [G loss: 1.673312]\n",
      "[D loss: 0.808257] [G loss: 1.453606]\n",
      "[D loss: 0.768206] [G loss: 1.699902]\n",
      "[D loss: 0.769259] [G loss: 1.619100]\n",
      "[D loss: 0.546316] [G loss: 1.666763]\n",
      "[D loss: 0.741774] [G loss: 1.520635]\n",
      "[D loss: 0.902144] [G loss: 1.866158]\n",
      "[D loss: 1.050166] [G loss: 1.418723]\n",
      "[D loss: 0.912597] [G loss: 1.541531]\n",
      "[D loss: 0.815233] [G loss: 1.390920]\n",
      "[D loss: 0.826438] [G loss: 1.650241]\n",
      "[D loss: 0.915645] [G loss: 1.618582]\n",
      "[D loss: 0.786300] [G loss: 1.651434]\n",
      "[D loss: 0.858709] [G loss: 1.621428]\n",
      "[D loss: 0.704331] [G loss: 1.340580]\n",
      "[D loss: 0.904590] [G loss: 1.273362]\n",
      "[D loss: 0.809318] [G loss: 1.483564]\n",
      "[D loss: 0.650042] [G loss: 1.841827]\n",
      "[D loss: 0.771035] [G loss: 1.674625]\n",
      "[D loss: 0.802340] [G loss: 1.673077]\n",
      "[D loss: 0.985303] [G loss: 1.647962]\n",
      "[D loss: 0.631864] [G loss: 1.398030]\n",
      "[D loss: 0.821831] [G loss: 1.495090]\n",
      "[D loss: 0.770753] [G loss: 1.561685]\n",
      "[D loss: 0.637917] [G loss: 1.590897]\n",
      "[D loss: 0.920131] [G loss: 1.453759]\n",
      "[D loss: 0.820487] [G loss: 1.315282]\n",
      "[D loss: 0.879203] [G loss: 1.345074]\n",
      "[D loss: 0.761560] [G loss: 1.619109]\n",
      "[D loss: 1.029675] [G loss: 1.635146]\n",
      "[D loss: 0.713147] [G loss: 1.743345]\n",
      "[D loss: 0.659693] [G loss: 1.807884]\n",
      "[D loss: 0.732436] [G loss: 1.638597]\n",
      "[D loss: 0.748767] [G loss: 1.465460]\n",
      "[D loss: 0.908430] [G loss: 1.468273]\n",
      "[D loss: 0.712241] [G loss: 1.657215]\n",
      "[D loss: 0.711137] [G loss: 1.468388]\n",
      "[D loss: 0.886811] [G loss: 1.571469]\n",
      "[D loss: 0.699556] [G loss: 1.616632]\n",
      "[D loss: 0.979090] [G loss: 1.387331]\n",
      "[D loss: 0.602379] [G loss: 1.417717]\n",
      "[D loss: 0.955841] [G loss: 1.437389]\n",
      "[D loss: 0.658100] [G loss: 1.692839]\n",
      "[D loss: 0.573121] [G loss: 1.597814]\n",
      "[D loss: 0.709599] [G loss: 1.471616]\n",
      "[D loss: 0.704647] [G loss: 1.678073]\n",
      "[D loss: 0.963442] [G loss: 1.480656]\n",
      "[D loss: 0.721356] [G loss: 1.779638]\n",
      "[D loss: 0.802703] [G loss: 1.886179]\n",
      "[D loss: 0.751124] [G loss: 1.347859]\n",
      "[D loss: 0.824066] [G loss: 1.526634]\n",
      "[D loss: 0.999759] [G loss: 1.305113]\n",
      "[D loss: 0.748757] [G loss: 1.783159]\n",
      "[D loss: 0.747954] [G loss: 1.425886]\n",
      "[D loss: 0.799677] [G loss: 1.527313]\n",
      "[D loss: 0.809679] [G loss: 1.376348]\n",
      "[D loss: 0.863683] [G loss: 1.511417]\n",
      "[D loss: 0.797134] [G loss: 1.726877]\n",
      "[D loss: 0.733620] [G loss: 1.623126]\n",
      "[D loss: 0.842690] [G loss: 1.567937]\n",
      "[D loss: 0.817780] [G loss: 1.547548]\n",
      "[D loss: 0.831848] [G loss: 1.436594]\n",
      "[D loss: 1.200727] [G loss: 1.392894]\n",
      "[D loss: 0.931163] [G loss: 1.412066]\n",
      "[D loss: 0.776714] [G loss: 1.508640]\n",
      "[D loss: 0.933126] [G loss: 1.487285]\n",
      "[D loss: 0.816934] [G loss: 1.305711]\n",
      "[D loss: 0.799549] [G loss: 1.322178]\n",
      "[D loss: 0.831089] [G loss: 1.363902]\n",
      "[D loss: 0.733953] [G loss: 1.533940]\n",
      "[D loss: 0.923074] [G loss: 1.511880]\n",
      "[D loss: 0.852583] [G loss: 1.554276]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.723549] [G loss: 1.557932]\n",
      "[D loss: 0.798520] [G loss: 1.729381]\n",
      "[D loss: 0.886765] [G loss: 1.664231]\n",
      "[D loss: 0.959741] [G loss: 1.289928]\n",
      "[D loss: 0.823771] [G loss: 1.189796]\n",
      "[D loss: 0.768412] [G loss: 1.377026]\n",
      "[D loss: 0.852305] [G loss: 1.665681]\n",
      "[D loss: 0.943244] [G loss: 1.496730]\n",
      "[D loss: 0.800476] [G loss: 1.537119]\n",
      "[D loss: 0.871380] [G loss: 1.539278]\n",
      "[D loss: 0.844325] [G loss: 1.410980]\n",
      "[D loss: 0.766625] [G loss: 1.556597]\n",
      "[D loss: 0.719112] [G loss: 1.516168]\n",
      "[D loss: 0.803777] [G loss: 1.397773]\n",
      "[D loss: 0.982249] [G loss: 1.409371]\n",
      "[D loss: 0.906425] [G loss: 1.289900]\n",
      "[D loss: 0.811986] [G loss: 1.526711]\n",
      "[D loss: 0.655940] [G loss: 1.502918]\n",
      "[D loss: 0.826424] [G loss: 1.425348]\n",
      "[D loss: 0.908174] [G loss: 1.472033]\n",
      "[D loss: 0.749613] [G loss: 1.512263]\n",
      "[D loss: 0.702873] [G loss: 1.761065]\n",
      "[D loss: 0.888021] [G loss: 1.436181]\n",
      "[D loss: 0.761452] [G loss: 1.448787]\n",
      "[D loss: 0.655921] [G loss: 1.380962]\n",
      "[D loss: 0.797754] [G loss: 1.324960]\n",
      "[D loss: 0.725460] [G loss: 1.760544]\n",
      "[D loss: 0.857317] [G loss: 1.654985]\n",
      "[D loss: 1.042894] [G loss: 1.888132]\n",
      "[D loss: 0.758941] [G loss: 1.442480]\n",
      "[D loss: 0.743594] [G loss: 1.570554]\n",
      "[D loss: 0.604281] [G loss: 1.557823]\n",
      "[D loss: 0.808584] [G loss: 1.477514]\n",
      "[D loss: 0.836927] [G loss: 1.853678]\n",
      "[D loss: 0.857619] [G loss: 1.694563]\n",
      "[D loss: 0.652490] [G loss: 1.829773]\n",
      "[D loss: 0.568287] [G loss: 1.644943]\n",
      "[D loss: 0.748213] [G loss: 1.642608]\n",
      "[D loss: 0.833668] [G loss: 1.571491]\n",
      "[D loss: 1.075318] [G loss: 1.631859]\n",
      "[D loss: 0.810651] [G loss: 1.787013]\n",
      "[D loss: 0.720727] [G loss: 1.490533]\n",
      "[D loss: 0.678964] [G loss: 1.380192]\n",
      "[D loss: 0.808009] [G loss: 1.389161]\n",
      "[D loss: 0.952265] [G loss: 1.287660]\n",
      "[D loss: 0.608179] [G loss: 1.708732]\n",
      "[D loss: 0.743719] [G loss: 1.519253]\n",
      "[D loss: 0.697332] [G loss: 1.744197]\n",
      "[D loss: 0.920190] [G loss: 1.454780]\n",
      "[D loss: 0.911316] [G loss: 1.513649]\n",
      "[D loss: 0.665570] [G loss: 1.692390]\n",
      "[D loss: 0.655931] [G loss: 1.360448]\n",
      "[D loss: 0.840411] [G loss: 1.414166]\n",
      "[D loss: 0.970615] [G loss: 1.378923]\n",
      "[D loss: 0.701563] [G loss: 1.550011]\n",
      "[D loss: 0.801244] [G loss: 1.716476]\n",
      "[D loss: 0.747559] [G loss: 1.444101]\n",
      "[D loss: 0.793623] [G loss: 1.733703]\n",
      "[D loss: 0.793771] [G loss: 1.572112]\n",
      "[D loss: 0.973716] [G loss: 1.500707]\n",
      "[D loss: 0.815630] [G loss: 1.362065]\n",
      "[D loss: 0.576967] [G loss: 1.686728]\n",
      "[D loss: 0.690523] [G loss: 1.678900]\n",
      "[D loss: 0.876587] [G loss: 1.626085]\n",
      "[D loss: 0.747055] [G loss: 1.661268]\n",
      "[D loss: 0.780272] [G loss: 1.619579]\n",
      "[D loss: 1.060285] [G loss: 1.737904]\n",
      "[D loss: 1.207546] [G loss: 1.633639]\n",
      "[D loss: 0.823702] [G loss: 1.636503]\n",
      "[D loss: 0.917799] [G loss: 1.328339]\n",
      "[D loss: 0.698930] [G loss: 1.352412]\n",
      "[D loss: 0.861996] [G loss: 1.587811]\n",
      "[D loss: 0.811473] [G loss: 1.479833]\n",
      "[D loss: 0.663647] [G loss: 1.469141]\n",
      "[D loss: 0.758002] [G loss: 1.725461]\n",
      "[D loss: 0.472136] [G loss: 1.542436]\n",
      "[D loss: 0.680519] [G loss: 1.441009]\n",
      "[D loss: 0.844524] [G loss: 1.530735]\n",
      "[D loss: 0.911214] [G loss: 1.428705]\n",
      "[D loss: 0.847571] [G loss: 1.547830]\n",
      "[D loss: 0.681229] [G loss: 1.539367]\n",
      "[D loss: 0.827945] [G loss: 1.599997]\n",
      "[D loss: 0.921905] [G loss: 1.483515]\n",
      "[D loss: 0.844263] [G loss: 1.564615]\n",
      "[D loss: 0.704173] [G loss: 1.648817]\n",
      "[D loss: 0.950650] [G loss: 1.470120]\n",
      "[D loss: 0.849705] [G loss: 1.689005]\n",
      "[D loss: 0.910707] [G loss: 1.534769]\n",
      "[D loss: 0.792854] [G loss: 1.586400]\n",
      "[D loss: 0.875892] [G loss: 1.406631]\n",
      "[D loss: 0.683808] [G loss: 1.281778]\n",
      "[D loss: 0.672708] [G loss: 1.369735]\n",
      "[D loss: 0.822115] [G loss: 1.519717]\n",
      "[D loss: 0.988873] [G loss: 1.677601]\n",
      "[D loss: 0.569082] [G loss: 1.835163]\n",
      "[D loss: 0.907613] [G loss: 1.535974]\n",
      "[D loss: 0.594631] [G loss: 1.612022]\n",
      "[D loss: 0.647672] [G loss: 1.499141]\n",
      "[D loss: 0.995512] [G loss: 1.369912]\n",
      "[D loss: 0.711366] [G loss: 1.797463]\n",
      "[D loss: 0.780968] [G loss: 1.839834]\n",
      "[D loss: 1.005347] [G loss: 1.753939]\n",
      "[D loss: 0.827540] [G loss: 1.693769]\n",
      "[D loss: 0.919742] [G loss: 1.610854]\n",
      "[D loss: 0.847009] [G loss: 1.361582]\n",
      "[D loss: 0.832367] [G loss: 1.545561]\n",
      "[D loss: 1.107792] [G loss: 1.348149]\n",
      "[D loss: 1.126738] [G loss: 1.466786]\n",
      "[D loss: 0.796583] [G loss: 1.419507]\n",
      "[D loss: 0.774275] [G loss: 1.458040]\n",
      "[D loss: 0.929203] [G loss: 1.418099]\n",
      "[D loss: 1.254059] [G loss: 1.362124]\n",
      "[D loss: 0.798107] [G loss: 1.363496]\n",
      "[D loss: 0.992323] [G loss: 1.278600]\n",
      "[D loss: 0.767625] [G loss: 1.425085]\n",
      "[D loss: 0.632044] [G loss: 1.561271]\n",
      "[D loss: 0.865347] [G loss: 1.452372]\n",
      "[D loss: 0.960743] [G loss: 1.190128]\n",
      "[D loss: 0.878014] [G loss: 1.461392]\n",
      "[D loss: 0.848534] [G loss: 1.375603]\n",
      "[D loss: 0.831643] [G loss: 1.540431]\n",
      "[D loss: 0.872041] [G loss: 1.454618]\n",
      "[D loss: 0.754543] [G loss: 1.307104]\n",
      "[D loss: 0.869711] [G loss: 1.549387]\n",
      "[D loss: 0.557772] [G loss: 1.549614]\n",
      "[D loss: 0.945765] [G loss: 1.365003]\n",
      "[D loss: 0.860635] [G loss: 1.461049]\n",
      "[D loss: 1.021644] [G loss: 1.359418]\n",
      "[D loss: 0.931587] [G loss: 1.543630]\n",
      "[D loss: 0.785113] [G loss: 1.238673]\n",
      "[D loss: 0.588945] [G loss: 1.525713]\n",
      "[D loss: 0.787883] [G loss: 1.451043]\n",
      "[D loss: 0.720031] [G loss: 1.548334]\n",
      "[D loss: 0.788454] [G loss: 1.364430]\n",
      "[D loss: 0.818708] [G loss: 1.383760]\n",
      "[D loss: 0.649717] [G loss: 1.467459]\n",
      "[D loss: 0.836143] [G loss: 1.331627]\n",
      "[D loss: 0.905344] [G loss: 1.607522]\n",
      "[D loss: 0.817258] [G loss: 1.682043]\n",
      "[D loss: 0.929070] [G loss: 1.368896]\n",
      "[D loss: 0.721507] [G loss: 1.333288]\n",
      "[D loss: 0.725259] [G loss: 1.255858]\n",
      "[D loss: 0.758009] [G loss: 1.438092]\n",
      "[D loss: 0.863004] [G loss: 1.493299]\n",
      "[D loss: 0.863021] [G loss: 1.713047]\n",
      "[D loss: 0.686558] [G loss: 1.527239]\n",
      "[D loss: 0.925213] [G loss: 1.554418]\n",
      "[D loss: 0.894669] [G loss: 1.398658]\n",
      "[D loss: 0.827044] [G loss: 1.492670]\n",
      "[D loss: 1.051252] [G loss: 1.214457]\n",
      "[D loss: 0.980136] [G loss: 1.529084]\n",
      "[D loss: 0.792099] [G loss: 1.545189]\n",
      "[D loss: 0.830022] [G loss: 1.357468]\n",
      "[D loss: 0.880810] [G loss: 1.467782]\n",
      "[D loss: 0.963895] [G loss: 1.445771]\n",
      "[D loss: 1.035246] [G loss: 1.358678]\n",
      "[D loss: 0.619499] [G loss: 1.478167]\n",
      "[D loss: 0.824979] [G loss: 1.424717]\n",
      "[D loss: 0.930699] [G loss: 1.254391]\n",
      "[D loss: 0.669267] [G loss: 1.413458]\n",
      "[D loss: 0.889893] [G loss: 1.625312]\n",
      "[D loss: 0.768290] [G loss: 1.584573]\n",
      "[D loss: 0.968425] [G loss: 1.288760]\n",
      "[D loss: 0.868185] [G loss: 1.343705]\n",
      "[D loss: 0.850895] [G loss: 1.399822]\n",
      "[D loss: 0.652538] [G loss: 1.393846]\n",
      "[D loss: 0.819388] [G loss: 1.423573]\n",
      "[D loss: 0.819379] [G loss: 1.403751]\n",
      "[D loss: 0.681383] [G loss: 1.563049]\n",
      "[D loss: 0.789641] [G loss: 1.265219]\n",
      "[D loss: 0.730599] [G loss: 1.545192]\n",
      "[D loss: 0.941279] [G loss: 1.493797]\n",
      "[D loss: 0.843637] [G loss: 1.434374]\n",
      "[D loss: 0.740368] [G loss: 1.452325]\n",
      "[D loss: 0.952279] [G loss: 1.424817]\n",
      "[D loss: 0.866664] [G loss: 1.567889]\n",
      "[D loss: 0.779934] [G loss: 1.619361]\n",
      "[D loss: 0.704107] [G loss: 1.569454]\n",
      "[D loss: 0.906271] [G loss: 1.351943]\n",
      "[D loss: 0.751767] [G loss: 1.451051]\n",
      "[D loss: 1.152880] [G loss: 1.374182]\n",
      "[D loss: 0.867142] [G loss: 1.629343]\n",
      "[D loss: 0.772690] [G loss: 1.603472]\n",
      "[D loss: 1.025459] [G loss: 1.428180]\n",
      "[D loss: 0.950624] [G loss: 1.492042]\n",
      "[D loss: 0.847509] [G loss: 1.258762]\n",
      "[D loss: 0.899705] [G loss: 1.366320]\n",
      "[D loss: 0.951595] [G loss: 1.187731]\n",
      "[D loss: 0.844597] [G loss: 1.347505]\n",
      "[D loss: 0.754419] [G loss: 1.531028]\n",
      "[D loss: 0.851865] [G loss: 1.313560]\n",
      "[D loss: 0.831323] [G loss: 1.472439]\n",
      "[D loss: 0.690740] [G loss: 1.396609]\n",
      "[D loss: 0.745363] [G loss: 1.598224]\n",
      "[D loss: 0.676323] [G loss: 1.491626]\n",
      "[D loss: 0.824082] [G loss: 1.358201]\n",
      "[D loss: 0.568878] [G loss: 1.542482]\n",
      "[D loss: 0.910994] [G loss: 1.351481]\n",
      "[D loss: 0.948062] [G loss: 1.605788]\n",
      "[D loss: 1.137119] [G loss: 1.503095]\n",
      "[D loss: 0.952635] [G loss: 1.536454]\n",
      "[D loss: 0.997856] [G loss: 1.485110]\n",
      "[D loss: 0.890597] [G loss: 1.208436]\n",
      "[D loss: 0.998324] [G loss: 1.194899]\n",
      "[D loss: 0.728436] [G loss: 1.579931]\n",
      "[D loss: 0.692168] [G loss: 1.555706]\n",
      "[D loss: 1.317563] [G loss: 1.460681]\n",
      "[D loss: 0.760926] [G loss: 1.643935]\n",
      "[D loss: 0.720368] [G loss: 1.439491]\n",
      "[D loss: 0.758586] [G loss: 1.323624]\n",
      "[D loss: 0.888049] [G loss: 1.334225]\n",
      "[D loss: 0.795889] [G loss: 1.239337]\n",
      "[D loss: 0.697926] [G loss: 1.493281]\n",
      "[D loss: 0.781261] [G loss: 1.620855]\n",
      "[D loss: 0.700768] [G loss: 1.609997]\n",
      "[D loss: 0.946998] [G loss: 1.395427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.637487] [G loss: 1.551823]\n",
      "[D loss: 0.768232] [G loss: 1.506838]\n",
      "[D loss: 0.785025] [G loss: 1.777925]\n",
      "[D loss: 0.819928] [G loss: 1.404934]\n",
      "[D loss: 0.640919] [G loss: 1.367265]\n",
      "[D loss: 1.014395] [G loss: 1.443455]\n",
      "[D loss: 0.709235] [G loss: 1.530969]\n",
      "[D loss: 1.066529] [G loss: 1.177002]\n",
      "[D loss: 1.014582] [G loss: 1.571668]\n",
      "[D loss: 0.875969] [G loss: 1.649103]\n",
      "[D loss: 0.897587] [G loss: 1.701594]\n",
      "[D loss: 0.894404] [G loss: 1.538303]\n",
      "[D loss: 0.971276] [G loss: 1.396279]\n",
      "[D loss: 0.918634] [G loss: 1.131560]\n",
      "[D loss: 0.936442] [G loss: 1.284358]\n",
      "[D loss: 0.979549] [G loss: 1.523137]\n",
      "[D loss: 0.854216] [G loss: 1.478541]\n",
      "[D loss: 1.025502] [G loss: 1.309345]\n",
      "[D loss: 0.887319] [G loss: 1.457399]\n",
      "[D loss: 0.603140] [G loss: 1.714746]\n",
      "[D loss: 0.828943] [G loss: 1.465169]\n",
      "[D loss: 0.681077] [G loss: 1.711048]\n",
      "[D loss: 0.542520] [G loss: 1.627367]\n",
      "[D loss: 0.631786] [G loss: 1.479184]\n",
      "[D loss: 1.119737] [G loss: 1.636769]\n",
      "[D loss: 0.837018] [G loss: 1.445874]\n",
      "[D loss: 0.775929] [G loss: 1.297034]\n",
      "[D loss: 0.837939] [G loss: 1.346190]\n",
      "[D loss: 0.968333] [G loss: 1.537823]\n",
      "[D loss: 0.900966] [G loss: 1.473397]\n",
      "[D loss: 0.695847] [G loss: 1.366638]\n",
      "[D loss: 0.787884] [G loss: 1.489183]\n",
      "[D loss: 0.856298] [G loss: 1.355497]\n",
      "[D loss: 1.010889] [G loss: 1.353350]\n",
      "[D loss: 0.941066] [G loss: 1.413714]\n",
      "[D loss: 0.757837] [G loss: 1.396376]\n",
      "[D loss: 0.626708] [G loss: 1.443878]\n",
      "[D loss: 0.811520] [G loss: 1.662129]\n",
      "[D loss: 0.714822] [G loss: 1.522358]\n",
      "[D loss: 1.001524] [G loss: 1.469747]\n",
      "[D loss: 0.849644] [G loss: 1.278644]\n",
      "[D loss: 0.760911] [G loss: 1.691220]\n",
      "[D loss: 0.837350] [G loss: 1.636572]\n",
      "[D loss: 0.694311] [G loss: 1.633060]\n",
      "[D loss: 0.979232] [G loss: 1.377311]\n",
      "[D loss: 1.030252] [G loss: 1.237373]\n",
      "[D loss: 0.694692] [G loss: 1.592332]\n",
      "[D loss: 0.958428] [G loss: 1.488083]\n",
      "[D loss: 0.890758] [G loss: 1.583297]\n",
      "[D loss: 0.953385] [G loss: 1.604274]\n",
      "[D loss: 1.026697] [G loss: 1.182474]\n",
      "[D loss: 0.801712] [G loss: 1.370175]\n",
      "[D loss: 0.759482] [G loss: 1.313082]\n",
      "[D loss: 0.788290] [G loss: 1.350245]\n",
      "[D loss: 0.783346] [G loss: 1.240216]\n",
      "[D loss: 0.905264] [G loss: 1.341879]\n",
      "[D loss: 0.990673] [G loss: 1.382044]\n",
      "[D loss: 0.794781] [G loss: 1.492275]\n",
      "[D loss: 0.776055] [G loss: 1.420451]\n",
      "[D loss: 0.727305] [G loss: 1.611029]\n",
      "[D loss: 0.798075] [G loss: 1.467350]\n",
      "[D loss: 0.653818] [G loss: 1.565233]\n",
      "[D loss: 0.863939] [G loss: 1.425005]\n",
      "[D loss: 1.135417] [G loss: 1.221955]\n",
      "[D loss: 0.732935] [G loss: 1.468801]\n",
      "[D loss: 0.778919] [G loss: 1.808296]\n",
      "[D loss: 0.581710] [G loss: 1.569053]\n",
      "[D loss: 0.878930] [G loss: 1.371109]\n",
      "[D loss: 0.863973] [G loss: 1.482413]\n",
      "[D loss: 1.100091] [G loss: 1.417008]\n",
      "[D loss: 0.715707] [G loss: 1.656494]\n",
      "[D loss: 1.055599] [G loss: 1.506249]\n",
      "[D loss: 0.769339] [G loss: 1.522789]\n",
      "[D loss: 0.641956] [G loss: 1.510348]\n",
      "[D loss: 0.955364] [G loss: 1.262033]\n",
      "[D loss: 0.979538] [G loss: 1.324264]\n",
      "[D loss: 0.796821] [G loss: 1.503512]\n",
      "[D loss: 0.863534] [G loss: 1.521048]\n",
      "[D loss: 0.798749] [G loss: 1.373015]\n",
      "[D loss: 0.529111] [G loss: 1.666888]\n",
      "[D loss: 1.045506] [G loss: 1.340222]\n",
      "[D loss: 0.788911] [G loss: 1.568252]\n",
      "[D loss: 0.770523] [G loss: 1.499514]\n",
      "[D loss: 0.730646] [G loss: 1.381235]\n",
      "[D loss: 0.967460] [G loss: 1.587972]\n",
      "[D loss: 0.778636] [G loss: 1.539722]\n",
      "[D loss: 0.887598] [G loss: 1.243260]\n",
      "[D loss: 0.629487] [G loss: 1.718404]\n",
      "[D loss: 0.725182] [G loss: 1.490206]\n",
      "[D loss: 0.843344] [G loss: 1.646506]\n",
      "[D loss: 0.982939] [G loss: 1.544061]\n",
      "[D loss: 0.795963] [G loss: 1.388197]\n",
      "[D loss: 1.064902] [G loss: 1.948850]\n",
      "[D loss: 0.984357] [G loss: 1.322891]\n",
      "[D loss: 0.961119] [G loss: 1.312461]\n",
      "[D loss: 0.657142] [G loss: 1.327863]\n",
      "[D loss: 0.850500] [G loss: 1.302192]\n",
      "[D loss: 0.983394] [G loss: 1.416222]\n",
      "[D loss: 0.834757] [G loss: 1.309711]\n",
      "[D loss: 0.967548] [G loss: 1.389135]\n",
      "[D loss: 0.875245] [G loss: 1.188949]\n",
      "[D loss: 1.032762] [G loss: 1.375527]\n",
      "[D loss: 0.787597] [G loss: 1.423391]\n",
      "[D loss: 0.847821] [G loss: 1.650231]\n",
      "[D loss: 0.794088] [G loss: 1.585079]\n",
      "[D loss: 0.940325] [G loss: 1.445150]\n",
      "[D loss: 0.874333] [G loss: 1.290778]\n",
      "[D loss: 0.806724] [G loss: 1.318118]\n",
      "[D loss: 0.737736] [G loss: 1.429154]\n",
      "[D loss: 0.894703] [G loss: 1.542709]\n",
      "[D loss: 0.641383] [G loss: 1.580966]\n",
      "[D loss: 0.802662] [G loss: 1.553307]\n",
      "[D loss: 0.657666] [G loss: 1.352612]\n",
      "[D loss: 0.742745] [G loss: 1.474680]\n",
      "[D loss: 0.832548] [G loss: 1.609259]\n",
      "[D loss: 0.809513] [G loss: 1.541372]\n",
      "[D loss: 0.670320] [G loss: 1.757434]\n",
      "[D loss: 0.674977] [G loss: 1.540371]\n",
      "[D loss: 0.753658] [G loss: 1.519954]\n",
      "[D loss: 0.940883] [G loss: 1.752889]\n",
      "[D loss: 0.714207] [G loss: 1.648189]\n",
      "[D loss: 0.999074] [G loss: 1.321068]\n",
      "[D loss: 0.837018] [G loss: 1.497694]\n",
      "[D loss: 0.763795] [G loss: 1.807256]\n",
      "[D loss: 0.764975] [G loss: 1.540424]\n",
      "[D loss: 0.640389] [G loss: 1.554677]\n",
      "[D loss: 0.717316] [G loss: 1.499951]\n",
      "[D loss: 0.819195] [G loss: 1.528208]\n",
      "[D loss: 0.758741] [G loss: 1.480760]\n",
      "[D loss: 1.108050] [G loss: 1.243156]\n",
      "[D loss: 0.693599] [G loss: 1.498523]\n",
      "[D loss: 1.018401] [G loss: 1.413364]\n",
      "[D loss: 0.843480] [G loss: 1.771925]\n",
      "[D loss: 0.730511] [G loss: 1.632565]\n",
      "[D loss: 0.716459] [G loss: 1.721314]\n",
      "[D loss: 0.785677] [G loss: 1.551977]\n",
      "[D loss: 0.789130] [G loss: 1.366613]\n",
      "[D loss: 0.838142] [G loss: 1.622192]\n",
      "[D loss: 0.904288] [G loss: 1.471923]\n",
      "[D loss: 0.965865] [G loss: 1.240818]\n",
      "[D loss: 0.721277] [G loss: 1.362043]\n",
      "[D loss: 0.700788] [G loss: 1.505666]\n",
      "[D loss: 1.074951] [G loss: 1.253910]\n",
      "[D loss: 0.887871] [G loss: 1.363156]\n",
      "[D loss: 0.821646] [G loss: 1.575588]\n",
      "[D loss: 0.726958] [G loss: 1.818069]\n",
      "[D loss: 0.832890] [G loss: 1.657519]\n",
      "[D loss: 0.804304] [G loss: 1.671693]\n",
      "[D loss: 0.787293] [G loss: 1.375974]\n",
      "[D loss: 0.823483] [G loss: 1.479836]\n",
      "[D loss: 0.602995] [G loss: 1.556850]\n",
      "[D loss: 0.731084] [G loss: 1.388635]\n",
      "[D loss: 0.520500] [G loss: 1.453850]\n",
      "[D loss: 0.830430] [G loss: 1.685540]\n",
      "[D loss: 0.988241] [G loss: 1.676654]\n",
      "[D loss: 0.822314] [G loss: 1.581912]\n",
      "[D loss: 0.915886] [G loss: 1.452520]\n",
      "[D loss: 0.802872] [G loss: 1.401254]\n",
      "[D loss: 0.830701] [G loss: 1.728338]\n",
      "[D loss: 0.952413] [G loss: 1.406954]\n",
      "[D loss: 0.977639] [G loss: 1.428674]\n",
      "[D loss: 0.918027] [G loss: 1.475524]\n",
      "[D loss: 1.077161] [G loss: 1.558462]\n",
      "[D loss: 1.001606] [G loss: 1.726936]\n",
      "[D loss: 0.984920] [G loss: 1.308163]\n",
      "[D loss: 0.583734] [G loss: 1.521457]\n",
      "[D loss: 0.923143] [G loss: 1.157333]\n",
      "[D loss: 0.840424] [G loss: 1.433471]\n",
      "[D loss: 0.949287] [G loss: 1.361320]\n",
      "[D loss: 0.810638] [G loss: 1.520508]\n",
      "[D loss: 0.868086] [G loss: 1.406563]\n",
      "[D loss: 0.895359] [G loss: 1.557761]\n",
      "[D loss: 0.847260] [G loss: 1.358607]\n",
      "[D loss: 0.891609] [G loss: 1.390818]\n",
      "[D loss: 0.764493] [G loss: 1.495063]\n",
      "[D loss: 0.965578] [G loss: 1.357856]\n",
      "[D loss: 0.879498] [G loss: 1.468115]\n",
      "[D loss: 0.822666] [G loss: 1.223722]\n",
      "[D loss: 0.822002] [G loss: 1.477975]\n",
      "[D loss: 1.080079] [G loss: 1.439160]\n",
      "[D loss: 0.941609] [G loss: 1.370196]\n",
      "[D loss: 0.900391] [G loss: 1.187394]\n",
      "[D loss: 0.776186] [G loss: 1.328203]\n",
      "[D loss: 0.828386] [G loss: 1.410546]\n",
      "[D loss: 0.939823] [G loss: 1.385999]\n",
      "[D loss: 0.849702] [G loss: 1.512021]\n",
      "[D loss: 0.824700] [G loss: 1.401704]\n",
      "[D loss: 0.800561] [G loss: 1.369985]\n",
      "[D loss: 0.870933] [G loss: 1.392226]\n",
      "[D loss: 0.736844] [G loss: 1.495338]\n",
      "[D loss: 0.758155] [G loss: 1.241335]\n",
      "[D loss: 0.693636] [G loss: 1.537463]\n",
      "[D loss: 0.723458] [G loss: 1.650710]\n",
      "[D loss: 0.800207] [G loss: 1.600487]\n",
      "[D loss: 0.673763] [G loss: 1.458140]\n",
      "[D loss: 0.790410] [G loss: 1.396401]\n",
      "[D loss: 0.953335] [G loss: 1.282333]\n",
      "[D loss: 0.608993] [G loss: 1.710626]\n",
      "[D loss: 0.672912] [G loss: 1.583961]\n",
      "[D loss: 0.974535] [G loss: 1.761693]\n",
      "[D loss: 0.798946] [G loss: 1.639095]\n",
      "[D loss: 0.975142] [G loss: 1.274705]\n",
      "[D loss: 0.680027] [G loss: 1.337879]\n",
      "[D loss: 1.280869] [G loss: 1.529550]\n",
      "[D loss: 0.722962] [G loss: 1.333826]\n",
      "[D loss: 0.922437] [G loss: 1.463880]\n",
      "[D loss: 0.699232] [G loss: 1.522908]\n",
      "[D loss: 0.922456] [G loss: 1.635411]\n",
      "[D loss: 0.763684] [G loss: 1.617145]\n",
      "[D loss: 0.857894] [G loss: 1.433288]\n",
      "[D loss: 0.923939] [G loss: 1.532978]\n",
      "[D loss: 0.779382] [G loss: 1.342131]\n",
      "[D loss: 0.798166] [G loss: 1.364782]\n",
      "[D loss: 0.720056] [G loss: 1.546559]\n",
      "[D loss: 0.976691] [G loss: 1.417506]\n",
      "[D loss: 0.629969] [G loss: 1.481595]\n",
      "[D loss: 0.880021] [G loss: 1.478621]\n",
      "[D loss: 0.770795] [G loss: 1.513682]\n",
      "[D loss: 0.824554] [G loss: 1.386535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 1.041894] [G loss: 1.394992]\n",
      "[D loss: 0.589759] [G loss: 1.693563]\n",
      "[D loss: 0.982176] [G loss: 1.503137]\n",
      "[D loss: 0.814378] [G loss: 1.456980]\n",
      "[D loss: 1.089983] [G loss: 1.360669]\n",
      "[D loss: 0.632349] [G loss: 1.388009]\n",
      "[D loss: 0.626473] [G loss: 1.758851]\n",
      "[D loss: 0.801428] [G loss: 1.530488]\n",
      "[D loss: 0.731613] [G loss: 1.600174]\n",
      "[D loss: 0.775297] [G loss: 1.447770]\n",
      "[D loss: 0.797565] [G loss: 1.537518]\n",
      "[D loss: 0.619224] [G loss: 1.738178]\n",
      "[D loss: 0.564943] [G loss: 1.750299]\n",
      "[D loss: 1.069625] [G loss: 1.297834]\n",
      "[D loss: 0.811229] [G loss: 1.851067]\n",
      "[D loss: 0.742775] [G loss: 1.840970]\n",
      "[D loss: 0.677792] [G loss: 1.509211]\n",
      "[D loss: 0.768114] [G loss: 1.425481]\n",
      "[D loss: 0.908747] [G loss: 1.445147]\n",
      "[D loss: 0.719477] [G loss: 1.524707]\n",
      "[D loss: 0.882464] [G loss: 1.367068]\n",
      "[D loss: 0.710277] [G loss: 1.618975]\n",
      "[D loss: 0.905127] [G loss: 1.522012]\n",
      "[D loss: 0.826533] [G loss: 1.459980]\n",
      "[D loss: 1.006944] [G loss: 1.643373]\n",
      "[D loss: 0.602240] [G loss: 1.586647]\n",
      "[D loss: 0.839505] [G loss: 1.542676]\n",
      "[D loss: 0.878262] [G loss: 1.530224]\n",
      "[D loss: 0.894216] [G loss: 1.559007]\n",
      "[D loss: 0.947465] [G loss: 1.310129]\n",
      "[D loss: 0.797734] [G loss: 1.469196]\n",
      "[D loss: 0.914353] [G loss: 1.189788]\n",
      "[D loss: 0.883792] [G loss: 1.352276]\n",
      "[D loss: 0.767726] [G loss: 1.400482]\n",
      "[D loss: 0.532253] [G loss: 1.427597]\n",
      "[D loss: 0.869145] [G loss: 1.315241]\n",
      "[D loss: 1.051738] [G loss: 1.398154]\n",
      "[D loss: 0.498601] [G loss: 1.473811]\n",
      "[D loss: 0.786524] [G loss: 1.689705]\n",
      "[D loss: 0.737989] [G loss: 1.635708]\n",
      "[D loss: 0.934539] [G loss: 1.489354]\n",
      "[D loss: 0.761894] [G loss: 1.599271]\n",
      "[D loss: 0.634810] [G loss: 1.533058]\n",
      "[D loss: 0.956338] [G loss: 1.521521]\n",
      "[D loss: 0.957456] [G loss: 1.286945]\n",
      "[D loss: 1.128563] [G loss: 1.351880]\n",
      "[D loss: 0.889611] [G loss: 1.396828]\n",
      "[D loss: 0.643235] [G loss: 1.597035]\n",
      "[D loss: 0.787521] [G loss: 1.453878]\n",
      "[D loss: 0.800987] [G loss: 1.377288]\n",
      "[D loss: 0.602074] [G loss: 1.659573]\n",
      "[D loss: 0.901726] [G loss: 1.481505]\n",
      "[D loss: 0.723779] [G loss: 1.502827]\n",
      "[D loss: 0.981612] [G loss: 1.499663]\n",
      "[D loss: 0.852023] [G loss: 1.521304]\n",
      "[D loss: 0.830016] [G loss: 1.459739]\n",
      "[D loss: 1.026034] [G loss: 1.219093]\n",
      "[D loss: 0.977901] [G loss: 1.633963]\n",
      "[D loss: 0.584214] [G loss: 1.640450]\n",
      "[D loss: 0.824682] [G loss: 1.341134]\n",
      "[D loss: 0.876129] [G loss: 1.301915]\n",
      "[D loss: 0.939271] [G loss: 1.577260]\n",
      "[D loss: 0.778923] [G loss: 1.526981]\n",
      "[D loss: 0.837455] [G loss: 1.513465]\n",
      "[D loss: 1.041490] [G loss: 1.262384]\n",
      "[D loss: 0.873438] [G loss: 1.376391]\n",
      "[D loss: 0.760042] [G loss: 1.495999]\n",
      "[D loss: 0.939366] [G loss: 1.584868]\n",
      "[D loss: 0.659530] [G loss: 1.413842]\n",
      "[D loss: 0.712048] [G loss: 1.432484]\n",
      "[D loss: 1.056301] [G loss: 1.489383]\n",
      "[D loss: 0.801537] [G loss: 1.337248]\n",
      "[D loss: 0.736448] [G loss: 1.416634]\n",
      "[D loss: 0.943200] [G loss: 1.278142]\n",
      "[D loss: 0.681266] [G loss: 1.427689]\n",
      "[D loss: 0.924080] [G loss: 1.427212]\n",
      "[D loss: 0.718184] [G loss: 1.300935]\n",
      "[D loss: 0.869494] [G loss: 1.395918]\n",
      "[D loss: 0.622219] [G loss: 1.548212]\n",
      "[D loss: 0.716956] [G loss: 1.587016]\n",
      "[D loss: 0.954600] [G loss: 1.423597]\n",
      "[D loss: 0.948678] [G loss: 1.515744]\n",
      "[D loss: 0.811932] [G loss: 1.319539]\n",
      "[D loss: 0.697111] [G loss: 1.507980]\n",
      "[D loss: 0.649226] [G loss: 1.352863]\n",
      "[D loss: 0.775139] [G loss: 1.534048]\n",
      "[D loss: 0.836975] [G loss: 1.397830]\n",
      "[D loss: 0.903329] [G loss: 1.482585]\n",
      "[D loss: 0.795685] [G loss: 1.530772]\n",
      "[D loss: 0.770656] [G loss: 1.383061]\n",
      "[D loss: 0.935114] [G loss: 1.312882]\n",
      "[D loss: 0.545235] [G loss: 1.384884]\n",
      "[D loss: 0.771542] [G loss: 1.317863]\n",
      "[D loss: 0.696991] [G loss: 1.565858]\n",
      "[D loss: 0.774173] [G loss: 1.611859]\n",
      "[D loss: 0.809357] [G loss: 1.474749]\n",
      "[D loss: 0.937508] [G loss: 1.314752]\n",
      "[D loss: 0.719325] [G loss: 1.605474]\n",
      "[D loss: 0.748689] [G loss: 1.636736]\n",
      "[D loss: 0.754274] [G loss: 1.539036]\n",
      "[D loss: 0.722975] [G loss: 1.621153]\n",
      "[D loss: 0.731465] [G loss: 1.500455]\n",
      "[D loss: 0.813242] [G loss: 1.374948]\n",
      "[D loss: 0.887626] [G loss: 1.477793]\n",
      "[D loss: 0.735605] [G loss: 1.826192]\n",
      "[D loss: 0.901218] [G loss: 1.404617]\n",
      "[D loss: 0.884395] [G loss: 1.353118]\n",
      "[D loss: 0.958119] [G loss: 1.443748]\n",
      "[D loss: 0.814326] [G loss: 1.561144]\n",
      "[D loss: 0.830354] [G loss: 1.350915]\n",
      "[D loss: 0.918362] [G loss: 1.368476]\n",
      "[D loss: 0.854361] [G loss: 1.370201]\n",
      "[D loss: 1.005919] [G loss: 1.289446]\n",
      "[D loss: 0.743366] [G loss: 1.396810]\n",
      "[D loss: 0.892157] [G loss: 1.585794]\n",
      "[D loss: 0.895101] [G loss: 1.464114]\n",
      "epoch:1, g_loss:2827.0810546875,d_loss:1528.5880126953125\n",
      "[D loss: 1.018953] [G loss: 1.548963]\n",
      "[D loss: 0.703427] [G loss: 1.538981]\n",
      "[D loss: 0.773781] [G loss: 1.452229]\n",
      "[D loss: 0.685511] [G loss: 1.337352]\n",
      "[D loss: 1.040728] [G loss: 1.528044]\n",
      "[D loss: 0.787301] [G loss: 1.698803]\n",
      "[D loss: 0.793425] [G loss: 1.302121]\n",
      "[D loss: 0.717405] [G loss: 1.432366]\n",
      "[D loss: 0.777997] [G loss: 1.565013]\n",
      "[D loss: 0.712574] [G loss: 1.482970]\n",
      "[D loss: 0.823419] [G loss: 1.583777]\n",
      "[D loss: 0.858284] [G loss: 1.491960]\n",
      "[D loss: 0.779853] [G loss: 1.505921]\n",
      "[D loss: 0.688224] [G loss: 1.628763]\n",
      "[D loss: 0.964098] [G loss: 1.606480]\n",
      "[D loss: 0.852210] [G loss: 1.562319]\n",
      "[D loss: 1.024622] [G loss: 1.672909]\n",
      "[D loss: 0.781439] [G loss: 1.651590]\n",
      "[D loss: 0.674129] [G loss: 1.409865]\n",
      "[D loss: 0.725810] [G loss: 1.381526]\n",
      "[D loss: 0.787275] [G loss: 1.372836]\n",
      "[D loss: 0.784582] [G loss: 1.334125]\n",
      "[D loss: 0.730943] [G loss: 1.562954]\n",
      "[D loss: 0.599940] [G loss: 1.710315]\n",
      "[D loss: 0.837209] [G loss: 1.603763]\n",
      "[D loss: 0.822927] [G loss: 1.414602]\n",
      "[D loss: 0.936192] [G loss: 1.699474]\n",
      "[D loss: 0.859476] [G loss: 1.518622]\n",
      "[D loss: 0.812266] [G loss: 1.159395]\n",
      "[D loss: 0.853219] [G loss: 1.429826]\n",
      "[D loss: 0.662040] [G loss: 1.503386]\n",
      "[D loss: 0.811769] [G loss: 1.369082]\n",
      "[D loss: 1.043011] [G loss: 1.417118]\n",
      "[D loss: 0.652658] [G loss: 1.594717]\n",
      "[D loss: 0.787006] [G loss: 1.481422]\n",
      "[D loss: 1.031927] [G loss: 1.218794]\n",
      "[D loss: 0.862880] [G loss: 1.383723]\n",
      "[D loss: 0.631521] [G loss: 1.629131]\n",
      "[D loss: 0.634596] [G loss: 1.503932]\n",
      "[D loss: 0.943921] [G loss: 1.493134]\n",
      "[D loss: 0.838474] [G loss: 1.567403]\n",
      "[D loss: 0.828035] [G loss: 1.209799]\n",
      "[D loss: 0.822497] [G loss: 1.639394]\n",
      "[D loss: 0.680639] [G loss: 1.599189]\n",
      "[D loss: 1.045092] [G loss: 1.356410]\n",
      "[D loss: 0.502641] [G loss: 1.663866]\n",
      "[D loss: 0.780633] [G loss: 1.539791]\n",
      "[D loss: 0.862915] [G loss: 1.470245]\n",
      "[D loss: 0.827736] [G loss: 1.523764]\n",
      "[D loss: 0.976832] [G loss: 1.508155]\n",
      "[D loss: 1.022331] [G loss: 1.229711]\n",
      "[D loss: 0.763733] [G loss: 1.541875]\n",
      "[D loss: 0.585065] [G loss: 1.711229]\n",
      "[D loss: 0.963319] [G loss: 1.861078]\n",
      "[D loss: 0.722199] [G loss: 1.458451]\n",
      "[D loss: 0.641540] [G loss: 1.420419]\n",
      "[D loss: 0.765424] [G loss: 1.416047]\n",
      "[D loss: 0.851785] [G loss: 1.390968]\n",
      "[D loss: 0.745608] [G loss: 1.593633]\n",
      "[D loss: 0.615708] [G loss: 1.579641]\n",
      "[D loss: 0.962972] [G loss: 1.474519]\n",
      "[D loss: 0.989264] [G loss: 1.590172]\n",
      "[D loss: 0.659498] [G loss: 1.695582]\n",
      "[D loss: 0.877990] [G loss: 1.350836]\n",
      "[D loss: 0.918634] [G loss: 1.391785]\n",
      "[D loss: 0.964670] [G loss: 1.419885]\n",
      "[D loss: 0.649867] [G loss: 1.802907]\n",
      "[D loss: 0.549967] [G loss: 1.403823]\n",
      "[D loss: 0.633245] [G loss: 1.753996]\n",
      "[D loss: 0.835371] [G loss: 1.405693]\n",
      "[D loss: 0.613618] [G loss: 1.606956]\n",
      "[D loss: 0.945697] [G loss: 1.575874]\n",
      "[D loss: 0.683089] [G loss: 1.439923]\n",
      "[D loss: 0.637509] [G loss: 1.513638]\n",
      "[D loss: 0.866840] [G loss: 1.535185]\n",
      "[D loss: 0.740483] [G loss: 1.589578]\n",
      "[D loss: 0.952372] [G loss: 1.351530]\n",
      "[D loss: 0.806513] [G loss: 1.291143]\n",
      "[D loss: 0.717338] [G loss: 1.468778]\n",
      "[D loss: 0.590560] [G loss: 1.721619]\n",
      "[D loss: 0.565970] [G loss: 1.867439]\n",
      "[D loss: 0.823693] [G loss: 1.588393]\n",
      "[D loss: 0.863378] [G loss: 1.484728]\n",
      "[D loss: 0.842359] [G loss: 1.488767]\n",
      "[D loss: 0.831714] [G loss: 1.409788]\n",
      "[D loss: 0.810721] [G loss: 1.604487]\n",
      "[D loss: 0.984855] [G loss: 1.485481]\n",
      "[D loss: 0.942513] [G loss: 1.553153]\n",
      "[D loss: 0.711659] [G loss: 1.627693]\n",
      "[D loss: 0.814110] [G loss: 1.773706]\n",
      "[D loss: 0.940937] [G loss: 1.635532]\n",
      "[D loss: 0.819562] [G loss: 1.461918]\n",
      "[D loss: 0.751035] [G loss: 1.352211]\n",
      "[D loss: 0.807786] [G loss: 1.281044]\n",
      "[D loss: 0.989874] [G loss: 1.450904]\n",
      "[D loss: 0.640951] [G loss: 1.509822]\n",
      "[D loss: 0.960151] [G loss: 1.327385]\n",
      "[D loss: 0.810600] [G loss: 1.560171]\n",
      "[D loss: 0.834036] [G loss: 1.562123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.962156] [G loss: 1.434780]\n",
      "[D loss: 0.786769] [G loss: 1.427829]\n",
      "[D loss: 0.929663] [G loss: 1.207173]\n",
      "[D loss: 0.780791] [G loss: 1.295920]\n",
      "[D loss: 0.739082] [G loss: 1.396435]\n",
      "[D loss: 0.636210] [G loss: 1.915098]\n",
      "[D loss: 0.853992] [G loss: 1.648414]\n",
      "[D loss: 0.903020] [G loss: 1.595953]\n",
      "[D loss: 0.639856] [G loss: 1.414808]\n",
      "[D loss: 0.930288] [G loss: 1.455432]\n",
      "[D loss: 0.970772] [G loss: 1.414691]\n",
      "[D loss: 0.797128] [G loss: 1.485054]\n",
      "[D loss: 0.846868] [G loss: 1.540923]\n",
      "[D loss: 0.965819] [G loss: 1.540348]\n",
      "[D loss: 0.901870] [G loss: 1.645456]\n",
      "[D loss: 0.966312] [G loss: 1.551275]\n",
      "[D loss: 0.837575] [G loss: 1.354875]\n",
      "[D loss: 0.859952] [G loss: 1.516531]\n",
      "[D loss: 0.911544] [G loss: 1.487416]\n",
      "[D loss: 0.790550] [G loss: 1.391281]\n",
      "[D loss: 0.717251] [G loss: 1.363657]\n",
      "[D loss: 0.737371] [G loss: 1.402895]\n",
      "[D loss: 0.732525] [G loss: 1.559159]\n",
      "[D loss: 0.680389] [G loss: 1.610530]\n",
      "[D loss: 1.215246] [G loss: 1.438878]\n",
      "[D loss: 0.886498] [G loss: 1.504870]\n",
      "[D loss: 0.980599] [G loss: 1.363943]\n",
      "[D loss: 0.667466] [G loss: 1.783608]\n",
      "[D loss: 0.652701] [G loss: 1.490044]\n",
      "[D loss: 0.761610] [G loss: 1.556093]\n",
      "[D loss: 0.734868] [G loss: 1.626218]\n",
      "[D loss: 0.720879] [G loss: 1.667997]\n",
      "[D loss: 0.710009] [G loss: 1.683593]\n",
      "[D loss: 0.820339] [G loss: 1.574984]\n",
      "[D loss: 0.835942] [G loss: 1.701097]\n",
      "[D loss: 0.831551] [G loss: 1.441432]\n",
      "[D loss: 0.721031] [G loss: 1.441761]\n",
      "[D loss: 0.783710] [G loss: 1.711062]\n",
      "[D loss: 0.834256] [G loss: 1.550961]\n",
      "[D loss: 0.845962] [G loss: 1.789588]\n",
      "[D loss: 0.717237] [G loss: 1.855849]\n",
      "[D loss: 0.691649] [G loss: 1.541536]\n",
      "[D loss: 0.796696] [G loss: 1.537733]\n",
      "[D loss: 0.625496] [G loss: 1.778673]\n",
      "[D loss: 0.619902] [G loss: 1.610830]\n",
      "[D loss: 0.878440] [G loss: 1.436111]\n",
      "[D loss: 0.877896] [G loss: 1.572317]\n",
      "[D loss: 0.881461] [G loss: 1.901692]\n",
      "[D loss: 0.803025] [G loss: 1.863338]\n",
      "[D loss: 1.029751] [G loss: 1.625216]\n",
      "[D loss: 1.004599] [G loss: 1.424411]\n",
      "[D loss: 0.915914] [G loss: 1.654125]\n",
      "[D loss: 0.685589] [G loss: 1.576748]\n",
      "[D loss: 0.856229] [G loss: 1.316360]\n",
      "[D loss: 0.752780] [G loss: 1.434415]\n",
      "[D loss: 0.471226] [G loss: 1.798572]\n",
      "[D loss: 0.895126] [G loss: 1.846660]\n",
      "[D loss: 0.704795] [G loss: 1.694594]\n",
      "[D loss: 0.856280] [G loss: 1.481026]\n",
      "[D loss: 0.878365] [G loss: 1.317969]\n",
      "[D loss: 0.734291] [G loss: 1.452164]\n",
      "[D loss: 0.599759] [G loss: 1.644701]\n",
      "[D loss: 0.679308] [G loss: 1.686186]\n",
      "[D loss: 1.033623] [G loss: 1.421256]\n",
      "[D loss: 0.695931] [G loss: 1.621102]\n",
      "[D loss: 0.676833] [G loss: 1.544617]\n",
      "[D loss: 0.789976] [G loss: 1.670172]\n",
      "[D loss: 0.761366] [G loss: 1.646769]\n",
      "[D loss: 0.607241] [G loss: 1.553764]\n",
      "[D loss: 0.838342] [G loss: 1.443130]\n",
      "[D loss: 0.555802] [G loss: 1.820835]\n",
      "[D loss: 0.945097] [G loss: 1.404345]\n",
      "[D loss: 0.930582] [G loss: 1.703537]\n",
      "[D loss: 0.740720] [G loss: 1.588991]\n",
      "[D loss: 0.801907] [G loss: 1.547553]\n",
      "[D loss: 0.822362] [G loss: 1.379453]\n",
      "[D loss: 0.717078] [G loss: 1.678714]\n",
      "[D loss: 0.840670] [G loss: 1.389607]\n",
      "[D loss: 0.754342] [G loss: 1.564593]\n",
      "[D loss: 0.883280] [G loss: 1.699440]\n",
      "[D loss: 0.876460] [G loss: 1.422017]\n",
      "[D loss: 0.900896] [G loss: 1.468482]\n",
      "[D loss: 0.661825] [G loss: 1.640825]\n",
      "[D loss: 0.583392] [G loss: 1.477008]\n",
      "[D loss: 0.707825] [G loss: 1.380611]\n",
      "[D loss: 0.842971] [G loss: 1.498940]\n",
      "[D loss: 0.945787] [G loss: 1.562620]\n",
      "[D loss: 0.791670] [G loss: 1.598631]\n",
      "[D loss: 0.867023] [G loss: 1.590701]\n",
      "[D loss: 0.875129] [G loss: 1.591045]\n",
      "[D loss: 0.840386] [G loss: 1.491685]\n",
      "[D loss: 0.833649] [G loss: 1.437182]\n",
      "[D loss: 0.794378] [G loss: 1.402418]\n",
      "[D loss: 0.731253] [G loss: 1.493789]\n",
      "[D loss: 0.790478] [G loss: 1.606997]\n",
      "[D loss: 0.725775] [G loss: 1.539060]\n",
      "[D loss: 0.675213] [G loss: 1.517035]\n",
      "[D loss: 0.807460] [G loss: 1.320127]\n",
      "[D loss: 0.965978] [G loss: 1.271727]\n",
      "[D loss: 0.660193] [G loss: 1.561987]\n",
      "[D loss: 0.786282] [G loss: 1.467834]\n",
      "[D loss: 0.888312] [G loss: 1.649494]\n",
      "[D loss: 0.790715] [G loss: 1.560157]\n",
      "[D loss: 0.715460] [G loss: 1.456660]\n",
      "[D loss: 0.629483] [G loss: 1.571139]\n",
      "[D loss: 0.849558] [G loss: 1.388978]\n",
      "[D loss: 0.839368] [G loss: 1.359841]\n",
      "[D loss: 0.734188] [G loss: 1.433371]\n",
      "[D loss: 0.661588] [G loss: 1.258362]\n",
      "[D loss: 0.801094] [G loss: 1.866674]\n",
      "[D loss: 0.864745] [G loss: 1.634319]\n",
      "[D loss: 0.680600] [G loss: 1.815243]\n",
      "[D loss: 0.755132] [G loss: 1.737482]\n",
      "[D loss: 0.719625] [G loss: 1.587513]\n",
      "[D loss: 0.768539] [G loss: 1.362726]\n",
      "[D loss: 0.646222] [G loss: 1.439841]\n",
      "[D loss: 0.506668] [G loss: 1.590890]\n",
      "[D loss: 0.689056] [G loss: 1.826211]\n",
      "[D loss: 0.880387] [G loss: 1.573075]\n",
      "[D loss: 0.840375] [G loss: 1.570335]\n",
      "[D loss: 0.909172] [G loss: 1.403401]\n",
      "[D loss: 0.547586] [G loss: 1.597248]\n",
      "[D loss: 0.795700] [G loss: 1.438043]\n",
      "[D loss: 0.746927] [G loss: 1.709152]\n",
      "[D loss: 0.750835] [G loss: 1.616149]\n",
      "[D loss: 0.497470] [G loss: 2.003126]\n",
      "[D loss: 0.686013] [G loss: 2.055503]\n",
      "[D loss: 0.706311] [G loss: 1.529799]\n",
      "[D loss: 0.983124] [G loss: 1.588547]\n",
      "[D loss: 0.944510] [G loss: 1.282068]\n",
      "[D loss: 0.677737] [G loss: 1.546105]\n",
      "[D loss: 0.869598] [G loss: 1.608063]\n",
      "[D loss: 0.760577] [G loss: 1.503343]\n",
      "[D loss: 0.609523] [G loss: 1.807457]\n",
      "[D loss: 0.617176] [G loss: 1.814303]\n",
      "[D loss: 0.983648] [G loss: 1.781622]\n",
      "[D loss: 0.626598] [G loss: 1.640798]\n",
      "[D loss: 0.799397] [G loss: 1.575141]\n",
      "[D loss: 0.671167] [G loss: 1.631639]\n",
      "[D loss: 0.824929] [G loss: 1.607808]\n",
      "[D loss: 0.744544] [G loss: 1.542636]\n",
      "[D loss: 0.666248] [G loss: 1.553913]\n",
      "[D loss: 0.820426] [G loss: 1.457462]\n",
      "[D loss: 0.664609] [G loss: 1.673324]\n",
      "[D loss: 0.772621] [G loss: 1.825099]\n",
      "[D loss: 0.845999] [G loss: 1.683972]\n",
      "[D loss: 0.884523] [G loss: 1.387121]\n",
      "[D loss: 0.591412] [G loss: 1.624311]\n",
      "[D loss: 1.108728] [G loss: 1.564815]\n",
      "[D loss: 0.832926] [G loss: 1.489362]\n",
      "[D loss: 0.694810] [G loss: 1.538641]\n",
      "[D loss: 0.833061] [G loss: 1.543887]\n",
      "[D loss: 0.772230] [G loss: 1.806561]\n",
      "[D loss: 0.857083] [G loss: 1.591991]\n",
      "[D loss: 0.648032] [G loss: 1.613556]\n",
      "[D loss: 0.844308] [G loss: 1.398440]\n",
      "[D loss: 0.855222] [G loss: 1.720478]\n",
      "[D loss: 0.724553] [G loss: 1.650165]\n",
      "[D loss: 0.963124] [G loss: 1.419770]\n",
      "[D loss: 1.000885] [G loss: 1.583345]\n",
      "[D loss: 0.694746] [G loss: 1.450253]\n",
      "[D loss: 0.750950] [G loss: 1.513827]\n",
      "[D loss: 0.808661] [G loss: 1.619700]\n",
      "[D loss: 0.678515] [G loss: 1.581451]\n",
      "[D loss: 0.757333] [G loss: 1.429710]\n",
      "[D loss: 0.828450] [G loss: 1.583333]\n",
      "[D loss: 0.695191] [G loss: 1.304393]\n",
      "[D loss: 0.718866] [G loss: 1.644729]\n",
      "[D loss: 0.614234] [G loss: 1.688526]\n",
      "[D loss: 0.926705] [G loss: 1.647515]\n",
      "[D loss: 0.599082] [G loss: 1.648017]\n",
      "[D loss: 1.002281] [G loss: 1.501339]\n",
      "[D loss: 0.787616] [G loss: 1.474094]\n",
      "[D loss: 0.703727] [G loss: 1.600368]\n",
      "[D loss: 0.668297] [G loss: 1.768303]\n",
      "[D loss: 0.864993] [G loss: 1.370886]\n",
      "[D loss: 0.878890] [G loss: 1.410874]\n",
      "[D loss: 0.611112] [G loss: 1.994596]\n",
      "[D loss: 0.746294] [G loss: 1.831206]\n",
      "[D loss: 0.918132] [G loss: 1.671556]\n",
      "[D loss: 0.625608] [G loss: 1.849893]\n",
      "[D loss: 0.793626] [G loss: 1.419484]\n",
      "[D loss: 0.708262] [G loss: 1.492370]\n",
      "[D loss: 0.749401] [G loss: 1.927767]\n",
      "[D loss: 0.689912] [G loss: 1.715575]\n",
      "[D loss: 0.680865] [G loss: 1.885488]\n",
      "[D loss: 0.639324] [G loss: 1.452454]\n",
      "[D loss: 0.971248] [G loss: 1.491658]\n",
      "[D loss: 0.860480] [G loss: 1.568435]\n",
      "[D loss: 0.840560] [G loss: 1.516965]\n",
      "[D loss: 0.944001] [G loss: 1.463083]\n",
      "[D loss: 0.985225] [G loss: 1.558414]\n",
      "[D loss: 0.936190] [G loss: 1.379243]\n",
      "[D loss: 0.729233] [G loss: 1.862838]\n",
      "[D loss: 1.051823] [G loss: 1.565825]\n",
      "[D loss: 0.918877] [G loss: 1.233213]\n",
      "[D loss: 0.872875] [G loss: 1.483140]\n",
      "[D loss: 0.728467] [G loss: 1.734172]\n",
      "[D loss: 0.923088] [G loss: 1.385295]\n",
      "[D loss: 0.608531] [G loss: 1.504739]\n",
      "[D loss: 0.729519] [G loss: 1.495749]\n",
      "[D loss: 0.846469] [G loss: 1.302633]\n",
      "[D loss: 0.812925] [G loss: 1.503066]\n",
      "[D loss: 0.829647] [G loss: 1.420361]\n",
      "[D loss: 0.994156] [G loss: 1.765780]\n",
      "[D loss: 0.719169] [G loss: 1.586122]\n",
      "[D loss: 1.267974] [G loss: 1.219926]\n",
      "[D loss: 0.836869] [G loss: 1.522143]\n",
      "[D loss: 0.515841] [G loss: 1.530039]\n",
      "[D loss: 0.580638] [G loss: 1.583031]\n",
      "[D loss: 0.584493] [G loss: 1.741488]\n",
      "[D loss: 0.551265] [G loss: 1.542835]\n",
      "[D loss: 0.898663] [G loss: 1.424019]\n",
      "[D loss: 0.818588] [G loss: 1.762924]\n",
      "[D loss: 0.982935] [G loss: 1.878342]\n",
      "[D loss: 0.635439] [G loss: 1.508538]\n",
      "[D loss: 0.709660] [G loss: 1.225896]\n",
      "[D loss: 0.686876] [G loss: 1.422718]\n",
      "[D loss: 0.979839] [G loss: 1.535745]\n",
      "[D loss: 0.908473] [G loss: 1.708438]\n",
      "[D loss: 0.664635] [G loss: 1.583761]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.846139] [G loss: 1.383476]\n",
      "[D loss: 0.766221] [G loss: 1.543501]\n",
      "[D loss: 0.992211] [G loss: 1.420821]\n",
      "[D loss: 0.887940] [G loss: 1.264569]\n",
      "[D loss: 0.742420] [G loss: 1.685367]\n",
      "[D loss: 0.707500] [G loss: 1.771745]\n",
      "[D loss: 0.634806] [G loss: 1.579085]\n",
      "[D loss: 0.772580] [G loss: 1.361513]\n",
      "[D loss: 0.982964] [G loss: 1.673923]\n",
      "[D loss: 0.891312] [G loss: 1.386526]\n",
      "[D loss: 0.989594] [G loss: 1.628596]\n",
      "[D loss: 0.936768] [G loss: 1.668967]\n",
      "[D loss: 0.696505] [G loss: 1.969855]\n",
      "[D loss: 0.696865] [G loss: 1.795125]\n",
      "[D loss: 0.909393] [G loss: 1.311048]\n",
      "[D loss: 0.798536] [G loss: 1.578047]\n",
      "[D loss: 0.814177] [G loss: 1.349566]\n",
      "[D loss: 0.945913] [G loss: 1.490223]\n",
      "[D loss: 0.719406] [G loss: 1.381532]\n",
      "[D loss: 0.736326] [G loss: 1.463151]\n",
      "[D loss: 0.627340] [G loss: 1.514516]\n",
      "[D loss: 1.019613] [G loss: 1.246476]\n",
      "[D loss: 0.631391] [G loss: 1.571095]\n",
      "[D loss: 0.679902] [G loss: 1.599596]\n",
      "[D loss: 0.756583] [G loss: 1.548774]\n",
      "[D loss: 0.756176] [G loss: 1.538820]\n",
      "[D loss: 0.681751] [G loss: 1.540639]\n",
      "[D loss: 0.860550] [G loss: 1.615897]\n",
      "[D loss: 0.792639] [G loss: 1.393703]\n",
      "[D loss: 0.947636] [G loss: 1.750123]\n",
      "[D loss: 0.570667] [G loss: 1.757129]\n",
      "[D loss: 0.874500] [G loss: 1.536411]\n",
      "[D loss: 0.721849] [G loss: 1.586066]\n",
      "[D loss: 0.882753] [G loss: 1.410716]\n",
      "[D loss: 0.580217] [G loss: 1.998445]\n",
      "[D loss: 1.057935] [G loss: 1.500506]\n",
      "[D loss: 0.836282] [G loss: 1.517315]\n",
      "[D loss: 1.037000] [G loss: 1.439115]\n",
      "[D loss: 0.685422] [G loss: 1.625598]\n",
      "[D loss: 0.832609] [G loss: 1.623779]\n",
      "[D loss: 0.818738] [G loss: 1.406894]\n",
      "[D loss: 0.844877] [G loss: 1.696808]\n",
      "[D loss: 0.901087] [G loss: 1.569295]\n",
      "[D loss: 0.820820] [G loss: 1.454868]\n",
      "[D loss: 1.100573] [G loss: 1.270446]\n",
      "[D loss: 0.604717] [G loss: 1.565071]\n",
      "[D loss: 0.966438] [G loss: 1.449121]\n",
      "[D loss: 0.945110] [G loss: 1.481594]\n",
      "[D loss: 0.727640] [G loss: 1.518729]\n",
      "[D loss: 0.856999] [G loss: 1.394071]\n",
      "[D loss: 0.836994] [G loss: 1.535836]\n",
      "[D loss: 0.950623] [G loss: 1.467674]\n",
      "[D loss: 1.071471] [G loss: 1.631713]\n",
      "[D loss: 0.899091] [G loss: 1.240390]\n",
      "[D loss: 0.807197] [G loss: 1.263489]\n",
      "[D loss: 0.891399] [G loss: 1.418460]\n",
      "[D loss: 0.733348] [G loss: 1.497648]\n",
      "[D loss: 0.782170] [G loss: 1.481596]\n",
      "[D loss: 0.667193] [G loss: 1.609025]\n",
      "[D loss: 0.915031] [G loss: 1.443439]\n",
      "[D loss: 0.812767] [G loss: 1.477877]\n",
      "[D loss: 0.630471] [G loss: 1.442593]\n",
      "[D loss: 0.927061] [G loss: 1.316360]\n",
      "[D loss: 0.749780] [G loss: 1.332622]\n",
      "[D loss: 0.977874] [G loss: 1.222147]\n",
      "[D loss: 0.906762] [G loss: 1.795315]\n",
      "[D loss: 0.705073] [G loss: 1.835950]\n",
      "[D loss: 0.738390] [G loss: 1.711644]\n",
      "[D loss: 0.865432] [G loss: 1.515798]\n",
      "[D loss: 0.796344] [G loss: 1.509887]\n",
      "[D loss: 0.692877] [G loss: 1.282610]\n",
      "[D loss: 0.836204] [G loss: 1.430760]\n",
      "[D loss: 0.662385] [G loss: 1.517176]\n",
      "[D loss: 0.701228] [G loss: 1.623319]\n",
      "[D loss: 0.944266] [G loss: 1.892675]\n",
      "[D loss: 0.640677] [G loss: 1.611300]\n",
      "[D loss: 1.000177] [G loss: 1.492323]\n",
      "[D loss: 0.931132] [G loss: 1.188930]\n",
      "[D loss: 0.666748] [G loss: 1.637692]\n",
      "[D loss: 0.828761] [G loss: 1.552822]\n",
      "[D loss: 0.685088] [G loss: 1.689146]\n",
      "[D loss: 0.777817] [G loss: 1.712116]\n",
      "[D loss: 0.714948] [G loss: 1.319944]\n",
      "[D loss: 0.699088] [G loss: 1.673401]\n",
      "[D loss: 0.910015] [G loss: 1.487650]\n",
      "[D loss: 0.899863] [G loss: 1.494912]\n",
      "[D loss: 1.144417] [G loss: 1.173631]\n",
      "[D loss: 0.646421] [G loss: 1.703640]\n",
      "[D loss: 0.689313] [G loss: 1.556156]\n",
      "[D loss: 0.850810] [G loss: 1.557224]\n",
      "[D loss: 0.718890] [G loss: 1.419957]\n",
      "[D loss: 0.679808] [G loss: 1.467297]\n",
      "[D loss: 0.859782] [G loss: 1.439727]\n",
      "[D loss: 0.697227] [G loss: 1.784916]\n",
      "[D loss: 0.767952] [G loss: 1.801971]\n",
      "[D loss: 0.668145] [G loss: 1.621887]\n",
      "[D loss: 0.936666] [G loss: 1.563558]\n",
      "[D loss: 0.780956] [G loss: 1.294780]\n",
      "[D loss: 1.024980] [G loss: 1.329888]\n",
      "[D loss: 0.647253] [G loss: 1.638827]\n",
      "[D loss: 0.964104] [G loss: 1.476482]\n",
      "[D loss: 0.698410] [G loss: 1.453238]\n",
      "[D loss: 0.922086] [G loss: 1.423246]\n",
      "[D loss: 0.982779] [G loss: 1.440619]\n",
      "[D loss: 0.781680] [G loss: 1.578688]\n",
      "[D loss: 0.824322] [G loss: 1.574975]\n",
      "[D loss: 0.650032] [G loss: 1.542532]\n",
      "[D loss: 0.705307] [G loss: 1.433140]\n",
      "[D loss: 0.701357] [G loss: 1.448301]\n",
      "[D loss: 0.634254] [G loss: 1.720455]\n",
      "[D loss: 0.901075] [G loss: 1.321219]\n",
      "[D loss: 0.744237] [G loss: 1.562158]\n",
      "[D loss: 1.007282] [G loss: 1.597306]\n",
      "[D loss: 1.040970] [G loss: 1.616953]\n",
      "[D loss: 0.724474] [G loss: 1.472423]\n",
      "[D loss: 1.164154] [G loss: 1.333806]\n",
      "[D loss: 0.879870] [G loss: 1.444075]\n",
      "[D loss: 0.560341] [G loss: 1.576092]\n",
      "[D loss: 0.795341] [G loss: 1.483187]\n",
      "[D loss: 0.754197] [G loss: 1.451184]\n",
      "[D loss: 1.095205] [G loss: 1.448729]\n",
      "[D loss: 0.869559] [G loss: 1.397565]\n",
      "[D loss: 0.907804] [G loss: 1.766483]\n",
      "[D loss: 0.876171] [G loss: 1.448569]\n",
      "[D loss: 0.761513] [G loss: 1.730232]\n",
      "[D loss: 1.001077] [G loss: 1.414295]\n",
      "[D loss: 0.661423] [G loss: 1.671868]\n",
      "[D loss: 0.816299] [G loss: 1.980819]\n",
      "[D loss: 0.892709] [G loss: 1.401731]\n",
      "[D loss: 0.741659] [G loss: 1.561038]\n",
      "[D loss: 0.734307] [G loss: 1.465256]\n",
      "[D loss: 0.898824] [G loss: 1.563222]\n",
      "[D loss: 0.826499] [G loss: 1.400785]\n",
      "[D loss: 0.734183] [G loss: 1.257383]\n",
      "[D loss: 0.765497] [G loss: 1.270780]\n",
      "[D loss: 0.681357] [G loss: 1.505012]\n",
      "[D loss: 0.850611] [G loss: 1.648517]\n",
      "[D loss: 0.663115] [G loss: 1.716520]\n",
      "[D loss: 1.138641] [G loss: 1.106206]\n",
      "[D loss: 0.878526] [G loss: 1.667859]\n",
      "[D loss: 0.718027] [G loss: 1.715864]\n",
      "[D loss: 0.825929] [G loss: 1.460757]\n",
      "[D loss: 0.742779] [G loss: 1.402776]\n",
      "[D loss: 0.619017] [G loss: 1.579709]\n",
      "[D loss: 0.676137] [G loss: 1.618807]\n",
      "[D loss: 0.929709] [G loss: 1.373854]\n",
      "[D loss: 0.770616] [G loss: 1.357211]\n",
      "[D loss: 0.887214] [G loss: 1.195545]\n",
      "[D loss: 0.693980] [G loss: 1.557942]\n",
      "[D loss: 0.937934] [G loss: 1.366762]\n",
      "[D loss: 0.953206] [G loss: 1.315002]\n",
      "[D loss: 0.584769] [G loss: 1.523875]\n",
      "[D loss: 0.587957] [G loss: 1.537063]\n",
      "[D loss: 0.862713] [G loss: 1.475760]\n",
      "[D loss: 0.643415] [G loss: 1.630187]\n",
      "[D loss: 0.696665] [G loss: 1.541535]\n",
      "[D loss: 0.740641] [G loss: 1.701719]\n",
      "[D loss: 0.667144] [G loss: 1.609791]\n",
      "[D loss: 0.964111] [G loss: 1.546125]\n",
      "[D loss: 0.443326] [G loss: 1.899920]\n",
      "[D loss: 0.818202] [G loss: 1.569868]\n",
      "[D loss: 0.787500] [G loss: 1.685153]\n",
      "[D loss: 0.783058] [G loss: 1.523835]\n",
      "[D loss: 0.609401] [G loss: 1.739760]\n",
      "[D loss: 0.763060] [G loss: 1.771072]\n",
      "[D loss: 0.772568] [G loss: 2.163741]\n",
      "[D loss: 0.865992] [G loss: 1.956349]\n",
      "[D loss: 0.844175] [G loss: 1.604379]\n",
      "[D loss: 0.768275] [G loss: 1.687791]\n",
      "[D loss: 0.540958] [G loss: 1.652335]\n",
      "[D loss: 0.751834] [G loss: 1.443334]\n",
      "[D loss: 0.671385] [G loss: 1.661582]\n",
      "[D loss: 0.464977] [G loss: 1.912507]\n",
      "[D loss: 0.611002] [G loss: 1.829229]\n",
      "[D loss: 0.921352] [G loss: 1.734315]\n",
      "[D loss: 0.677809] [G loss: 1.793597]\n",
      "[D loss: 0.817085] [G loss: 1.704674]\n",
      "[D loss: 0.826591] [G loss: 1.352465]\n",
      "[D loss: 0.822858] [G loss: 1.450215]\n",
      "[D loss: 0.877097] [G loss: 1.587616]\n",
      "[D loss: 0.706792] [G loss: 1.677401]\n",
      "[D loss: 0.824247] [G loss: 1.795397]\n",
      "[D loss: 0.718953] [G loss: 1.662975]\n",
      "[D loss: 0.936762] [G loss: 1.556816]\n",
      "[D loss: 0.612864] [G loss: 1.488210]\n",
      "[D loss: 0.675133] [G loss: 1.578140]\n",
      "[D loss: 0.549136] [G loss: 1.482137]\n",
      "[D loss: 1.023806] [G loss: 1.621261]\n",
      "[D loss: 0.827264] [G loss: 1.696172]\n",
      "[D loss: 0.691738] [G loss: 1.447275]\n",
      "[D loss: 1.064071] [G loss: 1.638491]\n",
      "[D loss: 0.763231] [G loss: 1.518896]\n",
      "[D loss: 1.025888] [G loss: 1.345899]\n",
      "[D loss: 0.598216] [G loss: 1.468996]\n",
      "[D loss: 0.871239] [G loss: 1.523398]\n",
      "[D loss: 0.779583] [G loss: 1.469258]\n",
      "[D loss: 0.780338] [G loss: 1.395155]\n",
      "[D loss: 0.714902] [G loss: 1.682982]\n",
      "[D loss: 0.633605] [G loss: 1.634830]\n",
      "[D loss: 0.882930] [G loss: 1.708472]\n",
      "[D loss: 0.717782] [G loss: 1.340469]\n",
      "[D loss: 0.979228] [G loss: 1.270344]\n",
      "[D loss: 0.548142] [G loss: 1.547197]\n",
      "[D loss: 0.820644] [G loss: 1.444386]\n",
      "[D loss: 0.772035] [G loss: 1.407980]\n",
      "[D loss: 0.856087] [G loss: 1.582569]\n",
      "[D loss: 0.892068] [G loss: 1.584958]\n",
      "[D loss: 0.673888] [G loss: 1.596320]\n",
      "[D loss: 0.812616] [G loss: 1.491662]\n",
      "[D loss: 1.071346] [G loss: 1.163686]\n",
      "[D loss: 0.931652] [G loss: 1.288002]\n",
      "[D loss: 0.930363] [G loss: 1.727276]\n",
      "[D loss: 0.856806] [G loss: 1.535522]\n",
      "[D loss: 0.573084] [G loss: 1.594709]\n",
      "[D loss: 0.695602] [G loss: 1.489856]\n",
      "[D loss: 0.684215] [G loss: 1.559816]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.786597] [G loss: 1.702841]\n",
      "[D loss: 0.821411] [G loss: 1.672228]\n",
      "[D loss: 0.608325] [G loss: 1.571090]\n",
      "[D loss: 0.674294] [G loss: 1.522025]\n",
      "[D loss: 0.704489] [G loss: 1.464421]\n",
      "[D loss: 0.870031] [G loss: 1.849501]\n",
      "[D loss: 1.021600] [G loss: 1.815684]\n",
      "[D loss: 0.889051] [G loss: 1.559441]\n",
      "[D loss: 1.002882] [G loss: 1.316599]\n",
      "[D loss: 0.683608] [G loss: 1.520674]\n",
      "[D loss: 0.858910] [G loss: 1.575137]\n",
      "[D loss: 0.829457] [G loss: 1.421881]\n",
      "[D loss: 0.862172] [G loss: 1.307110]\n",
      "[D loss: 0.733437] [G loss: 1.482071]\n",
      "[D loss: 0.859702] [G loss: 1.683488]\n",
      "[D loss: 0.869548] [G loss: 1.552370]\n",
      "[D loss: 1.037224] [G loss: 1.208453]\n",
      "[D loss: 0.719199] [G loss: 1.559692]\n",
      "[D loss: 0.572981] [G loss: 1.525540]\n",
      "[D loss: 0.704238] [G loss: 1.716504]\n",
      "[D loss: 1.248095] [G loss: 1.502291]\n",
      "[D loss: 0.691558] [G loss: 1.482891]\n",
      "[D loss: 1.003784] [G loss: 1.600034]\n",
      "[D loss: 1.054429] [G loss: 1.375248]\n",
      "[D loss: 0.850404] [G loss: 1.368460]\n",
      "[D loss: 0.725013] [G loss: 1.369583]\n",
      "[D loss: 0.807461] [G loss: 1.298847]\n",
      "[D loss: 0.723370] [G loss: 1.612666]\n",
      "[D loss: 0.879400] [G loss: 1.470129]\n",
      "[D loss: 0.736794] [G loss: 1.324161]\n",
      "[D loss: 0.754095] [G loss: 1.742666]\n",
      "[D loss: 1.116162] [G loss: 1.354146]\n",
      "[D loss: 0.765340] [G loss: 1.486843]\n",
      "[D loss: 0.603822] [G loss: 1.576576]\n",
      "[D loss: 0.917595] [G loss: 1.447047]\n",
      "[D loss: 0.885940] [G loss: 1.388718]\n",
      "[D loss: 0.861478] [G loss: 1.300749]\n",
      "[D loss: 0.954123] [G loss: 1.365230]\n",
      "[D loss: 0.856251] [G loss: 1.446859]\n",
      "[D loss: 0.928703] [G loss: 1.503653]\n",
      "[D loss: 0.681208] [G loss: 1.553202]\n",
      "[D loss: 0.863357] [G loss: 1.401683]\n",
      "[D loss: 0.741754] [G loss: 1.367267]\n",
      "[D loss: 0.694407] [G loss: 1.897397]\n",
      "[D loss: 0.847115] [G loss: 1.274507]\n",
      "[D loss: 0.670548] [G loss: 1.600208]\n",
      "[D loss: 0.821246] [G loss: 1.782219]\n",
      "[D loss: 0.752087] [G loss: 1.301659]\n",
      "[D loss: 0.817792] [G loss: 1.399393]\n",
      "[D loss: 0.600243] [G loss: 1.700856]\n",
      "[D loss: 0.803711] [G loss: 1.579875]\n",
      "[D loss: 0.531433] [G loss: 1.686222]\n",
      "[D loss: 0.666120] [G loss: 1.728260]\n",
      "[D loss: 0.938399] [G loss: 1.423920]\n",
      "[D loss: 0.962827] [G loss: 1.362970]\n",
      "[D loss: 0.971512] [G loss: 1.487988]\n",
      "[D loss: 0.804193] [G loss: 1.645083]\n",
      "[D loss: 0.798859] [G loss: 1.911419]\n",
      "[D loss: 0.844039] [G loss: 1.511051]\n",
      "[D loss: 0.737885] [G loss: 1.524990]\n",
      "[D loss: 0.817499] [G loss: 1.758153]\n",
      "[D loss: 0.797216] [G loss: 1.714026]\n",
      "[D loss: 0.899827] [G loss: 1.449081]\n",
      "[D loss: 0.817644] [G loss: 1.469259]\n",
      "[D loss: 0.748201] [G loss: 1.539663]\n",
      "[D loss: 0.910876] [G loss: 1.446091]\n",
      "[D loss: 0.806674] [G loss: 1.460914]\n",
      "[D loss: 0.747372] [G loss: 1.484883]\n",
      "[D loss: 0.767160] [G loss: 1.723796]\n",
      "[D loss: 0.799248] [G loss: 1.478098]\n",
      "[D loss: 0.928978] [G loss: 1.209106]\n",
      "[D loss: 0.892785] [G loss: 1.396673]\n",
      "[D loss: 0.796342] [G loss: 1.568357]\n",
      "[D loss: 0.832401] [G loss: 1.607462]\n",
      "[D loss: 0.720080] [G loss: 1.634102]\n",
      "[D loss: 0.890967] [G loss: 1.625152]\n",
      "[D loss: 0.714477] [G loss: 1.406043]\n",
      "[D loss: 1.095373] [G loss: 1.226343]\n",
      "[D loss: 0.734425] [G loss: 1.645149]\n",
      "[D loss: 1.052312] [G loss: 1.529611]\n",
      "[D loss: 0.905504] [G loss: 1.615990]\n",
      "[D loss: 0.690300] [G loss: 1.409664]\n",
      "[D loss: 0.784886] [G loss: 1.489911]\n",
      "[D loss: 0.695871] [G loss: 1.495476]\n",
      "[D loss: 0.539750] [G loss: 1.694521]\n",
      "[D loss: 0.670205] [G loss: 1.592914]\n",
      "[D loss: 0.770875] [G loss: 1.653425]\n",
      "[D loss: 0.872214] [G loss: 1.379434]\n",
      "[D loss: 0.955229] [G loss: 1.419988]\n",
      "[D loss: 0.686856] [G loss: 1.519828]\n",
      "[D loss: 0.886587] [G loss: 1.681878]\n",
      "[D loss: 0.730444] [G loss: 1.479575]\n",
      "[D loss: 0.692126] [G loss: 1.846813]\n",
      "[D loss: 0.751333] [G loss: 1.459440]\n",
      "[D loss: 0.697355] [G loss: 1.308750]\n",
      "[D loss: 0.695910] [G loss: 1.446615]\n",
      "[D loss: 0.718058] [G loss: 1.564376]\n",
      "[D loss: 0.615147] [G loss: 1.677627]\n",
      "[D loss: 0.837696] [G loss: 1.449552]\n",
      "[D loss: 0.838059] [G loss: 1.479004]\n",
      "[D loss: 0.779095] [G loss: 1.469148]\n",
      "[D loss: 0.802652] [G loss: 1.422219]\n",
      "[D loss: 0.925538] [G loss: 1.564550]\n",
      "[D loss: 1.025461] [G loss: 1.408611]\n",
      "[D loss: 0.659864] [G loss: 1.525216]\n",
      "[D loss: 1.018250] [G loss: 1.376940]\n",
      "[D loss: 0.714999] [G loss: 1.597231]\n",
      "[D loss: 0.787418] [G loss: 1.500581]\n",
      "[D loss: 0.823174] [G loss: 1.738008]\n",
      "[D loss: 0.872767] [G loss: 1.405350]\n",
      "[D loss: 0.748470] [G loss: 1.674868]\n",
      "[D loss: 0.642195] [G loss: 1.564240]\n",
      "[D loss: 0.717464] [G loss: 1.619513]\n",
      "[D loss: 1.130491] [G loss: 1.645550]\n",
      "[D loss: 0.576874] [G loss: 1.717368]\n",
      "[D loss: 0.737965] [G loss: 1.549683]\n",
      "[D loss: 0.906508] [G loss: 1.376953]\n",
      "[D loss: 0.798491] [G loss: 1.449381]\n",
      "[D loss: 0.883326] [G loss: 1.587992]\n",
      "[D loss: 0.890780] [G loss: 1.631054]\n",
      "[D loss: 0.839691] [G loss: 1.524118]\n",
      "[D loss: 0.712382] [G loss: 1.394384]\n",
      "[D loss: 0.707386] [G loss: 1.637716]\n",
      "[D loss: 0.860945] [G loss: 1.428967]\n",
      "[D loss: 0.798201] [G loss: 1.431454]\n",
      "[D loss: 1.058137] [G loss: 1.252572]\n",
      "[D loss: 0.810266] [G loss: 1.756274]\n",
      "[D loss: 0.797952] [G loss: 1.631665]\n",
      "[D loss: 0.763904] [G loss: 1.371120]\n",
      "[D loss: 0.929021] [G loss: 1.451502]\n",
      "[D loss: 0.864331] [G loss: 1.647651]\n",
      "[D loss: 1.044444] [G loss: 1.466222]\n",
      "[D loss: 0.781002] [G loss: 1.557938]\n",
      "[D loss: 0.925366] [G loss: 1.530764]\n",
      "[D loss: 0.856314] [G loss: 1.420875]\n",
      "[D loss: 0.700256] [G loss: 1.343144]\n",
      "[D loss: 0.839142] [G loss: 1.613185]\n",
      "[D loss: 1.064543] [G loss: 1.444983]\n",
      "[D loss: 0.715426] [G loss: 1.779566]\n",
      "[D loss: 0.809631] [G loss: 1.553040]\n",
      "[D loss: 0.836736] [G loss: 1.472126]\n",
      "[D loss: 0.825608] [G loss: 1.655326]\n",
      "[D loss: 0.804591] [G loss: 1.479396]\n",
      "[D loss: 0.967094] [G loss: 1.279365]\n",
      "[D loss: 0.715191] [G loss: 1.409182]\n",
      "[D loss: 0.935149] [G loss: 1.600745]\n",
      "[D loss: 0.677162] [G loss: 1.516772]\n",
      "[D loss: 0.796362] [G loss: 1.674615]\n",
      "[D loss: 0.831150] [G loss: 1.585119]\n",
      "[D loss: 0.863606] [G loss: 1.508503]\n",
      "[D loss: 0.541073] [G loss: 1.584864]\n",
      "[D loss: 0.739122] [G loss: 1.547554]\n",
      "[D loss: 0.973600] [G loss: 1.576463]\n",
      "[D loss: 0.726158] [G loss: 1.715827]\n",
      "[D loss: 0.878473] [G loss: 1.592200]\n",
      "[D loss: 0.941794] [G loss: 1.477059]\n",
      "[D loss: 0.817645] [G loss: 1.471419]\n",
      "[D loss: 0.747260] [G loss: 1.544245]\n",
      "[D loss: 0.783629] [G loss: 1.408300]\n",
      "[D loss: 0.915390] [G loss: 1.546903]\n",
      "[D loss: 0.766078] [G loss: 1.517321]\n",
      "[D loss: 0.944396] [G loss: 1.415938]\n",
      "[D loss: 0.744227] [G loss: 1.474259]\n",
      "[D loss: 0.964308] [G loss: 1.382817]\n",
      "[D loss: 0.952082] [G loss: 1.676488]\n",
      "[D loss: 0.817834] [G loss: 1.357905]\n",
      "[D loss: 0.993145] [G loss: 1.288466]\n",
      "[D loss: 0.899623] [G loss: 1.412417]\n",
      "[D loss: 0.759208] [G loss: 1.468689]\n",
      "[D loss: 0.735067] [G loss: 1.401462]\n",
      "[D loss: 0.786907] [G loss: 1.471864]\n",
      "[D loss: 0.833421] [G loss: 1.385434]\n",
      "[D loss: 0.521954] [G loss: 1.640839]\n",
      "[D loss: 0.485287] [G loss: 1.528176]\n",
      "[D loss: 0.635843] [G loss: 1.663196]\n",
      "[D loss: 0.977320] [G loss: 1.496386]\n",
      "[D loss: 0.748947] [G loss: 1.574365]\n",
      "[D loss: 0.678759] [G loss: 1.447650]\n",
      "[D loss: 0.698602] [G loss: 1.526215]\n",
      "[D loss: 0.906243] [G loss: 1.481004]\n",
      "[D loss: 0.675283] [G loss: 1.444306]\n",
      "[D loss: 0.620327] [G loss: 1.574229]\n",
      "[D loss: 0.694503] [G loss: 1.363693]\n",
      "[D loss: 0.671854] [G loss: 1.535370]\n",
      "[D loss: 0.883386] [G loss: 1.639520]\n",
      "[D loss: 0.801904] [G loss: 1.491600]\n",
      "[D loss: 0.743511] [G loss: 1.638388]\n",
      "[D loss: 0.751763] [G loss: 1.533021]\n",
      "[D loss: 0.559331] [G loss: 1.459349]\n",
      "[D loss: 0.712482] [G loss: 1.613509]\n",
      "[D loss: 0.957229] [G loss: 1.569383]\n",
      "[D loss: 0.681255] [G loss: 1.685971]\n",
      "[D loss: 0.605463] [G loss: 1.649683]\n",
      "[D loss: 0.925233] [G loss: 1.393956]\n",
      "[D loss: 0.811756] [G loss: 1.507477]\n",
      "[D loss: 0.934493] [G loss: 1.870562]\n",
      "[D loss: 0.696349] [G loss: 1.736378]\n",
      "[D loss: 1.371495] [G loss: 1.187432]\n",
      "[D loss: 0.933089] [G loss: 1.503854]\n",
      "[D loss: 0.776357] [G loss: 1.534812]\n",
      "[D loss: 0.940122] [G loss: 1.209776]\n",
      "[D loss: 0.720892] [G loss: 1.618688]\n",
      "[D loss: 0.770667] [G loss: 1.846835]\n",
      "[D loss: 0.838116] [G loss: 1.747919]\n",
      "[D loss: 0.900933] [G loss: 1.330318]\n",
      "[D loss: 0.942022] [G loss: 1.337487]\n",
      "[D loss: 0.832513] [G loss: 1.562875]\n",
      "[D loss: 0.795173] [G loss: 1.608632]\n",
      "[D loss: 0.842430] [G loss: 1.434952]\n",
      "[D loss: 0.813012] [G loss: 1.518106]\n",
      "[D loss: 0.687182] [G loss: 1.673560]\n",
      "[D loss: 0.750834] [G loss: 1.605349]\n",
      "[D loss: 0.971155] [G loss: 1.467524]\n",
      "[D loss: 0.824376] [G loss: 1.529835]\n",
      "[D loss: 1.055521] [G loss: 1.349633]\n",
      "[D loss: 0.939786] [G loss: 1.469651]\n",
      "[D loss: 0.749059] [G loss: 1.321120]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.680995] [G loss: 1.554894]\n",
      "[D loss: 0.680728] [G loss: 1.315549]\n",
      "[D loss: 0.598992] [G loss: 1.653508]\n",
      "[D loss: 0.816236] [G loss: 1.416122]\n",
      "[D loss: 0.765627] [G loss: 1.502617]\n",
      "[D loss: 0.778645] [G loss: 1.519089]\n",
      "[D loss: 0.934987] [G loss: 1.491093]\n",
      "[D loss: 0.708872] [G loss: 1.612385]\n",
      "[D loss: 0.946689] [G loss: 1.542750]\n",
      "[D loss: 0.826833] [G loss: 1.317836]\n",
      "[D loss: 0.558750] [G loss: 1.520031]\n",
      "[D loss: 0.702932] [G loss: 1.547347]\n",
      "[D loss: 1.028335] [G loss: 1.436532]\n",
      "[D loss: 0.897634] [G loss: 1.528047]\n",
      "[D loss: 0.742102] [G loss: 1.605613]\n",
      "[D loss: 0.675534] [G loss: 1.379976]\n",
      "[D loss: 0.845583] [G loss: 1.319263]\n",
      "[D loss: 0.737148] [G loss: 1.596406]\n",
      "[D loss: 0.841161] [G loss: 1.801426]\n",
      "[D loss: 0.745829] [G loss: 1.485194]\n",
      "[D loss: 0.649037] [G loss: 1.719748]\n",
      "[D loss: 1.014130] [G loss: 1.630769]\n",
      "[D loss: 0.916952] [G loss: 1.504268]\n",
      "[D loss: 0.937409] [G loss: 1.283651]\n",
      "[D loss: 0.845841] [G loss: 1.265751]\n",
      "[D loss: 0.696654] [G loss: 1.364542]\n",
      "[D loss: 0.709633] [G loss: 1.512942]\n",
      "[D loss: 0.724611] [G loss: 1.265819]\n",
      "[D loss: 1.015674] [G loss: 1.317368]\n",
      "[D loss: 0.669955] [G loss: 1.429778]\n",
      "[D loss: 0.920682] [G loss: 1.619278]\n",
      "[D loss: 0.609455] [G loss: 1.561145]\n",
      "[D loss: 0.630425] [G loss: 1.627329]\n",
      "[D loss: 0.693282] [G loss: 1.808559]\n",
      "[D loss: 0.670145] [G loss: 1.636584]\n",
      "[D loss: 0.816155] [G loss: 1.510208]\n",
      "[D loss: 0.733602] [G loss: 1.390946]\n",
      "[D loss: 0.781792] [G loss: 1.574617]\n",
      "[D loss: 1.067199] [G loss: 1.747461]\n",
      "[D loss: 1.045176] [G loss: 1.482049]\n",
      "[D loss: 0.848588] [G loss: 1.350735]\n",
      "[D loss: 0.904227] [G loss: 1.433975]\n",
      "[D loss: 1.158871] [G loss: 1.561418]\n",
      "[D loss: 0.754474] [G loss: 1.623344]\n",
      "[D loss: 0.992265] [G loss: 1.708276]\n",
      "[D loss: 0.783899] [G loss: 1.470304]\n",
      "[D loss: 0.750959] [G loss: 1.418347]\n",
      "[D loss: 0.911310] [G loss: 1.401383]\n",
      "[D loss: 0.705751] [G loss: 1.656421]\n",
      "[D loss: 0.967712] [G loss: 0.949960]\n",
      "[D loss: 0.928392] [G loss: 1.469401]\n",
      "[D loss: 0.745510] [G loss: 1.412794]\n",
      "[D loss: 0.945224] [G loss: 1.465668]\n",
      "[D loss: 0.884277] [G loss: 1.511253]\n",
      "[D loss: 0.639997] [G loss: 1.510358]\n",
      "[D loss: 0.644699] [G loss: 1.403394]\n",
      "[D loss: 0.966962] [G loss: 1.418269]\n",
      "[D loss: 0.589514] [G loss: 1.608130]\n",
      "[D loss: 0.926441] [G loss: 1.459806]\n",
      "[D loss: 0.712394] [G loss: 1.714640]\n",
      "[D loss: 0.731913] [G loss: 1.680956]\n",
      "[D loss: 0.859542] [G loss: 1.542433]\n",
      "[D loss: 0.747080] [G loss: 1.717317]\n",
      "[D loss: 0.740197] [G loss: 1.543890]\n",
      "[D loss: 0.779467] [G loss: 1.502260]\n",
      "[D loss: 0.849455] [G loss: 1.530851]\n",
      "[D loss: 0.921887] [G loss: 1.320894]\n",
      "[D loss: 0.889619] [G loss: 1.985896]\n",
      "[D loss: 0.728577] [G loss: 1.834321]\n",
      "[D loss: 0.849731] [G loss: 1.539608]\n",
      "[D loss: 0.675386] [G loss: 1.541627]\n",
      "[D loss: 0.832990] [G loss: 1.577302]\n",
      "[D loss: 0.858392] [G loss: 1.530718]\n",
      "[D loss: 0.713693] [G loss: 1.555212]\n",
      "[D loss: 0.802390] [G loss: 1.954840]\n",
      "[D loss: 0.720797] [G loss: 1.670486]\n",
      "[D loss: 0.839446] [G loss: 1.344299]\n",
      "[D loss: 1.151432] [G loss: 1.787857]\n",
      "[D loss: 0.883649] [G loss: 1.592233]\n",
      "[D loss: 0.857487] [G loss: 1.303794]\n",
      "[D loss: 0.677235] [G loss: 1.517247]\n",
      "[D loss: 0.679088] [G loss: 1.407572]\n",
      "[D loss: 0.532140] [G loss: 1.473986]\n",
      "[D loss: 0.715284] [G loss: 1.628412]\n",
      "[D loss: 1.123708] [G loss: 1.498152]\n",
      "[D loss: 0.780026] [G loss: 1.622164]\n",
      "[D loss: 0.808915] [G loss: 1.666879]\n",
      "[D loss: 0.817219] [G loss: 1.542176]\n",
      "[D loss: 0.807162] [G loss: 1.742936]\n",
      "[D loss: 0.697055] [G loss: 1.738409]\n",
      "[D loss: 0.953950] [G loss: 1.842995]\n",
      "[D loss: 0.915338] [G loss: 1.467486]\n",
      "[D loss: 0.815286] [G loss: 1.365864]\n",
      "[D loss: 0.797856] [G loss: 1.333516]\n",
      "[D loss: 0.732678] [G loss: 1.463437]\n",
      "[D loss: 0.722769] [G loss: 1.737956]\n",
      "[D loss: 0.781240] [G loss: 1.689830]\n",
      "[D loss: 1.039093] [G loss: 1.623233]\n",
      "[D loss: 0.672526] [G loss: 1.575217]\n",
      "[D loss: 0.979127] [G loss: 1.575318]\n",
      "[D loss: 0.525794] [G loss: 1.785774]\n",
      "[D loss: 0.705288] [G loss: 1.613580]\n",
      "[D loss: 0.663229] [G loss: 1.525265]\n",
      "[D loss: 0.870023] [G loss: 1.524156]\n",
      "[D loss: 0.862874] [G loss: 1.843028]\n",
      "[D loss: 0.743983] [G loss: 1.616248]\n",
      "[D loss: 0.944278] [G loss: 1.617616]\n",
      "[D loss: 0.967614] [G loss: 1.480785]\n",
      "[D loss: 1.033529] [G loss: 1.391567]\n",
      "[D loss: 0.880467] [G loss: 1.547745]\n",
      "[D loss: 0.883308] [G loss: 1.378685]\n",
      "[D loss: 0.992275] [G loss: 1.234985]\n",
      "[D loss: 0.745205] [G loss: 1.447318]\n",
      "[D loss: 0.621761] [G loss: 1.648521]\n",
      "[D loss: 1.013173] [G loss: 1.616131]\n",
      "[D loss: 1.067470] [G loss: 1.277376]\n",
      "[D loss: 0.633732] [G loss: 1.401619]\n",
      "[D loss: 1.077898] [G loss: 1.355605]\n",
      "[D loss: 0.719114] [G loss: 1.322947]\n",
      "[D loss: 0.856639] [G loss: 1.372749]\n",
      "[D loss: 0.992753] [G loss: 1.414349]\n",
      "[D loss: 0.621685] [G loss: 1.541792]\n",
      "[D loss: 0.803450] [G loss: 1.316158]\n",
      "[D loss: 0.820830] [G loss: 1.330980]\n",
      "[D loss: 0.913014] [G loss: 1.327115]\n",
      "[D loss: 0.642152] [G loss: 1.445850]\n",
      "[D loss: 0.894921] [G loss: 1.417369]\n",
      "[D loss: 0.802906] [G loss: 1.558170]\n",
      "[D loss: 0.986137] [G loss: 1.345086]\n",
      "[D loss: 1.053868] [G loss: 1.530885]\n",
      "[D loss: 0.762165] [G loss: 1.540258]\n",
      "[D loss: 0.870294] [G loss: 1.565750]\n",
      "[D loss: 0.737416] [G loss: 1.446904]\n",
      "[D loss: 0.954678] [G loss: 1.398163]\n",
      "[D loss: 0.665256] [G loss: 1.469543]\n",
      "[D loss: 0.884544] [G loss: 1.320437]\n",
      "[D loss: 0.535524] [G loss: 1.512404]\n",
      "[D loss: 0.826403] [G loss: 1.544207]\n",
      "[D loss: 0.794857] [G loss: 1.539678]\n",
      "[D loss: 0.745518] [G loss: 1.587019]\n",
      "[D loss: 0.799309] [G loss: 1.477993]\n",
      "[D loss: 0.928672] [G loss: 1.479756]\n",
      "[D loss: 0.776558] [G loss: 1.514089]\n",
      "[D loss: 0.874099] [G loss: 1.645132]\n",
      "[D loss: 1.068445] [G loss: 1.520057]\n",
      "[D loss: 0.895509] [G loss: 1.458439]\n",
      "[D loss: 0.819183] [G loss: 1.366364]\n",
      "[D loss: 0.810059] [G loss: 1.378014]\n",
      "[D loss: 0.790976] [G loss: 1.334437]\n",
      "[D loss: 0.809839] [G loss: 1.551620]\n",
      "[D loss: 0.681643] [G loss: 1.391915]\n",
      "[D loss: 0.564401] [G loss: 1.585634]\n",
      "[D loss: 0.739673] [G loss: 1.504933]\n",
      "[D loss: 0.725763] [G loss: 1.500044]\n",
      "[D loss: 0.759714] [G loss: 1.935550]\n",
      "[D loss: 0.842948] [G loss: 1.739792]\n",
      "[D loss: 1.018695] [G loss: 1.366325]\n",
      "[D loss: 0.835100] [G loss: 1.674281]\n",
      "[D loss: 0.848013] [G loss: 1.358963]\n",
      "[D loss: 0.672065] [G loss: 1.373920]\n",
      "[D loss: 0.967191] [G loss: 1.367928]\n",
      "[D loss: 0.894180] [G loss: 1.535807]\n",
      "[D loss: 0.726850] [G loss: 1.442412]\n",
      "[D loss: 1.028557] [G loss: 1.703950]\n",
      "[D loss: 0.939692] [G loss: 1.293036]\n",
      "[D loss: 1.013427] [G loss: 1.291188]\n",
      "[D loss: 0.993235] [G loss: 1.421487]\n",
      "[D loss: 1.022295] [G loss: 1.194289]\n",
      "[D loss: 0.851660] [G loss: 1.323782]\n",
      "[D loss: 0.728426] [G loss: 1.413719]\n",
      "[D loss: 0.955903] [G loss: 1.411919]\n",
      "[D loss: 0.720424] [G loss: 1.555024]\n",
      "[D loss: 0.922027] [G loss: 1.500943]\n",
      "[D loss: 0.820110] [G loss: 1.363305]\n",
      "[D loss: 0.950814] [G loss: 1.337398]\n",
      "[D loss: 0.933570] [G loss: 1.299082]\n",
      "[D loss: 0.746780] [G loss: 1.520690]\n",
      "[D loss: 0.630656] [G loss: 1.591423]\n",
      "[D loss: 0.708912] [G loss: 1.501433]\n",
      "[D loss: 0.686947] [G loss: 1.799747]\n",
      "[D loss: 0.854055] [G loss: 1.681372]\n",
      "[D loss: 0.665705] [G loss: 1.505103]\n",
      "[D loss: 1.019747] [G loss: 1.336228]\n",
      "[D loss: 1.001590] [G loss: 1.502807]\n",
      "[D loss: 0.830451] [G loss: 1.533967]\n",
      "[D loss: 0.880904] [G loss: 1.351980]\n",
      "[D loss: 0.964243] [G loss: 1.149934]\n",
      "[D loss: 0.706877] [G loss: 1.363493]\n",
      "[D loss: 0.799094] [G loss: 1.490289]\n",
      "[D loss: 0.727915] [G loss: 1.237207]\n",
      "[D loss: 0.916102] [G loss: 1.379914]\n",
      "[D loss: 0.998192] [G loss: 1.375998]\n",
      "[D loss: 0.716438] [G loss: 1.605317]\n",
      "[D loss: 0.613572] [G loss: 1.474901]\n",
      "[D loss: 0.731129] [G loss: 1.448927]\n",
      "[D loss: 0.747275] [G loss: 1.681900]\n",
      "[D loss: 0.764551] [G loss: 1.732965]\n",
      "[D loss: 0.978475] [G loss: 1.474903]\n",
      "[D loss: 0.857736] [G loss: 1.419071]\n",
      "[D loss: 0.961223] [G loss: 1.688102]\n",
      "[D loss: 0.631914] [G loss: 1.685829]\n",
      "[D loss: 0.988457] [G loss: 1.512960]\n",
      "[D loss: 0.996150] [G loss: 1.333067]\n",
      "[D loss: 0.772575] [G loss: 1.456922]\n",
      "[D loss: 0.936922] [G loss: 1.472780]\n",
      "[D loss: 0.743828] [G loss: 1.680167]\n",
      "[D loss: 0.647178] [G loss: 1.430096]\n",
      "[D loss: 0.720033] [G loss: 1.524145]\n",
      "[D loss: 0.768991] [G loss: 1.555848]\n",
      "[D loss: 0.815377] [G loss: 1.444797]\n",
      "[D loss: 0.689054] [G loss: 1.609908]\n",
      "[D loss: 0.838514] [G loss: 1.629820]\n",
      "[D loss: 0.778061] [G loss: 1.547892]\n",
      "[D loss: 0.963096] [G loss: 1.400080]\n",
      "[D loss: 1.188640] [G loss: 1.469889]\n",
      "[D loss: 0.535824] [G loss: 1.582046]\n",
      "[D loss: 0.778286] [G loss: 1.708365]\n",
      "[D loss: 0.978005] [G loss: 1.557008]\n",
      "[D loss: 1.085167] [G loss: 1.319123]\n",
      "[D loss: 0.715649] [G loss: 1.506099]\n",
      "[D loss: 0.653682] [G loss: 1.492412]\n",
      "[D loss: 0.853521] [G loss: 1.220874]\n",
      "[D loss: 0.884719] [G loss: 1.706615]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.682933] [G loss: 1.666609]\n",
      "[D loss: 0.934377] [G loss: 1.766780]\n",
      "[D loss: 0.628665] [G loss: 1.479142]\n",
      "[D loss: 0.704064] [G loss: 1.481281]\n",
      "[D loss: 1.128675] [G loss: 1.418265]\n",
      "[D loss: 0.762821] [G loss: 1.488501]\n",
      "[D loss: 0.802697] [G loss: 1.487847]\n",
      "[D loss: 0.781657] [G loss: 1.319458]\n",
      "[D loss: 0.798147] [G loss: 1.533497]\n",
      "[D loss: 0.575954] [G loss: 1.402179]\n",
      "[D loss: 0.939809] [G loss: 1.086404]\n",
      "[D loss: 1.024585] [G loss: 1.455733]\n",
      "[D loss: 0.760823] [G loss: 1.628624]\n",
      "[D loss: 0.604429] [G loss: 1.357516]\n",
      "[D loss: 0.881065] [G loss: 1.380809]\n",
      "[D loss: 0.600165] [G loss: 1.609743]\n",
      "[D loss: 0.738190] [G loss: 1.642361]\n",
      "[D loss: 0.692941] [G loss: 1.723802]\n",
      "[D loss: 0.723288] [G loss: 1.538946]\n",
      "[D loss: 1.026061] [G loss: 1.637897]\n",
      "[D loss: 0.968345] [G loss: 1.570900]\n",
      "[D loss: 0.728342] [G loss: 1.432718]\n",
      "[D loss: 0.933623] [G loss: 1.274546]\n",
      "[D loss: 0.993399] [G loss: 1.128604]\n",
      "[D loss: 0.797409] [G loss: 1.663434]\n",
      "[D loss: 0.629393] [G loss: 1.760778]\n",
      "[D loss: 0.876546] [G loss: 1.308191]\n",
      "[D loss: 0.911266] [G loss: 1.459256]\n",
      "[D loss: 0.629281] [G loss: 1.586273]\n",
      "[D loss: 0.792469] [G loss: 1.566269]\n",
      "[D loss: 0.730642] [G loss: 1.528151]\n",
      "[D loss: 0.762087] [G loss: 1.462938]\n",
      "[D loss: 0.863283] [G loss: 1.527380]\n",
      "[D loss: 0.620887] [G loss: 1.557162]\n",
      "[D loss: 0.675083] [G loss: 1.350409]\n",
      "[D loss: 0.697500] [G loss: 1.448400]\n",
      "[D loss: 0.758880] [G loss: 1.622516]\n",
      "[D loss: 1.042732] [G loss: 1.690091]\n",
      "[D loss: 0.833932] [G loss: 1.548433]\n",
      "[D loss: 0.862814] [G loss: 1.548013]\n",
      "[D loss: 0.764650] [G loss: 1.304235]\n",
      "[D loss: 0.762300] [G loss: 1.538484]\n",
      "[D loss: 0.859059] [G loss: 1.359638]\n",
      "[D loss: 0.714462] [G loss: 1.437408]\n",
      "[D loss: 0.642199] [G loss: 1.400577]\n",
      "[D loss: 0.574425] [G loss: 1.644544]\n",
      "[D loss: 1.079409] [G loss: 1.408877]\n",
      "[D loss: 0.942018] [G loss: 1.451905]\n",
      "[D loss: 0.821613] [G loss: 1.839323]\n",
      "[D loss: 0.784987] [G loss: 1.514438]\n",
      "[D loss: 0.922960] [G loss: 1.177151]\n",
      "[D loss: 0.746501] [G loss: 1.474733]\n",
      "[D loss: 0.753117] [G loss: 1.548008]\n",
      "[D loss: 0.862395] [G loss: 1.560111]\n",
      "[D loss: 0.796097] [G loss: 1.491217]\n",
      "[D loss: 0.636077] [G loss: 1.486410]\n",
      "[D loss: 0.845367] [G loss: 1.352705]\n",
      "[D loss: 0.937102] [G loss: 1.932313]\n",
      "[D loss: 0.738295] [G loss: 1.928293]\n",
      "[D loss: 0.851649] [G loss: 1.584192]\n",
      "[D loss: 0.711258] [G loss: 1.625158]\n",
      "[D loss: 0.917896] [G loss: 1.475789]\n",
      "[D loss: 0.813845] [G loss: 1.458884]\n",
      "[D loss: 0.738236] [G loss: 1.610408]\n",
      "[D loss: 0.671409] [G loss: 1.401372]\n",
      "[D loss: 0.821709] [G loss: 1.724569]\n",
      "[D loss: 0.800855] [G loss: 1.529150]\n",
      "[D loss: 0.946078] [G loss: 1.685515]\n",
      "[D loss: 0.704552] [G loss: 1.590402]\n",
      "[D loss: 0.688750] [G loss: 1.365535]\n",
      "[D loss: 0.857260] [G loss: 1.255179]\n",
      "[D loss: 0.646410] [G loss: 1.731958]\n",
      "[D loss: 0.687372] [G loss: 1.593837]\n",
      "[D loss: 0.647045] [G loss: 1.956511]\n",
      "[D loss: 0.721301] [G loss: 1.710765]\n",
      "[D loss: 0.816811] [G loss: 1.590728]\n",
      "[D loss: 0.828057] [G loss: 1.635233]\n",
      "[D loss: 0.687499] [G loss: 1.536930]\n",
      "[D loss: 0.645490] [G loss: 1.530154]\n",
      "[D loss: 0.863435] [G loss: 1.679614]\n",
      "[D loss: 0.834450] [G loss: 1.470486]\n",
      "[D loss: 0.690989] [G loss: 1.464833]\n",
      "[D loss: 0.878058] [G loss: 1.357573]\n",
      "[D loss: 0.843914] [G loss: 1.352639]\n",
      "[D loss: 0.835367] [G loss: 1.533251]\n",
      "[D loss: 0.782908] [G loss: 1.558253]\n",
      "[D loss: 0.809272] [G loss: 1.501049]\n",
      "[D loss: 0.855441] [G loss: 1.145485]\n",
      "[D loss: 0.786533] [G loss: 1.612067]\n",
      "[D loss: 1.115491] [G loss: 1.843874]\n",
      "[D loss: 0.793093] [G loss: 1.928300]\n",
      "[D loss: 0.799528] [G loss: 1.582584]\n",
      "[D loss: 0.950058] [G loss: 1.375583]\n",
      "[D loss: 1.089876] [G loss: 1.138900]\n",
      "[D loss: 0.677074] [G loss: 1.531359]\n",
      "[D loss: 0.860117] [G loss: 1.581943]\n",
      "[D loss: 0.783898] [G loss: 1.704528]\n",
      "[D loss: 0.791895] [G loss: 1.370832]\n",
      "[D loss: 0.516692] [G loss: 1.596438]\n",
      "[D loss: 0.842862] [G loss: 1.497385]\n",
      "[D loss: 0.917839] [G loss: 1.361530]\n",
      "[D loss: 0.931992] [G loss: 1.476746]\n",
      "[D loss: 0.930060] [G loss: 1.388565]\n",
      "[D loss: 0.669567] [G loss: 1.405185]\n",
      "[D loss: 0.780696] [G loss: 1.390729]\n",
      "[D loss: 0.673031] [G loss: 1.715491]\n",
      "[D loss: 0.776642] [G loss: 1.781327]\n",
      "[D loss: 0.732593] [G loss: 1.515893]\n",
      "[D loss: 0.830972] [G loss: 1.439566]\n",
      "[D loss: 0.731447] [G loss: 1.439738]\n",
      "[D loss: 0.695203] [G loss: 1.526042]\n",
      "[D loss: 0.947550] [G loss: 1.503863]\n",
      "[D loss: 0.618224] [G loss: 1.661376]\n",
      "[D loss: 0.699003] [G loss: 1.717987]\n",
      "[D loss: 0.599712] [G loss: 1.665372]\n",
      "[D loss: 0.779057] [G loss: 1.390885]\n",
      "[D loss: 0.840020] [G loss: 1.531317]\n",
      "[D loss: 0.711052] [G loss: 1.575579]\n",
      "[D loss: 0.858821] [G loss: 1.491535]\n",
      "[D loss: 0.762123] [G loss: 1.529286]\n",
      "[D loss: 0.772686] [G loss: 1.695695]\n",
      "[D loss: 0.770691] [G loss: 1.926447]\n",
      "[D loss: 0.871830] [G loss: 1.751749]\n",
      "[D loss: 0.793738] [G loss: 1.483351]\n",
      "[D loss: 0.916303] [G loss: 1.357143]\n",
      "[D loss: 0.844845] [G loss: 1.376260]\n",
      "[D loss: 0.874192] [G loss: 1.477630]\n",
      "[D loss: 0.914671] [G loss: 1.369560]\n",
      "[D loss: 0.773478] [G loss: 1.470288]\n",
      "[D loss: 0.911265] [G loss: 1.459806]\n",
      "[D loss: 0.673613] [G loss: 1.659929]\n",
      "[D loss: 0.649590] [G loss: 1.565462]\n",
      "[D loss: 1.069466] [G loss: 1.538552]\n",
      "[D loss: 0.700145] [G loss: 1.530246]\n",
      "[D loss: 0.804515] [G loss: 1.508585]\n",
      "[D loss: 1.034487] [G loss: 1.315437]\n",
      "[D loss: 0.776142] [G loss: 1.660509]\n",
      "[D loss: 0.924544] [G loss: 1.483536]\n",
      "[D loss: 0.772822] [G loss: 1.561337]\n",
      "[D loss: 0.795294] [G loss: 1.516819]\n",
      "[D loss: 0.653477] [G loss: 1.556851]\n",
      "[D loss: 0.589084] [G loss: 1.497679]\n",
      "[D loss: 0.866406] [G loss: 1.323041]\n",
      "[D loss: 0.723239] [G loss: 1.606578]\n",
      "[D loss: 0.707742] [G loss: 1.479425]\n",
      "[D loss: 0.905289] [G loss: 1.541745]\n",
      "[D loss: 0.683879] [G loss: 1.471039]\n",
      "[D loss: 0.954229] [G loss: 1.774123]\n",
      "[D loss: 1.088721] [G loss: 1.671479]\n",
      "[D loss: 0.834118] [G loss: 1.739936]\n",
      "[D loss: 0.560923] [G loss: 1.669608]\n",
      "[D loss: 0.802204] [G loss: 1.483784]\n",
      "[D loss: 0.675303] [G loss: 1.676723]\n",
      "[D loss: 0.911998] [G loss: 1.557785]\n",
      "[D loss: 0.678228] [G loss: 1.592349]\n",
      "[D loss: 0.771311] [G loss: 1.461326]\n",
      "[D loss: 0.501107] [G loss: 1.616389]\n",
      "[D loss: 0.673612] [G loss: 1.560605]\n",
      "[D loss: 0.858524] [G loss: 1.724803]\n",
      "[D loss: 0.554257] [G loss: 1.647401]\n",
      "[D loss: 0.960594] [G loss: 1.451152]\n",
      "[D loss: 0.759018] [G loss: 1.657776]\n",
      "[D loss: 0.878820] [G loss: 1.586715]\n",
      "[D loss: 0.779314] [G loss: 1.350714]\n",
      "[D loss: 0.637050] [G loss: 1.643245]\n",
      "[D loss: 1.105050] [G loss: 1.470553]\n",
      "[D loss: 0.705381] [G loss: 1.435061]\n",
      "[D loss: 0.753755] [G loss: 1.469448]\n",
      "[D loss: 0.725485] [G loss: 1.523249]\n",
      "[D loss: 0.628806] [G loss: 1.826654]\n",
      "[D loss: 0.637022] [G loss: 1.693629]\n",
      "[D loss: 0.870829] [G loss: 1.546753]\n",
      "[D loss: 0.782899] [G loss: 1.689684]\n",
      "[D loss: 0.724750] [G loss: 1.422101]\n",
      "[D loss: 1.180497] [G loss: 1.653226]\n",
      "[D loss: 0.961513] [G loss: 1.685275]\n",
      "[D loss: 0.636974] [G loss: 1.361806]\n",
      "[D loss: 0.856733] [G loss: 1.467644]\n",
      "[D loss: 1.019545] [G loss: 1.550352]\n",
      "[D loss: 0.669365] [G loss: 1.571043]\n",
      "[D loss: 0.759182] [G loss: 1.892044]\n",
      "[D loss: 0.871833] [G loss: 1.442537]\n",
      "[D loss: 0.770191] [G loss: 1.463284]\n",
      "[D loss: 0.742119] [G loss: 1.460820]\n",
      "[D loss: 1.111947] [G loss: 1.473958]\n",
      "[D loss: 0.652604] [G loss: 1.556891]\n",
      "[D loss: 0.627956] [G loss: 1.520942]\n",
      "[D loss: 0.987521] [G loss: 1.686098]\n",
      "[D loss: 0.695232] [G loss: 1.711697]\n",
      "[D loss: 1.094876] [G loss: 1.522408]\n",
      "[D loss: 0.786658] [G loss: 1.354855]\n",
      "[D loss: 0.784000] [G loss: 1.495881]\n",
      "[D loss: 1.037276] [G loss: 1.312700]\n",
      "[D loss: 0.858690] [G loss: 1.474283]\n",
      "[D loss: 0.754282] [G loss: 1.493649]\n",
      "[D loss: 0.768359] [G loss: 1.681877]\n",
      "[D loss: 1.064473] [G loss: 1.237338]\n",
      "[D loss: 0.658619] [G loss: 1.513132]\n",
      "[D loss: 0.906365] [G loss: 1.607786]\n",
      "[D loss: 0.874144] [G loss: 1.531045]\n",
      "[D loss: 0.993159] [G loss: 1.543412]\n",
      "[D loss: 0.797100] [G loss: 1.560608]\n",
      "[D loss: 0.700453] [G loss: 1.313105]\n",
      "[D loss: 0.787810] [G loss: 1.617653]\n",
      "[D loss: 0.860655] [G loss: 1.460321]\n",
      "[D loss: 0.671664] [G loss: 1.406689]\n",
      "[D loss: 0.834074] [G loss: 1.465032]\n",
      "[D loss: 0.914233] [G loss: 1.428203]\n",
      "[D loss: 0.689160] [G loss: 1.511092]\n",
      "[D loss: 1.097698] [G loss: 1.420712]\n",
      "[D loss: 0.972154] [G loss: 1.284125]\n",
      "[D loss: 0.946255] [G loss: 1.603417]\n",
      "[D loss: 0.708407] [G loss: 1.487858]\n",
      "[D loss: 1.018095] [G loss: 1.520659]\n",
      "[D loss: 0.795717] [G loss: 1.404720]\n",
      "[D loss: 1.273794] [G loss: 1.268480]\n",
      "[D loss: 0.717586] [G loss: 1.602777]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.826856] [G loss: 1.518191]\n",
      "[D loss: 0.708960] [G loss: 1.510673]\n",
      "[D loss: 0.745280] [G loss: 1.462709]\n",
      "[D loss: 0.752530] [G loss: 1.495717]\n",
      "[D loss: 1.033420] [G loss: 1.341068]\n",
      "[D loss: 0.820234] [G loss: 1.715373]\n",
      "[D loss: 0.978159] [G loss: 1.428981]\n",
      "[D loss: 0.950718] [G loss: 1.342664]\n",
      "[D loss: 0.750612] [G loss: 1.493329]\n",
      "[D loss: 0.550044] [G loss: 1.538633]\n",
      "[D loss: 0.890172] [G loss: 1.551202]\n",
      "[D loss: 0.935554] [G loss: 1.510622]\n",
      "[D loss: 0.924472] [G loss: 1.728356]\n",
      "[D loss: 0.958513] [G loss: 1.460767]\n",
      "[D loss: 0.916352] [G loss: 1.543451]\n",
      "[D loss: 0.884533] [G loss: 1.541532]\n",
      "[D loss: 0.757628] [G loss: 1.329065]\n",
      "[D loss: 0.854028] [G loss: 1.463452]\n",
      "[D loss: 0.917198] [G loss: 1.657927]\n",
      "[D loss: 0.741622] [G loss: 1.494682]\n",
      "[D loss: 0.851265] [G loss: 1.572758]\n",
      "[D loss: 0.864942] [G loss: 1.332154]\n",
      "[D loss: 0.739089] [G loss: 1.213975]\n",
      "[D loss: 0.742139] [G loss: 1.402555]\n",
      "[D loss: 0.825978] [G loss: 1.431198]\n",
      "[D loss: 0.734162] [G loss: 1.403184]\n",
      "[D loss: 0.726354] [G loss: 1.530579]\n",
      "[D loss: 0.962579] [G loss: 1.578861]\n",
      "[D loss: 0.919118] [G loss: 1.503865]\n",
      "[D loss: 0.741223] [G loss: 1.490550]\n",
      "[D loss: 0.845784] [G loss: 1.453915]\n",
      "[D loss: 0.882819] [G loss: 1.456702]\n",
      "[D loss: 0.914839] [G loss: 1.346627]\n",
      "[D loss: 0.792111] [G loss: 1.666351]\n",
      "[D loss: 0.852163] [G loss: 1.681345]\n",
      "[D loss: 0.788877] [G loss: 1.534055]\n",
      "[D loss: 0.788121] [G loss: 1.481812]\n",
      "[D loss: 0.917576] [G loss: 1.260989]\n",
      "[D loss: 0.777555] [G loss: 1.567586]\n",
      "[D loss: 0.907901] [G loss: 1.572990]\n",
      "[D loss: 0.784402] [G loss: 1.501925]\n",
      "[D loss: 0.814158] [G loss: 1.442483]\n",
      "[D loss: 0.802860] [G loss: 1.330528]\n",
      "[D loss: 0.862113] [G loss: 1.357484]\n",
      "[D loss: 0.951458] [G loss: 1.402129]\n",
      "[D loss: 0.910731] [G loss: 1.405061]\n",
      "[D loss: 0.925626] [G loss: 1.394114]\n",
      "[D loss: 0.823224] [G loss: 1.429322]\n",
      "[D loss: 1.008117] [G loss: 1.539291]\n",
      "[D loss: 0.878258] [G loss: 1.432172]\n",
      "[D loss: 0.701950] [G loss: 1.401422]\n",
      "[D loss: 0.955386] [G loss: 1.387560]\n",
      "[D loss: 0.718478] [G loss: 1.437454]\n",
      "[D loss: 0.678641] [G loss: 1.727805]\n",
      "[D loss: 0.701346] [G loss: 1.672169]\n",
      "[D loss: 0.656787] [G loss: 1.541291]\n",
      "[D loss: 0.727971] [G loss: 1.396430]\n",
      "[D loss: 0.809414] [G loss: 1.357654]\n",
      "[D loss: 0.701971] [G loss: 1.752369]\n",
      "[D loss: 1.005579] [G loss: 1.641482]\n",
      "[D loss: 0.679795] [G loss: 1.532888]\n",
      "[D loss: 0.797415] [G loss: 1.805700]\n",
      "[D loss: 0.721883] [G loss: 1.209201]\n",
      "[D loss: 0.757088] [G loss: 1.446323]\n",
      "[D loss: 0.832340] [G loss: 1.587854]\n",
      "[D loss: 0.843423] [G loss: 1.662252]\n",
      "[D loss: 0.716155] [G loss: 1.707724]\n",
      "[D loss: 0.843620] [G loss: 1.672987]\n",
      "[D loss: 0.851682] [G loss: 1.518479]\n",
      "[D loss: 0.887374] [G loss: 1.273780]\n",
      "[D loss: 1.064979] [G loss: 1.207473]\n",
      "[D loss: 0.797369] [G loss: 1.477278]\n",
      "[D loss: 0.849543] [G loss: 1.621827]\n",
      "[D loss: 0.841205] [G loss: 1.770044]\n",
      "[D loss: 0.733616] [G loss: 1.582732]\n",
      "[D loss: 0.871708] [G loss: 1.357631]\n",
      "[D loss: 0.718938] [G loss: 1.333361]\n",
      "[D loss: 0.909637] [G loss: 1.440777]\n",
      "[D loss: 0.698709] [G loss: 1.796977]\n",
      "[D loss: 0.708458] [G loss: 1.260259]\n",
      "[D loss: 0.776256] [G loss: 1.452083]\n",
      "[D loss: 0.762813] [G loss: 1.515126]\n",
      "[D loss: 0.615363] [G loss: 1.683444]\n",
      "[D loss: 0.735739] [G loss: 1.635809]\n",
      "[D loss: 0.673792] [G loss: 1.661698]\n",
      "[D loss: 0.763055] [G loss: 1.453496]\n",
      "[D loss: 0.693773] [G loss: 1.444750]\n",
      "[D loss: 0.748981] [G loss: 1.694468]\n",
      "[D loss: 0.723950] [G loss: 1.493142]\n",
      "[D loss: 0.929183] [G loss: 1.377029]\n",
      "[D loss: 0.998302] [G loss: 1.558062]\n",
      "[D loss: 0.754142] [G loss: 1.250726]\n",
      "[D loss: 0.765422] [G loss: 1.590998]\n",
      "[D loss: 0.574087] [G loss: 1.593862]\n",
      "[D loss: 0.643887] [G loss: 1.772545]\n",
      "[D loss: 0.926969] [G loss: 1.635144]\n",
      "[D loss: 0.810145] [G loss: 1.638325]\n",
      "[D loss: 1.101519] [G loss: 1.799389]\n",
      "[D loss: 0.784983] [G loss: 1.451443]\n",
      "[D loss: 0.676416] [G loss: 1.584480]\n",
      "[D loss: 0.740327] [G loss: 1.586038]\n",
      "[D loss: 0.971457] [G loss: 1.485531]\n",
      "[D loss: 0.769444] [G loss: 1.833335]\n",
      "[D loss: 0.806072] [G loss: 1.636695]\n",
      "[D loss: 0.850806] [G loss: 1.542962]\n",
      "[D loss: 0.938021] [G loss: 1.419905]\n",
      "[D loss: 0.805025] [G loss: 1.446531]\n",
      "[D loss: 0.659732] [G loss: 1.587776]\n",
      "[D loss: 0.916738] [G loss: 1.441820]\n",
      "[D loss: 0.719986] [G loss: 1.410145]\n",
      "[D loss: 0.692468] [G loss: 1.681936]\n",
      "[D loss: 0.763160] [G loss: 1.559020]\n",
      "[D loss: 0.792170] [G loss: 1.624635]\n",
      "[D loss: 0.957150] [G loss: 1.485732]\n",
      "[D loss: 0.909990] [G loss: 1.479873]\n",
      "[D loss: 0.626619] [G loss: 1.829003]\n",
      "[D loss: 0.892829] [G loss: 1.556320]\n",
      "[D loss: 0.761163] [G loss: 1.594340]\n",
      "[D loss: 0.738581] [G loss: 1.734206]\n",
      "[D loss: 0.948620] [G loss: 1.403805]\n",
      "[D loss: 0.863433] [G loss: 1.741334]\n",
      "[D loss: 0.953324] [G loss: 1.352098]\n",
      "[D loss: 0.787459] [G loss: 1.430259]\n",
      "[D loss: 0.722829] [G loss: 1.799428]\n",
      "[D loss: 0.707005] [G loss: 1.525567]\n",
      "[D loss: 0.703160] [G loss: 1.492483]\n",
      "[D loss: 1.110743] [G loss: 1.364742]\n",
      "[D loss: 1.001965] [G loss: 1.330009]\n",
      "[D loss: 0.992665] [G loss: 1.468144]\n",
      "[D loss: 0.562339] [G loss: 1.395914]\n",
      "[D loss: 0.840424] [G loss: 1.515412]\n",
      "[D loss: 0.712452] [G loss: 1.528740]\n",
      "[D loss: 0.678338] [G loss: 1.606147]\n",
      "[D loss: 0.599252] [G loss: 1.715291]\n",
      "[D loss: 1.094310] [G loss: 1.625246]\n",
      "[D loss: 0.828785] [G loss: 1.569328]\n",
      "[D loss: 0.969858] [G loss: 1.487043]\n",
      "[D loss: 0.561312] [G loss: 1.506920]\n",
      "[D loss: 0.945613] [G loss: 1.380715]\n",
      "[D loss: 0.693004] [G loss: 1.372675]\n",
      "[D loss: 0.939426] [G loss: 1.209033]\n",
      "[D loss: 0.855331] [G loss: 1.521644]\n",
      "[D loss: 0.839178] [G loss: 1.311600]\n",
      "[D loss: 0.889881] [G loss: 1.447737]\n",
      "[D loss: 0.679583] [G loss: 1.413240]\n",
      "[D loss: 0.855920] [G loss: 1.319460]\n",
      "[D loss: 0.870722] [G loss: 1.537067]\n",
      "[D loss: 0.809071] [G loss: 1.488975]\n",
      "[D loss: 0.764015] [G loss: 1.770926]\n",
      "[D loss: 0.932129] [G loss: 1.614705]\n",
      "[D loss: 0.826989] [G loss: 1.401639]\n",
      "[D loss: 0.721158] [G loss: 1.510064]\n",
      "[D loss: 1.125694] [G loss: 1.342989]\n",
      "[D loss: 1.095739] [G loss: 1.613641]\n",
      "[D loss: 0.704227] [G loss: 1.411911]\n",
      "[D loss: 0.895626] [G loss: 1.564287]\n",
      "[D loss: 0.724410] [G loss: 1.376402]\n",
      "[D loss: 0.681975] [G loss: 1.418107]\n",
      "[D loss: 0.861575] [G loss: 1.508457]\n",
      "[D loss: 0.807518] [G loss: 1.731158]\n",
      "[D loss: 0.989004] [G loss: 1.311280]\n",
      "[D loss: 1.107808] [G loss: 1.411736]\n",
      "[D loss: 0.858380] [G loss: 1.521635]\n",
      "[D loss: 0.754250] [G loss: 1.349719]\n",
      "[D loss: 0.947912] [G loss: 1.459550]\n",
      "[D loss: 0.640463] [G loss: 1.581848]\n",
      "[D loss: 0.801877] [G loss: 1.418025]\n",
      "[D loss: 0.936380] [G loss: 1.356308]\n",
      "[D loss: 0.806578] [G loss: 1.377819]\n",
      "[D loss: 0.977028] [G loss: 1.420993]\n",
      "[D loss: 0.683536] [G loss: 1.591248]\n",
      "[D loss: 0.830687] [G loss: 1.501093]\n",
      "[D loss: 0.885883] [G loss: 1.417803]\n",
      "[D loss: 0.875835] [G loss: 1.394465]\n",
      "[D loss: 0.705610] [G loss: 1.466464]\n",
      "[D loss: 0.903187] [G loss: 1.411080]\n",
      "[D loss: 0.963625] [G loss: 1.452420]\n",
      "[D loss: 0.878543] [G loss: 1.365228]\n",
      "[D loss: 0.815882] [G loss: 1.551402]\n",
      "[D loss: 0.826945] [G loss: 1.505859]\n",
      "[D loss: 0.936598] [G loss: 1.626762]\n",
      "[D loss: 0.735118] [G loss: 1.524767]\n",
      "[D loss: 0.778762] [G loss: 1.412148]\n",
      "[D loss: 0.855255] [G loss: 1.463575]\n",
      "[D loss: 0.872139] [G loss: 1.645025]\n",
      "[D loss: 0.735075] [G loss: 1.603399]\n",
      "[D loss: 0.981527] [G loss: 1.587705]\n",
      "[D loss: 0.881991] [G loss: 1.455892]\n",
      "[D loss: 1.081682] [G loss: 1.119665]\n",
      "[D loss: 0.777662] [G loss: 1.636751]\n",
      "[D loss: 0.802664] [G loss: 1.581313]\n",
      "[D loss: 0.752940] [G loss: 1.672762]\n",
      "[D loss: 0.808315] [G loss: 1.488002]\n",
      "[D loss: 0.653528] [G loss: 1.472464]\n",
      "[D loss: 0.897540] [G loss: 1.275693]\n",
      "[D loss: 0.682912] [G loss: 1.661629]\n",
      "[D loss: 0.714051] [G loss: 1.614694]\n",
      "[D loss: 0.646275] [G loss: 1.668302]\n",
      "[D loss: 0.762749] [G loss: 1.610271]\n",
      "[D loss: 0.876926] [G loss: 1.408028]\n",
      "[D loss: 0.893168] [G loss: 1.644098]\n",
      "[D loss: 0.722222] [G loss: 1.427893]\n",
      "[D loss: 0.750587] [G loss: 1.714200]\n",
      "[D loss: 0.912281] [G loss: 1.461680]\n",
      "[D loss: 0.680650] [G loss: 1.366105]\n",
      "[D loss: 0.787594] [G loss: 1.504586]\n",
      "[D loss: 0.767481] [G loss: 1.689965]\n",
      "[D loss: 1.076539] [G loss: 1.527080]\n",
      "[D loss: 0.871227] [G loss: 1.466191]\n",
      "[D loss: 0.954518] [G loss: 1.353260]\n",
      "[D loss: 0.932559] [G loss: 1.479872]\n",
      "[D loss: 0.945512] [G loss: 1.345993]\n",
      "[D loss: 1.111130] [G loss: 1.442481]\n",
      "[D loss: 0.772608] [G loss: 1.281704]\n",
      "[D loss: 0.868820] [G loss: 1.439802]\n",
      "[D loss: 0.946115] [G loss: 1.275896]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.551260] [G loss: 1.342454]\n",
      "[D loss: 1.019184] [G loss: 1.114680]\n",
      "[D loss: 0.827291] [G loss: 1.686868]\n",
      "[D loss: 0.837458] [G loss: 1.556713]\n",
      "[D loss: 0.758918] [G loss: 1.533409]\n",
      "[D loss: 0.657844] [G loss: 1.475925]\n",
      "[D loss: 0.871646] [G loss: 1.586559]\n",
      "[D loss: 0.719621] [G loss: 1.517643]\n",
      "[D loss: 1.063021] [G loss: 1.189537]\n",
      "[D loss: 0.768568] [G loss: 1.452933]\n",
      "[D loss: 1.101648] [G loss: 1.182590]\n",
      "[D loss: 0.817113] [G loss: 1.620292]\n",
      "[D loss: 0.880184] [G loss: 1.726137]\n",
      "[D loss: 0.833216] [G loss: 1.356238]\n",
      "[D loss: 0.778792] [G loss: 1.530519]\n",
      "[D loss: 1.014352] [G loss: 1.577121]\n",
      "[D loss: 0.772676] [G loss: 1.245285]\n",
      "[D loss: 1.096625] [G loss: 1.509762]\n",
      "[D loss: 0.779895] [G loss: 1.561275]\n",
      "[D loss: 1.026224] [G loss: 1.407155]\n",
      "[D loss: 0.747669] [G loss: 1.455967]\n",
      "[D loss: 0.839975] [G loss: 1.404501]\n",
      "[D loss: 0.811836] [G loss: 1.526736]\n",
      "[D loss: 0.800051] [G loss: 1.576503]\n",
      "[D loss: 0.723333] [G loss: 1.353215]\n",
      "[D loss: 0.676472] [G loss: 1.483550]\n",
      "[D loss: 0.833805] [G loss: 1.399889]\n",
      "[D loss: 0.777515] [G loss: 1.568405]\n",
      "[D loss: 0.804142] [G loss: 1.401500]\n",
      "[D loss: 0.808516] [G loss: 1.263594]\n",
      "[D loss: 1.027734] [G loss: 1.497847]\n",
      "[D loss: 0.942116] [G loss: 1.510531]\n",
      "[D loss: 0.770700] [G loss: 1.428266]\n",
      "[D loss: 0.703809] [G loss: 1.512195]\n",
      "[D loss: 0.722753] [G loss: 1.497068]\n",
      "[D loss: 0.925750] [G loss: 1.294935]\n",
      "[D loss: 0.965159] [G loss: 1.312193]\n",
      "[D loss: 0.893477] [G loss: 1.314058]\n",
      "[D loss: 0.985133] [G loss: 1.531280]\n",
      "[D loss: 0.652857] [G loss: 1.397217]\n",
      "[D loss: 0.700673] [G loss: 1.317647]\n",
      "[D loss: 0.983974] [G loss: 1.556193]\n",
      "[D loss: 1.093472] [G loss: 1.317966]\n",
      "[D loss: 0.595349] [G loss: 1.487356]\n",
      "[D loss: 0.807190] [G loss: 1.432248]\n",
      "[D loss: 0.946762] [G loss: 1.487108]\n",
      "[D loss: 0.922746] [G loss: 1.578548]\n",
      "[D loss: 0.976781] [G loss: 1.343470]\n",
      "[D loss: 0.772098] [G loss: 1.585652]\n",
      "[D loss: 0.812721] [G loss: 1.472020]\n",
      "[D loss: 1.030435] [G loss: 1.512730]\n",
      "[D loss: 0.705691] [G loss: 1.465381]\n",
      "[D loss: 0.826088] [G loss: 1.570140]\n",
      "[D loss: 0.816786] [G loss: 1.464091]\n",
      "[D loss: 1.003897] [G loss: 1.302609]\n",
      "[D loss: 0.907693] [G loss: 1.711524]\n",
      "[D loss: 0.798410] [G loss: 1.509869]\n",
      "[D loss: 0.726875] [G loss: 1.394041]\n",
      "[D loss: 0.801117] [G loss: 1.490006]\n",
      "[D loss: 0.901744] [G loss: 1.610466]\n",
      "[D loss: 1.028177] [G loss: 1.340633]\n",
      "[D loss: 0.631268] [G loss: 1.679932]\n",
      "[D loss: 0.797879] [G loss: 1.433353]\n",
      "[D loss: 0.732516] [G loss: 1.688142]\n",
      "[D loss: 0.757585] [G loss: 1.494775]\n",
      "[D loss: 0.742860] [G loss: 1.649641]\n",
      "[D loss: 0.896573] [G loss: 1.553475]\n",
      "[D loss: 1.098276] [G loss: 1.619683]\n",
      "[D loss: 0.736462] [G loss: 1.570638]\n",
      "[D loss: 0.854961] [G loss: 1.469378]\n",
      "[D loss: 0.825650] [G loss: 1.400005]\n",
      "[D loss: 0.927524] [G loss: 1.112223]\n",
      "[D loss: 0.816591] [G loss: 1.301280]\n",
      "[D loss: 0.653082] [G loss: 1.553110]\n",
      "[D loss: 0.649081] [G loss: 1.666662]\n",
      "[D loss: 0.899129] [G loss: 1.562277]\n",
      "[D loss: 0.589489] [G loss: 1.418435]\n",
      "[D loss: 0.750342] [G loss: 1.362868]\n",
      "[D loss: 0.983232] [G loss: 1.592349]\n",
      "[D loss: 1.066627] [G loss: 1.613384]\n",
      "[D loss: 0.899340] [G loss: 1.368922]\n",
      "[D loss: 0.731064] [G loss: 1.527160]\n",
      "[D loss: 0.939672] [G loss: 1.365746]\n",
      "[D loss: 0.746346] [G loss: 1.388922]\n",
      "[D loss: 0.892370] [G loss: 1.655225]\n",
      "[D loss: 0.871204] [G loss: 1.373248]\n",
      "[D loss: 0.736962] [G loss: 1.326741]\n",
      "[D loss: 0.802210] [G loss: 1.570020]\n",
      "[D loss: 0.893802] [G loss: 1.475139]\n",
      "[D loss: 0.719522] [G loss: 1.468794]\n",
      "[D loss: 0.803817] [G loss: 1.527390]\n",
      "[D loss: 0.952694] [G loss: 1.324337]\n",
      "[D loss: 0.938780] [G loss: 1.299564]\n",
      "[D loss: 0.865845] [G loss: 1.348027]\n",
      "[D loss: 0.724851] [G loss: 1.384655]\n",
      "[D loss: 0.923520] [G loss: 1.418339]\n",
      "[D loss: 0.731107] [G loss: 1.349295]\n",
      "[D loss: 0.708848] [G loss: 1.444070]\n",
      "[D loss: 0.831901] [G loss: 1.474041]\n",
      "[D loss: 0.859397] [G loss: 1.452794]\n",
      "[D loss: 0.773986] [G loss: 1.371488]\n",
      "[D loss: 0.854970] [G loss: 1.329868]\n",
      "[D loss: 0.893143] [G loss: 1.321540]\n",
      "[D loss: 0.960844] [G loss: 1.271861]\n",
      "[D loss: 0.745984] [G loss: 1.408807]\n",
      "[D loss: 0.748409] [G loss: 1.500924]\n",
      "[D loss: 0.768171] [G loss: 1.749037]\n",
      "[D loss: 0.823358] [G loss: 1.420582]\n",
      "[D loss: 1.084934] [G loss: 1.423675]\n",
      "[D loss: 0.780395] [G loss: 1.451627]\n",
      "[D loss: 0.747121] [G loss: 1.360417]\n",
      "[D loss: 0.614126] [G loss: 1.507993]\n",
      "[D loss: 0.745180] [G loss: 1.465994]\n",
      "[D loss: 0.717220] [G loss: 1.645900]\n",
      "[D loss: 0.882989] [G loss: 1.682002]\n",
      "[D loss: 0.815711] [G loss: 1.528977]\n",
      "[D loss: 0.954518] [G loss: 1.720210]\n",
      "[D loss: 0.857893] [G loss: 1.530254]\n",
      "[D loss: 1.036964] [G loss: 1.253988]\n",
      "[D loss: 0.822045] [G loss: 1.367656]\n",
      "[D loss: 0.850886] [G loss: 1.523190]\n",
      "[D loss: 1.071101] [G loss: 1.435886]\n",
      "[D loss: 0.888188] [G loss: 1.345960]\n",
      "[D loss: 0.760113] [G loss: 1.312830]\n",
      "[D loss: 0.620749] [G loss: 1.533623]\n",
      "[D loss: 0.825455] [G loss: 1.342225]\n",
      "[D loss: 0.764562] [G loss: 1.315982]\n",
      "[D loss: 0.767314] [G loss: 1.438182]\n",
      "[D loss: 0.969399] [G loss: 1.484684]\n",
      "[D loss: 0.791017] [G loss: 1.638204]\n",
      "[D loss: 0.795043] [G loss: 1.476518]\n",
      "[D loss: 0.774389] [G loss: 1.492707]\n",
      "[D loss: 0.813469] [G loss: 1.269065]\n",
      "[D loss: 0.910559] [G loss: 1.324524]\n",
      "[D loss: 0.892323] [G loss: 1.431646]\n",
      "[D loss: 0.688094] [G loss: 1.459338]\n",
      "[D loss: 1.013006] [G loss: 1.400396]\n",
      "[D loss: 0.623951] [G loss: 1.329542]\n",
      "[D loss: 0.891752] [G loss: 1.261820]\n",
      "[D loss: 0.880866] [G loss: 1.521389]\n",
      "[D loss: 0.713098] [G loss: 1.340959]\n",
      "[D loss: 1.087746] [G loss: 1.281434]\n",
      "[D loss: 0.785817] [G loss: 1.245297]\n",
      "[D loss: 1.043361] [G loss: 1.214108]\n",
      "[D loss: 0.797055] [G loss: 1.400710]\n",
      "[D loss: 0.783639] [G loss: 1.427799]\n",
      "[D loss: 0.759652] [G loss: 1.511462]\n",
      "[D loss: 0.768836] [G loss: 1.292742]\n",
      "[D loss: 1.034736] [G loss: 1.409897]\n",
      "[D loss: 0.808244] [G loss: 1.487127]\n",
      "[D loss: 0.850109] [G loss: 1.525247]\n",
      "[D loss: 0.941525] [G loss: 1.396337]\n",
      "[D loss: 0.799073] [G loss: 1.346779]\n",
      "[D loss: 0.724725] [G loss: 1.737128]\n",
      "[D loss: 0.670080] [G loss: 1.717877]\n",
      "[D loss: 0.711198] [G loss: 1.550710]\n",
      "[D loss: 0.457370] [G loss: 1.548759]\n",
      "[D loss: 0.840940] [G loss: 1.316139]\n",
      "[D loss: 0.756004] [G loss: 1.627973]\n",
      "[D loss: 0.891697] [G loss: 1.630835]\n",
      "[D loss: 1.042068] [G loss: 1.516020]\n",
      "[D loss: 0.820363] [G loss: 1.403633]\n",
      "[D loss: 1.010165] [G loss: 1.332641]\n",
      "[D loss: 0.915031] [G loss: 1.481942]\n",
      "[D loss: 1.029811] [G loss: 1.342695]\n",
      "[D loss: 0.821375] [G loss: 1.588033]\n",
      "[D loss: 1.026808] [G loss: 1.528132]\n",
      "[D loss: 0.794814] [G loss: 1.315458]\n",
      "[D loss: 0.656111] [G loss: 1.452433]\n",
      "[D loss: 0.633088] [G loss: 1.529740]\n",
      "[D loss: 0.727217] [G loss: 1.404090]\n",
      "[D loss: 0.838656] [G loss: 1.539024]\n",
      "[D loss: 0.783099] [G loss: 1.510262]\n",
      "[D loss: 0.687270] [G loss: 1.452725]\n",
      "[D loss: 0.991044] [G loss: 1.280396]\n",
      "[D loss: 0.799615] [G loss: 1.551964]\n",
      "[D loss: 0.683156] [G loss: 1.606586]\n",
      "[D loss: 1.046149] [G loss: 1.272460]\n",
      "[D loss: 0.735577] [G loss: 1.417755]\n",
      "[D loss: 0.925212] [G loss: 1.381371]\n",
      "[D loss: 0.880718] [G loss: 1.594885]\n",
      "[D loss: 0.851736] [G loss: 1.450618]\n",
      "[D loss: 0.801452] [G loss: 1.591394]\n",
      "[D loss: 0.871971] [G loss: 1.531729]\n",
      "[D loss: 1.029814] [G loss: 1.635252]\n",
      "[D loss: 0.913320] [G loss: 1.433110]\n",
      "[D loss: 0.850653] [G loss: 1.605830]\n",
      "[D loss: 0.833993] [G loss: 1.392726]\n",
      "[D loss: 0.979921] [G loss: 1.234506]\n",
      "[D loss: 0.821226] [G loss: 1.417943]\n",
      "[D loss: 0.833578] [G loss: 1.682234]\n",
      "[D loss: 0.756759] [G loss: 1.813182]\n",
      "[D loss: 1.003553] [G loss: 1.382228]\n",
      "[D loss: 0.645185] [G loss: 1.350411]\n",
      "[D loss: 0.649727] [G loss: 1.775020]\n",
      "[D loss: 0.845021] [G loss: 1.489638]\n",
      "[D loss: 0.782996] [G loss: 1.697287]\n",
      "[D loss: 0.675287] [G loss: 1.712252]\n",
      "[D loss: 0.896065] [G loss: 1.500975]\n",
      "[D loss: 0.600054] [G loss: 1.515323]\n",
      "[D loss: 0.909859] [G loss: 1.259911]\n",
      "[D loss: 1.144307] [G loss: 1.336307]\n",
      "[D loss: 0.921323] [G loss: 1.575100]\n",
      "[D loss: 0.881013] [G loss: 1.628979]\n",
      "[D loss: 0.563422] [G loss: 1.464660]\n",
      "[D loss: 0.788953] [G loss: 1.577574]\n",
      "[D loss: 1.060633] [G loss: 1.169319]\n",
      "[D loss: 0.786716] [G loss: 1.579899]\n",
      "[D loss: 0.660656] [G loss: 1.461523]\n",
      "[D loss: 0.768334] [G loss: 1.433533]\n",
      "[D loss: 0.898245] [G loss: 1.408816]\n",
      "[D loss: 0.867733] [G loss: 1.479851]\n",
      "[D loss: 0.617873] [G loss: 1.582214]\n",
      "[D loss: 0.723238] [G loss: 1.467938]\n",
      "[D loss: 0.795143] [G loss: 1.478094]\n",
      "[D loss: 0.711406] [G loss: 1.490010]\n",
      "[D loss: 0.859908] [G loss: 1.536541]\n",
      "[D loss: 0.931095] [G loss: 1.452320]\n",
      "[D loss: 0.754235] [G loss: 1.468334]\n",
      "[D loss: 0.956946] [G loss: 1.553107]\n",
      "[D loss: 0.738061] [G loss: 1.368170]\n",
      "[D loss: 0.603989] [G loss: 1.506327]\n",
      "[D loss: 0.903849] [G loss: 1.779479]\n",
      "[D loss: 0.977209] [G loss: 1.473129]\n",
      "[D loss: 0.529627] [G loss: 1.573602]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.837701] [G loss: 1.398974]\n",
      "[D loss: 0.932043] [G loss: 1.105508]\n",
      "[D loss: 0.788310] [G loss: 1.480544]\n",
      "[D loss: 1.173495] [G loss: 1.282808]\n",
      "[D loss: 0.895922] [G loss: 1.361955]\n",
      "[D loss: 0.716676] [G loss: 1.569650]\n",
      "[D loss: 0.797943] [G loss: 1.532026]\n",
      "[D loss: 0.940030] [G loss: 1.557730]\n",
      "[D loss: 0.755588] [G loss: 1.512181]\n",
      "[D loss: 0.735967] [G loss: 1.303092]\n",
      "[D loss: 0.733234] [G loss: 1.304694]\n",
      "[D loss: 0.933732] [G loss: 1.529984]\n",
      "[D loss: 0.824429] [G loss: 1.538740]\n",
      "[D loss: 0.981364] [G loss: 1.367916]\n",
      "[D loss: 0.817579] [G loss: 1.395831]\n",
      "[D loss: 0.674619] [G loss: 1.779004]\n",
      "[D loss: 0.981586] [G loss: 1.577786]\n",
      "[D loss: 0.810680] [G loss: 1.429865]\n",
      "[D loss: 0.856986] [G loss: 1.303501]\n",
      "[D loss: 0.769836] [G loss: 1.437146]\n",
      "[D loss: 0.707183] [G loss: 1.224480]\n",
      "[D loss: 0.938572] [G loss: 1.410872]\n",
      "[D loss: 0.954056] [G loss: 1.421472]\n",
      "[D loss: 0.783327] [G loss: 1.548548]\n",
      "[D loss: 0.933221] [G loss: 1.544702]\n",
      "[D loss: 0.776756] [G loss: 1.446086]\n",
      "[D loss: 0.682217] [G loss: 1.519847]\n",
      "[D loss: 0.669962] [G loss: 1.448510]\n",
      "[D loss: 0.931471] [G loss: 1.359334]\n",
      "[D loss: 0.831180] [G loss: 1.573059]\n",
      "[D loss: 0.752179] [G loss: 1.565262]\n",
      "[D loss: 0.830543] [G loss: 1.615579]\n",
      "[D loss: 0.696823] [G loss: 1.487115]\n",
      "[D loss: 1.072747] [G loss: 1.451029]\n",
      "[D loss: 0.945321] [G loss: 1.480780]\n",
      "[D loss: 0.773064] [G loss: 1.525126]\n",
      "[D loss: 0.779501] [G loss: 1.484837]\n",
      "[D loss: 0.626714] [G loss: 1.678829]\n",
      "[D loss: 0.854014] [G loss: 1.650638]\n",
      "[D loss: 0.835302] [G loss: 1.512264]\n",
      "[D loss: 0.924828] [G loss: 1.493985]\n",
      "[D loss: 0.914841] [G loss: 1.576929]\n",
      "[D loss: 0.661689] [G loss: 1.622071]\n",
      "[D loss: 0.764025] [G loss: 1.627786]\n",
      "[D loss: 0.747607] [G loss: 1.600751]\n",
      "[D loss: 0.892336] [G loss: 1.658779]\n",
      "[D loss: 0.672601] [G loss: 1.620267]\n",
      "[D loss: 0.984043] [G loss: 1.450133]\n",
      "[D loss: 0.865002] [G loss: 1.623976]\n",
      "[D loss: 0.930566] [G loss: 1.586852]\n",
      "[D loss: 0.940014] [G loss: 1.339981]\n",
      "[D loss: 0.835437] [G loss: 1.427919]\n",
      "[D loss: 0.910819] [G loss: 1.448388]\n",
      "[D loss: 0.838636] [G loss: 1.462951]\n",
      "[D loss: 0.901946] [G loss: 1.470883]\n",
      "[D loss: 0.469935] [G loss: 1.588642]\n",
      "[D loss: 0.848130] [G loss: 1.613776]\n",
      "[D loss: 0.916662] [G loss: 1.522805]\n",
      "[D loss: 0.756986] [G loss: 1.637135]\n",
      "[D loss: 0.993455] [G loss: 1.509368]\n",
      "[D loss: 0.802795] [G loss: 1.422456]\n",
      "[D loss: 0.758394] [G loss: 1.325255]\n",
      "[D loss: 0.625560] [G loss: 1.398789]\n",
      "[D loss: 0.587067] [G loss: 1.578179]\n",
      "[D loss: 0.995042] [G loss: 1.450383]\n",
      "[D loss: 0.727953] [G loss: 1.451176]\n",
      "[D loss: 0.866260] [G loss: 1.513900]\n",
      "[D loss: 0.890721] [G loss: 1.556820]\n",
      "[D loss: 0.948718] [G loss: 1.625848]\n",
      "[D loss: 0.835667] [G loss: 1.452388]\n",
      "[D loss: 0.726883] [G loss: 1.435302]\n",
      "[D loss: 0.642760] [G loss: 1.665206]\n",
      "[D loss: 0.751422] [G loss: 1.597696]\n",
      "[D loss: 0.953995] [G loss: 1.452446]\n",
      "[D loss: 0.667535] [G loss: 1.723907]\n",
      "[D loss: 1.037188] [G loss: 1.255072]\n",
      "[D loss: 0.732645] [G loss: 1.548249]\n",
      "[D loss: 0.626864] [G loss: 1.534629]\n",
      "[D loss: 0.754296] [G loss: 1.763294]\n",
      "[D loss: 0.813904] [G loss: 1.449038]\n",
      "[D loss: 0.839398] [G loss: 1.529617]\n",
      "[D loss: 0.837725] [G loss: 1.375113]\n",
      "[D loss: 0.955565] [G loss: 1.275731]\n",
      "[D loss: 0.971334] [G loss: 1.355835]\n",
      "[D loss: 0.837694] [G loss: 1.383598]\n",
      "[D loss: 0.967635] [G loss: 1.665456]\n",
      "[D loss: 0.926770] [G loss: 1.361295]\n",
      "[D loss: 0.672571] [G loss: 1.439398]\n",
      "[D loss: 0.897430] [G loss: 1.165313]\n",
      "[D loss: 0.541321] [G loss: 1.432247]\n",
      "[D loss: 1.010437] [G loss: 1.388277]\n",
      "[D loss: 1.176711] [G loss: 1.127089]\n",
      "[D loss: 0.978490] [G loss: 1.508767]\n",
      "[D loss: 0.788440] [G loss: 1.507265]\n",
      "[D loss: 0.914764] [G loss: 1.455153]\n",
      "[D loss: 0.959942] [G loss: 1.400840]\n",
      "[D loss: 0.802969] [G loss: 1.476128]\n",
      "[D loss: 0.842564] [G loss: 1.262724]\n",
      "[D loss: 0.754498] [G loss: 1.399677]\n",
      "[D loss: 0.728147] [G loss: 1.470154]\n",
      "[D loss: 0.845838] [G loss: 1.336710]\n",
      "[D loss: 0.713427] [G loss: 1.417590]\n",
      "[D loss: 0.837350] [G loss: 1.363265]\n",
      "[D loss: 0.816879] [G loss: 1.430790]\n",
      "[D loss: 0.853383] [G loss: 1.486053]\n",
      "[D loss: 0.877156] [G loss: 1.439448]\n",
      "[D loss: 0.649236] [G loss: 1.623315]\n",
      "[D loss: 0.762960] [G loss: 1.488950]\n",
      "[D loss: 0.836558] [G loss: 1.463680]\n",
      "[D loss: 0.822950] [G loss: 1.478863]\n",
      "[D loss: 0.819774] [G loss: 1.418648]\n",
      "[D loss: 0.946110] [G loss: 1.635424]\n",
      "[D loss: 0.853383] [G loss: 1.363033]\n",
      "[D loss: 0.884965] [G loss: 1.352746]\n",
      "[D loss: 0.683260] [G loss: 1.555858]\n",
      "[D loss: 0.828173] [G loss: 1.430004]\n",
      "[D loss: 0.647415] [G loss: 1.707050]\n",
      "[D loss: 0.816168] [G loss: 1.601050]\n",
      "[D loss: 0.910549] [G loss: 1.348130]\n",
      "[D loss: 0.892240] [G loss: 1.382874]\n",
      "[D loss: 0.591078] [G loss: 1.563481]\n",
      "[D loss: 0.768605] [G loss: 1.420425]\n",
      "[D loss: 0.889289] [G loss: 1.585194]\n",
      "[D loss: 0.695389] [G loss: 1.615217]\n",
      "[D loss: 0.694726] [G loss: 1.661110]\n",
      "[D loss: 0.639576] [G loss: 1.593441]\n",
      "[D loss: 0.771127] [G loss: 1.423639]\n",
      "[D loss: 0.940207] [G loss: 1.580200]\n",
      "[D loss: 0.719155] [G loss: 1.522757]\n",
      "[D loss: 0.866295] [G loss: 1.733956]\n",
      "[D loss: 0.595241] [G loss: 1.534489]\n",
      "[D loss: 0.940997] [G loss: 1.481952]\n",
      "[D loss: 0.821317] [G loss: 1.405992]\n",
      "[D loss: 0.976199] [G loss: 1.474208]\n",
      "[D loss: 0.777910] [G loss: 1.785134]\n",
      "[D loss: 0.856540] [G loss: 1.479142]\n",
      "[D loss: 0.899734] [G loss: 1.499941]\n",
      "[D loss: 0.971221] [G loss: 1.431351]\n",
      "[D loss: 0.681547] [G loss: 1.359437]\n",
      "[D loss: 0.968569] [G loss: 1.422266]\n",
      "[D loss: 0.889592] [G loss: 1.263854]\n",
      "[D loss: 0.677605] [G loss: 1.700069]\n",
      "[D loss: 0.762245] [G loss: 1.498245]\n",
      "[D loss: 0.771766] [G loss: 1.721979]\n",
      "[D loss: 0.840057] [G loss: 1.636283]\n",
      "[D loss: 0.930916] [G loss: 1.533368]\n",
      "[D loss: 1.014909] [G loss: 1.497049]\n",
      "[D loss: 0.874558] [G loss: 1.337358]\n",
      "[D loss: 0.782508] [G loss: 1.465131]\n",
      "[D loss: 0.896689] [G loss: 1.516425]\n",
      "[D loss: 0.958611] [G loss: 1.280232]\n",
      "[D loss: 1.121227] [G loss: 1.261100]\n",
      "[D loss: 0.720723] [G loss: 1.559657]\n",
      "[D loss: 0.957145] [G loss: 1.364028]\n",
      "[D loss: 0.809869] [G loss: 1.408230]\n",
      "[D loss: 0.881268] [G loss: 1.578140]\n",
      "[D loss: 0.962006] [G loss: 1.399622]\n",
      "[D loss: 0.918109] [G loss: 1.473446]\n",
      "[D loss: 0.723746] [G loss: 1.399334]\n",
      "[D loss: 0.834536] [G loss: 1.397797]\n",
      "[D loss: 0.794754] [G loss: 1.797258]\n",
      "[D loss: 0.869180] [G loss: 1.455881]\n",
      "[D loss: 0.888005] [G loss: 1.377392]\n",
      "[D loss: 0.654269] [G loss: 1.397234]\n",
      "[D loss: 0.881962] [G loss: 1.535195]\n",
      "[D loss: 0.836917] [G loss: 1.467345]\n",
      "[D loss: 0.754588] [G loss: 1.717495]\n",
      "[D loss: 0.550241] [G loss: 1.640455]\n",
      "[D loss: 0.936087] [G loss: 1.319041]\n",
      "[D loss: 0.872778] [G loss: 1.360149]\n",
      "[D loss: 0.680939] [G loss: 1.585242]\n",
      "[D loss: 0.731483] [G loss: 1.443931]\n",
      "[D loss: 0.949548] [G loss: 1.503047]\n",
      "[D loss: 0.941570] [G loss: 1.461745]\n",
      "[D loss: 1.048525] [G loss: 1.377530]\n",
      "[D loss: 0.938641] [G loss: 1.454801]\n",
      "[D loss: 0.834156] [G loss: 1.423188]\n",
      "[D loss: 1.037874] [G loss: 1.234844]\n",
      "[D loss: 0.967065] [G loss: 1.310135]\n",
      "[D loss: 0.785786] [G loss: 1.448780]\n",
      "[D loss: 0.709084] [G loss: 1.384530]\n",
      "[D loss: 0.977181] [G loss: 1.222349]\n",
      "[D loss: 0.902535] [G loss: 1.381838]\n",
      "[D loss: 0.808198] [G loss: 1.270094]\n",
      "[D loss: 0.756075] [G loss: 1.373673]\n",
      "[D loss: 0.770524] [G loss: 1.408737]\n",
      "[D loss: 0.971192] [G loss: 1.341778]\n",
      "[D loss: 0.794170] [G loss: 1.528986]\n",
      "[D loss: 1.050277] [G loss: 1.306164]\n",
      "[D loss: 0.708805] [G loss: 1.568093]\n",
      "[D loss: 1.031679] [G loss: 1.343366]\n",
      "[D loss: 0.813255] [G loss: 1.419307]\n",
      "[D loss: 0.788402] [G loss: 1.372155]\n",
      "[D loss: 0.926723] [G loss: 1.458927]\n",
      "[D loss: 0.868503] [G loss: 1.310610]\n",
      "[D loss: 1.088572] [G loss: 1.356647]\n",
      "[D loss: 0.756235] [G loss: 1.641896]\n",
      "[D loss: 0.918288] [G loss: 1.332795]\n",
      "[D loss: 0.752914] [G loss: 1.442569]\n",
      "[D loss: 0.866123] [G loss: 1.375484]\n",
      "[D loss: 0.885626] [G loss: 1.532102]\n",
      "[D loss: 0.920945] [G loss: 1.252256]\n",
      "[D loss: 0.741332] [G loss: 1.555198]\n",
      "[D loss: 0.910743] [G loss: 1.404012]\n",
      "[D loss: 0.929487] [G loss: 1.540797]\n",
      "[D loss: 0.804380] [G loss: 1.680460]\n",
      "[D loss: 0.857577] [G loss: 1.427098]\n",
      "[D loss: 0.776656] [G loss: 1.586915]\n",
      "[D loss: 0.928086] [G loss: 1.533511]\n",
      "[D loss: 0.859934] [G loss: 1.227319]\n",
      "[D loss: 0.876092] [G loss: 1.002473]\n",
      "[D loss: 0.772362] [G loss: 1.480351]\n",
      "[D loss: 0.661195] [G loss: 1.634800]\n",
      "[D loss: 0.766374] [G loss: 1.340771]\n",
      "[D loss: 0.629501] [G loss: 1.479067]\n",
      "[D loss: 0.984107] [G loss: 1.481724]\n",
      "[D loss: 0.820516] [G loss: 1.517672]\n",
      "[D loss: 0.788003] [G loss: 1.509959]\n",
      "[D loss: 1.209937] [G loss: 1.154011]\n",
      "[D loss: 1.066424] [G loss: 1.429189]\n",
      "[D loss: 1.137457] [G loss: 1.424976]\n",
      "[D loss: 1.038250] [G loss: 1.353332]\n",
      "[D loss: 1.090780] [G loss: 1.196335]\n",
      "[D loss: 0.788150] [G loss: 1.282571]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.805832] [G loss: 1.542848]\n",
      "[D loss: 0.634461] [G loss: 1.413185]\n",
      "[D loss: 0.870413] [G loss: 1.201447]\n",
      "[D loss: 0.874193] [G loss: 1.429572]\n",
      "[D loss: 0.788034] [G loss: 1.489181]\n",
      "[D loss: 0.828256] [G loss: 1.514508]\n",
      "[D loss: 0.888400] [G loss: 1.407470]\n",
      "[D loss: 0.734558] [G loss: 1.490599]\n",
      "[D loss: 0.885086] [G loss: 1.375904]\n",
      "[D loss: 0.866958] [G loss: 1.189082]\n",
      "[D loss: 0.654162] [G loss: 1.324635]\n",
      "[D loss: 0.731691] [G loss: 1.521010]\n",
      "[D loss: 0.571489] [G loss: 1.337341]\n",
      "[D loss: 0.892866] [G loss: 1.421206]\n",
      "[D loss: 0.751886] [G loss: 1.433410]\n",
      "[D loss: 1.155423] [G loss: 1.269852]\n",
      "[D loss: 0.704593] [G loss: 1.462929]\n",
      "epoch:2, g_loss:2831.18017578125,d_loss:1528.5284423828125\n",
      "[D loss: 1.143171] [G loss: 1.715560]\n",
      "[D loss: 0.949874] [G loss: 1.656309]\n",
      "[D loss: 0.730758] [G loss: 1.303098]\n",
      "[D loss: 0.848868] [G loss: 1.352613]\n",
      "[D loss: 0.855799] [G loss: 1.470998]\n",
      "[D loss: 1.008276] [G loss: 1.678648]\n",
      "[D loss: 0.844217] [G loss: 1.455968]\n",
      "[D loss: 0.632353] [G loss: 1.484961]\n",
      "[D loss: 1.123199] [G loss: 1.279870]\n",
      "[D loss: 0.810268] [G loss: 1.459422]\n",
      "[D loss: 0.979048] [G loss: 1.335326]\n",
      "[D loss: 0.822234] [G loss: 1.257374]\n",
      "[D loss: 0.827192] [G loss: 1.624546]\n",
      "[D loss: 0.819604] [G loss: 1.289496]\n",
      "[D loss: 0.855961] [G loss: 1.492283]\n",
      "[D loss: 0.744336] [G loss: 1.384124]\n",
      "[D loss: 0.964640] [G loss: 1.473082]\n",
      "[D loss: 1.085289] [G loss: 1.464109]\n",
      "[D loss: 0.793461] [G loss: 1.456843]\n",
      "[D loss: 0.810006] [G loss: 1.422241]\n",
      "[D loss: 0.649671] [G loss: 1.546120]\n",
      "[D loss: 0.807701] [G loss: 1.485991]\n",
      "[D loss: 0.878354] [G loss: 1.405795]\n",
      "[D loss: 0.670338] [G loss: 1.536995]\n",
      "[D loss: 0.866747] [G loss: 1.462603]\n",
      "[D loss: 0.758943] [G loss: 1.374808]\n",
      "[D loss: 0.892063] [G loss: 1.226121]\n",
      "[D loss: 1.122707] [G loss: 1.335410]\n",
      "[D loss: 0.784323] [G loss: 1.467775]\n",
      "[D loss: 1.109735] [G loss: 1.392590]\n",
      "[D loss: 0.717161] [G loss: 1.519395]\n",
      "[D loss: 0.852686] [G loss: 1.595454]\n",
      "[D loss: 0.715306] [G loss: 1.375173]\n",
      "[D loss: 0.811105] [G loss: 1.309457]\n",
      "[D loss: 0.903795] [G loss: 1.305454]\n",
      "[D loss: 0.868039] [G loss: 1.502948]\n",
      "[D loss: 0.680727] [G loss: 1.451142]\n",
      "[D loss: 0.740369] [G loss: 1.400812]\n",
      "[D loss: 0.782023] [G loss: 1.404449]\n",
      "[D loss: 0.850879] [G loss: 1.326122]\n",
      "[D loss: 0.940098] [G loss: 1.474494]\n",
      "[D loss: 0.668368] [G loss: 1.717158]\n",
      "[D loss: 0.859308] [G loss: 1.488263]\n",
      "[D loss: 0.686097] [G loss: 1.409882]\n",
      "[D loss: 0.856618] [G loss: 1.387201]\n",
      "[D loss: 0.862900] [G loss: 1.423828]\n",
      "[D loss: 0.896045] [G loss: 1.457853]\n",
      "[D loss: 0.871883] [G loss: 1.508646]\n",
      "[D loss: 0.864762] [G loss: 1.449958]\n",
      "[D loss: 1.063070] [G loss: 1.566505]\n",
      "[D loss: 0.639066] [G loss: 1.450611]\n",
      "[D loss: 0.837924] [G loss: 1.666285]\n",
      "[D loss: 0.865195] [G loss: 1.421926]\n",
      "[D loss: 0.873753] [G loss: 1.392633]\n",
      "[D loss: 0.788147] [G loss: 1.478185]\n",
      "[D loss: 0.695287] [G loss: 1.335348]\n",
      "[D loss: 0.778716] [G loss: 1.424890]\n",
      "[D loss: 0.785578] [G loss: 1.700004]\n",
      "[D loss: 1.125827] [G loss: 1.304201]\n",
      "[D loss: 0.903772] [G loss: 1.521101]\n",
      "[D loss: 0.859308] [G loss: 1.340982]\n",
      "[D loss: 0.804373] [G loss: 1.347591]\n",
      "[D loss: 0.718075] [G loss: 1.375919]\n",
      "[D loss: 0.717816] [G loss: 1.577445]\n",
      "[D loss: 0.781373] [G loss: 1.525081]\n",
      "[D loss: 0.683134] [G loss: 1.469885]\n",
      "[D loss: 0.924912] [G loss: 1.677902]\n",
      "[D loss: 0.831671] [G loss: 1.533016]\n",
      "[D loss: 0.906429] [G loss: 1.363482]\n",
      "[D loss: 0.939671] [G loss: 1.485943]\n",
      "[D loss: 0.869552] [G loss: 1.425222]\n",
      "[D loss: 0.840138] [G loss: 1.882434]\n",
      "[D loss: 0.963480] [G loss: 1.467423]\n",
      "[D loss: 0.969986] [G loss: 1.234313]\n",
      "[D loss: 0.901043] [G loss: 1.466608]\n",
      "[D loss: 0.821373] [G loss: 1.557930]\n",
      "[D loss: 0.654148] [G loss: 1.443877]\n",
      "[D loss: 0.907670] [G loss: 1.514725]\n",
      "[D loss: 0.650355] [G loss: 1.333786]\n",
      "[D loss: 0.680236] [G loss: 1.480050]\n",
      "[D loss: 0.960381] [G loss: 1.364162]\n",
      "[D loss: 0.777630] [G loss: 1.494900]\n",
      "[D loss: 0.673598] [G loss: 1.475722]\n",
      "[D loss: 0.795824] [G loss: 1.409044]\n",
      "[D loss: 0.765228] [G loss: 1.708717]\n",
      "[D loss: 0.725309] [G loss: 1.682718]\n",
      "[D loss: 0.692751] [G loss: 1.604615]\n",
      "[D loss: 0.788982] [G loss: 1.512020]\n",
      "[D loss: 0.612154] [G loss: 1.530882]\n",
      "[D loss: 1.030435] [G loss: 1.610479]\n",
      "[D loss: 0.854770] [G loss: 1.305764]\n",
      "[D loss: 1.136968] [G loss: 1.208685]\n",
      "[D loss: 0.720410] [G loss: 1.544474]\n",
      "[D loss: 0.633550] [G loss: 1.451144]\n",
      "[D loss: 1.092191] [G loss: 1.184303]\n",
      "[D loss: 0.906263] [G loss: 1.375574]\n",
      "[D loss: 0.922434] [G loss: 1.549253]\n",
      "[D loss: 0.750585] [G loss: 1.435476]\n",
      "[D loss: 0.796620] [G loss: 1.600772]\n",
      "[D loss: 0.888140] [G loss: 1.493075]\n",
      "[D loss: 1.036117] [G loss: 1.373300]\n",
      "[D loss: 0.868354] [G loss: 1.443719]\n",
      "[D loss: 0.830927] [G loss: 1.509131]\n",
      "[D loss: 0.677537] [G loss: 1.266297]\n",
      "[D loss: 0.808557] [G loss: 1.386566]\n",
      "[D loss: 0.920506] [G loss: 1.310543]\n",
      "[D loss: 0.837375] [G loss: 1.662247]\n",
      "[D loss: 0.823974] [G loss: 1.289448]\n",
      "[D loss: 0.597909] [G loss: 1.553367]\n",
      "[D loss: 1.089828] [G loss: 1.402743]\n",
      "[D loss: 0.840671] [G loss: 1.275801]\n",
      "[D loss: 0.872932] [G loss: 1.427089]\n",
      "[D loss: 1.148290] [G loss: 1.485716]\n",
      "[D loss: 0.616556] [G loss: 1.528465]\n",
      "[D loss: 0.899192] [G loss: 1.577977]\n",
      "[D loss: 0.947441] [G loss: 1.445926]\n",
      "[D loss: 0.849681] [G loss: 1.265884]\n",
      "[D loss: 0.827319] [G loss: 1.466097]\n",
      "[D loss: 0.814544] [G loss: 1.387631]\n",
      "[D loss: 0.806796] [G loss: 1.644676]\n",
      "[D loss: 0.929572] [G loss: 1.417580]\n",
      "[D loss: 0.887511] [G loss: 1.353011]\n",
      "[D loss: 0.747044] [G loss: 1.449683]\n",
      "[D loss: 0.890361] [G loss: 1.296741]\n",
      "[D loss: 0.828907] [G loss: 1.444318]\n",
      "[D loss: 0.842668] [G loss: 1.925237]\n",
      "[D loss: 1.103077] [G loss: 1.520876]\n",
      "[D loss: 0.777940] [G loss: 1.494608]\n",
      "[D loss: 1.156968] [G loss: 1.305868]\n",
      "[D loss: 0.916653] [G loss: 1.260230]\n",
      "[D loss: 0.772499] [G loss: 1.344328]\n",
      "[D loss: 0.735452] [G loss: 1.359855]\n",
      "[D loss: 0.694007] [G loss: 1.288621]\n",
      "[D loss: 0.726192] [G loss: 1.577003]\n",
      "[D loss: 1.010434] [G loss: 1.309488]\n",
      "[D loss: 1.026940] [G loss: 1.453133]\n",
      "[D loss: 0.868228] [G loss: 1.284508]\n",
      "[D loss: 0.719396] [G loss: 1.549301]\n",
      "[D loss: 0.697554] [G loss: 1.620925]\n",
      "[D loss: 0.767305] [G loss: 1.565158]\n",
      "[D loss: 0.580691] [G loss: 1.625999]\n",
      "[D loss: 0.665466] [G loss: 1.561674]\n",
      "[D loss: 0.903917] [G loss: 1.427877]\n",
      "[D loss: 0.812041] [G loss: 1.339495]\n",
      "[D loss: 0.900295] [G loss: 1.567560]\n",
      "[D loss: 0.677158] [G loss: 1.480875]\n",
      "[D loss: 0.707472] [G loss: 1.388141]\n",
      "[D loss: 0.703739] [G loss: 1.485353]\n",
      "[D loss: 1.029137] [G loss: 1.472053]\n",
      "[D loss: 0.948157] [G loss: 1.482348]\n",
      "[D loss: 0.631581] [G loss: 1.504408]\n",
      "[D loss: 0.800613] [G loss: 1.541863]\n",
      "[D loss: 0.851182] [G loss: 1.491378]\n",
      "[D loss: 0.636704] [G loss: 1.621092]\n",
      "[D loss: 0.803068] [G loss: 1.441090]\n",
      "[D loss: 0.879800] [G loss: 1.354672]\n",
      "[D loss: 0.768971] [G loss: 1.559613]\n",
      "[D loss: 0.695295] [G loss: 1.597125]\n",
      "[D loss: 0.922532] [G loss: 1.492703]\n",
      "[D loss: 0.717146] [G loss: 1.216584]\n",
      "[D loss: 0.869511] [G loss: 1.448535]\n",
      "[D loss: 0.741546] [G loss: 1.474994]\n",
      "[D loss: 0.890223] [G loss: 1.415374]\n",
      "[D loss: 0.728021] [G loss: 1.530869]\n",
      "[D loss: 0.735221] [G loss: 1.581292]\n",
      "[D loss: 0.662338] [G loss: 1.434521]\n",
      "[D loss: 0.860902] [G loss: 1.334283]\n",
      "[D loss: 0.913447] [G loss: 1.179221]\n",
      "[D loss: 0.838470] [G loss: 1.350654]\n",
      "[D loss: 0.781533] [G loss: 1.743430]\n",
      "[D loss: 0.887176] [G loss: 1.703462]\n",
      "[D loss: 0.750351] [G loss: 1.673310]\n",
      "[D loss: 0.941937] [G loss: 1.764355]\n",
      "[D loss: 1.039445] [G loss: 1.447788]\n",
      "[D loss: 0.588136] [G loss: 1.480471]\n",
      "[D loss: 0.730117] [G loss: 1.312160]\n",
      "[D loss: 0.785111] [G loss: 1.618955]\n",
      "[D loss: 0.787200] [G loss: 1.422872]\n",
      "[D loss: 0.795775] [G loss: 1.477915]\n",
      "[D loss: 1.024070] [G loss: 1.263513]\n",
      "[D loss: 0.903299] [G loss: 1.599365]\n",
      "[D loss: 0.936907] [G loss: 1.692140]\n",
      "[D loss: 0.743664] [G loss: 1.421626]\n",
      "[D loss: 1.129830] [G loss: 1.336338]\n",
      "[D loss: 0.829164] [G loss: 1.411005]\n",
      "[D loss: 0.963242] [G loss: 1.659049]\n",
      "[D loss: 1.002359] [G loss: 1.406058]\n",
      "[D loss: 1.140933] [G loss: 1.173577]\n",
      "[D loss: 0.963814] [G loss: 1.417721]\n",
      "[D loss: 0.830869] [G loss: 1.403060]\n",
      "[D loss: 0.786365] [G loss: 1.405615]\n",
      "[D loss: 0.949207] [G loss: 1.230508]\n",
      "[D loss: 0.835445] [G loss: 1.275369]\n",
      "[D loss: 0.784455] [G loss: 1.292804]\n",
      "[D loss: 0.856631] [G loss: 1.375326]\n",
      "[D loss: 0.670634] [G loss: 1.426898]\n",
      "[D loss: 0.942022] [G loss: 1.480385]\n",
      "[D loss: 0.791433] [G loss: 1.502888]\n",
      "[D loss: 0.821230] [G loss: 1.471232]\n",
      "[D loss: 0.795105] [G loss: 1.394734]\n",
      "[D loss: 0.715814] [G loss: 1.546153]\n",
      "[D loss: 0.731898] [G loss: 1.408338]\n",
      "[D loss: 0.771132] [G loss: 1.524107]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 1.007000] [G loss: 1.408458]\n",
      "[D loss: 0.858703] [G loss: 1.350350]\n",
      "[D loss: 1.050197] [G loss: 1.243734]\n",
      "[D loss: 1.230369] [G loss: 1.250983]\n",
      "[D loss: 0.625763] [G loss: 1.514783]\n",
      "[D loss: 0.852142] [G loss: 1.472113]\n",
      "[D loss: 0.696575] [G loss: 1.615363]\n",
      "[D loss: 0.804204] [G loss: 1.459572]\n",
      "[D loss: 0.643455] [G loss: 1.495448]\n",
      "[D loss: 0.857025] [G loss: 1.471060]\n",
      "[D loss: 0.804263] [G loss: 1.367366]\n",
      "[D loss: 0.717464] [G loss: 1.529259]\n",
      "[D loss: 0.571956] [G loss: 1.510609]\n",
      "[D loss: 0.905362] [G loss: 1.359032]\n",
      "[D loss: 0.741998] [G loss: 1.442687]\n",
      "[D loss: 0.860014] [G loss: 1.653358]\n",
      "[D loss: 0.813146] [G loss: 1.356022]\n",
      "[D loss: 0.601101] [G loss: 1.523201]\n",
      "[D loss: 0.813214] [G loss: 1.551918]\n",
      "[D loss: 0.771157] [G loss: 1.546115]\n",
      "[D loss: 0.544073] [G loss: 1.679522]\n",
      "[D loss: 0.824355] [G loss: 1.472187]\n",
      "[D loss: 0.731588] [G loss: 1.325280]\n",
      "[D loss: 0.887573] [G loss: 1.546331]\n",
      "[D loss: 0.793732] [G loss: 1.398453]\n",
      "[D loss: 0.767868] [G loss: 1.367096]\n",
      "[D loss: 0.974372] [G loss: 1.449070]\n",
      "[D loss: 0.778957] [G loss: 1.729868]\n",
      "[D loss: 0.978279] [G loss: 1.623281]\n",
      "[D loss: 0.902834] [G loss: 1.499501]\n",
      "[D loss: 0.854479] [G loss: 1.268388]\n",
      "[D loss: 0.688675] [G loss: 1.456820]\n",
      "[D loss: 0.776846] [G loss: 1.360411]\n",
      "[D loss: 0.754692] [G loss: 1.471384]\n",
      "[D loss: 0.813663] [G loss: 1.614013]\n",
      "[D loss: 0.472942] [G loss: 1.626188]\n",
      "[D loss: 0.916300] [G loss: 1.213618]\n",
      "[D loss: 0.885476] [G loss: 1.559216]\n",
      "[D loss: 0.715558] [G loss: 1.507893]\n",
      "[D loss: 0.811376] [G loss: 1.577997]\n",
      "[D loss: 0.939728] [G loss: 1.629191]\n",
      "[D loss: 0.936320] [G loss: 1.505611]\n",
      "[D loss: 0.953600] [G loss: 1.329198]\n",
      "[D loss: 0.950977] [G loss: 1.588021]\n",
      "[D loss: 0.901182] [G loss: 1.418238]\n",
      "[D loss: 0.680965] [G loss: 1.477426]\n",
      "[D loss: 0.767587] [G loss: 1.692558]\n",
      "[D loss: 0.783799] [G loss: 1.724805]\n",
      "[D loss: 0.901451] [G loss: 1.524036]\n",
      "[D loss: 0.754888] [G loss: 1.807210]\n",
      "[D loss: 0.699253] [G loss: 1.495963]\n",
      "[D loss: 0.689274] [G loss: 1.427070]\n",
      "[D loss: 0.849391] [G loss: 1.488231]\n",
      "[D loss: 0.790928] [G loss: 1.687188]\n",
      "[D loss: 0.790923] [G loss: 1.533118]\n",
      "[D loss: 0.652689] [G loss: 1.547497]\n",
      "[D loss: 0.847054] [G loss: 1.362534]\n",
      "[D loss: 0.882763] [G loss: 1.526362]\n",
      "[D loss: 0.811513] [G loss: 1.219947]\n",
      "[D loss: 0.594113] [G loss: 1.720673]\n",
      "[D loss: 0.851167] [G loss: 1.390162]\n",
      "[D loss: 0.721189] [G loss: 1.822177]\n",
      "[D loss: 0.986928] [G loss: 1.741576]\n",
      "[D loss: 0.912326] [G loss: 1.375051]\n",
      "[D loss: 0.719030] [G loss: 1.409285]\n",
      "[D loss: 0.878248] [G loss: 1.354787]\n",
      "[D loss: 0.791058] [G loss: 1.489678]\n",
      "[D loss: 0.679978] [G loss: 1.588213]\n",
      "[D loss: 0.727765] [G loss: 1.728713]\n",
      "[D loss: 0.852575] [G loss: 1.517224]\n",
      "[D loss: 1.080303] [G loss: 1.385865]\n",
      "[D loss: 0.526724] [G loss: 1.619874]\n",
      "[D loss: 0.670889] [G loss: 1.445339]\n",
      "[D loss: 0.816598] [G loss: 1.602858]\n",
      "[D loss: 0.831798] [G loss: 1.682145]\n",
      "[D loss: 0.794473] [G loss: 1.519997]\n",
      "[D loss: 0.801390] [G loss: 1.411829]\n",
      "[D loss: 0.882415] [G loss: 1.344969]\n",
      "[D loss: 0.754522] [G loss: 1.437319]\n",
      "[D loss: 0.789563] [G loss: 1.419071]\n",
      "[D loss: 1.006003] [G loss: 1.323884]\n",
      "[D loss: 0.911384] [G loss: 1.286533]\n",
      "[D loss: 0.851673] [G loss: 1.479861]\n",
      "[D loss: 0.828667] [G loss: 1.478341]\n",
      "[D loss: 0.808368] [G loss: 1.684473]\n",
      "[D loss: 0.959691] [G loss: 1.380529]\n",
      "[D loss: 0.877871] [G loss: 1.454473]\n",
      "[D loss: 0.725580] [G loss: 1.496665]\n",
      "[D loss: 0.561796] [G loss: 1.530270]\n",
      "[D loss: 0.655817] [G loss: 1.507639]\n",
      "[D loss: 0.714761] [G loss: 1.412436]\n",
      "[D loss: 0.753338] [G loss: 1.521925]\n",
      "[D loss: 0.886147] [G loss: 1.564396]\n",
      "[D loss: 0.749778] [G loss: 1.436107]\n",
      "[D loss: 0.778346] [G loss: 1.421104]\n",
      "[D loss: 0.504076] [G loss: 1.625840]\n",
      "[D loss: 0.726009] [G loss: 1.522053]\n",
      "[D loss: 0.701195] [G loss: 1.848344]\n",
      "[D loss: 0.859061] [G loss: 1.754802]\n",
      "[D loss: 0.678715] [G loss: 1.590830]\n",
      "[D loss: 0.834102] [G loss: 1.474394]\n",
      "[D loss: 0.846483] [G loss: 1.360083]\n",
      "[D loss: 0.965853] [G loss: 1.255415]\n",
      "[D loss: 0.845645] [G loss: 1.565323]\n",
      "[D loss: 0.640296] [G loss: 1.556289]\n",
      "[D loss: 0.622308] [G loss: 1.704228]\n",
      "[D loss: 1.011290] [G loss: 1.505322]\n",
      "[D loss: 0.785949] [G loss: 1.530647]\n",
      "[D loss: 0.807643] [G loss: 1.533198]\n",
      "[D loss: 0.602695] [G loss: 1.798973]\n",
      "[D loss: 0.987901] [G loss: 1.356315]\n",
      "[D loss: 0.902556] [G loss: 1.403993]\n",
      "[D loss: 0.848902] [G loss: 1.534234]\n",
      "[D loss: 0.932129] [G loss: 1.368279]\n",
      "[D loss: 0.570603] [G loss: 1.703344]\n",
      "[D loss: 0.744315] [G loss: 1.556097]\n",
      "[D loss: 0.663980] [G loss: 1.641506]\n",
      "[D loss: 0.919945] [G loss: 1.485085]\n",
      "[D loss: 0.851845] [G loss: 1.342525]\n",
      "[D loss: 0.778864] [G loss: 1.502327]\n",
      "[D loss: 0.679581] [G loss: 1.907625]\n",
      "[D loss: 0.844472] [G loss: 1.777136]\n",
      "[D loss: 0.660222] [G loss: 1.841952]\n",
      "[D loss: 0.857581] [G loss: 1.618579]\n",
      "[D loss: 0.822494] [G loss: 1.276386]\n",
      "[D loss: 0.688987] [G loss: 1.397485]\n",
      "[D loss: 0.838437] [G loss: 1.806864]\n",
      "[D loss: 0.787160] [G loss: 1.869301]\n",
      "[D loss: 0.668513] [G loss: 1.948669]\n",
      "[D loss: 0.955171] [G loss: 1.253392]\n",
      "[D loss: 0.876599] [G loss: 1.406564]\n",
      "[D loss: 0.773994] [G loss: 1.851826]\n",
      "[D loss: 0.846312] [G loss: 1.676075]\n",
      "[D loss: 0.749522] [G loss: 1.740976]\n",
      "[D loss: 0.876797] [G loss: 1.555035]\n",
      "[D loss: 0.826441] [G loss: 1.493370]\n",
      "[D loss: 0.785842] [G loss: 1.607362]\n",
      "[D loss: 0.823991] [G loss: 1.411056]\n",
      "[D loss: 0.906725] [G loss: 1.438543]\n",
      "[D loss: 0.827633] [G loss: 1.623247]\n",
      "[D loss: 0.822302] [G loss: 1.457536]\n",
      "[D loss: 0.923651] [G loss: 1.590415]\n",
      "[D loss: 0.802732] [G loss: 1.302411]\n",
      "[D loss: 0.820071] [G loss: 1.446668]\n",
      "[D loss: 0.999458] [G loss: 1.260425]\n",
      "[D loss: 0.887079] [G loss: 1.319084]\n",
      "[D loss: 0.874613] [G loss: 1.531270]\n",
      "[D loss: 0.837779] [G loss: 1.481466]\n",
      "[D loss: 0.747396] [G loss: 1.440371]\n",
      "[D loss: 0.689986] [G loss: 1.606869]\n",
      "[D loss: 0.776741] [G loss: 1.512161]\n",
      "[D loss: 0.662598] [G loss: 1.587820]\n",
      "[D loss: 0.699723] [G loss: 1.668950]\n",
      "[D loss: 0.933280] [G loss: 1.442168]\n",
      "[D loss: 0.717958] [G loss: 1.449685]\n",
      "[D loss: 0.641267] [G loss: 1.478633]\n",
      "[D loss: 0.536324] [G loss: 1.755015]\n",
      "[D loss: 0.664966] [G loss: 1.522668]\n",
      "[D loss: 0.771167] [G loss: 1.723504]\n",
      "[D loss: 0.599802] [G loss: 1.859496]\n",
      "[D loss: 0.910431] [G loss: 1.284454]\n",
      "[D loss: 0.784259] [G loss: 1.689598]\n",
      "[D loss: 0.683824] [G loss: 1.496370]\n",
      "[D loss: 0.518755] [G loss: 1.655479]\n",
      "[D loss: 0.849562] [G loss: 1.445495]\n",
      "[D loss: 0.775127] [G loss: 1.634728]\n",
      "[D loss: 0.852302] [G loss: 1.435847]\n",
      "[D loss: 0.847973] [G loss: 1.717579]\n",
      "[D loss: 0.812944] [G loss: 1.543669]\n",
      "[D loss: 0.811736] [G loss: 1.647135]\n",
      "[D loss: 0.852342] [G loss: 1.682225]\n",
      "[D loss: 0.920097] [G loss: 1.620542]\n",
      "[D loss: 0.938552] [G loss: 1.531680]\n",
      "[D loss: 0.643973] [G loss: 1.577630]\n",
      "[D loss: 0.668629] [G loss: 1.598594]\n",
      "[D loss: 0.623711] [G loss: 1.572502]\n",
      "[D loss: 0.974731] [G loss: 1.430034]\n",
      "[D loss: 0.650553] [G loss: 1.489579]\n",
      "[D loss: 0.971598] [G loss: 1.358000]\n",
      "[D loss: 0.846908] [G loss: 1.077048]\n",
      "[D loss: 1.077137] [G loss: 1.590809]\n",
      "[D loss: 0.771371] [G loss: 1.643940]\n",
      "[D loss: 0.773615] [G loss: 1.620163]\n",
      "[D loss: 0.769944] [G loss: 1.492355]\n",
      "[D loss: 0.597703] [G loss: 1.429740]\n",
      "[D loss: 0.820572] [G loss: 1.442285]\n",
      "[D loss: 0.774459] [G loss: 1.474286]\n",
      "[D loss: 1.040073] [G loss: 1.332317]\n",
      "[D loss: 0.700456] [G loss: 1.627867]\n",
      "[D loss: 0.686545] [G loss: 1.695287]\n",
      "[D loss: 0.956455] [G loss: 1.303761]\n",
      "[D loss: 0.904185] [G loss: 1.457785]\n",
      "[D loss: 0.794106] [G loss: 1.475804]\n",
      "[D loss: 0.689685] [G loss: 1.644735]\n",
      "[D loss: 0.771958] [G loss: 1.559676]\n",
      "[D loss: 0.615712] [G loss: 1.574609]\n",
      "[D loss: 0.866069] [G loss: 1.451455]\n",
      "[D loss: 0.690434] [G loss: 1.363520]\n",
      "[D loss: 0.699217] [G loss: 1.495605]\n",
      "[D loss: 0.669686] [G loss: 1.608052]\n",
      "[D loss: 0.674620] [G loss: 1.637623]\n",
      "[D loss: 0.763821] [G loss: 1.717273]\n",
      "[D loss: 0.748279] [G loss: 1.441316]\n",
      "[D loss: 0.963388] [G loss: 1.446772]\n",
      "[D loss: 0.909859] [G loss: 1.455494]\n",
      "[D loss: 0.976276] [G loss: 1.345387]\n",
      "[D loss: 0.751357] [G loss: 1.549030]\n",
      "[D loss: 0.879994] [G loss: 1.617561]\n",
      "[D loss: 0.871871] [G loss: 1.609730]\n",
      "[D loss: 0.828840] [G loss: 1.459785]\n",
      "[D loss: 0.703165] [G loss: 1.529920]\n",
      "[D loss: 0.805255] [G loss: 1.448473]\n",
      "[D loss: 0.802787] [G loss: 1.353235]\n",
      "[D loss: 0.906527] [G loss: 1.402543]\n",
      "[D loss: 0.852737] [G loss: 1.210806]\n",
      "[D loss: 0.859587] [G loss: 1.294597]\n",
      "[D loss: 0.811381] [G loss: 1.632764]\n",
      "[D loss: 0.672558] [G loss: 1.508924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.783523] [G loss: 1.447380]\n",
      "[D loss: 0.871793] [G loss: 1.781896]\n",
      "[D loss: 0.895167] [G loss: 1.600036]\n",
      "[D loss: 0.920332] [G loss: 1.494029]\n",
      "[D loss: 0.944886] [G loss: 1.164880]\n",
      "[D loss: 0.739779] [G loss: 1.344595]\n",
      "[D loss: 0.976026] [G loss: 1.365953]\n",
      "[D loss: 0.579684] [G loss: 1.626950]\n",
      "[D loss: 0.868559] [G loss: 1.724922]\n",
      "[D loss: 0.791733] [G loss: 1.504272]\n",
      "[D loss: 1.056700] [G loss: 1.424048]\n",
      "[D loss: 0.857128] [G loss: 1.473821]\n",
      "[D loss: 0.753629] [G loss: 1.343632]\n",
      "[D loss: 0.722149] [G loss: 1.647152]\n",
      "[D loss: 0.799465] [G loss: 1.702811]\n",
      "[D loss: 0.866449] [G loss: 1.523727]\n",
      "[D loss: 0.662975] [G loss: 1.787249]\n",
      "[D loss: 0.774477] [G loss: 1.575321]\n",
      "[D loss: 0.809682] [G loss: 1.522574]\n",
      "[D loss: 0.858467] [G loss: 1.619092]\n",
      "[D loss: 1.114713] [G loss: 1.337445]\n",
      "[D loss: 0.761231] [G loss: 1.721314]\n",
      "[D loss: 0.713499] [G loss: 1.476833]\n",
      "[D loss: 0.790004] [G loss: 1.244636]\n",
      "[D loss: 0.763559] [G loss: 1.533810]\n",
      "[D loss: 0.661590] [G loss: 1.729287]\n",
      "[D loss: 0.771927] [G loss: 1.460281]\n",
      "[D loss: 0.632505] [G loss: 1.399117]\n",
      "[D loss: 0.682191] [G loss: 1.429973]\n",
      "[D loss: 0.628981] [G loss: 1.592459]\n",
      "[D loss: 0.698897] [G loss: 1.658257]\n",
      "[D loss: 0.916064] [G loss: 1.592423]\n",
      "[D loss: 0.790523] [G loss: 1.568378]\n",
      "[D loss: 1.033379] [G loss: 1.191342]\n",
      "[D loss: 0.877483] [G loss: 1.676393]\n",
      "[D loss: 0.737605] [G loss: 1.523336]\n",
      "[D loss: 0.750413] [G loss: 1.796834]\n",
      "[D loss: 0.628634] [G loss: 1.626000]\n",
      "[D loss: 0.801915] [G loss: 1.329711]\n",
      "[D loss: 0.729878] [G loss: 1.403092]\n",
      "[D loss: 0.824675] [G loss: 1.306194]\n",
      "[D loss: 0.862484] [G loss: 1.776744]\n",
      "[D loss: 0.857240] [G loss: 1.521394]\n",
      "[D loss: 0.615525] [G loss: 1.676553]\n",
      "[D loss: 0.906712] [G loss: 1.619769]\n",
      "[D loss: 0.909460] [G loss: 1.588372]\n",
      "[D loss: 0.576122] [G loss: 1.594050]\n",
      "[D loss: 0.905582] [G loss: 1.470026]\n",
      "[D loss: 0.685335] [G loss: 1.762781]\n",
      "[D loss: 0.838371] [G loss: 1.558442]\n",
      "[D loss: 0.843048] [G loss: 1.717621]\n",
      "[D loss: 0.901561] [G loss: 1.388217]\n",
      "[D loss: 0.911334] [G loss: 1.420361]\n",
      "[D loss: 0.897968] [G loss: 1.606661]\n",
      "[D loss: 0.938041] [G loss: 1.108421]\n",
      "[D loss: 0.849558] [G loss: 1.487226]\n",
      "[D loss: 0.853505] [G loss: 1.612983]\n",
      "[D loss: 0.842892] [G loss: 1.906310]\n",
      "[D loss: 0.859819] [G loss: 1.456531]\n",
      "[D loss: 0.758531] [G loss: 1.418479]\n",
      "[D loss: 0.832493] [G loss: 1.415322]\n",
      "[D loss: 0.770594] [G loss: 1.594431]\n",
      "[D loss: 0.750219] [G loss: 1.673604]\n",
      "[D loss: 0.870845] [G loss: 1.610352]\n",
      "[D loss: 0.929913] [G loss: 1.665600]\n",
      "[D loss: 0.830838] [G loss: 1.508124]\n",
      "[D loss: 0.798573] [G loss: 1.415818]\n",
      "[D loss: 0.523953] [G loss: 1.521421]\n",
      "[D loss: 0.843127] [G loss: 1.644502]\n",
      "[D loss: 0.802883] [G loss: 1.579595]\n",
      "[D loss: 0.766762] [G loss: 1.494772]\n",
      "[D loss: 0.725997] [G loss: 1.518937]\n",
      "[D loss: 0.873129] [G loss: 1.534547]\n",
      "[D loss: 0.747159] [G loss: 1.548252]\n",
      "[D loss: 0.907473] [G loss: 1.519095]\n",
      "[D loss: 0.905614] [G loss: 1.561925]\n",
      "[D loss: 0.535245] [G loss: 1.449757]\n",
      "[D loss: 0.826037] [G loss: 1.518813]\n",
      "[D loss: 0.620989] [G loss: 1.641674]\n",
      "[D loss: 0.640140] [G loss: 1.698737]\n",
      "[D loss: 0.760061] [G loss: 1.418422]\n",
      "[D loss: 0.738954] [G loss: 1.577014]\n",
      "[D loss: 1.039943] [G loss: 1.490379]\n",
      "[D loss: 0.669992] [G loss: 1.583654]\n",
      "[D loss: 0.656637] [G loss: 1.733655]\n",
      "[D loss: 0.738521] [G loss: 1.473536]\n",
      "[D loss: 0.975889] [G loss: 1.678142]\n",
      "[D loss: 0.833423] [G loss: 1.695800]\n",
      "[D loss: 0.870982] [G loss: 1.494246]\n",
      "[D loss: 0.695712] [G loss: 1.518924]\n",
      "[D loss: 0.756602] [G loss: 1.348789]\n",
      "[D loss: 0.873155] [G loss: 1.784452]\n",
      "[D loss: 0.917987] [G loss: 1.590583]\n",
      "[D loss: 1.013620] [G loss: 1.584373]\n",
      "[D loss: 0.651941] [G loss: 1.410951]\n",
      "[D loss: 0.772564] [G loss: 1.305120]\n",
      "[D loss: 0.732398] [G loss: 1.441359]\n",
      "[D loss: 0.732525] [G loss: 1.731652]\n",
      "[D loss: 0.671296] [G loss: 1.596380]\n",
      "[D loss: 0.883244] [G loss: 1.413516]\n",
      "[D loss: 0.845962] [G loss: 1.461738]\n",
      "[D loss: 0.617720] [G loss: 1.528987]\n",
      "[D loss: 1.016495] [G loss: 1.620918]\n",
      "[D loss: 0.798213] [G loss: 1.515031]\n",
      "[D loss: 0.990244] [G loss: 1.243252]\n",
      "[D loss: 0.782823] [G loss: 1.707982]\n",
      "[D loss: 0.762316] [G loss: 1.681118]\n",
      "[D loss: 0.587245] [G loss: 1.729007]\n",
      "[D loss: 0.913852] [G loss: 1.554402]\n",
      "[D loss: 0.830553] [G loss: 1.745078]\n",
      "[D loss: 0.728446] [G loss: 1.589320]\n",
      "[D loss: 0.974685] [G loss: 1.412579]\n",
      "[D loss: 0.744232] [G loss: 1.321164]\n",
      "[D loss: 0.677823] [G loss: 1.461081]\n",
      "[D loss: 0.620810] [G loss: 1.630715]\n",
      "[D loss: 0.645017] [G loss: 1.774674]\n",
      "[D loss: 0.818183] [G loss: 1.552104]\n",
      "[D loss: 0.894753] [G loss: 1.624728]\n",
      "[D loss: 0.628550] [G loss: 1.604265]\n",
      "[D loss: 0.894506] [G loss: 1.400180]\n",
      "[D loss: 0.862691] [G loss: 1.514987]\n",
      "[D loss: 0.882377] [G loss: 1.419712]\n",
      "[D loss: 0.756570] [G loss: 1.555062]\n",
      "[D loss: 0.646502] [G loss: 1.474204]\n",
      "[D loss: 0.991860] [G loss: 1.451875]\n",
      "[D loss: 0.660943] [G loss: 1.437874]\n",
      "[D loss: 0.694726] [G loss: 1.468461]\n",
      "[D loss: 1.033947] [G loss: 1.412468]\n",
      "[D loss: 0.855476] [G loss: 1.672486]\n",
      "[D loss: 0.681985] [G loss: 1.623185]\n",
      "[D loss: 0.752477] [G loss: 1.730594]\n",
      "[D loss: 0.703710] [G loss: 1.353960]\n",
      "[D loss: 0.913705] [G loss: 1.407539]\n",
      "[D loss: 0.996790] [G loss: 1.253032]\n",
      "[D loss: 0.756666] [G loss: 1.532516]\n",
      "[D loss: 0.774724] [G loss: 1.487934]\n",
      "[D loss: 0.525170] [G loss: 1.663091]\n",
      "[D loss: 0.826207] [G loss: 1.624791]\n",
      "[D loss: 0.507325] [G loss: 1.826046]\n",
      "[D loss: 0.785842] [G loss: 1.779805]\n",
      "[D loss: 1.182732] [G loss: 1.374472]\n",
      "[D loss: 0.635224] [G loss: 1.870318]\n",
      "[D loss: 0.799533] [G loss: 1.535039]\n",
      "[D loss: 0.815195] [G loss: 1.662229]\n",
      "[D loss: 0.862241] [G loss: 1.456513]\n",
      "[D loss: 0.735731] [G loss: 1.281848]\n",
      "[D loss: 0.716046] [G loss: 1.456465]\n",
      "[D loss: 0.947514] [G loss: 1.770110]\n",
      "[D loss: 0.665442] [G loss: 1.694836]\n",
      "[D loss: 0.985231] [G loss: 1.575207]\n",
      "[D loss: 0.770148] [G loss: 1.707827]\n",
      "[D loss: 0.867907] [G loss: 1.461981]\n",
      "[D loss: 0.845723] [G loss: 1.599089]\n",
      "[D loss: 1.139269] [G loss: 1.533697]\n",
      "[D loss: 0.835948] [G loss: 1.501409]\n",
      "[D loss: 0.966695] [G loss: 1.546523]\n",
      "[D loss: 0.755043] [G loss: 1.472612]\n",
      "[D loss: 0.661674] [G loss: 1.287463]\n",
      "[D loss: 0.810587] [G loss: 1.586154]\n",
      "[D loss: 0.932135] [G loss: 1.508100]\n",
      "[D loss: 0.687761] [G loss: 1.590208]\n",
      "[D loss: 0.774748] [G loss: 1.435983]\n",
      "[D loss: 0.862299] [G loss: 1.213368]\n",
      "[D loss: 0.765262] [G loss: 1.470312]\n",
      "[D loss: 0.778546] [G loss: 1.401961]\n",
      "[D loss: 0.636515] [G loss: 1.626707]\n",
      "[D loss: 0.866335] [G loss: 1.420683]\n",
      "[D loss: 0.596435] [G loss: 1.518707]\n",
      "[D loss: 0.773596] [G loss: 1.381932]\n",
      "[D loss: 0.803093] [G loss: 1.637626]\n",
      "[D loss: 0.784030] [G loss: 1.429705]\n",
      "[D loss: 0.796690] [G loss: 1.380316]\n",
      "[D loss: 0.846543] [G loss: 1.376985]\n",
      "[D loss: 0.844220] [G loss: 1.614380]\n",
      "[D loss: 1.037408] [G loss: 1.577990]\n",
      "[D loss: 0.753431] [G loss: 1.518341]\n",
      "[D loss: 0.795378] [G loss: 1.515016]\n",
      "[D loss: 0.778013] [G loss: 1.447377]\n",
      "[D loss: 0.736992] [G loss: 1.818369]\n",
      "[D loss: 0.752189] [G loss: 1.611700]\n",
      "[D loss: 0.857763] [G loss: 1.485995]\n",
      "[D loss: 0.942995] [G loss: 1.345889]\n",
      "[D loss: 0.921796] [G loss: 1.520749]\n",
      "[D loss: 0.886070] [G loss: 1.714775]\n",
      "[D loss: 0.796584] [G loss: 1.595070]\n",
      "[D loss: 0.770673] [G loss: 1.555252]\n",
      "[D loss: 0.705661] [G loss: 1.667765]\n",
      "[D loss: 0.978229] [G loss: 1.525776]\n",
      "[D loss: 1.053546] [G loss: 0.970672]\n",
      "[D loss: 1.087231] [G loss: 1.347434]\n",
      "[D loss: 0.736042] [G loss: 1.734998]\n",
      "[D loss: 0.650221] [G loss: 1.572310]\n",
      "[D loss: 0.920868] [G loss: 1.479162]\n",
      "[D loss: 0.598766] [G loss: 1.477583]\n",
      "[D loss: 0.796625] [G loss: 1.505197]\n",
      "[D loss: 1.020423] [G loss: 1.295720]\n",
      "[D loss: 0.665767] [G loss: 1.613905]\n",
      "[D loss: 0.728380] [G loss: 1.505209]\n",
      "[D loss: 0.532503] [G loss: 1.439057]\n",
      "[D loss: 0.823645] [G loss: 1.610886]\n",
      "[D loss: 0.844031] [G loss: 1.483779]\n",
      "[D loss: 0.841542] [G loss: 1.564662]\n",
      "[D loss: 0.841348] [G loss: 1.464512]\n",
      "[D loss: 0.774516] [G loss: 1.609486]\n",
      "[D loss: 0.648380] [G loss: 1.610840]\n",
      "[D loss: 0.962571] [G loss: 1.634038]\n",
      "[D loss: 0.577604] [G loss: 1.520322]\n",
      "[D loss: 0.838186] [G loss: 1.624924]\n",
      "[D loss: 0.700143] [G loss: 1.439654]\n",
      "[D loss: 0.766111] [G loss: 1.985446]\n",
      "[D loss: 0.749021] [G loss: 1.449407]\n",
      "[D loss: 0.744413] [G loss: 1.512999]\n",
      "[D loss: 0.903217] [G loss: 1.571051]\n",
      "[D loss: 0.911780] [G loss: 1.359379]\n",
      "[D loss: 0.918742] [G loss: 1.341227]\n",
      "[D loss: 0.915113] [G loss: 1.686934]\n",
      "[D loss: 0.888113] [G loss: 1.640754]\n",
      "[D loss: 0.675120] [G loss: 1.540689]\n",
      "[D loss: 0.864049] [G loss: 1.511004]\n",
      "[D loss: 0.771055] [G loss: 1.427696]\n",
      "[D loss: 0.780521] [G loss: 1.414113]\n",
      "[D loss: 0.644752] [G loss: 1.606169]\n",
      "[D loss: 0.930976] [G loss: 1.735567]\n",
      "[D loss: 0.870426] [G loss: 1.556384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.896951] [G loss: 1.357461]\n",
      "[D loss: 0.878522] [G loss: 1.471205]\n",
      "[D loss: 0.576082] [G loss: 1.749770]\n",
      "[D loss: 0.779530] [G loss: 1.467951]\n",
      "[D loss: 1.052628] [G loss: 1.289751]\n",
      "[D loss: 0.968998] [G loss: 1.366400]\n",
      "[D loss: 0.918018] [G loss: 1.769096]\n",
      "[D loss: 0.905540] [G loss: 1.688558]\n",
      "[D loss: 0.864377] [G loss: 1.349608]\n",
      "[D loss: 0.633920] [G loss: 1.506592]\n",
      "[D loss: 0.824676] [G loss: 1.447160]\n",
      "[D loss: 0.700727] [G loss: 1.507386]\n",
      "[D loss: 0.873463] [G loss: 1.300526]\n",
      "[D loss: 1.190533] [G loss: 1.481101]\n",
      "[D loss: 0.747268] [G loss: 1.659663]\n",
      "[D loss: 0.718154] [G loss: 1.945162]\n",
      "[D loss: 0.908384] [G loss: 1.781689]\n",
      "[D loss: 0.710492] [G loss: 1.547642]\n",
      "[D loss: 0.863236] [G loss: 1.451361]\n",
      "[D loss: 0.758752] [G loss: 1.461667]\n",
      "[D loss: 0.744703] [G loss: 1.534406]\n",
      "[D loss: 1.026930] [G loss: 1.538804]\n",
      "[D loss: 0.864700] [G loss: 1.335779]\n",
      "[D loss: 0.679625] [G loss: 1.630289]\n",
      "[D loss: 0.619585] [G loss: 1.507167]\n",
      "[D loss: 0.696791] [G loss: 1.478409]\n",
      "[D loss: 0.775286] [G loss: 1.515284]\n",
      "[D loss: 0.675901] [G loss: 1.527345]\n",
      "[D loss: 0.627026] [G loss: 1.799033]\n",
      "[D loss: 0.861695] [G loss: 1.619459]\n",
      "[D loss: 1.139497] [G loss: 1.600729]\n",
      "[D loss: 0.909358] [G loss: 1.475203]\n",
      "[D loss: 0.724199] [G loss: 1.771870]\n",
      "[D loss: 0.817964] [G loss: 1.638208]\n",
      "[D loss: 0.812827] [G loss: 1.688102]\n",
      "[D loss: 1.026868] [G loss: 1.414953]\n",
      "[D loss: 0.762957] [G loss: 1.597835]\n",
      "[D loss: 0.756597] [G loss: 1.390579]\n",
      "[D loss: 1.043676] [G loss: 1.229592]\n",
      "[D loss: 0.838534] [G loss: 1.329106]\n",
      "[D loss: 0.738442] [G loss: 1.559746]\n",
      "[D loss: 0.863252] [G loss: 1.330539]\n",
      "[D loss: 0.777292] [G loss: 1.755113]\n",
      "[D loss: 0.925279] [G loss: 1.472414]\n",
      "[D loss: 0.979753] [G loss: 1.449838]\n",
      "[D loss: 0.861760] [G loss: 1.294513]\n",
      "[D loss: 1.000781] [G loss: 1.293984]\n",
      "[D loss: 0.790003] [G loss: 1.391380]\n",
      "[D loss: 0.677849] [G loss: 1.404672]\n",
      "[D loss: 0.820817] [G loss: 1.527838]\n",
      "[D loss: 0.804397] [G loss: 1.401002]\n",
      "[D loss: 0.793822] [G loss: 1.339251]\n",
      "[D loss: 0.829951] [G loss: 1.257361]\n",
      "[D loss: 0.830136] [G loss: 1.350421]\n",
      "[D loss: 0.809049] [G loss: 1.414757]\n",
      "[D loss: 0.691889] [G loss: 1.580505]\n",
      "[D loss: 0.967502] [G loss: 1.841205]\n",
      "[D loss: 0.710902] [G loss: 1.521256]\n",
      "[D loss: 0.879325] [G loss: 1.267350]\n",
      "[D loss: 0.846070] [G loss: 1.487357]\n",
      "[D loss: 0.706169] [G loss: 1.434136]\n",
      "[D loss: 0.773685] [G loss: 1.526863]\n",
      "[D loss: 0.729218] [G loss: 1.776531]\n",
      "[D loss: 0.605166] [G loss: 1.552781]\n",
      "[D loss: 1.186877] [G loss: 1.140994]\n",
      "[D loss: 0.571993] [G loss: 1.518001]\n",
      "[D loss: 0.784431] [G loss: 1.573887]\n",
      "[D loss: 0.821032] [G loss: 1.684221]\n",
      "[D loss: 0.864079] [G loss: 1.625822]\n",
      "[D loss: 0.711864] [G loss: 1.557058]\n",
      "[D loss: 0.805786] [G loss: 1.719003]\n",
      "[D loss: 0.979249] [G loss: 1.424040]\n",
      "[D loss: 0.825371] [G loss: 1.570157]\n",
      "[D loss: 0.943972] [G loss: 1.357440]\n",
      "[D loss: 0.891031] [G loss: 1.380694]\n",
      "[D loss: 0.593318] [G loss: 1.696395]\n",
      "[D loss: 1.114970] [G loss: 1.708460]\n",
      "[D loss: 0.661584] [G loss: 1.937296]\n",
      "[D loss: 0.861709] [G loss: 1.612819]\n",
      "[D loss: 0.968534] [G loss: 1.677031]\n",
      "[D loss: 0.755799] [G loss: 1.458819]\n",
      "[D loss: 0.726635] [G loss: 1.454508]\n",
      "[D loss: 0.849596] [G loss: 1.479667]\n",
      "[D loss: 0.715067] [G loss: 1.468883]\n",
      "[D loss: 0.877113] [G loss: 1.603038]\n",
      "[D loss: 0.693888] [G loss: 1.872821]\n",
      "[D loss: 0.728030] [G loss: 1.809312]\n",
      "[D loss: 0.692823] [G loss: 1.596715]\n",
      "[D loss: 0.786743] [G loss: 1.591593]\n",
      "[D loss: 0.740785] [G loss: 1.442542]\n",
      "[D loss: 0.858255] [G loss: 1.654220]\n",
      "[D loss: 0.774585] [G loss: 1.479518]\n",
      "[D loss: 0.642360] [G loss: 1.484065]\n",
      "[D loss: 0.502727] [G loss: 1.545315]\n",
      "[D loss: 0.869594] [G loss: 1.537548]\n",
      "[D loss: 0.721579] [G loss: 1.539870]\n",
      "[D loss: 0.755522] [G loss: 1.416342]\n",
      "[D loss: 0.615841] [G loss: 1.629832]\n",
      "[D loss: 0.734798] [G loss: 1.591464]\n",
      "[D loss: 0.837946] [G loss: 1.637875]\n",
      "[D loss: 0.843892] [G loss: 1.654363]\n",
      "[D loss: 0.549710] [G loss: 1.609383]\n",
      "[D loss: 0.750159] [G loss: 1.375220]\n",
      "[D loss: 0.527482] [G loss: 1.518738]\n",
      "[D loss: 0.750915] [G loss: 1.678348]\n",
      "[D loss: 0.780192] [G loss: 1.697159]\n",
      "[D loss: 0.687092] [G loss: 1.793802]\n",
      "[D loss: 0.956726] [G loss: 1.619435]\n",
      "[D loss: 1.002932] [G loss: 1.494987]\n",
      "[D loss: 0.856416] [G loss: 1.698395]\n",
      "[D loss: 0.734737] [G loss: 1.481563]\n",
      "[D loss: 0.684824] [G loss: 1.633556]\n",
      "[D loss: 0.621854] [G loss: 1.502545]\n",
      "[D loss: 0.936260] [G loss: 1.412325]\n",
      "[D loss: 0.687139] [G loss: 1.583367]\n",
      "[D loss: 0.919451] [G loss: 1.299361]\n",
      "[D loss: 0.586143] [G loss: 1.386375]\n",
      "[D loss: 0.880865] [G loss: 1.811832]\n",
      "[D loss: 1.018023] [G loss: 1.519220]\n",
      "[D loss: 0.700629] [G loss: 1.561501]\n",
      "[D loss: 0.794657] [G loss: 1.652313]\n",
      "[D loss: 1.047737] [G loss: 1.222804]\n",
      "[D loss: 0.949062] [G loss: 1.348930]\n",
      "[D loss: 0.513889] [G loss: 1.493724]\n",
      "[D loss: 1.023785] [G loss: 1.574043]\n",
      "[D loss: 0.637961] [G loss: 1.548537]\n",
      "[D loss: 1.030326] [G loss: 1.270078]\n",
      "[D loss: 0.753633] [G loss: 1.765880]\n",
      "[D loss: 0.782504] [G loss: 1.506970]\n",
      "[D loss: 0.822866] [G loss: 1.714564]\n",
      "[D loss: 0.922184] [G loss: 1.599505]\n",
      "[D loss: 0.874850] [G loss: 1.471386]\n",
      "[D loss: 0.774003] [G loss: 1.656517]\n",
      "[D loss: 0.870283] [G loss: 1.633026]\n",
      "[D loss: 0.810929] [G loss: 1.340986]\n",
      "[D loss: 0.863700] [G loss: 1.337467]\n",
      "[D loss: 0.893231] [G loss: 1.445974]\n",
      "[D loss: 0.883393] [G loss: 1.389340]\n",
      "[D loss: 0.653832] [G loss: 1.594074]\n",
      "[D loss: 0.717930] [G loss: 1.433488]\n",
      "[D loss: 0.904619] [G loss: 1.657077]\n",
      "[D loss: 0.934642] [G loss: 1.626444]\n",
      "[D loss: 0.934911] [G loss: 1.428176]\n",
      "[D loss: 0.766249] [G loss: 1.648140]\n",
      "[D loss: 0.911405] [G loss: 1.501871]\n",
      "[D loss: 0.757197] [G loss: 1.497927]\n",
      "[D loss: 0.777048] [G loss: 1.369479]\n",
      "[D loss: 0.868931] [G loss: 1.488729]\n",
      "[D loss: 0.843048] [G loss: 1.480613]\n",
      "[D loss: 0.659997] [G loss: 1.766789]\n",
      "[D loss: 0.715616] [G loss: 1.658211]\n",
      "[D loss: 0.994125] [G loss: 1.410441]\n",
      "[D loss: 0.710894] [G loss: 1.428235]\n",
      "[D loss: 0.827976] [G loss: 1.581645]\n",
      "[D loss: 0.862300] [G loss: 1.295976]\n",
      "[D loss: 0.917289] [G loss: 1.458532]\n",
      "[D loss: 0.894020] [G loss: 1.353565]\n",
      "[D loss: 0.827292] [G loss: 1.710386]\n",
      "[D loss: 0.928490] [G loss: 1.646399]\n",
      "[D loss: 0.747753] [G loss: 1.548856]\n",
      "[D loss: 0.599418] [G loss: 1.552382]\n",
      "[D loss: 0.952377] [G loss: 1.583399]\n",
      "[D loss: 0.975382] [G loss: 1.282946]\n",
      "[D loss: 0.751221] [G loss: 1.551239]\n",
      "[D loss: 0.723463] [G loss: 1.593030]\n",
      "[D loss: 0.997220] [G loss: 1.243015]\n",
      "[D loss: 0.871039] [G loss: 1.527703]\n",
      "[D loss: 1.005430] [G loss: 1.400851]\n",
      "[D loss: 0.688860] [G loss: 1.444837]\n",
      "[D loss: 0.795874] [G loss: 1.366579]\n",
      "[D loss: 0.958574] [G loss: 1.412647]\n",
      "[D loss: 0.903206] [G loss: 1.607154]\n",
      "[D loss: 0.638251] [G loss: 1.462310]\n",
      "[D loss: 0.716711] [G loss: 1.612653]\n",
      "[D loss: 0.615021] [G loss: 1.662563]\n",
      "[D loss: 0.976705] [G loss: 1.282920]\n",
      "[D loss: 0.836082] [G loss: 1.830856]\n",
      "[D loss: 0.875195] [G loss: 1.422287]\n",
      "[D loss: 0.915977] [G loss: 1.447820]\n",
      "[D loss: 0.843349] [G loss: 1.491872]\n",
      "[D loss: 0.740130] [G loss: 1.415783]\n",
      "[D loss: 0.733381] [G loss: 1.403880]\n",
      "[D loss: 0.880663] [G loss: 1.710951]\n",
      "[D loss: 0.967289] [G loss: 1.662807]\n",
      "[D loss: 0.875055] [G loss: 1.609632]\n",
      "[D loss: 0.903977] [G loss: 1.534153]\n",
      "[D loss: 0.847033] [G loss: 1.314338]\n",
      "[D loss: 0.687290] [G loss: 1.458841]\n",
      "[D loss: 1.132202] [G loss: 1.361970]\n",
      "[D loss: 0.816242] [G loss: 1.476952]\n",
      "[D loss: 0.587783] [G loss: 1.617627]\n",
      "[D loss: 0.883854] [G loss: 1.445560]\n",
      "[D loss: 0.903658] [G loss: 1.538633]\n",
      "[D loss: 0.972511] [G loss: 1.237518]\n",
      "[D loss: 0.821072] [G loss: 1.390671]\n",
      "[D loss: 0.563948] [G loss: 1.650969]\n",
      "[D loss: 0.735639] [G loss: 1.550229]\n",
      "[D loss: 0.753838] [G loss: 1.545401]\n",
      "[D loss: 0.655942] [G loss: 1.515516]\n",
      "[D loss: 0.937209] [G loss: 1.343139]\n",
      "[D loss: 0.867248] [G loss: 1.727374]\n",
      "[D loss: 0.746921] [G loss: 1.601186]\n",
      "[D loss: 0.755093] [G loss: 1.712424]\n",
      "[D loss: 0.786887] [G loss: 1.435976]\n",
      "[D loss: 0.562202] [G loss: 1.513984]\n",
      "[D loss: 0.686210] [G loss: 1.648188]\n",
      "[D loss: 0.739113] [G loss: 1.770792]\n",
      "[D loss: 0.632860] [G loss: 1.695379]\n",
      "[D loss: 0.769824] [G loss: 1.430800]\n",
      "[D loss: 0.686044] [G loss: 1.620131]\n",
      "[D loss: 0.835576] [G loss: 1.831211]\n",
      "[D loss: 0.698410] [G loss: 1.746868]\n",
      "[D loss: 0.828505] [G loss: 1.442706]\n",
      "[D loss: 1.034582] [G loss: 1.313238]\n",
      "[D loss: 0.772098] [G loss: 1.460052]\n",
      "[D loss: 0.628700] [G loss: 1.541340]\n",
      "[D loss: 0.695647] [G loss: 1.693654]\n",
      "[D loss: 0.760651] [G loss: 1.666936]\n",
      "[D loss: 0.857423] [G loss: 1.817061]\n",
      "[D loss: 1.059372] [G loss: 1.516731]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.868477] [G loss: 1.544946]\n",
      "[D loss: 0.915046] [G loss: 1.364012]\n",
      "[D loss: 0.910846] [G loss: 1.325734]\n",
      "[D loss: 0.892184] [G loss: 1.338958]\n",
      "[D loss: 0.662255] [G loss: 1.573752]\n",
      "[D loss: 0.963660] [G loss: 1.718451]\n",
      "[D loss: 0.822278] [G loss: 1.555946]\n",
      "[D loss: 0.828899] [G loss: 1.628205]\n",
      "[D loss: 0.805146] [G loss: 1.524716]\n",
      "[D loss: 0.919857] [G loss: 1.376349]\n",
      "[D loss: 0.752830] [G loss: 1.544310]\n",
      "[D loss: 0.937652] [G loss: 1.396233]\n",
      "[D loss: 0.767716] [G loss: 1.420454]\n",
      "[D loss: 1.075382] [G loss: 1.099593]\n",
      "[D loss: 0.929337] [G loss: 1.352410]\n",
      "[D loss: 0.927290] [G loss: 1.247151]\n",
      "[D loss: 0.714105] [G loss: 1.496048]\n",
      "[D loss: 0.915761] [G loss: 1.619393]\n",
      "[D loss: 0.724871] [G loss: 1.482000]\n",
      "[D loss: 0.985690] [G loss: 1.278984]\n",
      "[D loss: 0.825379] [G loss: 1.313066]\n",
      "[D loss: 0.731664] [G loss: 1.503429]\n",
      "[D loss: 0.717086] [G loss: 1.372238]\n",
      "[D loss: 0.793579] [G loss: 1.496437]\n",
      "[D loss: 0.977619] [G loss: 1.313283]\n",
      "[D loss: 0.639734] [G loss: 1.399459]\n",
      "[D loss: 0.946016] [G loss: 1.210054]\n",
      "[D loss: 0.882817] [G loss: 1.544062]\n",
      "[D loss: 0.694277] [G loss: 1.558619]\n",
      "[D loss: 0.833914] [G loss: 1.452241]\n",
      "[D loss: 0.722548] [G loss: 1.548313]\n",
      "[D loss: 0.450304] [G loss: 1.437592]\n",
      "[D loss: 0.793579] [G loss: 1.529404]\n",
      "[D loss: 0.888531] [G loss: 1.500298]\n",
      "[D loss: 1.015435] [G loss: 1.533177]\n",
      "[D loss: 0.729837] [G loss: 1.354055]\n",
      "[D loss: 0.803231] [G loss: 1.734473]\n",
      "[D loss: 0.701539] [G loss: 1.553207]\n",
      "[D loss: 0.789880] [G loss: 1.332255]\n",
      "[D loss: 1.018399] [G loss: 1.366895]\n",
      "[D loss: 0.832538] [G loss: 1.630057]\n",
      "[D loss: 0.711135] [G loss: 1.733594]\n",
      "[D loss: 0.940094] [G loss: 1.754642]\n",
      "[D loss: 0.965629] [G loss: 1.452958]\n",
      "[D loss: 0.776954] [G loss: 1.206205]\n",
      "[D loss: 0.894376] [G loss: 1.429311]\n",
      "[D loss: 0.623116] [G loss: 1.577638]\n",
      "[D loss: 0.900372] [G loss: 1.420691]\n",
      "[D loss: 0.804705] [G loss: 1.588961]\n",
      "[D loss: 0.668340] [G loss: 1.346283]\n",
      "[D loss: 1.115351] [G loss: 1.137434]\n",
      "[D loss: 0.716414] [G loss: 1.373006]\n",
      "[D loss: 0.809058] [G loss: 1.382115]\n",
      "[D loss: 0.703495] [G loss: 1.706591]\n",
      "[D loss: 0.998119] [G loss: 1.762065]\n",
      "[D loss: 0.837241] [G loss: 1.528592]\n",
      "[D loss: 0.795768] [G loss: 1.499292]\n",
      "[D loss: 0.763869] [G loss: 1.396671]\n",
      "[D loss: 0.777207] [G loss: 1.329249]\n",
      "[D loss: 0.729628] [G loss: 1.483932]\n",
      "[D loss: 0.833947] [G loss: 1.609860]\n",
      "[D loss: 0.620025] [G loss: 1.646760]\n",
      "[D loss: 0.954017] [G loss: 1.773894]\n",
      "[D loss: 1.031533] [G loss: 1.363226]\n",
      "[D loss: 0.583113] [G loss: 1.624354]\n",
      "[D loss: 0.713648] [G loss: 1.699286]\n",
      "[D loss: 0.841265] [G loss: 1.298071]\n",
      "[D loss: 0.671684] [G loss: 1.311110]\n",
      "[D loss: 0.742807] [G loss: 1.616327]\n",
      "[D loss: 0.741766] [G loss: 1.815254]\n",
      "[D loss: 0.889859] [G loss: 1.671573]\n",
      "[D loss: 0.769260] [G loss: 1.698807]\n",
      "[D loss: 0.723643] [G loss: 1.588004]\n",
      "[D loss: 1.059778] [G loss: 1.536615]\n",
      "[D loss: 0.763952] [G loss: 1.404786]\n",
      "[D loss: 0.755216] [G loss: 1.570118]\n",
      "[D loss: 0.699250] [G loss: 1.775468]\n",
      "[D loss: 0.936378] [G loss: 1.521901]\n",
      "[D loss: 0.786502] [G loss: 1.395076]\n",
      "[D loss: 0.868741] [G loss: 1.523112]\n",
      "[D loss: 0.739071] [G loss: 1.603827]\n",
      "[D loss: 0.582864] [G loss: 1.874220]\n",
      "[D loss: 0.816490] [G loss: 1.302368]\n",
      "[D loss: 0.985493] [G loss: 1.471632]\n",
      "[D loss: 0.855627] [G loss: 1.584824]\n",
      "[D loss: 0.666900] [G loss: 1.569964]\n",
      "[D loss: 0.813311] [G loss: 1.418641]\n",
      "[D loss: 0.629503] [G loss: 1.617062]\n",
      "[D loss: 0.784553] [G loss: 1.659511]\n",
      "[D loss: 0.730533] [G loss: 1.556047]\n",
      "[D loss: 0.775106] [G loss: 1.598577]\n",
      "[D loss: 0.683992] [G loss: 1.637888]\n",
      "[D loss: 0.972942] [G loss: 1.358641]\n",
      "[D loss: 0.775555] [G loss: 1.398518]\n",
      "[D loss: 0.820007] [G loss: 1.658820]\n",
      "[D loss: 0.749624] [G loss: 1.539658]\n",
      "[D loss: 0.738346] [G loss: 1.806506]\n",
      "[D loss: 0.821845] [G loss: 1.782609]\n",
      "[D loss: 1.054604] [G loss: 1.463579]\n",
      "[D loss: 1.016494] [G loss: 1.262770]\n",
      "[D loss: 1.053626] [G loss: 1.398031]\n",
      "[D loss: 0.813216] [G loss: 1.435894]\n",
      "[D loss: 0.783242] [G loss: 1.481060]\n",
      "[D loss: 0.726111] [G loss: 1.381582]\n",
      "[D loss: 0.779215] [G loss: 1.532121]\n",
      "[D loss: 0.824578] [G loss: 1.652058]\n",
      "[D loss: 0.690112] [G loss: 1.467465]\n",
      "[D loss: 0.740340] [G loss: 1.364639]\n",
      "[D loss: 0.690462] [G loss: 1.449233]\n",
      "[D loss: 0.882537] [G loss: 1.328373]\n",
      "[D loss: 0.899855] [G loss: 1.498618]\n",
      "[D loss: 0.845793] [G loss: 1.626351]\n",
      "[D loss: 0.916895] [G loss: 1.478492]\n",
      "[D loss: 0.829819] [G loss: 1.336792]\n",
      "[D loss: 0.789976] [G loss: 1.441225]\n",
      "[D loss: 0.593132] [G loss: 1.780817]\n",
      "[D loss: 0.802642] [G loss: 1.423257]\n",
      "[D loss: 0.880980] [G loss: 1.507507]\n",
      "[D loss: 0.774481] [G loss: 1.516696]\n",
      "[D loss: 0.670770] [G loss: 1.719474]\n",
      "[D loss: 0.839899] [G loss: 1.432912]\n",
      "[D loss: 0.663183] [G loss: 1.505925]\n",
      "[D loss: 0.591137] [G loss: 1.698891]\n",
      "[D loss: 0.673085] [G loss: 1.317826]\n",
      "[D loss: 0.716952] [G loss: 1.666616]\n",
      "[D loss: 0.719404] [G loss: 1.695763]\n",
      "[D loss: 1.060892] [G loss: 1.460513]\n",
      "[D loss: 0.799034] [G loss: 1.563048]\n",
      "[D loss: 0.782648] [G loss: 1.601512]\n",
      "[D loss: 0.934716] [G loss: 1.398428]\n",
      "[D loss: 0.642382] [G loss: 1.726994]\n",
      "[D loss: 0.726403] [G loss: 1.370954]\n",
      "[D loss: 0.899891] [G loss: 1.545294]\n",
      "[D loss: 0.660457] [G loss: 1.508960]\n",
      "[D loss: 0.854644] [G loss: 1.562185]\n",
      "[D loss: 0.787497] [G loss: 1.722909]\n",
      "[D loss: 0.758703] [G loss: 1.767624]\n",
      "[D loss: 0.925828] [G loss: 1.670937]\n",
      "[D loss: 0.896803] [G loss: 1.627340]\n",
      "[D loss: 0.661232] [G loss: 1.397987]\n",
      "[D loss: 0.836089] [G loss: 1.516818]\n",
      "[D loss: 1.069807] [G loss: 1.611901]\n",
      "[D loss: 0.705799] [G loss: 1.635406]\n",
      "[D loss: 0.756331] [G loss: 1.492130]\n",
      "[D loss: 0.953256] [G loss: 1.224134]\n",
      "[D loss: 0.815725] [G loss: 1.359056]\n",
      "[D loss: 0.674211] [G loss: 1.416581]\n",
      "[D loss: 0.971216] [G loss: 1.478668]\n",
      "[D loss: 0.865844] [G loss: 1.229871]\n",
      "[D loss: 0.760766] [G loss: 1.682390]\n",
      "[D loss: 0.985879] [G loss: 1.555978]\n",
      "[D loss: 0.950082] [G loss: 1.564438]\n",
      "[D loss: 0.827158] [G loss: 1.456170]\n",
      "[D loss: 0.644096] [G loss: 1.397569]\n",
      "[D loss: 0.920609] [G loss: 1.425114]\n",
      "[D loss: 0.659019] [G loss: 1.545758]\n",
      "[D loss: 1.101437] [G loss: 1.400627]\n",
      "[D loss: 0.554192] [G loss: 1.455242]\n",
      "[D loss: 1.046536] [G loss: 1.347630]\n",
      "[D loss: 0.663595] [G loss: 1.619287]\n",
      "[D loss: 0.943796] [G loss: 1.542285]\n",
      "[D loss: 0.950914] [G loss: 1.600149]\n",
      "[D loss: 0.922399] [G loss: 1.241260]\n",
      "[D loss: 0.620476] [G loss: 1.544933]\n",
      "[D loss: 0.970048] [G loss: 1.310341]\n",
      "[D loss: 0.722793] [G loss: 1.626359]\n",
      "[D loss: 0.806555] [G loss: 1.525555]\n",
      "[D loss: 0.886054] [G loss: 1.571034]\n",
      "[D loss: 0.728551] [G loss: 1.716837]\n",
      "[D loss: 0.728192] [G loss: 1.722606]\n",
      "[D loss: 0.705674] [G loss: 1.480048]\n",
      "[D loss: 0.675609] [G loss: 1.511681]\n",
      "[D loss: 0.991082] [G loss: 1.584155]\n",
      "[D loss: 0.717613] [G loss: 1.698908]\n",
      "[D loss: 0.686041] [G loss: 1.585647]\n",
      "[D loss: 1.031887] [G loss: 1.401317]\n",
      "[D loss: 0.644042] [G loss: 1.465284]\n",
      "[D loss: 1.115468] [G loss: 1.557756]\n",
      "[D loss: 0.895140] [G loss: 1.413690]\n",
      "[D loss: 0.842328] [G loss: 1.487934]\n",
      "[D loss: 0.810297] [G loss: 1.288489]\n",
      "[D loss: 0.773609] [G loss: 1.691192]\n",
      "[D loss: 0.752683] [G loss: 1.714088]\n",
      "[D loss: 1.006629] [G loss: 1.476976]\n",
      "[D loss: 0.826795] [G loss: 1.603342]\n",
      "[D loss: 0.926189] [G loss: 1.354716]\n",
      "[D loss: 0.786415] [G loss: 1.340542]\n",
      "[D loss: 0.801915] [G loss: 1.396375]\n",
      "[D loss: 0.635574] [G loss: 1.410124]\n",
      "[D loss: 0.683697] [G loss: 1.413786]\n",
      "[D loss: 0.786440] [G loss: 1.526523]\n",
      "[D loss: 0.925481] [G loss: 1.387987]\n",
      "[D loss: 0.683185] [G loss: 1.403668]\n",
      "[D loss: 0.858579] [G loss: 1.567168]\n",
      "[D loss: 0.887478] [G loss: 1.573429]\n",
      "[D loss: 1.135959] [G loss: 1.378720]\n",
      "[D loss: 0.779897] [G loss: 1.512940]\n",
      "[D loss: 0.664498] [G loss: 1.754237]\n",
      "[D loss: 0.949399] [G loss: 1.537061]\n",
      "[D loss: 0.931442] [G loss: 1.491099]\n",
      "[D loss: 0.913460] [G loss: 1.600210]\n",
      "[D loss: 0.665354] [G loss: 1.514292]\n",
      "[D loss: 0.801084] [G loss: 1.194632]\n",
      "[D loss: 0.910644] [G loss: 1.533011]\n",
      "[D loss: 0.729732] [G loss: 1.455614]\n",
      "[D loss: 0.693258] [G loss: 1.494147]\n",
      "[D loss: 0.689589] [G loss: 1.582337]\n",
      "[D loss: 0.904432] [G loss: 1.549899]\n",
      "[D loss: 0.893756] [G loss: 1.060974]\n",
      "[D loss: 0.758095] [G loss: 1.541248]\n",
      "[D loss: 0.833750] [G loss: 1.512828]\n",
      "[D loss: 0.625449] [G loss: 1.441275]\n",
      "[D loss: 0.950573] [G loss: 1.428608]\n",
      "[D loss: 0.658993] [G loss: 1.520397]\n",
      "[D loss: 1.227654] [G loss: 1.402065]\n",
      "[D loss: 0.932121] [G loss: 1.646703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.949189] [G loss: 1.267311]\n",
      "[D loss: 1.005648] [G loss: 1.328058]\n",
      "[D loss: 0.908534] [G loss: 1.641486]\n",
      "[D loss: 0.909836] [G loss: 1.568024]\n",
      "[D loss: 0.828190] [G loss: 1.288026]\n",
      "[D loss: 0.878818] [G loss: 1.265348]\n",
      "[D loss: 0.797491] [G loss: 1.501873]\n",
      "[D loss: 0.575637] [G loss: 1.805857]\n",
      "[D loss: 0.942691] [G loss: 1.432122]\n",
      "[D loss: 0.910627] [G loss: 1.686666]\n",
      "[D loss: 0.856350] [G loss: 1.285297]\n",
      "[D loss: 1.151857] [G loss: 1.252563]\n",
      "[D loss: 0.969971] [G loss: 1.388597]\n",
      "[D loss: 0.775749] [G loss: 1.285201]\n",
      "[D loss: 0.796327] [G loss: 1.609670]\n",
      "[D loss: 0.687836] [G loss: 1.651768]\n",
      "[D loss: 0.822665] [G loss: 1.560343]\n",
      "[D loss: 0.785285] [G loss: 1.803378]\n",
      "[D loss: 0.721125] [G loss: 1.544417]\n",
      "[D loss: 0.919062] [G loss: 1.354505]\n",
      "[D loss: 0.828984] [G loss: 1.388186]\n",
      "[D loss: 0.798267] [G loss: 1.480886]\n",
      "[D loss: 0.734410] [G loss: 1.831676]\n",
      "[D loss: 0.611353] [G loss: 1.612937]\n",
      "[D loss: 0.705823] [G loss: 1.482338]\n",
      "[D loss: 0.950928] [G loss: 1.286704]\n",
      "[D loss: 0.823300] [G loss: 1.392947]\n",
      "[D loss: 0.828878] [G loss: 1.502845]\n",
      "[D loss: 0.887332] [G loss: 1.356584]\n",
      "[D loss: 0.677563] [G loss: 1.725500]\n",
      "[D loss: 0.763314] [G loss: 1.734373]\n",
      "[D loss: 0.823035] [G loss: 1.493166]\n",
      "[D loss: 0.853691] [G loss: 1.376199]\n",
      "[D loss: 0.719278] [G loss: 1.576786]\n",
      "[D loss: 0.907026] [G loss: 1.411947]\n",
      "[D loss: 0.740460] [G loss: 1.609196]\n",
      "[D loss: 0.737832] [G loss: 1.542086]\n",
      "[D loss: 0.691232] [G loss: 1.673127]\n",
      "[D loss: 0.713404] [G loss: 1.460261]\n",
      "[D loss: 0.940216] [G loss: 1.452650]\n",
      "[D loss: 1.071718] [G loss: 1.391344]\n",
      "[D loss: 0.606059] [G loss: 1.677345]\n",
      "[D loss: 0.873896] [G loss: 1.750039]\n",
      "[D loss: 0.627656] [G loss: 1.520823]\n",
      "[D loss: 0.657104] [G loss: 1.606232]\n",
      "[D loss: 0.672706] [G loss: 1.503448]\n",
      "[D loss: 0.583194] [G loss: 1.611415]\n",
      "[D loss: 0.877240] [G loss: 1.663373]\n",
      "[D loss: 0.739476] [G loss: 1.685370]\n",
      "[D loss: 0.891221] [G loss: 1.531747]\n",
      "[D loss: 0.619253] [G loss: 1.578625]\n",
      "[D loss: 0.712901] [G loss: 1.429361]\n",
      "[D loss: 0.727699] [G loss: 1.687123]\n",
      "[D loss: 0.803367] [G loss: 1.772588]\n",
      "[D loss: 0.669301] [G loss: 1.618760]\n",
      "[D loss: 0.726310] [G loss: 1.657104]\n",
      "[D loss: 0.937145] [G loss: 1.610555]\n",
      "[D loss: 0.703534] [G loss: 1.529516]\n",
      "[D loss: 0.783116] [G loss: 1.556784]\n",
      "[D loss: 0.901093] [G loss: 1.623454]\n",
      "[D loss: 0.867264] [G loss: 1.735472]\n",
      "[D loss: 0.945805] [G loss: 1.413338]\n",
      "[D loss: 0.798879] [G loss: 1.309904]\n",
      "[D loss: 0.892391] [G loss: 1.395044]\n",
      "[D loss: 0.986650] [G loss: 1.223040]\n",
      "[D loss: 0.864799] [G loss: 1.408691]\n",
      "[D loss: 0.958250] [G loss: 1.460490]\n",
      "[D loss: 0.904908] [G loss: 1.536483]\n",
      "[D loss: 1.093389] [G loss: 1.530457]\n",
      "[D loss: 0.831044] [G loss: 1.438156]\n",
      "[D loss: 1.041312] [G loss: 1.435624]\n",
      "[D loss: 0.746384] [G loss: 1.374700]\n",
      "[D loss: 0.661557] [G loss: 1.553468]\n",
      "[D loss: 0.670844] [G loss: 1.267347]\n",
      "[D loss: 0.726470] [G loss: 1.259182]\n",
      "[D loss: 0.731520] [G loss: 1.419546]\n",
      "[D loss: 0.912875] [G loss: 1.413444]\n",
      "[D loss: 0.798034] [G loss: 1.414107]\n",
      "[D loss: 0.906931] [G loss: 1.511879]\n",
      "[D loss: 0.699705] [G loss: 1.504890]\n",
      "[D loss: 0.982207] [G loss: 1.491891]\n",
      "[D loss: 0.675153] [G loss: 1.693450]\n",
      "[D loss: 0.803458] [G loss: 1.363917]\n",
      "[D loss: 0.811659] [G loss: 1.408717]\n",
      "[D loss: 0.785976] [G loss: 1.452351]\n",
      "[D loss: 0.780487] [G loss: 1.862956]\n",
      "[D loss: 0.791384] [G loss: 1.682266]\n",
      "[D loss: 1.047478] [G loss: 1.295488]\n",
      "[D loss: 0.922815] [G loss: 1.295557]\n",
      "[D loss: 0.755617] [G loss: 1.423180]\n",
      "[D loss: 0.922818] [G loss: 1.356046]\n",
      "[D loss: 0.921468] [G loss: 1.358787]\n",
      "[D loss: 0.935988] [G loss: 1.386574]\n",
      "[D loss: 0.767765] [G loss: 1.764900]\n",
      "[D loss: 0.817072] [G loss: 1.308980]\n",
      "[D loss: 0.654472] [G loss: 1.574237]\n",
      "[D loss: 0.815242] [G loss: 1.505033]\n",
      "[D loss: 0.629708] [G loss: 1.554391]\n",
      "[D loss: 0.856365] [G loss: 1.605463]\n",
      "[D loss: 0.881318] [G loss: 1.364995]\n",
      "[D loss: 0.993802] [G loss: 1.499128]\n",
      "[D loss: 0.892415] [G loss: 1.445231]\n",
      "[D loss: 0.793844] [G loss: 1.330888]\n",
      "[D loss: 0.770902] [G loss: 1.430813]\n",
      "[D loss: 1.126344] [G loss: 1.559947]\n",
      "[D loss: 0.929735] [G loss: 1.419964]\n",
      "[D loss: 0.705320] [G loss: 1.573312]\n",
      "[D loss: 0.825658] [G loss: 1.632797]\n",
      "[D loss: 0.945963] [G loss: 1.357785]\n",
      "[D loss: 0.847964] [G loss: 1.440073]\n",
      "[D loss: 0.696369] [G loss: 1.656873]\n",
      "[D loss: 0.734703] [G loss: 1.308856]\n",
      "[D loss: 0.770632] [G loss: 1.341139]\n",
      "[D loss: 0.879334] [G loss: 1.353397]\n",
      "[D loss: 0.953249] [G loss: 1.438704]\n",
      "[D loss: 0.575036] [G loss: 1.635146]\n",
      "[D loss: 0.734404] [G loss: 1.428035]\n",
      "[D loss: 0.806034] [G loss: 1.419059]\n",
      "[D loss: 0.709355] [G loss: 1.247739]\n",
      "[D loss: 0.846459] [G loss: 1.207078]\n",
      "[D loss: 0.901417] [G loss: 1.638090]\n",
      "[D loss: 0.874820] [G loss: 1.584566]\n",
      "[D loss: 0.526289] [G loss: 1.679649]\n",
      "[D loss: 0.887098] [G loss: 1.588102]\n",
      "[D loss: 0.914693] [G loss: 1.306467]\n",
      "[D loss: 0.759109] [G loss: 1.468701]\n",
      "[D loss: 0.804866] [G loss: 1.515897]\n",
      "[D loss: 0.888867] [G loss: 1.441685]\n",
      "[D loss: 1.015734] [G loss: 1.537420]\n",
      "[D loss: 0.783263] [G loss: 1.342579]\n",
      "[D loss: 0.771191] [G loss: 1.607636]\n",
      "[D loss: 0.695245] [G loss: 1.332077]\n",
      "[D loss: 0.777339] [G loss: 1.561299]\n",
      "[D loss: 0.657073] [G loss: 1.497805]\n",
      "[D loss: 0.766141] [G loss: 1.587380]\n",
      "[D loss: 0.778977] [G loss: 1.402514]\n",
      "[D loss: 0.827582] [G loss: 1.551556]\n",
      "[D loss: 0.844360] [G loss: 1.455921]\n",
      "[D loss: 0.855983] [G loss: 1.418423]\n",
      "[D loss: 0.797662] [G loss: 1.445606]\n",
      "[D loss: 0.876220] [G loss: 1.354012]\n",
      "[D loss: 0.743865] [G loss: 1.460746]\n",
      "[D loss: 0.948845] [G loss: 1.321367]\n",
      "[D loss: 0.664984] [G loss: 1.700274]\n",
      "[D loss: 0.791355] [G loss: 1.506528]\n",
      "[D loss: 0.657639] [G loss: 1.434638]\n",
      "[D loss: 0.793596] [G loss: 1.517018]\n",
      "[D loss: 0.784967] [G loss: 1.477688]\n",
      "[D loss: 0.602634] [G loss: 1.367959]\n",
      "[D loss: 0.858047] [G loss: 1.576815]\n",
      "[D loss: 0.498999] [G loss: 1.557029]\n",
      "[D loss: 0.781330] [G loss: 1.417496]\n",
      "[D loss: 0.851355] [G loss: 1.640247]\n",
      "[D loss: 0.722919] [G loss: 1.410469]\n",
      "[D loss: 0.565775] [G loss: 1.789189]\n",
      "[D loss: 0.538937] [G loss: 1.836060]\n",
      "[D loss: 0.793813] [G loss: 1.577374]\n",
      "[D loss: 0.934716] [G loss: 1.339989]\n",
      "[D loss: 0.939000] [G loss: 1.893452]\n",
      "[D loss: 0.692971] [G loss: 1.829470]\n",
      "[D loss: 0.888397] [G loss: 1.524706]\n",
      "[D loss: 0.601485] [G loss: 1.725457]\n",
      "[D loss: 0.809273] [G loss: 1.617666]\n",
      "[D loss: 0.636592] [G loss: 1.416822]\n",
      "[D loss: 0.764329] [G loss: 1.476426]\n",
      "[D loss: 0.762948] [G loss: 1.375610]\n",
      "[D loss: 0.882200] [G loss: 1.644476]\n",
      "[D loss: 0.819012] [G loss: 1.562421]\n",
      "[D loss: 0.718531] [G loss: 1.676402]\n",
      "[D loss: 0.751033] [G loss: 1.419534]\n",
      "[D loss: 0.984360] [G loss: 1.083251]\n",
      "[D loss: 0.714296] [G loss: 1.500950]\n",
      "[D loss: 0.604794] [G loss: 1.631620]\n",
      "[D loss: 0.674080] [G loss: 1.773274]\n",
      "[D loss: 0.601153] [G loss: 1.500582]\n",
      "[D loss: 0.908192] [G loss: 1.502147]\n",
      "[D loss: 0.699351] [G loss: 1.406436]\n",
      "[D loss: 0.975781] [G loss: 1.440206]\n",
      "[D loss: 0.925419] [G loss: 1.696368]\n",
      "[D loss: 0.809868] [G loss: 1.572842]\n",
      "[D loss: 0.830510] [G loss: 1.534235]\n",
      "[D loss: 0.711627] [G loss: 1.440382]\n",
      "[D loss: 0.858638] [G loss: 1.445590]\n",
      "[D loss: 0.628901] [G loss: 1.545530]\n",
      "[D loss: 0.793563] [G loss: 1.723408]\n",
      "[D loss: 0.791567] [G loss: 1.483451]\n",
      "[D loss: 1.084762] [G loss: 1.512731]\n",
      "[D loss: 0.639641] [G loss: 1.475988]\n",
      "[D loss: 0.725786] [G loss: 1.512057]\n",
      "[D loss: 0.730078] [G loss: 1.675887]\n",
      "[D loss: 0.765106] [G loss: 1.748437]\n",
      "[D loss: 0.741234] [G loss: 1.704159]\n",
      "[D loss: 0.676949] [G loss: 1.570079]\n",
      "[D loss: 0.904300] [G loss: 1.708692]\n",
      "[D loss: 0.866583] [G loss: 1.456077]\n",
      "[D loss: 0.960977] [G loss: 1.393321]\n",
      "[D loss: 0.761841] [G loss: 1.630143]\n",
      "[D loss: 1.127754] [G loss: 1.303895]\n",
      "[D loss: 0.782342] [G loss: 1.408807]\n",
      "[D loss: 0.913680] [G loss: 1.286884]\n",
      "[D loss: 1.004401] [G loss: 1.421417]\n",
      "[D loss: 0.571882] [G loss: 1.352946]\n",
      "[D loss: 0.996340] [G loss: 1.558939]\n",
      "[D loss: 0.946147] [G loss: 1.409064]\n",
      "[D loss: 0.825796] [G loss: 1.440295]\n",
      "[D loss: 0.807948] [G loss: 1.335374]\n",
      "[D loss: 0.869434] [G loss: 1.435897]\n",
      "[D loss: 1.041983] [G loss: 1.412936]\n",
      "[D loss: 0.802959] [G loss: 1.490443]\n",
      "[D loss: 0.671341] [G loss: 1.350305]\n",
      "[D loss: 0.574132] [G loss: 1.465559]\n",
      "[D loss: 0.739405] [G loss: 1.567481]\n",
      "[D loss: 0.900469] [G loss: 1.441414]\n",
      "[D loss: 0.832202] [G loss: 1.399414]\n",
      "[D loss: 0.952755] [G loss: 1.450645]\n",
      "[D loss: 0.702291] [G loss: 1.360416]\n",
      "[D loss: 0.950739] [G loss: 1.470445]\n",
      "[D loss: 0.967449] [G loss: 1.589852]\n",
      "[D loss: 0.691797] [G loss: 1.536844]\n",
      "[D loss: 0.684244] [G loss: 1.501189]\n",
      "[D loss: 0.794607] [G loss: 1.700490]\n",
      "[D loss: 1.066750] [G loss: 1.454407]\n",
      "[D loss: 1.112413] [G loss: 1.185111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.856536] [G loss: 1.579656]\n",
      "[D loss: 0.852782] [G loss: 1.690710]\n",
      "[D loss: 0.842172] [G loss: 1.473226]\n",
      "[D loss: 1.064622] [G loss: 1.318073]\n",
      "[D loss: 0.884356] [G loss: 1.396210]\n",
      "[D loss: 0.789461] [G loss: 1.316152]\n",
      "[D loss: 0.866032] [G loss: 1.279734]\n",
      "[D loss: 0.752415] [G loss: 1.336524]\n",
      "[D loss: 0.711464] [G loss: 1.368484]\n",
      "[D loss: 0.818632] [G loss: 1.548524]\n",
      "[D loss: 0.867772] [G loss: 1.365313]\n",
      "[D loss: 0.811861] [G loss: 1.532777]\n",
      "[D loss: 0.680131] [G loss: 1.665900]\n",
      "[D loss: 0.636575] [G loss: 1.661682]\n",
      "[D loss: 0.736639] [G loss: 1.537750]\n",
      "[D loss: 0.956101] [G loss: 1.366779]\n",
      "[D loss: 0.818740] [G loss: 1.516629]\n",
      "[D loss: 0.773214] [G loss: 1.737603]\n",
      "[D loss: 0.755639] [G loss: 1.659150]\n",
      "[D loss: 0.639639] [G loss: 1.606392]\n",
      "[D loss: 0.753182] [G loss: 1.545399]\n",
      "[D loss: 0.827053] [G loss: 1.577608]\n",
      "[D loss: 1.036361] [G loss: 1.314563]\n",
      "[D loss: 0.697057] [G loss: 1.413790]\n",
      "[D loss: 0.738516] [G loss: 1.624496]\n",
      "[D loss: 0.999230] [G loss: 1.415894]\n",
      "[D loss: 0.942653] [G loss: 1.723182]\n",
      "[D loss: 1.186309] [G loss: 1.199741]\n",
      "[D loss: 0.622900] [G loss: 1.430884]\n",
      "[D loss: 0.732516] [G loss: 1.284215]\n",
      "[D loss: 0.679308] [G loss: 1.555576]\n",
      "[D loss: 0.929981] [G loss: 1.135192]\n",
      "[D loss: 0.778951] [G loss: 1.411350]\n",
      "[D loss: 0.891316] [G loss: 1.571354]\n",
      "[D loss: 0.830237] [G loss: 1.644339]\n",
      "[D loss: 0.815563] [G loss: 1.659750]\n",
      "[D loss: 0.836950] [G loss: 1.537603]\n",
      "[D loss: 0.797641] [G loss: 1.597726]\n",
      "[D loss: 0.720860] [G loss: 1.626022]\n",
      "[D loss: 0.837266] [G loss: 1.666899]\n",
      "[D loss: 0.716619] [G loss: 1.472300]\n",
      "[D loss: 0.782290] [G loss: 1.430702]\n",
      "[D loss: 0.653635] [G loss: 1.355953]\n",
      "[D loss: 0.738683] [G loss: 1.412401]\n",
      "[D loss: 0.669016] [G loss: 1.639064]\n",
      "[D loss: 0.741654] [G loss: 1.428475]\n",
      "[D loss: 0.904013] [G loss: 1.381109]\n",
      "[D loss: 0.676757] [G loss: 1.747155]\n",
      "[D loss: 0.748218] [G loss: 1.668304]\n",
      "[D loss: 0.713542] [G loss: 1.638858]\n",
      "[D loss: 0.739273] [G loss: 1.376910]\n",
      "[D loss: 1.115544] [G loss: 1.701962]\n",
      "[D loss: 0.746298] [G loss: 1.658119]\n",
      "[D loss: 0.666925] [G loss: 1.555566]\n",
      "[D loss: 0.851841] [G loss: 1.382184]\n",
      "[D loss: 1.012043] [G loss: 1.160383]\n",
      "[D loss: 0.887807] [G loss: 1.552332]\n",
      "[D loss: 0.809964] [G loss: 1.510164]\n",
      "[D loss: 0.894105] [G loss: 1.352527]\n",
      "[D loss: 0.730508] [G loss: 1.477077]\n",
      "[D loss: 0.818832] [G loss: 1.337311]\n",
      "[D loss: 0.675729] [G loss: 1.324920]\n",
      "[D loss: 0.879906] [G loss: 1.528298]\n",
      "[D loss: 0.850931] [G loss: 1.574913]\n",
      "[D loss: 0.901761] [G loss: 1.596818]\n",
      "[D loss: 0.762705] [G loss: 1.354109]\n",
      "[D loss: 0.806187] [G loss: 1.601948]\n",
      "[D loss: 0.745448] [G loss: 1.422135]\n",
      "[D loss: 1.113444] [G loss: 1.318606]\n",
      "[D loss: 0.947640] [G loss: 1.398254]\n",
      "[D loss: 0.739624] [G loss: 1.443683]\n",
      "[D loss: 0.747688] [G loss: 1.323903]\n",
      "[D loss: 1.064933] [G loss: 1.398091]\n",
      "[D loss: 0.747262] [G loss: 1.542400]\n",
      "[D loss: 0.701676] [G loss: 1.781865]\n",
      "[D loss: 0.971140] [G loss: 1.362077]\n",
      "[D loss: 1.042116] [G loss: 1.391367]\n",
      "[D loss: 1.094125] [G loss: 1.467205]\n",
      "[D loss: 1.015957] [G loss: 1.569020]\n",
      "[D loss: 1.003984] [G loss: 1.360375]\n",
      "[D loss: 0.985975] [G loss: 1.403686]\n",
      "[D loss: 0.818789] [G loss: 1.196737]\n",
      "[D loss: 0.696959] [G loss: 1.568796]\n",
      "[D loss: 0.874584] [G loss: 1.570927]\n",
      "[D loss: 0.736109] [G loss: 1.797704]\n",
      "[D loss: 0.858338] [G loss: 1.420540]\n",
      "[D loss: 0.891196] [G loss: 1.238483]\n",
      "[D loss: 0.595798] [G loss: 1.564331]\n",
      "[D loss: 0.926122] [G loss: 1.378466]\n",
      "[D loss: 0.981298] [G loss: 1.504703]\n",
      "[D loss: 1.064749] [G loss: 1.472702]\n",
      "[D loss: 1.072489] [G loss: 1.433199]\n",
      "[D loss: 0.890107] [G loss: 1.548517]\n",
      "[D loss: 0.699169] [G loss: 1.436705]\n",
      "[D loss: 0.887760] [G loss: 1.242251]\n",
      "[D loss: 0.921385] [G loss: 1.367760]\n",
      "[D loss: 0.817885] [G loss: 1.360403]\n",
      "[D loss: 0.923713] [G loss: 1.430072]\n",
      "[D loss: 1.019104] [G loss: 1.279849]\n",
      "[D loss: 0.870112] [G loss: 1.349450]\n",
      "[D loss: 0.794229] [G loss: 1.263094]\n",
      "[D loss: 0.944879] [G loss: 1.494859]\n",
      "[D loss: 0.888641] [G loss: 1.465606]\n",
      "[D loss: 0.811666] [G loss: 1.441460]\n",
      "[D loss: 0.874809] [G loss: 1.523304]\n",
      "[D loss: 0.897386] [G loss: 1.238681]\n",
      "[D loss: 0.770684] [G loss: 1.427922]\n",
      "[D loss: 0.830217] [G loss: 1.393039]\n",
      "[D loss: 0.977802] [G loss: 1.696641]\n",
      "[D loss: 0.543926] [G loss: 1.629928]\n",
      "[D loss: 0.975993] [G loss: 1.533941]\n",
      "[D loss: 0.696552] [G loss: 1.368389]\n",
      "[D loss: 0.715269] [G loss: 1.246740]\n",
      "[D loss: 0.737866] [G loss: 1.428995]\n",
      "[D loss: 0.620219] [G loss: 1.595887]\n",
      "[D loss: 0.820258] [G loss: 1.465962]\n",
      "[D loss: 0.830345] [G loss: 1.551224]\n",
      "[D loss: 0.753453] [G loss: 1.584846]\n",
      "[D loss: 0.694177] [G loss: 1.624688]\n",
      "[D loss: 0.768951] [G loss: 1.438817]\n",
      "[D loss: 0.807114] [G loss: 1.513130]\n",
      "[D loss: 0.658376] [G loss: 1.409168]\n",
      "[D loss: 1.035902] [G loss: 1.336249]\n",
      "[D loss: 0.889225] [G loss: 1.626000]\n",
      "[D loss: 0.828480] [G loss: 1.248837]\n",
      "[D loss: 0.704622] [G loss: 1.407147]\n",
      "[D loss: 0.814649] [G loss: 1.622686]\n",
      "[D loss: 0.792728] [G loss: 1.392321]\n",
      "[D loss: 1.055862] [G loss: 1.483704]\n",
      "[D loss: 0.803943] [G loss: 1.312716]\n",
      "[D loss: 0.803303] [G loss: 1.539864]\n",
      "[D loss: 0.624251] [G loss: 1.465851]\n",
      "[D loss: 0.788536] [G loss: 1.591279]\n",
      "[D loss: 0.738205] [G loss: 1.650582]\n",
      "[D loss: 0.845410] [G loss: 1.413107]\n",
      "[D loss: 1.038826] [G loss: 1.414985]\n",
      "[D loss: 0.869755] [G loss: 1.653806]\n",
      "[D loss: 0.606597] [G loss: 1.630549]\n",
      "[D loss: 0.696480] [G loss: 1.621848]\n",
      "[D loss: 0.935180] [G loss: 1.522249]\n",
      "[D loss: 0.982755] [G loss: 1.938013]\n",
      "[D loss: 0.808626] [G loss: 1.694275]\n",
      "[D loss: 0.706483] [G loss: 1.475229]\n",
      "[D loss: 1.022035] [G loss: 1.301302]\n",
      "[D loss: 0.888179] [G loss: 1.459110]\n",
      "[D loss: 0.708425] [G loss: 1.438624]\n",
      "[D loss: 0.972737] [G loss: 1.406888]\n",
      "[D loss: 1.058718] [G loss: 1.559479]\n",
      "[D loss: 0.935074] [G loss: 1.400945]\n",
      "[D loss: 0.826038] [G loss: 1.480347]\n",
      "[D loss: 0.886652] [G loss: 1.412589]\n",
      "[D loss: 0.768065] [G loss: 1.373412]\n",
      "[D loss: 0.787343] [G loss: 1.278948]\n",
      "[D loss: 0.652105] [G loss: 1.476244]\n",
      "[D loss: 0.798827] [G loss: 1.608472]\n",
      "[D loss: 0.967703] [G loss: 1.422573]\n",
      "[D loss: 0.924824] [G loss: 1.551478]\n",
      "[D loss: 1.031979] [G loss: 1.334562]\n",
      "[D loss: 0.913317] [G loss: 1.545894]\n",
      "[D loss: 0.696482] [G loss: 1.554896]\n",
      "[D loss: 1.168310] [G loss: 1.303958]\n",
      "[D loss: 0.610627] [G loss: 1.406119]\n",
      "[D loss: 0.765843] [G loss: 1.474421]\n",
      "[D loss: 0.852352] [G loss: 1.310166]\n",
      "[D loss: 0.964151] [G loss: 1.393964]\n",
      "[D loss: 0.974543] [G loss: 1.331995]\n",
      "[D loss: 0.892230] [G loss: 1.363768]\n",
      "[D loss: 0.843811] [G loss: 1.583455]\n",
      "[D loss: 0.834218] [G loss: 1.593108]\n",
      "[D loss: 0.700008] [G loss: 1.288693]\n",
      "[D loss: 0.803358] [G loss: 1.415393]\n",
      "[D loss: 0.833522] [G loss: 1.535716]\n",
      "[D loss: 0.827588] [G loss: 1.501953]\n",
      "[D loss: 0.923995] [G loss: 1.411379]\n",
      "[D loss: 0.707679] [G loss: 1.447059]\n",
      "[D loss: 0.873535] [G loss: 1.469291]\n",
      "[D loss: 0.885532] [G loss: 1.526733]\n",
      "[D loss: 0.798758] [G loss: 1.251203]\n",
      "[D loss: 0.803468] [G loss: 1.450973]\n",
      "[D loss: 0.622422] [G loss: 1.477471]\n",
      "[D loss: 1.016714] [G loss: 1.627833]\n",
      "[D loss: 0.785150] [G loss: 1.583120]\n",
      "[D loss: 0.857054] [G loss: 1.271943]\n",
      "[D loss: 0.930080] [G loss: 1.382724]\n",
      "[D loss: 0.952258] [G loss: 1.429198]\n",
      "[D loss: 1.027994] [G loss: 1.460316]\n",
      "[D loss: 0.752899] [G loss: 1.444596]\n",
      "[D loss: 0.811374] [G loss: 1.432189]\n",
      "[D loss: 0.833003] [G loss: 1.512986]\n",
      "[D loss: 0.644266] [G loss: 1.613547]\n",
      "[D loss: 0.746387] [G loss: 1.373471]\n",
      "[D loss: 0.938608] [G loss: 1.290308]\n",
      "[D loss: 0.816275] [G loss: 1.650150]\n",
      "[D loss: 0.886431] [G loss: 1.580417]\n",
      "[D loss: 1.019898] [G loss: 1.467101]\n",
      "[D loss: 0.724454] [G loss: 1.263719]\n",
      "[D loss: 1.010921] [G loss: 1.534577]\n",
      "[D loss: 0.775669] [G loss: 1.458324]\n",
      "[D loss: 0.810303] [G loss: 1.559858]\n",
      "[D loss: 0.853006] [G loss: 1.424571]\n",
      "[D loss: 0.636298] [G loss: 1.436217]\n",
      "[D loss: 1.156296] [G loss: 1.366460]\n",
      "[D loss: 0.775241] [G loss: 1.516661]\n",
      "[D loss: 1.000484] [G loss: 1.525496]\n",
      "[D loss: 0.903765] [G loss: 1.589594]\n",
      "[D loss: 1.002689] [G loss: 1.277279]\n",
      "[D loss: 1.175988] [G loss: 1.419804]\n",
      "[D loss: 0.969280] [G loss: 1.292436]\n",
      "[D loss: 0.947219] [G loss: 1.367188]\n",
      "[D loss: 0.931662] [G loss: 1.305976]\n",
      "[D loss: 0.840265] [G loss: 1.558959]\n",
      "[D loss: 0.854029] [G loss: 1.343549]\n",
      "[D loss: 0.867177] [G loss: 1.319129]\n",
      "[D loss: 0.860401] [G loss: 1.436694]\n",
      "[D loss: 0.815520] [G loss: 1.304252]\n",
      "[D loss: 0.762465] [G loss: 1.352529]\n",
      "[D loss: 0.767058] [G loss: 1.285599]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.832542] [G loss: 1.409314]\n",
      "[D loss: 0.995754] [G loss: 1.443497]\n",
      "[D loss: 0.972946] [G loss: 1.381979]\n",
      "[D loss: 0.670682] [G loss: 1.448869]\n",
      "[D loss: 0.807025] [G loss: 1.318756]\n",
      "[D loss: 0.781551] [G loss: 1.396027]\n",
      "[D loss: 0.708679] [G loss: 1.382506]\n",
      "[D loss: 0.709305] [G loss: 1.322253]\n",
      "[D loss: 0.593192] [G loss: 1.688556]\n",
      "[D loss: 1.000161] [G loss: 1.398582]\n",
      "[D loss: 0.765049] [G loss: 1.432396]\n",
      "[D loss: 0.714868] [G loss: 1.454084]\n",
      "[D loss: 0.641805] [G loss: 1.511846]\n",
      "[D loss: 0.970071] [G loss: 1.456959]\n",
      "[D loss: 0.679205] [G loss: 1.538954]\n",
      "[D loss: 0.787675] [G loss: 1.535414]\n",
      "[D loss: 0.792779] [G loss: 1.462406]\n",
      "[D loss: 0.979962] [G loss: 1.538718]\n",
      "[D loss: 0.716735] [G loss: 1.492849]\n",
      "[D loss: 0.750980] [G loss: 1.466682]\n",
      "[D loss: 0.861356] [G loss: 1.581940]\n",
      "[D loss: 0.998580] [G loss: 1.465000]\n",
      "[D loss: 0.682804] [G loss: 1.422587]\n",
      "[D loss: 0.743635] [G loss: 1.541845]\n",
      "[D loss: 0.906128] [G loss: 1.433777]\n",
      "[D loss: 1.108615] [G loss: 1.282821]\n",
      "[D loss: 0.890897] [G loss: 1.460336]\n",
      "[D loss: 0.877593] [G loss: 1.635650]\n",
      "[D loss: 0.970059] [G loss: 1.428611]\n",
      "[D loss: 0.980401] [G loss: 1.476432]\n",
      "[D loss: 0.837463] [G loss: 1.447606]\n",
      "[D loss: 0.661003] [G loss: 1.410709]\n",
      "[D loss: 0.981009] [G loss: 1.312837]\n",
      "[D loss: 0.848317] [G loss: 1.452413]\n",
      "[D loss: 0.883935] [G loss: 1.377379]\n",
      "[D loss: 0.803240] [G loss: 1.361279]\n",
      "[D loss: 0.896910] [G loss: 1.474556]\n",
      "[D loss: 0.636680] [G loss: 1.446312]\n",
      "[D loss: 1.129040] [G loss: 1.468152]\n",
      "[D loss: 0.890324] [G loss: 1.625081]\n",
      "[D loss: 0.722715] [G loss: 1.534317]\n",
      "[D loss: 0.861568] [G loss: 1.451501]\n",
      "[D loss: 0.978081] [G loss: 1.406348]\n",
      "[D loss: 0.744960] [G loss: 1.399062]\n",
      "[D loss: 0.952566] [G loss: 1.353589]\n",
      "[D loss: 0.716671] [G loss: 1.464953]\n",
      "[D loss: 0.688200] [G loss: 1.503986]\n",
      "[D loss: 0.675498] [G loss: 1.628983]\n",
      "[D loss: 0.732983] [G loss: 1.389516]\n",
      "[D loss: 1.046206] [G loss: 1.369174]\n",
      "[D loss: 1.085020] [G loss: 1.313950]\n",
      "[D loss: 0.601785] [G loss: 1.568375]\n",
      "[D loss: 0.909736] [G loss: 1.402667]\n",
      "[D loss: 1.155995] [G loss: 1.303682]\n",
      "[D loss: 0.887411] [G loss: 1.272942]\n",
      "[D loss: 0.819411] [G loss: 1.428858]\n",
      "[D loss: 0.971624] [G loss: 1.584864]\n",
      "[D loss: 0.799031] [G loss: 1.311182]\n",
      "[D loss: 0.832291] [G loss: 1.093345]\n",
      "[D loss: 0.735813] [G loss: 1.475835]\n",
      "[D loss: 0.804989] [G loss: 1.376689]\n",
      "[D loss: 0.955222] [G loss: 1.425301]\n",
      "[D loss: 0.788573] [G loss: 1.303943]\n",
      "[D loss: 0.737683] [G loss: 1.513270]\n",
      "[D loss: 0.840451] [G loss: 1.398174]\n",
      "[D loss: 0.891636] [G loss: 1.377932]\n",
      "[D loss: 0.751291] [G loss: 1.474177]\n",
      "[D loss: 0.684550] [G loss: 1.540962]\n",
      "[D loss: 0.646240] [G loss: 1.700082]\n",
      "[D loss: 0.817320] [G loss: 1.408505]\n",
      "[D loss: 0.957490] [G loss: 1.561470]\n",
      "[D loss: 0.861891] [G loss: 1.611494]\n",
      "[D loss: 0.650617] [G loss: 1.527128]\n",
      "[D loss: 0.711000] [G loss: 1.524781]\n",
      "[D loss: 0.878967] [G loss: 1.669994]\n",
      "[D loss: 0.723108] [G loss: 1.545600]\n",
      "[D loss: 0.849200] [G loss: 1.454340]\n",
      "[D loss: 0.716275] [G loss: 1.621392]\n",
      "[D loss: 0.793950] [G loss: 1.391405]\n",
      "[D loss: 0.773045] [G loss: 1.410732]\n",
      "[D loss: 0.805647] [G loss: 1.578375]\n",
      "[D loss: 0.967701] [G loss: 1.619537]\n",
      "[D loss: 0.806140] [G loss: 1.314431]\n",
      "[D loss: 0.696686] [G loss: 1.591982]\n",
      "[D loss: 0.797455] [G loss: 1.538028]\n",
      "[D loss: 1.030429] [G loss: 1.431041]\n",
      "[D loss: 0.939295] [G loss: 1.399323]\n",
      "[D loss: 1.036722] [G loss: 1.532619]\n",
      "[D loss: 0.905150] [G loss: 1.389382]\n",
      "[D loss: 1.070137] [G loss: 1.533540]\n",
      "[D loss: 0.786341] [G loss: 1.369213]\n",
      "[D loss: 0.861638] [G loss: 1.403961]\n",
      "[D loss: 0.898259] [G loss: 1.316697]\n",
      "[D loss: 0.722990] [G loss: 1.414583]\n",
      "[D loss: 0.797925] [G loss: 1.419127]\n",
      "[D loss: 1.027705] [G loss: 1.251349]\n",
      "[D loss: 0.659073] [G loss: 1.574327]\n",
      "[D loss: 0.743720] [G loss: 1.633553]\n",
      "[D loss: 0.952914] [G loss: 1.458497]\n",
      "[D loss: 0.862170] [G loss: 1.461551]\n",
      "[D loss: 0.688304] [G loss: 1.361317]\n",
      "[D loss: 0.923989] [G loss: 1.429029]\n",
      "[D loss: 0.781463] [G loss: 1.386919]\n",
      "[D loss: 0.897166] [G loss: 1.359150]\n",
      "[D loss: 0.969769] [G loss: 1.520899]\n",
      "[D loss: 0.840856] [G loss: 1.557643]\n",
      "[D loss: 0.747238] [G loss: 1.485458]\n",
      "[D loss: 0.686706] [G loss: 1.274482]\n",
      "[D loss: 0.923735] [G loss: 1.338059]\n",
      "[D loss: 0.996072] [G loss: 1.503340]\n",
      "[D loss: 0.924362] [G loss: 1.512708]\n",
      "[D loss: 0.669028] [G loss: 1.445250]\n",
      "[D loss: 0.841718] [G loss: 1.375608]\n",
      "[D loss: 0.506975] [G loss: 1.461670]\n",
      "[D loss: 0.797613] [G loss: 1.556481]\n",
      "[D loss: 0.827438] [G loss: 1.662711]\n",
      "[D loss: 0.983495] [G loss: 1.374807]\n",
      "[D loss: 0.967438] [G loss: 1.471877]\n",
      "[D loss: 0.777895] [G loss: 1.632636]\n",
      "[D loss: 0.895823] [G loss: 1.621441]\n",
      "[D loss: 0.767796] [G loss: 1.524959]\n",
      "[D loss: 1.015863] [G loss: 1.536153]\n",
      "[D loss: 0.775488] [G loss: 1.561942]\n",
      "[D loss: 0.653013] [G loss: 1.643527]\n",
      "[D loss: 0.826106] [G loss: 1.438808]\n",
      "[D loss: 0.813106] [G loss: 1.302172]\n",
      "[D loss: 0.718790] [G loss: 1.374759]\n",
      "[D loss: 0.682249] [G loss: 1.805758]\n",
      "[D loss: 0.788580] [G loss: 1.616816]\n",
      "[D loss: 0.738401] [G loss: 1.583220]\n",
      "[D loss: 1.119091] [G loss: 1.319282]\n",
      "[D loss: 0.796502] [G loss: 1.532466]\n",
      "[D loss: 0.761866] [G loss: 1.578820]\n",
      "[D loss: 0.851794] [G loss: 1.428554]\n",
      "[D loss: 0.812339] [G loss: 1.608284]\n",
      "[D loss: 0.825700] [G loss: 1.416261]\n",
      "[D loss: 1.006610] [G loss: 1.497775]\n",
      "[D loss: 1.016973] [G loss: 1.666417]\n",
      "[D loss: 1.045256] [G loss: 1.389978]\n",
      "[D loss: 0.860250] [G loss: 1.462190]\n",
      "[D loss: 0.798446] [G loss: 1.322662]\n",
      "[D loss: 0.709841] [G loss: 1.499499]\n",
      "[D loss: 0.796839] [G loss: 1.374435]\n",
      "[D loss: 0.628165] [G loss: 1.475973]\n",
      "[D loss: 1.030300] [G loss: 1.398016]\n",
      "[D loss: 1.017727] [G loss: 1.386433]\n",
      "[D loss: 0.876202] [G loss: 1.305923]\n",
      "[D loss: 0.910407] [G loss: 1.487109]\n",
      "[D loss: 0.915476] [G loss: 1.306648]\n",
      "[D loss: 1.003000] [G loss: 1.315729]\n",
      "[D loss: 0.807049] [G loss: 1.422021]\n",
      "[D loss: 0.792482] [G loss: 1.164490]\n",
      "[D loss: 0.546658] [G loss: 1.432813]\n",
      "[D loss: 0.808884] [G loss: 1.435003]\n",
      "[D loss: 0.905826] [G loss: 1.469427]\n",
      "[D loss: 0.704204] [G loss: 1.714842]\n",
      "[D loss: 0.519706] [G loss: 1.543783]\n",
      "[D loss: 1.005478] [G loss: 1.559689]\n",
      "[D loss: 0.738826] [G loss: 1.498612]\n",
      "[D loss: 0.974387] [G loss: 1.320751]\n",
      "[D loss: 0.978116] [G loss: 1.721984]\n",
      "[D loss: 0.904052] [G loss: 1.555941]\n",
      "[D loss: 0.663534] [G loss: 1.554371]\n",
      "[D loss: 0.791197] [G loss: 1.395191]\n",
      "[D loss: 1.070152] [G loss: 1.180814]\n",
      "[D loss: 0.810952] [G loss: 1.547194]\n",
      "[D loss: 0.736542] [G loss: 1.393397]\n",
      "[D loss: 0.655720] [G loss: 1.388199]\n",
      "[D loss: 0.797144] [G loss: 1.469573]\n",
      "[D loss: 0.733236] [G loss: 1.522078]\n",
      "[D loss: 0.754335] [G loss: 1.541034]\n",
      "[D loss: 1.181763] [G loss: 1.086670]\n",
      "[D loss: 0.972300] [G loss: 1.500256]\n",
      "[D loss: 0.822943] [G loss: 1.569438]\n",
      "[D loss: 0.789929] [G loss: 1.661129]\n",
      "[D loss: 1.093197] [G loss: 1.265445]\n",
      "[D loss: 0.764339] [G loss: 1.521891]\n",
      "[D loss: 0.743333] [G loss: 1.459735]\n",
      "[D loss: 0.966783] [G loss: 1.501365]\n",
      "[D loss: 0.979180] [G loss: 1.284112]\n",
      "[D loss: 0.599403] [G loss: 1.378133]\n",
      "[D loss: 0.775614] [G loss: 1.408080]\n",
      "[D loss: 0.892639] [G loss: 1.526826]\n",
      "[D loss: 0.814981] [G loss: 1.174472]\n",
      "[D loss: 1.030811] [G loss: 1.493669]\n",
      "[D loss: 0.790309] [G loss: 1.257845]\n",
      "[D loss: 0.915005] [G loss: 1.188293]\n",
      "[D loss: 0.664554] [G loss: 1.580767]\n",
      "[D loss: 0.844104] [G loss: 1.406597]\n",
      "[D loss: 0.791266] [G loss: 1.458993]\n",
      "[D loss: 0.957539] [G loss: 1.423331]\n",
      "[D loss: 1.174299] [G loss: 1.329100]\n",
      "[D loss: 0.873426] [G loss: 1.396315]\n",
      "[D loss: 0.742593] [G loss: 1.519110]\n",
      "[D loss: 0.707570] [G loss: 1.548495]\n",
      "[D loss: 0.688554] [G loss: 1.363342]\n",
      "[D loss: 0.729654] [G loss: 1.465956]\n",
      "[D loss: 0.805654] [G loss: 1.497334]\n",
      "[D loss: 0.879373] [G loss: 1.426159]\n",
      "[D loss: 1.044956] [G loss: 1.244960]\n",
      "[D loss: 0.815445] [G loss: 1.563093]\n",
      "[D loss: 0.749379] [G loss: 1.455531]\n",
      "[D loss: 0.902995] [G loss: 1.337595]\n",
      "[D loss: 0.953732] [G loss: 1.194336]\n",
      "[D loss: 0.999468] [G loss: 1.416017]\n",
      "[D loss: 0.842923] [G loss: 1.501129]\n",
      "[D loss: 0.680194] [G loss: 1.424661]\n",
      "[D loss: 1.009449] [G loss: 1.472948]\n",
      "[D loss: 0.933634] [G loss: 1.295314]\n",
      "[D loss: 1.046961] [G loss: 1.226614]\n",
      "[D loss: 0.751430] [G loss: 1.506527]\n",
      "[D loss: 0.891729] [G loss: 1.748295]\n",
      "[D loss: 0.908915] [G loss: 1.640520]\n",
      "[D loss: 0.912914] [G loss: 1.504171]\n",
      "[D loss: 1.003316] [G loss: 1.334942]\n",
      "[D loss: 0.753332] [G loss: 1.276556]\n",
      "[D loss: 0.860180] [G loss: 1.378736]\n",
      "[D loss: 0.788327] [G loss: 1.595035]\n",
      "[D loss: 0.840398] [G loss: 1.549422]\n",
      "[D loss: 0.875978] [G loss: 1.453544]\n",
      "[D loss: 0.855913] [G loss: 1.518311]\n",
      "[D loss: 1.079328] [G loss: 1.130974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.854566] [G loss: 1.513502]\n",
      "[D loss: 0.702147] [G loss: 1.424732]\n",
      "[D loss: 0.853140] [G loss: 1.613959]\n",
      "[D loss: 0.949162] [G loss: 1.445361]\n",
      "[D loss: 0.945681] [G loss: 1.321238]\n",
      "[D loss: 0.919039] [G loss: 1.476284]\n",
      "[D loss: 0.906199] [G loss: 1.367489]\n",
      "[D loss: 0.693496] [G loss: 1.617864]\n",
      "[D loss: 0.896635] [G loss: 1.242928]\n",
      "[D loss: 0.753106] [G loss: 1.487286]\n",
      "[D loss: 0.999076] [G loss: 1.349875]\n",
      "[D loss: 0.901742] [G loss: 1.325331]\n",
      "[D loss: 1.073422] [G loss: 1.104684]\n",
      "[D loss: 1.062129] [G loss: 1.407724]\n",
      "[D loss: 1.204859] [G loss: 1.282411]\n",
      "[D loss: 0.631073] [G loss: 1.342649]\n",
      "[D loss: 0.800096] [G loss: 1.516156]\n",
      "[D loss: 0.938923] [G loss: 1.337403]\n",
      "[D loss: 0.810721] [G loss: 1.204819]\n",
      "[D loss: 1.006411] [G loss: 1.191349]\n",
      "[D loss: 1.041229] [G loss: 1.328412]\n",
      "[D loss: 0.793756] [G loss: 1.518299]\n",
      "[D loss: 0.791711] [G loss: 1.582119]\n",
      "[D loss: 0.983770] [G loss: 1.445268]\n",
      "[D loss: 0.900749] [G loss: 1.354294]\n",
      "[D loss: 0.733855] [G loss: 1.368383]\n",
      "[D loss: 0.878629] [G loss: 1.321031]\n",
      "[D loss: 0.841679] [G loss: 1.326162]\n",
      "[D loss: 0.824178] [G loss: 1.459576]\n",
      "[D loss: 0.664163] [G loss: 1.572617]\n",
      "[D loss: 0.869448] [G loss: 1.473550]\n",
      "[D loss: 0.822673] [G loss: 1.416167]\n",
      "[D loss: 0.799495] [G loss: 1.502674]\n",
      "[D loss: 0.856951] [G loss: 1.487295]\n",
      "[D loss: 0.922847] [G loss: 1.275202]\n",
      "[D loss: 0.901218] [G loss: 1.392980]\n",
      "[D loss: 0.699310] [G loss: 1.427946]\n",
      "[D loss: 0.905779] [G loss: 1.362346]\n",
      "[D loss: 0.783560] [G loss: 1.247580]\n",
      "[D loss: 0.661973] [G loss: 1.533372]\n",
      "[D loss: 0.781350] [G loss: 1.380494]\n",
      "[D loss: 0.903272] [G loss: 1.419571]\n",
      "[D loss: 0.842946] [G loss: 1.388330]\n",
      "[D loss: 0.872009] [G loss: 1.507843]\n",
      "[D loss: 1.031060] [G loss: 1.566441]\n",
      "[D loss: 0.860797] [G loss: 1.464871]\n",
      "[D loss: 0.879795] [G loss: 1.500358]\n",
      "[D loss: 0.848059] [G loss: 1.457113]\n",
      "[D loss: 0.797306] [G loss: 1.556949]\n",
      "[D loss: 0.896887] [G loss: 1.596665]\n",
      "[D loss: 0.797958] [G loss: 1.532228]\n",
      "[D loss: 0.892730] [G loss: 1.403536]\n",
      "[D loss: 0.940490] [G loss: 1.148561]\n",
      "[D loss: 0.983930] [G loss: 1.161190]\n",
      "[D loss: 0.470773] [G loss: 1.400564]\n",
      "[D loss: 0.876315] [G loss: 1.328163]\n",
      "[D loss: 0.713899] [G loss: 1.590511]\n",
      "[D loss: 0.755416] [G loss: 1.413109]\n",
      "[D loss: 0.829206] [G loss: 1.452765]\n",
      "[D loss: 1.001159] [G loss: 1.373267]\n",
      "[D loss: 0.820365] [G loss: 1.342813]\n",
      "[D loss: 0.764529] [G loss: 1.543290]\n",
      "[D loss: 0.916342] [G loss: 1.549372]\n",
      "[D loss: 0.739169] [G loss: 1.471662]\n",
      "[D loss: 0.623835] [G loss: 1.391124]\n",
      "[D loss: 0.738995] [G loss: 1.364914]\n",
      "[D loss: 0.713834] [G loss: 1.495409]\n",
      "[D loss: 1.112306] [G loss: 1.663919]\n",
      "[D loss: 0.814790] [G loss: 1.392212]\n",
      "[D loss: 0.835264] [G loss: 1.470277]\n",
      "[D loss: 0.790394] [G loss: 1.294519]\n",
      "[D loss: 0.793812] [G loss: 1.529014]\n",
      "[D loss: 0.706358] [G loss: 1.664151]\n",
      "[D loss: 0.655842] [G loss: 1.529588]\n",
      "[D loss: 0.912282] [G loss: 1.617685]\n",
      "[D loss: 0.613870] [G loss: 1.518838]\n",
      "[D loss: 0.843667] [G loss: 1.453709]\n",
      "[D loss: 0.983770] [G loss: 1.496676]\n",
      "[D loss: 1.012477] [G loss: 1.411142]\n",
      "[D loss: 0.845402] [G loss: 1.280496]\n",
      "[D loss: 0.934783] [G loss: 1.394006]\n",
      "[D loss: 0.965903] [G loss: 1.403267]\n",
      "[D loss: 0.702441] [G loss: 1.502243]\n",
      "[D loss: 0.798910] [G loss: 1.357054]\n",
      "[D loss: 0.764523] [G loss: 1.423231]\n",
      "[D loss: 1.085065] [G loss: 1.473650]\n",
      "[D loss: 0.700975] [G loss: 1.485906]\n",
      "[D loss: 0.812574] [G loss: 1.348036]\n",
      "[D loss: 1.052901] [G loss: 1.373481]\n",
      "[D loss: 0.857431] [G loss: 1.329526]\n",
      "[D loss: 0.668383] [G loss: 1.330429]\n",
      "[D loss: 0.661876] [G loss: 1.636812]\n",
      "[D loss: 0.898771] [G loss: 1.337598]\n",
      "[D loss: 0.840483] [G loss: 1.507711]\n",
      "[D loss: 0.895767] [G loss: 1.366164]\n",
      "[D loss: 0.994554] [G loss: 1.467490]\n",
      "[D loss: 0.866813] [G loss: 1.353891]\n",
      "[D loss: 0.796273] [G loss: 1.325698]\n",
      "[D loss: 0.657986] [G loss: 1.468052]\n",
      "[D loss: 0.975758] [G loss: 1.413570]\n",
      "[D loss: 0.709391] [G loss: 1.327397]\n",
      "[D loss: 0.840666] [G loss: 1.410744]\n",
      "[D loss: 0.702205] [G loss: 1.441176]\n",
      "[D loss: 0.891756] [G loss: 1.488546]\n",
      "[D loss: 0.929501] [G loss: 1.495442]\n",
      "[D loss: 0.823577] [G loss: 1.455035]\n",
      "[D loss: 0.719167] [G loss: 1.375390]\n",
      "[D loss: 0.822697] [G loss: 1.371659]\n",
      "[D loss: 0.778211] [G loss: 1.420476]\n",
      "[D loss: 0.753943] [G loss: 1.624119]\n",
      "[D loss: 0.682685] [G loss: 1.643027]\n",
      "[D loss: 0.931992] [G loss: 1.497110]\n",
      "[D loss: 0.725626] [G loss: 1.428523]\n",
      "[D loss: 0.855665] [G loss: 1.386259]\n",
      "[D loss: 0.733490] [G loss: 1.371907]\n",
      "[D loss: 0.693020] [G loss: 1.437528]\n",
      "[D loss: 1.093505] [G loss: 1.332960]\n",
      "[D loss: 0.874851] [G loss: 1.384224]\n",
      "[D loss: 0.790737] [G loss: 1.270928]\n",
      "[D loss: 0.819134] [G loss: 1.402787]\n",
      "[D loss: 1.064200] [G loss: 1.645186]\n",
      "[D loss: 0.670333] [G loss: 1.630452]\n",
      "[D loss: 0.752595] [G loss: 1.525356]\n",
      "[D loss: 0.918383] [G loss: 1.261482]\n",
      "[D loss: 0.745713] [G loss: 1.541361]\n",
      "[D loss: 0.828415] [G loss: 1.555705]\n",
      "[D loss: 1.048794] [G loss: 1.421736]\n",
      "[D loss: 0.721467] [G loss: 1.333189]\n",
      "[D loss: 0.721931] [G loss: 1.567485]\n",
      "[D loss: 0.645544] [G loss: 1.692263]\n",
      "[D loss: 0.833758] [G loss: 1.513603]\n",
      "[D loss: 0.936025] [G loss: 1.343271]\n",
      "epoch:3, g_loss:2795.3896484375,d_loss:1539.662353515625\n",
      "[D loss: 0.794310] [G loss: 1.174187]\n",
      "[D loss: 0.766879] [G loss: 1.350039]\n",
      "[D loss: 0.727402] [G loss: 1.694278]\n",
      "[D loss: 0.686667] [G loss: 1.526756]\n",
      "[D loss: 0.783523] [G loss: 1.539122]\n",
      "[D loss: 0.638251] [G loss: 1.707230]\n",
      "[D loss: 0.757899] [G loss: 1.787590]\n",
      "[D loss: 0.993041] [G loss: 1.477662]\n",
      "[D loss: 0.866601] [G loss: 1.371388]\n",
      "[D loss: 0.770521] [G loss: 1.552548]\n",
      "[D loss: 0.967535] [G loss: 1.332093]\n",
      "[D loss: 0.859026] [G loss: 1.507933]\n",
      "[D loss: 0.879013] [G loss: 1.619221]\n",
      "[D loss: 0.912582] [G loss: 1.443002]\n",
      "[D loss: 0.875969] [G loss: 1.609523]\n",
      "[D loss: 0.784518] [G loss: 1.506617]\n",
      "[D loss: 0.821627] [G loss: 1.356733]\n",
      "[D loss: 0.732779] [G loss: 1.376802]\n",
      "[D loss: 0.935449] [G loss: 1.486049]\n",
      "[D loss: 1.039710] [G loss: 1.307310]\n",
      "[D loss: 0.723911] [G loss: 1.623562]\n",
      "[D loss: 0.670318] [G loss: 1.562824]\n",
      "[D loss: 0.776229] [G loss: 1.611706]\n",
      "[D loss: 0.772233] [G loss: 1.673752]\n",
      "[D loss: 0.973591] [G loss: 1.470823]\n",
      "[D loss: 0.971126] [G loss: 1.687246]\n",
      "[D loss: 0.700397] [G loss: 1.456015]\n",
      "[D loss: 0.755346] [G loss: 1.514338]\n",
      "[D loss: 0.758378] [G loss: 1.654698]\n",
      "[D loss: 0.933571] [G loss: 1.596308]\n",
      "[D loss: 0.553985] [G loss: 1.387179]\n",
      "[D loss: 0.792846] [G loss: 1.280879]\n",
      "[D loss: 0.760397] [G loss: 1.437088]\n",
      "[D loss: 0.673603] [G loss: 1.663193]\n",
      "[D loss: 0.774882] [G loss: 1.780045]\n",
      "[D loss: 0.935052] [G loss: 1.700083]\n",
      "[D loss: 0.664608] [G loss: 1.432755]\n",
      "[D loss: 1.019660] [G loss: 1.266918]\n",
      "[D loss: 0.663834] [G loss: 1.598191]\n",
      "[D loss: 0.869371] [G loss: 1.343146]\n",
      "[D loss: 0.859656] [G loss: 1.414829]\n",
      "[D loss: 0.931359] [G loss: 1.483978]\n",
      "[D loss: 0.789271] [G loss: 1.490164]\n",
      "[D loss: 0.613665] [G loss: 1.452881]\n",
      "[D loss: 0.821528] [G loss: 1.511442]\n",
      "[D loss: 0.898085] [G loss: 1.539020]\n",
      "[D loss: 0.898882] [G loss: 1.324050]\n",
      "[D loss: 0.709082] [G loss: 1.520777]\n",
      "[D loss: 0.979454] [G loss: 1.433832]\n",
      "[D loss: 1.097173] [G loss: 1.294533]\n",
      "[D loss: 0.966629] [G loss: 1.449685]\n",
      "[D loss: 0.723449] [G loss: 1.587991]\n",
      "[D loss: 0.793903] [G loss: 1.516741]\n",
      "[D loss: 0.865152] [G loss: 1.374656]\n",
      "[D loss: 0.960733] [G loss: 1.355967]\n",
      "[D loss: 0.932831] [G loss: 1.266442]\n",
      "[D loss: 0.711439] [G loss: 1.520830]\n",
      "[D loss: 0.634955] [G loss: 1.391937]\n",
      "[D loss: 0.749558] [G loss: 1.480081]\n",
      "[D loss: 0.990428] [G loss: 1.466132]\n",
      "[D loss: 0.739071] [G loss: 1.776611]\n",
      "[D loss: 0.780048] [G loss: 1.429382]\n",
      "[D loss: 0.832062] [G loss: 1.532481]\n",
      "[D loss: 0.870677] [G loss: 1.649569]\n",
      "[D loss: 1.008955] [G loss: 1.199267]\n",
      "[D loss: 0.720100] [G loss: 1.398459]\n",
      "[D loss: 0.767346] [G loss: 1.335825]\n",
      "[D loss: 0.923283] [G loss: 1.451070]\n",
      "[D loss: 0.680009] [G loss: 1.318806]\n",
      "[D loss: 0.482534] [G loss: 1.701897]\n",
      "[D loss: 0.852360] [G loss: 1.553889]\n",
      "[D loss: 0.925691] [G loss: 1.498588]\n",
      "[D loss: 0.909925] [G loss: 1.477284]\n",
      "[D loss: 0.844678] [G loss: 1.432214]\n",
      "[D loss: 0.685287] [G loss: 1.687864]\n",
      "[D loss: 0.982380] [G loss: 1.347007]\n",
      "[D loss: 0.568630] [G loss: 1.680123]\n",
      "[D loss: 0.876604] [G loss: 1.457080]\n",
      "[D loss: 0.798586] [G loss: 1.581866]\n",
      "[D loss: 0.861170] [G loss: 1.361355]\n",
      "[D loss: 0.921620] [G loss: 1.556450]\n",
      "[D loss: 0.847742] [G loss: 1.519536]\n",
      "[D loss: 0.993846] [G loss: 1.569415]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 1.187035] [G loss: 1.269688]\n",
      "[D loss: 0.739471] [G loss: 1.591087]\n",
      "[D loss: 1.045164] [G loss: 1.736548]\n",
      "[D loss: 0.767934] [G loss: 1.625433]\n",
      "[D loss: 0.780474] [G loss: 1.394078]\n",
      "[D loss: 0.912210] [G loss: 1.290792]\n",
      "[D loss: 0.798338] [G loss: 1.475012]\n",
      "[D loss: 0.895370] [G loss: 1.442002]\n",
      "[D loss: 0.899460] [G loss: 1.349216]\n",
      "[D loss: 0.688295] [G loss: 1.461700]\n",
      "[D loss: 0.785392] [G loss: 1.442779]\n",
      "[D loss: 0.846819] [G loss: 1.685063]\n",
      "[D loss: 0.716493] [G loss: 1.601861]\n",
      "[D loss: 0.826101] [G loss: 1.554104]\n",
      "[D loss: 0.832057] [G loss: 1.653542]\n",
      "[D loss: 0.992239] [G loss: 1.212489]\n",
      "[D loss: 0.867961] [G loss: 1.519179]\n",
      "[D loss: 0.624220] [G loss: 1.353793]\n",
      "[D loss: 0.894841] [G loss: 1.630846]\n",
      "[D loss: 0.747417] [G loss: 1.486459]\n",
      "[D loss: 0.746911] [G loss: 1.389651]\n",
      "[D loss: 0.788744] [G loss: 1.681387]\n",
      "[D loss: 0.608700] [G loss: 1.583468]\n",
      "[D loss: 0.890434] [G loss: 1.376982]\n",
      "[D loss: 0.916792] [G loss: 1.282329]\n",
      "[D loss: 0.829180] [G loss: 1.572476]\n",
      "[D loss: 1.089168] [G loss: 1.391912]\n",
      "[D loss: 0.924939] [G loss: 1.799221]\n",
      "[D loss: 0.676975] [G loss: 1.467086]\n",
      "[D loss: 0.836937] [G loss: 1.714626]\n",
      "[D loss: 0.818694] [G loss: 1.580918]\n",
      "[D loss: 0.732679] [G loss: 1.286252]\n",
      "[D loss: 0.867458] [G loss: 1.427871]\n",
      "[D loss: 0.748411] [G loss: 1.624380]\n",
      "[D loss: 0.894678] [G loss: 1.514852]\n",
      "[D loss: 0.803890] [G loss: 1.433293]\n",
      "[D loss: 0.650741] [G loss: 1.494131]\n",
      "[D loss: 0.777678] [G loss: 1.375248]\n",
      "[D loss: 0.638345] [G loss: 1.517683]\n",
      "[D loss: 0.655965] [G loss: 1.467188]\n",
      "[D loss: 0.807052] [G loss: 1.598200]\n",
      "[D loss: 0.989992] [G loss: 1.490703]\n",
      "[D loss: 0.858236] [G loss: 1.509178]\n",
      "[D loss: 0.855829] [G loss: 1.536737]\n",
      "[D loss: 0.753281] [G loss: 1.796122]\n",
      "[D loss: 0.719145] [G loss: 1.789077]\n",
      "[D loss: 0.873898] [G loss: 1.423569]\n",
      "[D loss: 0.800318] [G loss: 1.500369]\n",
      "[D loss: 0.687255] [G loss: 1.757179]\n",
      "[D loss: 0.860532] [G loss: 1.607476]\n",
      "[D loss: 0.885218] [G loss: 1.609551]\n",
      "[D loss: 0.976953] [G loss: 1.421978]\n",
      "[D loss: 0.862907] [G loss: 1.443027]\n",
      "[D loss: 0.833223] [G loss: 1.255249]\n",
      "[D loss: 0.785848] [G loss: 1.621583]\n",
      "[D loss: 0.819125] [G loss: 1.499404]\n",
      "[D loss: 0.729719] [G loss: 1.521349]\n",
      "[D loss: 0.785388] [G loss: 1.613798]\n",
      "[D loss: 0.773371] [G loss: 1.603814]\n",
      "[D loss: 0.801549] [G loss: 1.647965]\n",
      "[D loss: 0.757406] [G loss: 1.278389]\n",
      "[D loss: 0.761113] [G loss: 1.471369]\n",
      "[D loss: 0.907513] [G loss: 1.492042]\n",
      "[D loss: 0.853574] [G loss: 1.660909]\n",
      "[D loss: 0.958963] [G loss: 1.339272]\n",
      "[D loss: 0.729255] [G loss: 1.569349]\n",
      "[D loss: 0.766619] [G loss: 1.570405]\n",
      "[D loss: 0.843799] [G loss: 1.741471]\n",
      "[D loss: 0.913677] [G loss: 1.536010]\n",
      "[D loss: 0.925524] [G loss: 1.441845]\n",
      "[D loss: 0.929686] [G loss: 1.473364]\n",
      "[D loss: 0.739950] [G loss: 1.627711]\n",
      "[D loss: 0.846353] [G loss: 1.578884]\n",
      "[D loss: 0.780976] [G loss: 1.560755]\n",
      "[D loss: 0.878091] [G loss: 1.622743]\n",
      "[D loss: 0.780225] [G loss: 1.438488]\n",
      "[D loss: 0.964233] [G loss: 1.540986]\n",
      "[D loss: 0.814376] [G loss: 1.397915]\n",
      "[D loss: 0.723977] [G loss: 1.325664]\n",
      "[D loss: 0.887562] [G loss: 1.526178]\n",
      "[D loss: 0.695484] [G loss: 1.672507]\n",
      "[D loss: 1.025241] [G loss: 1.735677]\n",
      "[D loss: 0.834129] [G loss: 1.603021]\n",
      "[D loss: 1.014405] [G loss: 1.264999]\n",
      "[D loss: 1.035368] [G loss: 1.181664]\n",
      "[D loss: 0.671060] [G loss: 1.348385]\n",
      "[D loss: 0.796326] [G loss: 1.387259]\n",
      "[D loss: 0.873973] [G loss: 1.763962]\n",
      "[D loss: 1.004226] [G loss: 1.514362]\n",
      "[D loss: 0.802626] [G loss: 1.260197]\n",
      "[D loss: 0.696896] [G loss: 1.611544]\n",
      "[D loss: 0.742169] [G loss: 1.305343]\n",
      "[D loss: 0.612707] [G loss: 1.644356]\n",
      "[D loss: 0.734361] [G loss: 1.404525]\n",
      "[D loss: 0.779493] [G loss: 1.429982]\n",
      "[D loss: 1.133042] [G loss: 1.371568]\n",
      "[D loss: 0.756327] [G loss: 1.461168]\n",
      "[D loss: 0.882676] [G loss: 1.499292]\n",
      "[D loss: 0.947912] [G loss: 1.270071]\n",
      "[D loss: 0.990802] [G loss: 1.239897]\n",
      "[D loss: 0.720730] [G loss: 1.361497]\n",
      "[D loss: 0.826129] [G loss: 1.438130]\n",
      "[D loss: 1.079460] [G loss: 1.512385]\n",
      "[D loss: 0.890832] [G loss: 1.507275]\n",
      "[D loss: 0.783096] [G loss: 1.375986]\n",
      "[D loss: 0.665272] [G loss: 1.583732]\n",
      "[D loss: 0.665043] [G loss: 1.572077]\n",
      "[D loss: 0.793581] [G loss: 1.580325]\n",
      "[D loss: 0.735641] [G loss: 1.569267]\n",
      "[D loss: 0.709892] [G loss: 1.594720]\n",
      "[D loss: 0.750589] [G loss: 1.438283]\n",
      "[D loss: 1.004550] [G loss: 1.539538]\n",
      "[D loss: 0.897900] [G loss: 1.435648]\n",
      "[D loss: 0.929000] [G loss: 1.198837]\n",
      "[D loss: 0.889806] [G loss: 1.490476]\n",
      "[D loss: 0.733482] [G loss: 1.657754]\n",
      "[D loss: 0.653077] [G loss: 1.558397]\n",
      "[D loss: 0.734905] [G loss: 1.258708]\n",
      "[D loss: 0.810342] [G loss: 1.605515]\n",
      "[D loss: 0.867554] [G loss: 1.866770]\n",
      "[D loss: 0.637752] [G loss: 1.577763]\n",
      "[D loss: 0.593240] [G loss: 1.432274]\n",
      "[D loss: 0.719628] [G loss: 1.694098]\n",
      "[D loss: 0.769061] [G loss: 1.680470]\n",
      "[D loss: 1.000118] [G loss: 1.671095]\n",
      "[D loss: 0.731885] [G loss: 1.652344]\n",
      "[D loss: 0.679312] [G loss: 1.628261]\n",
      "[D loss: 0.926922] [G loss: 1.493191]\n",
      "[D loss: 0.886450] [G loss: 1.410165]\n",
      "[D loss: 0.928987] [G loss: 1.526329]\n",
      "[D loss: 0.732228] [G loss: 1.517449]\n",
      "[D loss: 0.969276] [G loss: 1.228359]\n",
      "[D loss: 0.820428] [G loss: 1.456920]\n",
      "[D loss: 0.831457] [G loss: 1.443668]\n",
      "[D loss: 0.678877] [G loss: 1.611868]\n",
      "[D loss: 1.102611] [G loss: 1.474878]\n",
      "[D loss: 0.833653] [G loss: 1.620237]\n",
      "[D loss: 0.991396] [G loss: 1.249692]\n",
      "[D loss: 0.679318] [G loss: 1.425334]\n",
      "[D loss: 0.905198] [G loss: 1.190043]\n",
      "[D loss: 0.709644] [G loss: 1.559386]\n",
      "[D loss: 0.743730] [G loss: 1.502356]\n",
      "[D loss: 0.795733] [G loss: 1.367094]\n",
      "[D loss: 0.640264] [G loss: 1.462866]\n",
      "[D loss: 0.690316] [G loss: 1.509441]\n",
      "[D loss: 0.821805] [G loss: 1.496545]\n",
      "[D loss: 0.855700] [G loss: 1.386284]\n",
      "[D loss: 0.860655] [G loss: 1.381787]\n",
      "[D loss: 0.682899] [G loss: 1.466345]\n",
      "[D loss: 0.914271] [G loss: 1.624510]\n",
      "[D loss: 0.898798] [G loss: 1.609556]\n",
      "[D loss: 0.835314] [G loss: 1.634929]\n",
      "[D loss: 0.701023] [G loss: 1.483545]\n",
      "[D loss: 0.712976] [G loss: 1.452022]\n",
      "[D loss: 0.716250] [G loss: 1.592825]\n",
      "[D loss: 0.831457] [G loss: 1.754099]\n",
      "[D loss: 0.895078] [G loss: 1.577557]\n",
      "[D loss: 0.914301] [G loss: 1.469141]\n",
      "[D loss: 0.672296] [G loss: 1.566127]\n",
      "[D loss: 0.700446] [G loss: 1.587894]\n",
      "[D loss: 0.773253] [G loss: 1.670087]\n",
      "[D loss: 0.744868] [G loss: 1.567530]\n",
      "[D loss: 0.791903] [G loss: 1.536137]\n",
      "[D loss: 0.903758] [G loss: 1.341993]\n",
      "[D loss: 0.807540] [G loss: 1.500391]\n",
      "[D loss: 0.738519] [G loss: 1.726243]\n",
      "[D loss: 0.997389] [G loss: 1.429531]\n",
      "[D loss: 0.639375] [G loss: 1.484106]\n",
      "[D loss: 0.932960] [G loss: 1.735265]\n",
      "[D loss: 0.790843] [G loss: 1.507704]\n",
      "[D loss: 0.850735] [G loss: 1.344805]\n",
      "[D loss: 0.824055] [G loss: 1.333604]\n",
      "[D loss: 0.754695] [G loss: 1.439663]\n",
      "[D loss: 0.800153] [G loss: 1.468862]\n",
      "[D loss: 0.779936] [G loss: 1.643891]\n",
      "[D loss: 0.849892] [G loss: 1.410676]\n",
      "[D loss: 0.745699] [G loss: 1.543687]\n",
      "[D loss: 0.816143] [G loss: 1.382160]\n",
      "[D loss: 0.818318] [G loss: 1.352917]\n",
      "[D loss: 0.756510] [G loss: 1.427028]\n",
      "[D loss: 0.789787] [G loss: 1.676900]\n",
      "[D loss: 0.854035] [G loss: 1.429201]\n",
      "[D loss: 0.644864] [G loss: 1.568114]\n",
      "[D loss: 0.785179] [G loss: 1.462980]\n",
      "[D loss: 0.947142] [G loss: 1.474549]\n",
      "[D loss: 0.701493] [G loss: 1.592614]\n",
      "[D loss: 0.782965] [G loss: 1.533372]\n",
      "[D loss: 0.593926] [G loss: 1.481511]\n",
      "[D loss: 0.644700] [G loss: 1.430053]\n",
      "[D loss: 0.760489] [G loss: 1.809159]\n",
      "[D loss: 0.968187] [G loss: 1.381647]\n",
      "[D loss: 0.623875] [G loss: 1.569583]\n",
      "[D loss: 0.948831] [G loss: 1.525120]\n",
      "[D loss: 0.622285] [G loss: 1.608210]\n",
      "[D loss: 0.864865] [G loss: 1.545461]\n",
      "[D loss: 0.761909] [G loss: 1.540040]\n",
      "[D loss: 0.811120] [G loss: 1.292128]\n",
      "[D loss: 0.778439] [G loss: 1.492284]\n",
      "[D loss: 0.779737] [G loss: 1.449826]\n",
      "[D loss: 0.817104] [G loss: 1.402874]\n",
      "[D loss: 0.701438] [G loss: 1.748714]\n",
      "[D loss: 0.626391] [G loss: 1.664833]\n",
      "[D loss: 0.838691] [G loss: 1.459798]\n",
      "[D loss: 0.909748] [G loss: 1.212290]\n",
      "[D loss: 0.909316] [G loss: 1.651534]\n",
      "[D loss: 0.966952] [G loss: 1.399596]\n",
      "[D loss: 1.037410] [G loss: 1.646008]\n",
      "[D loss: 0.919241] [G loss: 1.509444]\n",
      "[D loss: 0.736001] [G loss: 1.706971]\n",
      "[D loss: 0.777796] [G loss: 1.524833]\n",
      "[D loss: 0.799896] [G loss: 1.424899]\n",
      "[D loss: 0.703430] [G loss: 1.390572]\n",
      "[D loss: 0.731611] [G loss: 1.443219]\n",
      "[D loss: 0.833705] [G loss: 1.560948]\n",
      "[D loss: 0.747982] [G loss: 1.590080]\n",
      "[D loss: 0.834643] [G loss: 1.484144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.825274] [G loss: 1.676877]\n",
      "[D loss: 0.772709] [G loss: 1.530390]\n",
      "[D loss: 0.603168] [G loss: 1.636595]\n",
      "[D loss: 0.812848] [G loss: 1.729382]\n",
      "[D loss: 0.874691] [G loss: 1.580631]\n",
      "[D loss: 0.691451] [G loss: 1.435530]\n",
      "[D loss: 0.610153] [G loss: 1.613346]\n",
      "[D loss: 0.667775] [G loss: 1.650436]\n",
      "[D loss: 0.932358] [G loss: 1.617965]\n",
      "[D loss: 0.735658] [G loss: 1.299804]\n",
      "[D loss: 0.720716] [G loss: 1.425478]\n",
      "[D loss: 0.748347] [G loss: 1.581349]\n",
      "[D loss: 0.730525] [G loss: 1.715820]\n",
      "[D loss: 0.655917] [G loss: 1.872986]\n",
      "[D loss: 0.901729] [G loss: 1.586607]\n",
      "[D loss: 0.742675] [G loss: 1.425246]\n",
      "[D loss: 0.764945] [G loss: 1.617306]\n",
      "[D loss: 0.767046] [G loss: 1.550558]\n",
      "[D loss: 0.850191] [G loss: 1.453571]\n",
      "[D loss: 0.721805] [G loss: 1.554023]\n",
      "[D loss: 0.835816] [G loss: 1.842776]\n",
      "[D loss: 0.733849] [G loss: 1.577178]\n",
      "[D loss: 1.021176] [G loss: 1.728640]\n",
      "[D loss: 1.060862] [G loss: 1.427030]\n",
      "[D loss: 0.977575] [G loss: 1.396996]\n",
      "[D loss: 0.929502] [G loss: 1.448649]\n",
      "[D loss: 0.672219] [G loss: 1.489246]\n",
      "[D loss: 0.831660] [G loss: 1.442477]\n",
      "[D loss: 0.752785] [G loss: 1.377479]\n",
      "[D loss: 1.278382] [G loss: 1.059456]\n",
      "[D loss: 0.589921] [G loss: 1.473680]\n",
      "[D loss: 0.608387] [G loss: 1.756818]\n",
      "[D loss: 0.693312] [G loss: 1.663739]\n",
      "[D loss: 0.822455] [G loss: 1.641132]\n",
      "[D loss: 0.731518] [G loss: 1.732283]\n",
      "[D loss: 0.970707] [G loss: 1.519219]\n",
      "[D loss: 0.746565] [G loss: 1.606059]\n",
      "[D loss: 0.794290] [G loss: 1.403088]\n",
      "[D loss: 0.637782] [G loss: 1.452543]\n",
      "[D loss: 1.041226] [G loss: 1.660754]\n",
      "[D loss: 0.653654] [G loss: 1.583255]\n",
      "[D loss: 1.012163] [G loss: 1.786612]\n",
      "[D loss: 0.973315] [G loss: 1.540623]\n",
      "[D loss: 0.504413] [G loss: 1.476427]\n",
      "[D loss: 0.809670] [G loss: 1.541933]\n",
      "[D loss: 0.868584] [G loss: 1.337395]\n",
      "[D loss: 0.733041] [G loss: 1.391026]\n",
      "[D loss: 0.904736] [G loss: 1.459448]\n",
      "[D loss: 0.823757] [G loss: 1.613057]\n",
      "[D loss: 0.769826] [G loss: 1.605193]\n",
      "[D loss: 0.926512] [G loss: 1.539636]\n",
      "[D loss: 0.759191] [G loss: 1.573617]\n",
      "[D loss: 0.620812] [G loss: 1.431783]\n",
      "[D loss: 0.543398] [G loss: 1.605581]\n",
      "[D loss: 0.872907] [G loss: 1.521535]\n",
      "[D loss: 0.571166] [G loss: 1.483771]\n",
      "[D loss: 0.671145] [G loss: 1.522459]\n",
      "[D loss: 0.866343] [G loss: 1.685991]\n",
      "[D loss: 0.746465] [G loss: 1.731584]\n",
      "[D loss: 0.903625] [G loss: 1.527139]\n",
      "[D loss: 1.050881] [G loss: 1.536855]\n",
      "[D loss: 0.689818] [G loss: 1.640986]\n",
      "[D loss: 0.763435] [G loss: 1.575544]\n",
      "[D loss: 0.768826] [G loss: 1.494318]\n",
      "[D loss: 0.806236] [G loss: 1.574642]\n",
      "[D loss: 0.650234] [G loss: 1.657652]\n",
      "[D loss: 0.775355] [G loss: 1.710846]\n",
      "[D loss: 0.769734] [G loss: 1.684566]\n",
      "[D loss: 0.817457] [G loss: 1.415769]\n",
      "[D loss: 0.718194] [G loss: 1.667849]\n",
      "[D loss: 0.791265] [G loss: 1.408006]\n",
      "[D loss: 0.764791] [G loss: 1.520958]\n",
      "[D loss: 0.798238] [G loss: 1.595924]\n",
      "[D loss: 0.767158] [G loss: 1.526903]\n",
      "[D loss: 0.787377] [G loss: 1.466810]\n",
      "[D loss: 0.895127] [G loss: 1.504374]\n",
      "[D loss: 0.913405] [G loss: 1.477529]\n",
      "[D loss: 1.227858] [G loss: 1.248327]\n",
      "[D loss: 0.755389] [G loss: 1.380600]\n",
      "[D loss: 0.947860] [G loss: 1.568628]\n",
      "[D loss: 0.796834] [G loss: 1.327595]\n",
      "[D loss: 0.726559] [G loss: 1.402781]\n",
      "[D loss: 0.887831] [G loss: 1.703968]\n",
      "[D loss: 0.597586] [G loss: 1.616614]\n",
      "[D loss: 0.962011] [G loss: 1.244838]\n",
      "[D loss: 0.505891] [G loss: 1.620002]\n",
      "[D loss: 0.759973] [G loss: 1.613162]\n",
      "[D loss: 0.834863] [G loss: 1.556476]\n",
      "[D loss: 0.538154] [G loss: 1.506981]\n",
      "[D loss: 0.804790] [G loss: 1.385716]\n",
      "[D loss: 1.045063] [G loss: 1.531157]\n",
      "[D loss: 0.714652] [G loss: 1.447568]\n",
      "[D loss: 0.805207] [G loss: 1.505318]\n",
      "[D loss: 0.811480] [G loss: 1.416723]\n",
      "[D loss: 0.808632] [G loss: 1.599182]\n",
      "[D loss: 1.026641] [G loss: 1.327412]\n",
      "[D loss: 0.934180] [G loss: 1.476672]\n",
      "[D loss: 0.890358] [G loss: 1.441977]\n",
      "[D loss: 0.798867] [G loss: 1.431305]\n",
      "[D loss: 0.770564] [G loss: 1.513301]\n",
      "[D loss: 0.560325] [G loss: 1.424676]\n",
      "[D loss: 0.682540] [G loss: 1.452688]\n",
      "[D loss: 0.783868] [G loss: 1.443227]\n",
      "[D loss: 0.549653] [G loss: 1.445852]\n",
      "[D loss: 0.735796] [G loss: 1.265132]\n",
      "[D loss: 0.618720] [G loss: 1.741981]\n",
      "[D loss: 0.740968] [G loss: 1.717701]\n",
      "[D loss: 0.830124] [G loss: 1.840882]\n",
      "[D loss: 1.103513] [G loss: 1.501605]\n",
      "[D loss: 0.752428] [G loss: 1.466320]\n",
      "[D loss: 0.699011] [G loss: 1.348792]\n",
      "[D loss: 1.069471] [G loss: 1.468905]\n",
      "[D loss: 0.726329] [G loss: 1.631938]\n",
      "[D loss: 0.987136] [G loss: 1.560837]\n",
      "[D loss: 0.893099] [G loss: 1.455674]\n",
      "[D loss: 0.942606] [G loss: 1.526668]\n",
      "[D loss: 0.912117] [G loss: 1.493956]\n",
      "[D loss: 0.748806] [G loss: 1.591514]\n",
      "[D loss: 0.761438] [G loss: 1.632062]\n",
      "[D loss: 0.866393] [G loss: 1.367714]\n",
      "[D loss: 0.872854] [G loss: 1.283092]\n",
      "[D loss: 0.724273] [G loss: 1.485501]\n",
      "[D loss: 0.743259] [G loss: 1.331834]\n",
      "[D loss: 0.749585] [G loss: 1.549645]\n",
      "[D loss: 0.709811] [G loss: 1.425105]\n",
      "[D loss: 0.810403] [G loss: 1.413908]\n",
      "[D loss: 1.003884] [G loss: 1.490865]\n",
      "[D loss: 0.937268] [G loss: 1.407874]\n",
      "[D loss: 0.866384] [G loss: 1.452412]\n",
      "[D loss: 0.775684] [G loss: 1.602408]\n",
      "[D loss: 0.832291] [G loss: 1.391058]\n",
      "[D loss: 0.817465] [G loss: 1.536157]\n",
      "[D loss: 0.779010] [G loss: 1.397429]\n",
      "[D loss: 0.787518] [G loss: 1.580632]\n",
      "[D loss: 1.034435] [G loss: 1.452529]\n",
      "[D loss: 0.929760] [G loss: 1.451928]\n",
      "[D loss: 0.935000] [G loss: 1.361058]\n",
      "[D loss: 0.840245] [G loss: 1.493365]\n",
      "[D loss: 0.703317] [G loss: 1.312971]\n",
      "[D loss: 0.795975] [G loss: 1.433072]\n",
      "[D loss: 0.993548] [G loss: 1.247363]\n",
      "[D loss: 0.709606] [G loss: 1.892105]\n",
      "[D loss: 0.841984] [G loss: 1.516879]\n",
      "[D loss: 0.732009] [G loss: 1.561459]\n",
      "[D loss: 0.564427] [G loss: 1.443911]\n",
      "[D loss: 0.713398] [G loss: 1.358992]\n",
      "[D loss: 0.779563] [G loss: 1.357682]\n",
      "[D loss: 0.730400] [G loss: 1.559965]\n",
      "[D loss: 0.960454] [G loss: 1.730318]\n",
      "[D loss: 0.729540] [G loss: 1.816417]\n",
      "[D loss: 0.656156] [G loss: 1.628268]\n",
      "[D loss: 0.705236] [G loss: 1.735251]\n",
      "[D loss: 0.875900] [G loss: 1.567425]\n",
      "[D loss: 0.716526] [G loss: 1.448389]\n",
      "[D loss: 0.887858] [G loss: 1.373521]\n",
      "[D loss: 1.145676] [G loss: 1.496856]\n",
      "[D loss: 0.858346] [G loss: 1.776382]\n",
      "[D loss: 0.813600] [G loss: 1.756730]\n",
      "[D loss: 0.895757] [G loss: 1.800183]\n",
      "[D loss: 0.892024] [G loss: 1.284187]\n",
      "[D loss: 0.791512] [G loss: 1.393176]\n",
      "[D loss: 0.758678] [G loss: 1.561505]\n",
      "[D loss: 1.004101] [G loss: 1.523821]\n",
      "[D loss: 0.614631] [G loss: 1.670686]\n",
      "[D loss: 0.473086] [G loss: 1.559451]\n",
      "[D loss: 0.794453] [G loss: 1.443447]\n",
      "[D loss: 0.776899] [G loss: 1.494171]\n",
      "[D loss: 0.760972] [G loss: 1.571351]\n",
      "[D loss: 0.711843] [G loss: 1.725575]\n",
      "[D loss: 1.054199] [G loss: 1.635625]\n",
      "[D loss: 0.917049] [G loss: 1.695058]\n",
      "[D loss: 0.861075] [G loss: 1.705490]\n",
      "[D loss: 0.669139] [G loss: 1.532449]\n",
      "[D loss: 0.937174] [G loss: 1.619978]\n",
      "[D loss: 0.773333] [G loss: 1.332271]\n",
      "[D loss: 0.924090] [G loss: 1.383345]\n",
      "[D loss: 0.810202] [G loss: 1.657139]\n",
      "[D loss: 0.784433] [G loss: 1.401117]\n",
      "[D loss: 1.086564] [G loss: 1.368439]\n",
      "[D loss: 0.773391] [G loss: 1.540800]\n",
      "[D loss: 1.012848] [G loss: 1.453052]\n",
      "[D loss: 0.825874] [G loss: 1.573671]\n",
      "[D loss: 0.978540] [G loss: 1.324446]\n",
      "[D loss: 0.651956] [G loss: 1.567789]\n",
      "[D loss: 0.791431] [G loss: 1.437355]\n",
      "[D loss: 1.055792] [G loss: 1.534719]\n",
      "[D loss: 1.072923] [G loss: 1.589437]\n",
      "[D loss: 0.907117] [G loss: 1.539767]\n",
      "[D loss: 0.760490] [G loss: 1.629531]\n",
      "[D loss: 0.947684] [G loss: 1.658458]\n",
      "[D loss: 0.938513] [G loss: 1.365638]\n",
      "[D loss: 0.756907] [G loss: 1.582345]\n",
      "[D loss: 0.823003] [G loss: 1.536301]\n",
      "[D loss: 0.961184] [G loss: 1.421743]\n",
      "[D loss: 0.756438] [G loss: 1.453895]\n",
      "[D loss: 0.838061] [G loss: 1.237261]\n",
      "[D loss: 0.917270] [G loss: 1.523478]\n",
      "[D loss: 0.777375] [G loss: 1.347529]\n",
      "[D loss: 0.781011] [G loss: 1.505557]\n",
      "[D loss: 1.020638] [G loss: 1.350996]\n",
      "[D loss: 1.100330] [G loss: 1.383621]\n",
      "[D loss: 0.819705] [G loss: 1.295308]\n",
      "[D loss: 0.872049] [G loss: 1.332255]\n",
      "[D loss: 0.956807] [G loss: 1.363314]\n",
      "[D loss: 0.951485] [G loss: 1.689229]\n",
      "[D loss: 0.725423] [G loss: 1.575746]\n",
      "[D loss: 0.732440] [G loss: 1.509233]\n",
      "[D loss: 0.687530] [G loss: 1.327601]\n",
      "[D loss: 0.884769] [G loss: 1.353776]\n",
      "[D loss: 0.803911] [G loss: 1.555491]\n",
      "[D loss: 0.785882] [G loss: 1.428497]\n",
      "[D loss: 0.833520] [G loss: 1.589196]\n",
      "[D loss: 1.009630] [G loss: 1.625042]\n",
      "[D loss: 0.894870] [G loss: 1.339773]\n",
      "[D loss: 0.804981] [G loss: 1.232354]\n",
      "[D loss: 0.923560] [G loss: 1.324142]\n",
      "[D loss: 0.885919] [G loss: 1.442713]\n",
      "[D loss: 0.728211] [G loss: 1.329464]\n",
      "[D loss: 0.704283] [G loss: 1.405986]\n",
      "[D loss: 0.755277] [G loss: 1.474132]\n",
      "[D loss: 0.742786] [G loss: 1.332739]\n",
      "[D loss: 0.741430] [G loss: 1.507443]\n",
      "[D loss: 0.746281] [G loss: 1.465761]\n",
      "[D loss: 0.952211] [G loss: 1.530579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 1.028125] [G loss: 1.438399]\n",
      "[D loss: 0.866334] [G loss: 1.587569]\n",
      "[D loss: 0.775801] [G loss: 1.306553]\n",
      "[D loss: 0.682009] [G loss: 1.489852]\n",
      "[D loss: 0.997156] [G loss: 1.470197]\n",
      "[D loss: 0.766919] [G loss: 1.470833]\n",
      "[D loss: 0.760971] [G loss: 1.420002]\n",
      "[D loss: 0.919326] [G loss: 1.486882]\n",
      "[D loss: 0.921052] [G loss: 1.505632]\n",
      "[D loss: 1.091170] [G loss: 1.307476]\n",
      "[D loss: 0.736419] [G loss: 1.468320]\n",
      "[D loss: 0.837474] [G loss: 1.358333]\n",
      "[D loss: 0.924776] [G loss: 1.371125]\n",
      "[D loss: 0.856232] [G loss: 1.428234]\n",
      "[D loss: 0.934431] [G loss: 1.396649]\n",
      "[D loss: 0.850973] [G loss: 1.296810]\n",
      "[D loss: 0.839445] [G loss: 1.489671]\n",
      "[D loss: 0.778735] [G loss: 1.547948]\n",
      "[D loss: 0.777706] [G loss: 1.380851]\n",
      "[D loss: 1.083249] [G loss: 1.370469]\n",
      "[D loss: 0.968919] [G loss: 1.448634]\n",
      "[D loss: 0.928029] [G loss: 1.475996]\n",
      "[D loss: 0.627934] [G loss: 1.550678]\n",
      "[D loss: 0.852653] [G loss: 1.581195]\n",
      "[D loss: 0.654083] [G loss: 1.458611]\n",
      "[D loss: 0.984761] [G loss: 1.268112]\n",
      "[D loss: 0.760224] [G loss: 1.444710]\n",
      "[D loss: 0.920043] [G loss: 1.448807]\n",
      "[D loss: 0.974496] [G loss: 1.419408]\n",
      "[D loss: 0.997074] [G loss: 1.321510]\n",
      "[D loss: 0.705574] [G loss: 1.312761]\n",
      "[D loss: 0.810964] [G loss: 1.591829]\n",
      "[D loss: 0.849263] [G loss: 1.434059]\n",
      "[D loss: 1.005433] [G loss: 1.367678]\n",
      "[D loss: 0.878265] [G loss: 1.253161]\n",
      "[D loss: 1.021887] [G loss: 1.146668]\n",
      "[D loss: 0.764485] [G loss: 1.456895]\n",
      "[D loss: 0.880366] [G loss: 1.415599]\n",
      "[D loss: 0.771951] [G loss: 1.353265]\n",
      "[D loss: 0.748077] [G loss: 1.414279]\n",
      "[D loss: 0.646636] [G loss: 1.333677]\n",
      "[D loss: 0.675326] [G loss: 1.516163]\n",
      "[D loss: 0.843728] [G loss: 1.412673]\n",
      "[D loss: 0.635137] [G loss: 1.369089]\n",
      "[D loss: 0.779207] [G loss: 1.463874]\n",
      "[D loss: 0.973890] [G loss: 1.512605]\n",
      "[D loss: 0.707295] [G loss: 1.563945]\n",
      "[D loss: 0.863587] [G loss: 1.630981]\n",
      "[D loss: 0.873491] [G loss: 1.555088]\n",
      "[D loss: 0.849702] [G loss: 1.394468]\n",
      "[D loss: 0.842508] [G loss: 1.629910]\n",
      "[D loss: 0.899559] [G loss: 1.441786]\n",
      "[D loss: 0.914331] [G loss: 1.446371]\n",
      "[D loss: 0.993899] [G loss: 1.438215]\n",
      "[D loss: 0.976155] [G loss: 1.322383]\n",
      "[D loss: 0.810971] [G loss: 1.644409]\n",
      "[D loss: 0.802439] [G loss: 1.763021]\n",
      "[D loss: 0.839770] [G loss: 1.304385]\n",
      "[D loss: 0.751598] [G loss: 1.341167]\n",
      "[D loss: 0.899540] [G loss: 1.379013]\n",
      "[D loss: 1.034344] [G loss: 1.340417]\n",
      "[D loss: 0.808871] [G loss: 1.275175]\n",
      "[D loss: 1.050680] [G loss: 1.117050]\n",
      "[D loss: 0.886938] [G loss: 1.505287]\n",
      "[D loss: 0.687841] [G loss: 1.353171]\n",
      "[D loss: 1.031646] [G loss: 1.448221]\n",
      "[D loss: 0.998308] [G loss: 1.423795]\n",
      "[D loss: 0.547681] [G loss: 1.370574]\n",
      "[D loss: 0.829724] [G loss: 1.449861]\n",
      "[D loss: 0.744110] [G loss: 1.312862]\n",
      "[D loss: 0.695267] [G loss: 1.518573]\n",
      "[D loss: 0.867024] [G loss: 1.477304]\n",
      "[D loss: 1.017262] [G loss: 1.575760]\n",
      "[D loss: 0.763602] [G loss: 1.446923]\n",
      "[D loss: 0.693477] [G loss: 1.456984]\n",
      "[D loss: 0.742237] [G loss: 1.477949]\n",
      "[D loss: 0.614630] [G loss: 1.671983]\n",
      "[D loss: 1.077339] [G loss: 1.514842]\n",
      "[D loss: 1.064858] [G loss: 1.213815]\n",
      "[D loss: 0.901585] [G loss: 1.277174]\n",
      "[D loss: 0.819562] [G loss: 1.532440]\n",
      "[D loss: 0.777716] [G loss: 1.625901]\n",
      "[D loss: 0.624157] [G loss: 1.488770]\n",
      "[D loss: 0.698015] [G loss: 1.478232]\n",
      "[D loss: 0.650969] [G loss: 1.453016]\n",
      "[D loss: 0.922156] [G loss: 1.419394]\n",
      "[D loss: 0.845580] [G loss: 1.347806]\n",
      "[D loss: 0.762431] [G loss: 1.304101]\n",
      "[D loss: 1.070209] [G loss: 1.513857]\n",
      "[D loss: 0.867579] [G loss: 1.374017]\n",
      "[D loss: 0.750526] [G loss: 1.546977]\n",
      "[D loss: 0.597658] [G loss: 1.561862]\n",
      "[D loss: 0.843473] [G loss: 1.487410]\n",
      "[D loss: 0.954337] [G loss: 1.512534]\n",
      "[D loss: 0.880758] [G loss: 1.767597]\n",
      "[D loss: 0.792752] [G loss: 1.550775]\n",
      "[D loss: 0.943643] [G loss: 1.424696]\n",
      "[D loss: 0.628049] [G loss: 1.436523]\n",
      "[D loss: 0.755977] [G loss: 1.467784]\n",
      "[D loss: 0.745629] [G loss: 1.467939]\n",
      "[D loss: 0.667862] [G loss: 1.442569]\n",
      "[D loss: 0.799238] [G loss: 1.505093]\n",
      "[D loss: 0.957064] [G loss: 1.639340]\n",
      "[D loss: 0.853970] [G loss: 1.602141]\n",
      "[D loss: 0.681326] [G loss: 1.604609]\n",
      "[D loss: 0.901393] [G loss: 1.307465]\n",
      "[D loss: 0.771910] [G loss: 1.580301]\n",
      "[D loss: 0.737203] [G loss: 1.370390]\n",
      "[D loss: 0.587225] [G loss: 1.494619]\n",
      "[D loss: 0.876017] [G loss: 1.849462]\n",
      "[D loss: 0.997742] [G loss: 1.424945]\n",
      "[D loss: 0.855764] [G loss: 1.422643]\n",
      "[D loss: 0.792379] [G loss: 1.614392]\n",
      "[D loss: 0.618106] [G loss: 1.345638]\n",
      "[D loss: 0.627655] [G loss: 1.666576]\n",
      "[D loss: 0.684127] [G loss: 1.514993]\n",
      "[D loss: 0.886641] [G loss: 1.632382]\n",
      "[D loss: 0.655453] [G loss: 1.595981]\n",
      "[D loss: 0.826825] [G loss: 1.611194]\n",
      "[D loss: 0.819248] [G loss: 1.632997]\n",
      "[D loss: 0.767667] [G loss: 1.629088]\n",
      "[D loss: 0.837932] [G loss: 1.403046]\n",
      "[D loss: 0.811306] [G loss: 1.497677]\n",
      "[D loss: 0.991122] [G loss: 1.318313]\n",
      "[D loss: 0.768511] [G loss: 1.295205]\n",
      "[D loss: 0.745455] [G loss: 1.624964]\n",
      "[D loss: 1.016132] [G loss: 1.629740]\n",
      "[D loss: 0.694385] [G loss: 1.755433]\n",
      "[D loss: 1.202113] [G loss: 1.357435]\n",
      "[D loss: 0.792059] [G loss: 1.378384]\n",
      "[D loss: 1.035207] [G loss: 1.400292]\n",
      "[D loss: 0.851827] [G loss: 1.416798]\n",
      "[D loss: 0.756780] [G loss: 1.579070]\n",
      "[D loss: 1.161966] [G loss: 1.422175]\n",
      "[D loss: 0.764467] [G loss: 1.337621]\n",
      "[D loss: 0.860307] [G loss: 1.160937]\n",
      "[D loss: 0.855500] [G loss: 1.297759]\n",
      "[D loss: 0.747935] [G loss: 1.595669]\n",
      "[D loss: 0.700361] [G loss: 1.514731]\n",
      "[D loss: 0.832880] [G loss: 1.331491]\n",
      "[D loss: 0.783584] [G loss: 1.377752]\n",
      "[D loss: 0.770408] [G loss: 1.511484]\n",
      "[D loss: 0.749851] [G loss: 1.590341]\n",
      "[D loss: 0.990866] [G loss: 1.343037]\n",
      "[D loss: 0.683446] [G loss: 1.874454]\n",
      "[D loss: 0.953840] [G loss: 1.377706]\n",
      "[D loss: 0.801975] [G loss: 1.508072]\n",
      "[D loss: 0.742993] [G loss: 1.702748]\n",
      "[D loss: 0.749144] [G loss: 1.799194]\n",
      "[D loss: 0.844197] [G loss: 1.663360]\n",
      "[D loss: 1.009323] [G loss: 1.202933]\n",
      "[D loss: 0.733351] [G loss: 1.625363]\n",
      "[D loss: 0.844364] [G loss: 1.593215]\n",
      "[D loss: 0.688085] [G loss: 1.531832]\n",
      "[D loss: 0.993122] [G loss: 1.440738]\n",
      "[D loss: 0.755060] [G loss: 1.669304]\n",
      "[D loss: 0.636127] [G loss: 1.599579]\n",
      "[D loss: 0.563677] [G loss: 1.504120]\n",
      "[D loss: 0.719280] [G loss: 1.557754]\n",
      "[D loss: 0.803096] [G loss: 1.616894]\n",
      "[D loss: 0.612163] [G loss: 1.373353]\n",
      "[D loss: 0.904706] [G loss: 1.320799]\n",
      "[D loss: 0.783110] [G loss: 1.584949]\n",
      "[D loss: 0.837053] [G loss: 1.370923]\n",
      "[D loss: 0.855117] [G loss: 1.553090]\n",
      "[D loss: 0.812220] [G loss: 1.423353]\n",
      "[D loss: 0.745013] [G loss: 1.501584]\n",
      "[D loss: 0.628225] [G loss: 1.439367]\n",
      "[D loss: 0.945573] [G loss: 1.273544]\n",
      "[D loss: 0.657476] [G loss: 1.510489]\n",
      "[D loss: 0.861843] [G loss: 1.751603]\n",
      "[D loss: 0.948397] [G loss: 1.717367]\n",
      "[D loss: 0.548607] [G loss: 1.532414]\n",
      "[D loss: 0.768383] [G loss: 1.399284]\n",
      "[D loss: 0.775766] [G loss: 1.434344]\n",
      "[D loss: 0.936494] [G loss: 1.477277]\n",
      "[D loss: 0.673960] [G loss: 1.273429]\n",
      "[D loss: 0.766336] [G loss: 1.289009]\n",
      "[D loss: 0.822859] [G loss: 1.467152]\n",
      "[D loss: 0.903168] [G loss: 1.448603]\n",
      "[D loss: 0.587739] [G loss: 1.804556]\n",
      "[D loss: 1.010588] [G loss: 1.277019]\n",
      "[D loss: 0.879225] [G loss: 1.364888]\n",
      "[D loss: 0.752224] [G loss: 1.330846]\n",
      "[D loss: 0.777069] [G loss: 1.396015]\n",
      "[D loss: 0.784987] [G loss: 1.698137]\n",
      "[D loss: 0.783041] [G loss: 1.398934]\n",
      "[D loss: 0.842471] [G loss: 1.519019]\n",
      "[D loss: 0.914649] [G loss: 1.635451]\n",
      "[D loss: 0.873347] [G loss: 1.331295]\n",
      "[D loss: 0.929756] [G loss: 1.490796]\n",
      "[D loss: 0.870302] [G loss: 1.359637]\n",
      "[D loss: 1.011036] [G loss: 1.509625]\n",
      "[D loss: 0.919340] [G loss: 1.387521]\n",
      "[D loss: 0.550731] [G loss: 1.594990]\n",
      "[D loss: 0.820270] [G loss: 1.475435]\n",
      "[D loss: 0.891260] [G loss: 1.430992]\n",
      "[D loss: 0.746397] [G loss: 1.563326]\n",
      "[D loss: 0.760373] [G loss: 1.376252]\n",
      "[D loss: 0.857427] [G loss: 1.521833]\n",
      "[D loss: 0.977304] [G loss: 1.423960]\n",
      "[D loss: 0.884165] [G loss: 1.675035]\n",
      "[D loss: 0.956838] [G loss: 1.497403]\n",
      "[D loss: 0.988474] [G loss: 1.601082]\n",
      "[D loss: 0.629626] [G loss: 1.439820]\n",
      "[D loss: 0.921878] [G loss: 1.304005]\n",
      "[D loss: 0.958355] [G loss: 1.271260]\n",
      "[D loss: 1.009591] [G loss: 1.211815]\n",
      "[D loss: 0.861858] [G loss: 1.308880]\n",
      "[D loss: 0.592340] [G loss: 1.476616]\n",
      "[D loss: 0.855486] [G loss: 1.359383]\n",
      "[D loss: 0.827156] [G loss: 1.541886]\n",
      "[D loss: 1.044706] [G loss: 1.405780]\n",
      "[D loss: 0.884251] [G loss: 1.405398]\n",
      "[D loss: 0.666067] [G loss: 1.356769]\n",
      "[D loss: 0.844094] [G loss: 1.379716]\n",
      "[D loss: 0.676761] [G loss: 1.511399]\n",
      "[D loss: 0.841175] [G loss: 1.300164]\n",
      "[D loss: 0.777322] [G loss: 1.476716]\n",
      "[D loss: 0.940492] [G loss: 1.446054]\n",
      "[D loss: 0.587273] [G loss: 1.623385]\n",
      "[D loss: 1.049093] [G loss: 1.569081]\n",
      "[D loss: 0.717221] [G loss: 1.515154]\n",
      "[D loss: 0.872735] [G loss: 1.493867]\n",
      "[D loss: 1.153373] [G loss: 1.245880]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.797652] [G loss: 1.585870]\n",
      "[D loss: 0.918755] [G loss: 1.522958]\n",
      "[D loss: 0.632536] [G loss: 1.474022]\n",
      "[D loss: 1.148875] [G loss: 1.508819]\n",
      "[D loss: 0.643428] [G loss: 1.391246]\n",
      "[D loss: 0.932268] [G loss: 1.219376]\n",
      "[D loss: 0.917702] [G loss: 1.389392]\n",
      "[D loss: 0.783385] [G loss: 1.430948]\n",
      "[D loss: 0.688953] [G loss: 1.488818]\n",
      "[D loss: 0.820327] [G loss: 1.432911]\n",
      "[D loss: 0.652871] [G loss: 1.451337]\n",
      "[D loss: 0.850806] [G loss: 1.350302]\n",
      "[D loss: 0.846027] [G loss: 1.312675]\n",
      "[D loss: 0.673794] [G loss: 1.599796]\n",
      "[D loss: 0.892671] [G loss: 1.269294]\n",
      "[D loss: 0.964202] [G loss: 1.467297]\n",
      "[D loss: 0.663231] [G loss: 1.460372]\n",
      "[D loss: 0.809459] [G loss: 1.397385]\n",
      "[D loss: 0.935344] [G loss: 1.465414]\n",
      "[D loss: 1.032863] [G loss: 1.518800]\n",
      "[D loss: 1.028725] [G loss: 1.342425]\n",
      "[D loss: 0.777698] [G loss: 1.768744]\n",
      "[D loss: 0.789441] [G loss: 1.648585]\n",
      "[D loss: 1.050462] [G loss: 1.556914]\n",
      "[D loss: 0.878572] [G loss: 1.270606]\n",
      "[D loss: 0.898144] [G loss: 1.217844]\n",
      "[D loss: 0.696512] [G loss: 1.291852]\n",
      "[D loss: 0.743848] [G loss: 1.397967]\n",
      "[D loss: 0.768339] [G loss: 1.429217]\n",
      "[D loss: 0.818900] [G loss: 1.549109]\n",
      "[D loss: 0.859692] [G loss: 1.547585]\n",
      "[D loss: 0.848071] [G loss: 1.601117]\n",
      "[D loss: 0.911860] [G loss: 1.373911]\n",
      "[D loss: 0.699488] [G loss: 1.387196]\n",
      "[D loss: 0.806361] [G loss: 1.604740]\n",
      "[D loss: 0.654222] [G loss: 1.492874]\n",
      "[D loss: 0.729339] [G loss: 1.622989]\n",
      "[D loss: 0.779327] [G loss: 1.423653]\n",
      "[D loss: 0.957686] [G loss: 1.174679]\n",
      "[D loss: 0.813964] [G loss: 1.525521]\n",
      "[D loss: 0.822389] [G loss: 1.571189]\n",
      "[D loss: 0.792696] [G loss: 1.690230]\n",
      "[D loss: 0.894170] [G loss: 1.519601]\n",
      "[D loss: 0.608559] [G loss: 1.405677]\n",
      "[D loss: 0.705135] [G loss: 1.429808]\n",
      "[D loss: 0.710339] [G loss: 1.353595]\n",
      "[D loss: 0.763965] [G loss: 1.370106]\n",
      "[D loss: 0.837961] [G loss: 1.500009]\n",
      "[D loss: 0.860900] [G loss: 1.510204]\n",
      "[D loss: 0.878953] [G loss: 1.523129]\n",
      "[D loss: 0.773764] [G loss: 1.324517]\n",
      "[D loss: 0.853706] [G loss: 1.652247]\n",
      "[D loss: 0.780762] [G loss: 1.453099]\n",
      "[D loss: 0.810416] [G loss: 1.467945]\n",
      "[D loss: 0.872966] [G loss: 1.496152]\n",
      "[D loss: 0.721785] [G loss: 1.578779]\n",
      "[D loss: 0.767928] [G loss: 1.293705]\n",
      "[D loss: 0.837215] [G loss: 1.479530]\n",
      "[D loss: 0.932795] [G loss: 1.481278]\n",
      "[D loss: 0.879491] [G loss: 1.488651]\n",
      "[D loss: 0.949741] [G loss: 1.531167]\n",
      "[D loss: 0.728794] [G loss: 1.765204]\n",
      "[D loss: 0.892746] [G loss: 1.563362]\n",
      "[D loss: 0.832534] [G loss: 1.563780]\n",
      "[D loss: 0.758165] [G loss: 1.557071]\n",
      "[D loss: 0.937721] [G loss: 1.412149]\n",
      "[D loss: 0.758329] [G loss: 1.636126]\n",
      "[D loss: 1.013653] [G loss: 1.326470]\n",
      "[D loss: 0.720767] [G loss: 1.427021]\n",
      "[D loss: 0.774608] [G loss: 1.627817]\n",
      "[D loss: 0.939992] [G loss: 1.356967]\n",
      "[D loss: 0.910075] [G loss: 1.602632]\n",
      "[D loss: 0.862258] [G loss: 1.754395]\n",
      "[D loss: 0.885265] [G loss: 1.498330]\n",
      "[D loss: 0.705990] [G loss: 1.546973]\n",
      "[D loss: 0.966480] [G loss: 1.360294]\n",
      "[D loss: 1.040946] [G loss: 1.437532]\n",
      "[D loss: 0.748357] [G loss: 1.372538]\n",
      "[D loss: 0.976808] [G loss: 1.218094]\n",
      "[D loss: 0.835140] [G loss: 1.297283]\n",
      "[D loss: 0.856392] [G loss: 1.447351]\n",
      "[D loss: 0.822474] [G loss: 1.462816]\n",
      "[D loss: 0.782285] [G loss: 1.380612]\n",
      "[D loss: 0.685929] [G loss: 1.570432]\n",
      "[D loss: 0.835755] [G loss: 1.466019]\n",
      "[D loss: 0.846820] [G loss: 1.363367]\n",
      "[D loss: 0.993666] [G loss: 1.313181]\n",
      "[D loss: 0.785270] [G loss: 1.580606]\n",
      "[D loss: 0.593361] [G loss: 1.397999]\n",
      "[D loss: 0.732047] [G loss: 1.530856]\n",
      "[D loss: 0.720029] [G loss: 1.422869]\n",
      "[D loss: 0.717482] [G loss: 1.327981]\n",
      "[D loss: 0.786119] [G loss: 1.376225]\n",
      "[D loss: 0.722298] [G loss: 1.581522]\n",
      "[D loss: 0.670961] [G loss: 1.696487]\n",
      "[D loss: 0.788117] [G loss: 1.552627]\n",
      "[D loss: 0.745584] [G loss: 1.468985]\n",
      "[D loss: 0.876752] [G loss: 1.735206]\n",
      "[D loss: 0.852810] [G loss: 1.710183]\n",
      "[D loss: 0.665542] [G loss: 1.537410]\n",
      "[D loss: 1.103387] [G loss: 1.561797]\n",
      "[D loss: 0.774515] [G loss: 1.436409]\n",
      "[D loss: 0.644701] [G loss: 1.310294]\n",
      "[D loss: 0.708149] [G loss: 1.367947]\n",
      "[D loss: 0.692792] [G loss: 1.855215]\n",
      "[D loss: 0.989103] [G loss: 1.566357]\n",
      "[D loss: 1.042287] [G loss: 1.395950]\n",
      "[D loss: 0.744285] [G loss: 1.522765]\n",
      "[D loss: 0.851365] [G loss: 1.572446]\n",
      "[D loss: 0.818649] [G loss: 1.618733]\n",
      "[D loss: 0.616893] [G loss: 1.743224]\n",
      "[D loss: 0.930436] [G loss: 1.300077]\n",
      "[D loss: 0.787399] [G loss: 1.774860]\n",
      "[D loss: 0.646367] [G loss: 1.561373]\n",
      "[D loss: 0.795728] [G loss: 1.751775]\n",
      "[D loss: 0.587637] [G loss: 1.614264]\n",
      "[D loss: 0.701858] [G loss: 1.361423]\n",
      "[D loss: 0.708672] [G loss: 1.493964]\n",
      "[D loss: 0.897118] [G loss: 1.469544]\n",
      "[D loss: 0.992832] [G loss: 1.454225]\n",
      "[D loss: 0.983753] [G loss: 1.615280]\n",
      "[D loss: 0.646922] [G loss: 1.357455]\n",
      "[D loss: 0.938878] [G loss: 1.556164]\n",
      "[D loss: 0.749529] [G loss: 1.473531]\n",
      "[D loss: 0.837696] [G loss: 1.410161]\n",
      "[D loss: 0.823523] [G loss: 1.646639]\n",
      "[D loss: 0.998862] [G loss: 1.721341]\n",
      "[D loss: 0.729481] [G loss: 1.542704]\n",
      "[D loss: 0.730666] [G loss: 1.314863]\n",
      "[D loss: 0.733727] [G loss: 1.351268]\n",
      "[D loss: 0.923692] [G loss: 1.570992]\n",
      "[D loss: 0.815040] [G loss: 1.542679]\n",
      "[D loss: 0.668738] [G loss: 1.754710]\n",
      "[D loss: 1.040522] [G loss: 1.603488]\n",
      "[D loss: 0.967652] [G loss: 1.517915]\n",
      "[D loss: 0.798934] [G loss: 1.454210]\n",
      "[D loss: 0.656146] [G loss: 1.494551]\n",
      "[D loss: 0.688670] [G loss: 1.311085]\n",
      "[D loss: 0.758237] [G loss: 1.512607]\n",
      "[D loss: 0.827036] [G loss: 1.425350]\n",
      "[D loss: 0.875106] [G loss: 1.393483]\n",
      "[D loss: 0.653337] [G loss: 1.556644]\n",
      "[D loss: 0.609018] [G loss: 1.500671]\n",
      "[D loss: 0.960569] [G loss: 1.624359]\n",
      "[D loss: 0.676830] [G loss: 1.488371]\n",
      "[D loss: 0.666038] [G loss: 1.547577]\n",
      "[D loss: 0.787219] [G loss: 1.502804]\n",
      "[D loss: 0.971167] [G loss: 1.293263]\n",
      "[D loss: 0.980554] [G loss: 1.575978]\n",
      "[D loss: 0.646417] [G loss: 1.527423]\n",
      "[D loss: 0.750877] [G loss: 1.469476]\n",
      "[D loss: 0.900648] [G loss: 1.521406]\n",
      "[D loss: 0.854646] [G loss: 1.425122]\n",
      "[D loss: 0.830175] [G loss: 1.580469]\n",
      "[D loss: 0.830381] [G loss: 1.721915]\n",
      "[D loss: 0.747745] [G loss: 1.572888]\n",
      "[D loss: 0.864287] [G loss: 1.459680]\n",
      "[D loss: 0.934935] [G loss: 1.418779]\n",
      "[D loss: 0.710311] [G loss: 1.398774]\n",
      "[D loss: 1.003713] [G loss: 1.364338]\n",
      "[D loss: 0.689643] [G loss: 1.436925]\n",
      "[D loss: 0.776822] [G loss: 1.529960]\n",
      "[D loss: 0.912388] [G loss: 1.541216]\n",
      "[D loss: 0.824166] [G loss: 1.470816]\n",
      "[D loss: 0.811866] [G loss: 1.678774]\n",
      "[D loss: 0.631065] [G loss: 1.491154]\n",
      "[D loss: 0.720256] [G loss: 1.430946]\n",
      "[D loss: 0.779116] [G loss: 1.368078]\n",
      "[D loss: 0.736381] [G loss: 1.305839]\n",
      "[D loss: 0.750230] [G loss: 1.315055]\n",
      "[D loss: 0.797184] [G loss: 1.487439]\n",
      "[D loss: 0.775409] [G loss: 1.929169]\n",
      "[D loss: 0.680669] [G loss: 1.521373]\n",
      "[D loss: 0.980165] [G loss: 1.617188]\n",
      "[D loss: 0.940620] [G loss: 1.292210]\n",
      "[D loss: 0.875136] [G loss: 1.418816]\n",
      "[D loss: 0.660394] [G loss: 1.573338]\n",
      "[D loss: 1.039419] [G loss: 1.363558]\n",
      "[D loss: 0.988196] [G loss: 1.490325]\n",
      "[D loss: 0.637954] [G loss: 1.436022]\n",
      "[D loss: 0.622128] [G loss: 1.775126]\n",
      "[D loss: 0.884546] [G loss: 1.734596]\n",
      "[D loss: 0.574700] [G loss: 1.553336]\n",
      "[D loss: 0.888954] [G loss: 1.497673]\n",
      "[D loss: 0.744664] [G loss: 1.627032]\n",
      "[D loss: 0.581418] [G loss: 1.567061]\n",
      "[D loss: 0.748071] [G loss: 1.498980]\n",
      "[D loss: 0.738334] [G loss: 1.508130]\n",
      "[D loss: 0.941233] [G loss: 1.551562]\n",
      "[D loss: 0.601011] [G loss: 1.498176]\n",
      "[D loss: 0.900267] [G loss: 1.575923]\n",
      "[D loss: 0.873378] [G loss: 1.430331]\n",
      "[D loss: 0.786802] [G loss: 1.701032]\n",
      "[D loss: 0.773050] [G loss: 1.669013]\n",
      "[D loss: 0.742377] [G loss: 1.619848]\n",
      "[D loss: 0.836236] [G loss: 1.649377]\n",
      "[D loss: 0.720349] [G loss: 1.548430]\n",
      "[D loss: 0.924898] [G loss: 1.649657]\n",
      "[D loss: 0.830451] [G loss: 1.492882]\n",
      "[D loss: 0.786491] [G loss: 1.602509]\n",
      "[D loss: 0.816298] [G loss: 1.478448]\n",
      "[D loss: 0.877625] [G loss: 1.410331]\n",
      "[D loss: 0.620037] [G loss: 1.630887]\n",
      "[D loss: 0.933657] [G loss: 1.460305]\n",
      "[D loss: 0.520003] [G loss: 1.592078]\n",
      "[D loss: 0.876582] [G loss: 1.552137]\n",
      "[D loss: 0.828660] [G loss: 1.420726]\n",
      "[D loss: 0.760134] [G loss: 1.472435]\n",
      "[D loss: 0.832926] [G loss: 1.339278]\n",
      "[D loss: 1.074800] [G loss: 1.334296]\n",
      "[D loss: 0.798874] [G loss: 1.646042]\n",
      "[D loss: 0.679293] [G loss: 1.532336]\n",
      "[D loss: 0.892756] [G loss: 1.446414]\n",
      "[D loss: 0.875538] [G loss: 1.431330]\n",
      "[D loss: 0.734151] [G loss: 1.542816]\n",
      "[D loss: 0.801090] [G loss: 1.621528]\n",
      "[D loss: 0.889050] [G loss: 1.547449]\n",
      "[D loss: 0.599346] [G loss: 1.468839]\n",
      "[D loss: 0.678300] [G loss: 1.415913]\n",
      "[D loss: 0.787605] [G loss: 1.446630]\n",
      "[D loss: 0.627018] [G loss: 1.326017]\n",
      "[D loss: 0.731709] [G loss: 1.605194]\n",
      "[D loss: 0.819844] [G loss: 1.400398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.881003] [G loss: 1.472824]\n",
      "[D loss: 1.036304] [G loss: 1.185072]\n",
      "[D loss: 0.964014] [G loss: 1.617622]\n",
      "[D loss: 0.799646] [G loss: 1.592752]\n",
      "[D loss: 1.061160] [G loss: 1.509628]\n",
      "[D loss: 0.821769] [G loss: 1.662592]\n",
      "[D loss: 0.830557] [G loss: 1.713745]\n",
      "[D loss: 0.841738] [G loss: 1.549960]\n",
      "[D loss: 0.819665] [G loss: 1.375499]\n",
      "[D loss: 0.893145] [G loss: 1.486559]\n",
      "[D loss: 0.830551] [G loss: 1.393920]\n",
      "[D loss: 0.956795] [G loss: 1.573251]\n",
      "[D loss: 0.823433] [G loss: 1.490771]\n",
      "[D loss: 0.884637] [G loss: 1.385054]\n",
      "[D loss: 0.963660] [G loss: 1.385413]\n",
      "[D loss: 0.817950] [G loss: 1.360615]\n",
      "[D loss: 0.767214] [G loss: 1.364316]\n",
      "[D loss: 0.655098] [G loss: 1.297160]\n",
      "[D loss: 1.112318] [G loss: 1.263845]\n",
      "[D loss: 0.774115] [G loss: 1.449778]\n",
      "[D loss: 0.634459] [G loss: 1.503632]\n",
      "[D loss: 0.733581] [G loss: 1.584543]\n",
      "[D loss: 0.807966] [G loss: 1.400372]\n",
      "[D loss: 0.959623] [G loss: 1.521327]\n",
      "[D loss: 1.006037] [G loss: 1.182595]\n",
      "[D loss: 0.897985] [G loss: 1.319331]\n",
      "[D loss: 0.623776] [G loss: 1.694503]\n",
      "[D loss: 0.649750] [G loss: 1.370086]\n",
      "[D loss: 0.531294] [G loss: 1.577076]\n",
      "[D loss: 0.948697] [G loss: 1.432262]\n",
      "[D loss: 0.738046] [G loss: 1.539558]\n",
      "[D loss: 0.837162] [G loss: 1.487133]\n",
      "[D loss: 0.952028] [G loss: 1.362367]\n",
      "[D loss: 0.789920] [G loss: 1.486718]\n",
      "[D loss: 0.875058] [G loss: 1.429563]\n",
      "[D loss: 0.640935] [G loss: 1.461984]\n",
      "[D loss: 0.807905] [G loss: 1.344085]\n",
      "[D loss: 0.807963] [G loss: 1.566124]\n",
      "[D loss: 0.905368] [G loss: 1.539066]\n",
      "[D loss: 0.835864] [G loss: 1.535007]\n",
      "[D loss: 0.690212] [G loss: 1.426267]\n",
      "[D loss: 0.675677] [G loss: 1.514523]\n",
      "[D loss: 0.791654] [G loss: 1.331767]\n",
      "[D loss: 0.736473] [G loss: 1.530001]\n",
      "[D loss: 0.556166] [G loss: 1.721311]\n",
      "[D loss: 0.787327] [G loss: 1.476596]\n",
      "[D loss: 0.662977] [G loss: 1.556228]\n",
      "[D loss: 1.014210] [G loss: 1.285289]\n",
      "[D loss: 0.990962] [G loss: 1.440565]\n",
      "[D loss: 0.848478] [G loss: 1.571128]\n",
      "[D loss: 1.094399] [G loss: 1.267606]\n",
      "[D loss: 0.848332] [G loss: 1.363558]\n",
      "[D loss: 1.113595] [G loss: 1.598542]\n",
      "[D loss: 0.861792] [G loss: 1.599038]\n",
      "[D loss: 0.775983] [G loss: 1.406470]\n",
      "[D loss: 0.998419] [G loss: 1.279654]\n",
      "[D loss: 0.854709] [G loss: 1.583297]\n",
      "[D loss: 0.869238] [G loss: 1.321141]\n",
      "[D loss: 0.762048] [G loss: 1.391562]\n",
      "[D loss: 0.707330] [G loss: 1.161629]\n",
      "[D loss: 0.821221] [G loss: 1.249500]\n",
      "[D loss: 0.799058] [G loss: 1.630582]\n",
      "[D loss: 0.740579] [G loss: 1.352917]\n",
      "[D loss: 0.832953] [G loss: 1.469121]\n",
      "[D loss: 0.650024] [G loss: 1.434655]\n",
      "[D loss: 0.780759] [G loss: 1.704159]\n",
      "[D loss: 0.973722] [G loss: 1.421862]\n",
      "[D loss: 0.908167] [G loss: 1.326209]\n",
      "[D loss: 0.851780] [G loss: 1.369291]\n",
      "[D loss: 0.837321] [G loss: 1.353952]\n",
      "[D loss: 0.945752] [G loss: 1.432337]\n",
      "[D loss: 0.743606] [G loss: 1.577565]\n",
      "[D loss: 0.654794] [G loss: 1.516552]\n",
      "[D loss: 0.803697] [G loss: 1.734869]\n",
      "[D loss: 0.708230] [G loss: 1.454709]\n",
      "[D loss: 0.693502] [G loss: 1.270570]\n",
      "[D loss: 0.768123] [G loss: 1.417928]\n",
      "[D loss: 0.901337] [G loss: 1.259647]\n",
      "[D loss: 0.750574] [G loss: 1.678732]\n",
      "[D loss: 0.627489] [G loss: 1.612575]\n",
      "[D loss: 0.862420] [G loss: 1.422395]\n",
      "[D loss: 0.890409] [G loss: 1.613985]\n",
      "[D loss: 0.853467] [G loss: 1.553526]\n",
      "[D loss: 0.924030] [G loss: 1.378222]\n",
      "[D loss: 0.927494] [G loss: 1.466390]\n",
      "[D loss: 0.989416] [G loss: 1.486090]\n",
      "[D loss: 0.838597] [G loss: 1.451279]\n",
      "[D loss: 0.848472] [G loss: 1.351056]\n",
      "[D loss: 0.728047] [G loss: 1.736773]\n",
      "[D loss: 0.774216] [G loss: 1.660702]\n",
      "[D loss: 0.931094] [G loss: 1.229668]\n",
      "[D loss: 0.742138] [G loss: 1.758124]\n",
      "[D loss: 0.769729] [G loss: 1.392923]\n",
      "[D loss: 0.903798] [G loss: 1.297982]\n",
      "[D loss: 0.664486] [G loss: 1.378476]\n",
      "[D loss: 0.871592] [G loss: 1.459134]\n",
      "[D loss: 0.968937] [G loss: 1.643574]\n",
      "[D loss: 0.791571] [G loss: 1.582951]\n",
      "[D loss: 1.142080] [G loss: 1.117167]\n",
      "[D loss: 0.844506] [G loss: 1.457719]\n",
      "[D loss: 0.937984] [G loss: 1.594407]\n",
      "[D loss: 0.810389] [G loss: 1.473372]\n",
      "[D loss: 0.697383] [G loss: 1.420239]\n",
      "[D loss: 0.790056] [G loss: 1.365212]\n",
      "[D loss: 0.690618] [G loss: 1.519105]\n",
      "[D loss: 0.813188] [G loss: 1.505453]\n",
      "[D loss: 0.640840] [G loss: 1.516994]\n",
      "[D loss: 0.841375] [G loss: 1.472903]\n",
      "[D loss: 0.806494] [G loss: 1.336522]\n",
      "[D loss: 0.958889] [G loss: 1.514773]\n",
      "[D loss: 0.761101] [G loss: 1.421145]\n",
      "[D loss: 0.873753] [G loss: 1.416046]\n",
      "[D loss: 0.837986] [G loss: 1.432243]\n",
      "[D loss: 0.967804] [G loss: 1.463797]\n",
      "[D loss: 0.830714] [G loss: 1.605277]\n",
      "[D loss: 0.598106] [G loss: 1.572374]\n",
      "[D loss: 0.945834] [G loss: 1.575360]\n",
      "[D loss: 0.536505] [G loss: 1.570481]\n",
      "[D loss: 0.746661] [G loss: 1.534963]\n",
      "[D loss: 0.777718] [G loss: 1.529052]\n",
      "[D loss: 0.778099] [G loss: 1.549349]\n",
      "[D loss: 0.929540] [G loss: 1.482399]\n",
      "[D loss: 1.006241] [G loss: 1.608691]\n",
      "[D loss: 0.706914] [G loss: 1.716715]\n",
      "[D loss: 1.059143] [G loss: 1.727468]\n",
      "[D loss: 0.578698] [G loss: 1.270590]\n",
      "[D loss: 0.758115] [G loss: 1.371198]\n",
      "[D loss: 0.817674] [G loss: 1.312685]\n",
      "[D loss: 0.801824] [G loss: 1.345457]\n",
      "[D loss: 0.912283] [G loss: 1.318800]\n",
      "[D loss: 0.840687] [G loss: 1.543530]\n",
      "[D loss: 0.690931] [G loss: 1.737005]\n",
      "[D loss: 0.973682] [G loss: 1.547523]\n",
      "[D loss: 0.877225] [G loss: 1.703562]\n",
      "[D loss: 0.727786] [G loss: 1.480371]\n",
      "[D loss: 0.669580] [G loss: 1.524562]\n",
      "[D loss: 0.881000] [G loss: 1.252548]\n",
      "[D loss: 0.821215] [G loss: 1.352422]\n",
      "[D loss: 0.832900] [G loss: 1.601144]\n",
      "[D loss: 0.701777] [G loss: 1.519031]\n",
      "[D loss: 0.737109] [G loss: 1.474076]\n",
      "[D loss: 0.694429] [G loss: 1.378652]\n",
      "[D loss: 0.465238] [G loss: 1.531847]\n",
      "[D loss: 0.863344] [G loss: 1.498573]\n",
      "[D loss: 1.000615] [G loss: 1.579074]\n",
      "[D loss: 0.646291] [G loss: 1.452321]\n",
      "[D loss: 0.767063] [G loss: 1.478942]\n",
      "[D loss: 1.130213] [G loss: 1.243145]\n",
      "[D loss: 1.132966] [G loss: 1.466384]\n",
      "[D loss: 1.042659] [G loss: 1.501695]\n",
      "[D loss: 0.615978] [G loss: 1.487768]\n",
      "[D loss: 0.894472] [G loss: 1.558381]\n",
      "[D loss: 1.141096] [G loss: 1.472676]\n",
      "[D loss: 0.845664] [G loss: 1.219479]\n",
      "[D loss: 0.987435] [G loss: 1.232509]\n",
      "[D loss: 0.887845] [G loss: 1.220420]\n",
      "[D loss: 0.854797] [G loss: 1.415159]\n",
      "[D loss: 0.807375] [G loss: 1.448435]\n",
      "[D loss: 0.896351] [G loss: 1.293879]\n",
      "[D loss: 0.700317] [G loss: 1.475359]\n",
      "[D loss: 0.531421] [G loss: 1.559308]\n",
      "[D loss: 0.643865] [G loss: 1.483205]\n",
      "[D loss: 0.828017] [G loss: 1.539301]\n",
      "[D loss: 0.950141] [G loss: 1.693095]\n",
      "[D loss: 0.871953] [G loss: 1.491960]\n",
      "[D loss: 0.814394] [G loss: 1.438379]\n",
      "[D loss: 1.033653] [G loss: 1.582175]\n",
      "[D loss: 0.765075] [G loss: 1.573353]\n",
      "[D loss: 1.011724] [G loss: 1.542095]\n",
      "[D loss: 0.794294] [G loss: 1.126324]\n",
      "[D loss: 0.680773] [G loss: 1.502103]\n",
      "[D loss: 0.928704] [G loss: 1.244567]\n",
      "[D loss: 0.735995] [G loss: 1.660495]\n",
      "[D loss: 0.724707] [G loss: 1.728091]\n",
      "[D loss: 0.705323] [G loss: 1.370376]\n",
      "[D loss: 0.855968] [G loss: 1.476171]\n",
      "[D loss: 0.766508] [G loss: 1.276665]\n",
      "[D loss: 0.689643] [G loss: 1.630690]\n",
      "[D loss: 0.772912] [G loss: 1.583977]\n",
      "[D loss: 0.874076] [G loss: 1.572617]\n",
      "[D loss: 0.685962] [G loss: 1.753719]\n",
      "[D loss: 0.919587] [G loss: 1.518594]\n",
      "[D loss: 0.788215] [G loss: 1.397163]\n",
      "[D loss: 0.723388] [G loss: 1.515667]\n",
      "[D loss: 0.621152] [G loss: 1.608426]\n",
      "[D loss: 1.010652] [G loss: 1.510866]\n",
      "[D loss: 1.021308] [G loss: 1.441492]\n",
      "[D loss: 0.754219] [G loss: 1.444613]\n",
      "[D loss: 0.723823] [G loss: 1.353814]\n",
      "[D loss: 0.842584] [G loss: 1.238682]\n",
      "[D loss: 0.817862] [G loss: 1.551399]\n",
      "[D loss: 0.654902] [G loss: 1.285205]\n",
      "[D loss: 0.874162] [G loss: 1.636212]\n",
      "[D loss: 0.804371] [G loss: 1.544202]\n",
      "[D loss: 0.616455] [G loss: 1.590636]\n",
      "[D loss: 0.852656] [G loss: 1.482383]\n",
      "[D loss: 0.490473] [G loss: 1.540380]\n",
      "[D loss: 0.682668] [G loss: 1.365518]\n",
      "[D loss: 0.607637] [G loss: 1.718719]\n",
      "[D loss: 0.807626] [G loss: 1.521531]\n",
      "[D loss: 0.957579] [G loss: 1.246148]\n",
      "[D loss: 0.721745] [G loss: 1.577675]\n",
      "[D loss: 0.831765] [G loss: 1.720309]\n",
      "[D loss: 0.767242] [G loss: 1.555035]\n",
      "[D loss: 0.881323] [G loss: 1.550084]\n",
      "[D loss: 0.596469] [G loss: 1.450038]\n",
      "[D loss: 0.780408] [G loss: 1.520954]\n",
      "[D loss: 1.227658] [G loss: 1.382028]\n",
      "[D loss: 0.802847] [G loss: 1.681290]\n",
      "[D loss: 0.846996] [G loss: 2.054453]\n",
      "[D loss: 0.748343] [G loss: 1.770764]\n",
      "[D loss: 0.703625] [G loss: 1.333037]\n",
      "[D loss: 1.250674] [G loss: 1.315964]\n",
      "[D loss: 0.611984] [G loss: 1.356757]\n",
      "[D loss: 0.824230] [G loss: 1.520876]\n",
      "[D loss: 0.892377] [G loss: 1.351202]\n",
      "[D loss: 0.773639] [G loss: 1.592060]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.736147] [G loss: 1.528008]\n",
      "[D loss: 1.134528] [G loss: 1.246174]\n",
      "[D loss: 0.849188] [G loss: 1.422842]\n",
      "[D loss: 0.951125] [G loss: 1.381937]\n",
      "[D loss: 0.980613] [G loss: 1.614613]\n",
      "[D loss: 0.834817] [G loss: 1.491347]\n",
      "[D loss: 0.676711] [G loss: 1.541823]\n",
      "[D loss: 0.691489] [G loss: 1.615923]\n",
      "[D loss: 1.044706] [G loss: 1.489999]\n",
      "[D loss: 1.053278] [G loss: 1.569482]\n",
      "[D loss: 0.905122] [G loss: 1.257967]\n",
      "[D loss: 0.849621] [G loss: 1.404822]\n",
      "[D loss: 0.740760] [G loss: 1.487157]\n",
      "[D loss: 0.718884] [G loss: 1.837518]\n",
      "[D loss: 0.683192] [G loss: 1.618392]\n",
      "[D loss: 0.997675] [G loss: 1.321089]\n",
      "[D loss: 0.664228] [G loss: 1.462991]\n",
      "[D loss: 0.793893] [G loss: 1.578579]\n",
      "[D loss: 0.830894] [G loss: 1.606597]\n",
      "[D loss: 0.821675] [G loss: 1.483644]\n",
      "[D loss: 0.604496] [G loss: 1.522460]\n",
      "[D loss: 0.658542] [G loss: 1.439590]\n",
      "[D loss: 0.932949] [G loss: 1.316625]\n",
      "[D loss: 0.884417] [G loss: 1.520257]\n",
      "[D loss: 0.713480] [G loss: 1.942416]\n",
      "[D loss: 0.887583] [G loss: 1.587571]\n",
      "[D loss: 0.718928] [G loss: 1.481640]\n",
      "[D loss: 1.162413] [G loss: 1.435752]\n",
      "[D loss: 0.955964] [G loss: 1.399711]\n",
      "[D loss: 0.913299] [G loss: 1.593558]\n",
      "[D loss: 0.772786] [G loss: 1.458030]\n",
      "[D loss: 1.080159] [G loss: 1.306046]\n",
      "[D loss: 0.765523] [G loss: 1.596889]\n",
      "[D loss: 1.159481] [G loss: 1.487231]\n",
      "[D loss: 0.867852] [G loss: 1.394421]\n",
      "[D loss: 0.749530] [G loss: 1.311482]\n",
      "[D loss: 0.968709] [G loss: 1.417484]\n",
      "[D loss: 0.924114] [G loss: 1.529758]\n",
      "[D loss: 0.844217] [G loss: 1.461942]\n",
      "[D loss: 0.977593] [G loss: 1.269788]\n",
      "[D loss: 0.998693] [G loss: 1.205600]\n",
      "[D loss: 0.752430] [G loss: 1.158708]\n",
      "[D loss: 0.758279] [G loss: 1.413104]\n",
      "[D loss: 0.880815] [G loss: 1.157733]\n",
      "[D loss: 0.915075] [G loss: 1.487213]\n",
      "[D loss: 0.709114] [G loss: 1.455848]\n",
      "[D loss: 0.756730] [G loss: 1.442273]\n",
      "[D loss: 0.792897] [G loss: 1.396539]\n",
      "[D loss: 0.994939] [G loss: 1.429127]\n",
      "[D loss: 0.848318] [G loss: 1.420008]\n",
      "[D loss: 0.885039] [G loss: 1.499936]\n",
      "[D loss: 0.902338] [G loss: 1.426608]\n",
      "[D loss: 0.978380] [G loss: 1.200647]\n",
      "[D loss: 0.674666] [G loss: 1.442779]\n",
      "[D loss: 0.589418] [G loss: 1.443866]\n",
      "[D loss: 0.866768] [G loss: 1.345714]\n",
      "[D loss: 1.049115] [G loss: 1.447251]\n",
      "[D loss: 0.800244] [G loss: 1.356678]\n",
      "[D loss: 0.781091] [G loss: 1.565937]\n",
      "[D loss: 0.870616] [G loss: 1.282515]\n",
      "[D loss: 0.703221] [G loss: 1.533931]\n",
      "[D loss: 0.976960] [G loss: 1.561414]\n",
      "[D loss: 0.769776] [G loss: 1.525091]\n",
      "[D loss: 0.826621] [G loss: 1.349856]\n",
      "[D loss: 0.871308] [G loss: 1.496397]\n",
      "[D loss: 0.549827] [G loss: 1.583260]\n",
      "[D loss: 1.009102] [G loss: 1.388899]\n",
      "[D loss: 0.655965] [G loss: 1.667303]\n",
      "[D loss: 1.030450] [G loss: 1.298734]\n",
      "[D loss: 0.824153] [G loss: 1.411001]\n",
      "[D loss: 1.002715] [G loss: 1.387595]\n",
      "[D loss: 0.871805] [G loss: 1.494830]\n",
      "[D loss: 0.846396] [G loss: 1.471113]\n",
      "[D loss: 0.750896] [G loss: 1.535589]\n",
      "[D loss: 1.006294] [G loss: 1.378416]\n",
      "[D loss: 0.840216] [G loss: 1.563237]\n",
      "[D loss: 0.832592] [G loss: 1.392169]\n",
      "[D loss: 0.676311] [G loss: 1.533161]\n",
      "[D loss: 0.786373] [G loss: 1.486069]\n",
      "[D loss: 1.014328] [G loss: 1.384740]\n",
      "[D loss: 1.078890] [G loss: 1.483326]\n",
      "[D loss: 0.783377] [G loss: 1.307135]\n",
      "[D loss: 0.692268] [G loss: 1.362648]\n",
      "[D loss: 0.780186] [G loss: 1.460066]\n",
      "[D loss: 0.852105] [G loss: 1.461016]\n",
      "[D loss: 0.720678] [G loss: 1.662217]\n",
      "[D loss: 0.710791] [G loss: 1.630258]\n",
      "[D loss: 0.985640] [G loss: 1.289310]\n",
      "[D loss: 0.815653] [G loss: 1.422773]\n",
      "[D loss: 0.955290] [G loss: 1.423449]\n",
      "[D loss: 1.071050] [G loss: 1.518379]\n",
      "[D loss: 0.934209] [G loss: 1.418911]\n",
      "[D loss: 0.831322] [G loss: 1.268858]\n",
      "[D loss: 0.856381] [G loss: 1.351900]\n",
      "[D loss: 0.919422] [G loss: 1.140038]\n",
      "[D loss: 0.799512] [G loss: 1.617901]\n",
      "[D loss: 0.678729] [G loss: 1.648994]\n",
      "[D loss: 0.887655] [G loss: 1.288742]\n",
      "[D loss: 0.649993] [G loss: 1.417392]\n",
      "[D loss: 0.737247] [G loss: 1.479696]\n",
      "[D loss: 0.893327] [G loss: 1.340313]\n",
      "[D loss: 0.911507] [G loss: 1.600543]\n",
      "[D loss: 0.876687] [G loss: 1.555566]\n",
      "[D loss: 1.059375] [G loss: 1.430666]\n",
      "[D loss: 0.644545] [G loss: 1.406138]\n",
      "[D loss: 0.715457] [G loss: 1.418312]\n",
      "[D loss: 0.950684] [G loss: 1.490346]\n",
      "[D loss: 0.878473] [G loss: 1.462577]\n",
      "[D loss: 0.755868] [G loss: 1.601004]\n",
      "[D loss: 0.708945] [G loss: 1.444992]\n",
      "[D loss: 0.783016] [G loss: 1.760982]\n",
      "[D loss: 0.776489] [G loss: 1.441268]\n",
      "[D loss: 0.855468] [G loss: 1.289663]\n",
      "[D loss: 0.973582] [G loss: 1.352197]\n",
      "[D loss: 0.849336] [G loss: 1.339401]\n",
      "[D loss: 1.011167] [G loss: 1.512863]\n",
      "[D loss: 0.789833] [G loss: 1.914103]\n",
      "[D loss: 0.609246] [G loss: 1.508369]\n",
      "[D loss: 0.853186] [G loss: 1.469328]\n",
      "[D loss: 1.108775] [G loss: 1.207078]\n",
      "[D loss: 0.847464] [G loss: 1.275380]\n",
      "[D loss: 0.942900] [G loss: 1.260956]\n",
      "[D loss: 1.123763] [G loss: 1.286341]\n",
      "[D loss: 0.970196] [G loss: 1.279430]\n",
      "[D loss: 1.009619] [G loss: 1.357477]\n",
      "[D loss: 0.705746] [G loss: 1.579393]\n",
      "[D loss: 0.738980] [G loss: 1.582968]\n",
      "[D loss: 0.680427] [G loss: 1.628115]\n",
      "[D loss: 0.680428] [G loss: 1.577720]\n",
      "[D loss: 0.941563] [G loss: 1.321979]\n",
      "[D loss: 0.883213] [G loss: 1.420019]\n",
      "[D loss: 0.739696] [G loss: 1.289300]\n",
      "[D loss: 0.652995] [G loss: 1.490595]\n",
      "[D loss: 0.727132] [G loss: 1.370801]\n",
      "[D loss: 0.640553] [G loss: 1.507612]\n",
      "[D loss: 1.110551] [G loss: 1.473347]\n",
      "[D loss: 0.769090] [G loss: 1.439055]\n",
      "[D loss: 0.872589] [G loss: 1.581762]\n",
      "[D loss: 0.837781] [G loss: 1.686877]\n",
      "[D loss: 1.138236] [G loss: 1.416493]\n",
      "[D loss: 0.930122] [G loss: 1.289648]\n",
      "[D loss: 1.008137] [G loss: 1.258198]\n",
      "[D loss: 0.892633] [G loss: 1.350280]\n",
      "[D loss: 0.976106] [G loss: 1.399243]\n",
      "[D loss: 0.918785] [G loss: 1.457773]\n",
      "[D loss: 0.853368] [G loss: 1.422628]\n",
      "[D loss: 0.892292] [G loss: 1.222907]\n",
      "[D loss: 0.796205] [G loss: 1.178618]\n",
      "[D loss: 0.753792] [G loss: 1.435846]\n",
      "[D loss: 0.655789] [G loss: 1.551759]\n",
      "[D loss: 0.783155] [G loss: 1.517038]\n",
      "[D loss: 0.841847] [G loss: 1.604548]\n",
      "[D loss: 0.890803] [G loss: 1.273180]\n",
      "[D loss: 0.952787] [G loss: 1.329000]\n",
      "[D loss: 0.832064] [G loss: 1.441952]\n",
      "[D loss: 0.610435] [G loss: 1.334818]\n",
      "[D loss: 0.850339] [G loss: 1.169760]\n",
      "[D loss: 0.638099] [G loss: 1.289267]\n",
      "[D loss: 0.837056] [G loss: 1.452583]\n",
      "[D loss: 0.888773] [G loss: 1.593779]\n",
      "[D loss: 0.689545] [G loss: 1.607592]\n",
      "[D loss: 0.741994] [G loss: 1.718415]\n",
      "[D loss: 0.780350] [G loss: 1.486085]\n",
      "[D loss: 0.560783] [G loss: 1.760498]\n",
      "[D loss: 1.003300] [G loss: 1.363414]\n",
      "[D loss: 0.582905] [G loss: 1.718597]\n",
      "[D loss: 0.777207] [G loss: 1.562847]\n",
      "[D loss: 0.944000] [G loss: 1.798178]\n",
      "[D loss: 0.765198] [G loss: 1.672671]\n",
      "[D loss: 1.008630] [G loss: 1.399228]\n",
      "[D loss: 0.801973] [G loss: 1.431801]\n",
      "[D loss: 0.676489] [G loss: 1.502919]\n",
      "[D loss: 0.964953] [G loss: 1.434819]\n",
      "[D loss: 0.792502] [G loss: 1.358576]\n",
      "[D loss: 0.613458] [G loss: 1.546580]\n",
      "[D loss: 0.982803] [G loss: 1.490707]\n",
      "[D loss: 0.642171] [G loss: 1.527245]\n",
      "[D loss: 0.715092] [G loss: 1.747777]\n",
      "[D loss: 0.890002] [G loss: 1.409146]\n",
      "[D loss: 0.730575] [G loss: 1.554144]\n",
      "[D loss: 0.808966] [G loss: 1.409997]\n",
      "[D loss: 0.797421] [G loss: 1.706843]\n",
      "[D loss: 0.806845] [G loss: 1.509409]\n",
      "[D loss: 0.872308] [G loss: 1.468442]\n",
      "[D loss: 0.919134] [G loss: 1.671737]\n",
      "[D loss: 0.800859] [G loss: 1.427020]\n",
      "[D loss: 0.650519] [G loss: 1.485290]\n",
      "[D loss: 0.882264] [G loss: 1.463500]\n",
      "[D loss: 0.755635] [G loss: 1.466664]\n",
      "[D loss: 0.717805] [G loss: 1.596822]\n",
      "[D loss: 1.000764] [G loss: 1.419500]\n",
      "[D loss: 1.031307] [G loss: 1.528777]\n",
      "[D loss: 0.870800] [G loss: 1.488963]\n",
      "[D loss: 0.860889] [G loss: 1.413171]\n",
      "[D loss: 0.872980] [G loss: 1.501591]\n",
      "[D loss: 0.732899] [G loss: 1.410443]\n",
      "[D loss: 0.755490] [G loss: 1.598105]\n",
      "[D loss: 0.876035] [G loss: 1.392798]\n",
      "[D loss: 0.714551] [G loss: 1.505082]\n",
      "[D loss: 1.018645] [G loss: 1.153267]\n",
      "[D loss: 0.898291] [G loss: 1.606041]\n",
      "[D loss: 0.783740] [G loss: 1.632228]\n",
      "[D loss: 1.012569] [G loss: 1.166700]\n",
      "[D loss: 0.691915] [G loss: 1.443709]\n",
      "[D loss: 0.614351] [G loss: 1.721540]\n",
      "[D loss: 0.701666] [G loss: 1.312397]\n",
      "[D loss: 0.607728] [G loss: 1.370462]\n",
      "[D loss: 0.767570] [G loss: 1.408530]\n",
      "[D loss: 0.814540] [G loss: 1.474723]\n",
      "[D loss: 1.127882] [G loss: 1.627204]\n",
      "[D loss: 0.763510] [G loss: 1.659994]\n",
      "[D loss: 1.006606] [G loss: 1.238885]\n",
      "[D loss: 0.782016] [G loss: 1.495598]\n",
      "[D loss: 0.933288] [G loss: 1.483476]\n",
      "[D loss: 0.684284] [G loss: 1.626340]\n",
      "[D loss: 0.712189] [G loss: 1.706058]\n",
      "[D loss: 0.879997] [G loss: 1.423834]\n",
      "[D loss: 0.695140] [G loss: 1.727810]\n",
      "[D loss: 0.622280] [G loss: 1.335186]\n",
      "[D loss: 0.824967] [G loss: 1.765865]\n",
      "[D loss: 0.742478] [G loss: 1.383470]\n",
      "[D loss: 1.000759] [G loss: 1.540222]\n",
      "[D loss: 0.835337] [G loss: 1.565810]\n",
      "[D loss: 0.584863] [G loss: 1.781892]\n",
      "[D loss: 0.921970] [G loss: 1.517963]\n",
      "[D loss: 0.936852] [G loss: 1.480447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.716935] [G loss: 1.394261]\n",
      "[D loss: 1.090013] [G loss: 1.353737]\n",
      "[D loss: 0.796448] [G loss: 1.274000]\n",
      "[D loss: 0.859955] [G loss: 1.351571]\n",
      "[D loss: 0.844656] [G loss: 1.596532]\n",
      "[D loss: 0.578020] [G loss: 1.625068]\n",
      "[D loss: 0.985848] [G loss: 1.655317]\n",
      "[D loss: 0.738973] [G loss: 1.479958]\n",
      "[D loss: 0.545972] [G loss: 1.398044]\n",
      "[D loss: 0.929364] [G loss: 1.672852]\n",
      "[D loss: 0.712433] [G loss: 1.639049]\n",
      "[D loss: 0.811821] [G loss: 1.508575]\n",
      "[D loss: 0.900278] [G loss: 1.618461]\n",
      "[D loss: 1.054562] [G loss: 1.363758]\n",
      "[D loss: 0.863022] [G loss: 1.571847]\n",
      "[D loss: 0.877010] [G loss: 1.333643]\n",
      "[D loss: 0.787847] [G loss: 1.421687]\n",
      "[D loss: 0.866089] [G loss: 1.426041]\n",
      "[D loss: 0.955167] [G loss: 1.447813]\n",
      "[D loss: 0.702831] [G loss: 1.528282]\n",
      "[D loss: 0.942133] [G loss: 1.662315]\n",
      "[D loss: 0.772654] [G loss: 1.359770]\n",
      "[D loss: 0.689128] [G loss: 1.647988]\n",
      "[D loss: 0.903944] [G loss: 1.218653]\n",
      "[D loss: 0.922078] [G loss: 1.539021]\n",
      "[D loss: 0.794331] [G loss: 1.441908]\n",
      "[D loss: 0.721012] [G loss: 1.515756]\n",
      "[D loss: 0.776989] [G loss: 1.611810]\n",
      "[D loss: 0.844361] [G loss: 1.694216]\n",
      "[D loss: 0.755883] [G loss: 1.405529]\n",
      "[D loss: 1.001205] [G loss: 1.470943]\n",
      "[D loss: 0.881642] [G loss: 1.459539]\n",
      "[D loss: 0.939839] [G loss: 1.283872]\n",
      "[D loss: 0.734800] [G loss: 1.685572]\n",
      "[D loss: 0.889046] [G loss: 1.551699]\n",
      "[D loss: 0.769403] [G loss: 1.569054]\n",
      "[D loss: 0.961819] [G loss: 1.537986]\n",
      "[D loss: 0.907476] [G loss: 1.457486]\n",
      "[D loss: 1.007748] [G loss: 1.463640]\n",
      "[D loss: 0.888017] [G loss: 1.438899]\n",
      "[D loss: 0.832259] [G loss: 1.487622]\n",
      "[D loss: 0.901531] [G loss: 1.543149]\n",
      "[D loss: 0.790962] [G loss: 1.372839]\n",
      "[D loss: 0.894867] [G loss: 1.303581]\n",
      "[D loss: 0.757415] [G loss: 1.517717]\n",
      "[D loss: 0.687464] [G loss: 1.494253]\n",
      "[D loss: 0.612740] [G loss: 1.583866]\n",
      "[D loss: 0.702240] [G loss: 1.359927]\n",
      "[D loss: 1.058789] [G loss: 1.388670]\n",
      "[D loss: 0.931587] [G loss: 1.410400]\n",
      "[D loss: 0.730732] [G loss: 1.476723]\n",
      "[D loss: 0.718964] [G loss: 1.658290]\n",
      "[D loss: 0.759553] [G loss: 1.522839]\n",
      "[D loss: 0.940706] [G loss: 1.415299]\n",
      "[D loss: 0.845674] [G loss: 1.389629]\n",
      "[D loss: 0.782750] [G loss: 1.622217]\n",
      "[D loss: 0.939600] [G loss: 1.479504]\n",
      "[D loss: 0.881981] [G loss: 1.388837]\n",
      "[D loss: 1.090812] [G loss: 1.441222]\n",
      "[D loss: 0.812375] [G loss: 1.481036]\n",
      "[D loss: 0.675037] [G loss: 1.647847]\n",
      "[D loss: 0.775721] [G loss: 1.669757]\n",
      "[D loss: 0.801888] [G loss: 1.337100]\n",
      "[D loss: 0.915718] [G loss: 1.314963]\n",
      "[D loss: 1.136337] [G loss: 1.365512]\n",
      "[D loss: 0.913993] [G loss: 1.408084]\n",
      "[D loss: 1.041549] [G loss: 1.513624]\n",
      "[D loss: 0.951065] [G loss: 1.490110]\n",
      "[D loss: 0.757966] [G loss: 1.400482]\n",
      "[D loss: 0.881564] [G loss: 1.277719]\n",
      "[D loss: 0.710746] [G loss: 1.399125]\n",
      "[D loss: 0.789300] [G loss: 1.520576]\n",
      "[D loss: 0.799489] [G loss: 1.597618]\n",
      "[D loss: 0.995755] [G loss: 1.438657]\n",
      "[D loss: 0.908823] [G loss: 1.523123]\n",
      "[D loss: 0.856811] [G loss: 1.202175]\n",
      "[D loss: 0.597628] [G loss: 1.481529]\n",
      "[D loss: 0.783881] [G loss: 1.534573]\n",
      "[D loss: 1.041193] [G loss: 1.101281]\n",
      "[D loss: 1.084094] [G loss: 1.371928]\n",
      "[D loss: 0.624826] [G loss: 1.715225]\n",
      "[D loss: 0.933261] [G loss: 1.590109]\n",
      "[D loss: 0.720895] [G loss: 1.441857]\n",
      "[D loss: 0.826873] [G loss: 1.350305]\n",
      "[D loss: 1.010728] [G loss: 1.427133]\n",
      "[D loss: 0.812177] [G loss: 1.363908]\n",
      "[D loss: 0.753607] [G loss: 1.395848]\n",
      "[D loss: 0.666759] [G loss: 1.349082]\n",
      "[D loss: 0.855039] [G loss: 1.406170]\n",
      "[D loss: 0.733015] [G loss: 1.544474]\n",
      "[D loss: 0.677212] [G loss: 1.647330]\n",
      "[D loss: 0.818104] [G loss: 1.411585]\n",
      "[D loss: 0.712238] [G loss: 1.702602]\n",
      "[D loss: 0.960865] [G loss: 1.574915]\n",
      "[D loss: 0.952027] [G loss: 1.594993]\n",
      "[D loss: 0.846383] [G loss: 1.396273]\n",
      "[D loss: 0.843792] [G loss: 1.316138]\n",
      "[D loss: 0.722370] [G loss: 1.448763]\n",
      "[D loss: 0.846538] [G loss: 1.477204]\n",
      "[D loss: 0.745972] [G loss: 1.325092]\n",
      "[D loss: 1.106965] [G loss: 1.545779]\n",
      "[D loss: 0.795649] [G loss: 1.325828]\n",
      "[D loss: 1.236613] [G loss: 1.406561]\n",
      "[D loss: 0.898657] [G loss: 1.604611]\n",
      "[D loss: 0.632147] [G loss: 1.535705]\n",
      "[D loss: 0.708301] [G loss: 1.761996]\n",
      "[D loss: 0.994544] [G loss: 1.328865]\n",
      "[D loss: 0.916205] [G loss: 1.590670]\n",
      "[D loss: 0.897207] [G loss: 1.438171]\n",
      "[D loss: 1.087703] [G loss: 1.322181]\n",
      "[D loss: 0.910590] [G loss: 1.327673]\n",
      "[D loss: 0.800475] [G loss: 1.373159]\n",
      "[D loss: 0.789349] [G loss: 1.291414]\n",
      "[D loss: 0.945909] [G loss: 1.170544]\n",
      "[D loss: 0.898612] [G loss: 1.396509]\n",
      "[D loss: 0.723121] [G loss: 1.400155]\n",
      "[D loss: 0.923978] [G loss: 1.666391]\n",
      "[D loss: 0.857310] [G loss: 1.385471]\n",
      "[D loss: 1.028629] [G loss: 1.677152]\n",
      "[D loss: 0.726747] [G loss: 1.314165]\n",
      "[D loss: 0.936635] [G loss: 1.223934]\n",
      "[D loss: 1.050821] [G loss: 1.289832]\n",
      "[D loss: 0.865454] [G loss: 1.607246]\n",
      "[D loss: 0.739815] [G loss: 1.544997]\n",
      "[D loss: 0.849657] [G loss: 1.556769]\n",
      "[D loss: 0.981681] [G loss: 1.650845]\n",
      "[D loss: 0.731020] [G loss: 1.274091]\n",
      "[D loss: 0.819568] [G loss: 1.404217]\n",
      "[D loss: 0.888850] [G loss: 1.264372]\n",
      "[D loss: 0.766848] [G loss: 1.545750]\n",
      "[D loss: 1.033439] [G loss: 1.223790]\n",
      "[D loss: 0.873437] [G loss: 1.381810]\n",
      "[D loss: 0.798810] [G loss: 1.353523]\n",
      "[D loss: 0.753697] [G loss: 1.344908]\n",
      "[D loss: 0.995519] [G loss: 1.521672]\n",
      "[D loss: 0.735864] [G loss: 1.589697]\n",
      "[D loss: 0.827530] [G loss: 1.587605]\n",
      "[D loss: 0.896565] [G loss: 1.208477]\n",
      "[D loss: 0.863354] [G loss: 1.373562]\n",
      "[D loss: 0.835665] [G loss: 1.366520]\n",
      "[D loss: 0.930532] [G loss: 1.522626]\n",
      "[D loss: 1.212486] [G loss: 1.393280]\n",
      "[D loss: 0.829878] [G loss: 1.480293]\n",
      "[D loss: 0.941811] [G loss: 1.232138]\n",
      "[D loss: 0.746467] [G loss: 1.382236]\n",
      "[D loss: 0.571175] [G loss: 1.583751]\n",
      "[D loss: 1.070686] [G loss: 1.449344]\n",
      "[D loss: 0.731350] [G loss: 1.713817]\n",
      "[D loss: 0.591983] [G loss: 1.563425]\n",
      "[D loss: 0.699165] [G loss: 1.570525]\n",
      "[D loss: 0.923424] [G loss: 1.384911]\n",
      "[D loss: 0.868465] [G loss: 1.565202]\n",
      "[D loss: 0.772810] [G loss: 1.286944]\n",
      "[D loss: 0.842276] [G loss: 1.297619]\n",
      "[D loss: 0.900915] [G loss: 1.494354]\n",
      "[D loss: 0.774989] [G loss: 1.573773]\n",
      "[D loss: 0.824453] [G loss: 1.647900]\n",
      "[D loss: 0.833184] [G loss: 1.522082]\n",
      "[D loss: 0.656609] [G loss: 1.653120]\n",
      "[D loss: 0.840057] [G loss: 1.549509]\n",
      "[D loss: 0.745641] [G loss: 1.481655]\n",
      "[D loss: 0.860376] [G loss: 1.395191]\n",
      "[D loss: 0.823474] [G loss: 1.534185]\n",
      "[D loss: 1.025041] [G loss: 1.324484]\n",
      "[D loss: 0.792583] [G loss: 1.361762]\n",
      "[D loss: 0.799562] [G loss: 1.756896]\n",
      "[D loss: 0.660910] [G loss: 1.638281]\n",
      "[D loss: 0.642437] [G loss: 1.700665]\n",
      "[D loss: 0.748026] [G loss: 1.651770]\n",
      "[D loss: 0.849544] [G loss: 1.473843]\n",
      "[D loss: 0.833127] [G loss: 1.402778]\n",
      "[D loss: 0.646287] [G loss: 1.448851]\n",
      "[D loss: 0.570327] [G loss: 1.825298]\n",
      "[D loss: 0.805467] [G loss: 1.655460]\n",
      "[D loss: 0.904704] [G loss: 1.636344]\n",
      "[D loss: 0.997073] [G loss: 1.343285]\n",
      "[D loss: 0.775389] [G loss: 1.479368]\n",
      "[D loss: 0.735495] [G loss: 1.328411]\n",
      "[D loss: 0.757957] [G loss: 1.461971]\n",
      "[D loss: 0.816856] [G loss: 1.831300]\n",
      "[D loss: 0.529050] [G loss: 1.732072]\n",
      "[D loss: 0.844630] [G loss: 1.418768]\n",
      "[D loss: 0.960046] [G loss: 1.424103]\n",
      "[D loss: 0.846726] [G loss: 1.477497]\n",
      "[D loss: 0.603514] [G loss: 1.635494]\n",
      "[D loss: 0.840145] [G loss: 1.393257]\n",
      "[D loss: 0.840008] [G loss: 1.427146]\n",
      "[D loss: 0.694618] [G loss: 1.314835]\n",
      "[D loss: 0.890006] [G loss: 1.389493]\n",
      "[D loss: 0.596368] [G loss: 1.512018]\n",
      "[D loss: 0.825029] [G loss: 1.660480]\n",
      "[D loss: 0.978855] [G loss: 1.428009]\n",
      "[D loss: 0.663571] [G loss: 1.640770]\n",
      "[D loss: 0.906914] [G loss: 1.417563]\n",
      "[D loss: 0.822790] [G loss: 1.638031]\n",
      "[D loss: 0.957508] [G loss: 1.393950]\n",
      "[D loss: 0.801040] [G loss: 1.367661]\n",
      "[D loss: 0.662952] [G loss: 1.515270]\n",
      "[D loss: 0.916174] [G loss: 1.463230]\n",
      "[D loss: 0.669324] [G loss: 1.354506]\n",
      "[D loss: 0.805886] [G loss: 1.492867]\n",
      "[D loss: 0.686941] [G loss: 1.615688]\n",
      "[D loss: 0.669745] [G loss: 1.783446]\n",
      "[D loss: 0.876981] [G loss: 1.629276]\n",
      "[D loss: 0.641537] [G loss: 1.696625]\n",
      "[D loss: 1.009303] [G loss: 1.532127]\n",
      "[D loss: 0.905164] [G loss: 1.244662]\n",
      "[D loss: 0.763783] [G loss: 1.507732]\n",
      "[D loss: 0.862109] [G loss: 1.574251]\n",
      "[D loss: 0.755322] [G loss: 1.495090]\n",
      "[D loss: 0.881270] [G loss: 1.318858]\n",
      "[D loss: 0.824948] [G loss: 1.306169]\n",
      "[D loss: 0.986333] [G loss: 1.356349]\n",
      "[D loss: 0.806285] [G loss: 1.451923]\n",
      "[D loss: 0.984915] [G loss: 1.529118]\n",
      "[D loss: 0.924930] [G loss: 1.488657]\n",
      "[D loss: 0.803864] [G loss: 1.538962]\n",
      "[D loss: 0.785013] [G loss: 1.686103]\n",
      "[D loss: 0.912231] [G loss: 1.489981]\n",
      "[D loss: 0.805697] [G loss: 1.323864]\n",
      "[D loss: 0.802820] [G loss: 1.329527]\n",
      "[D loss: 0.923951] [G loss: 1.363321]\n",
      "[D loss: 0.630695] [G loss: 1.437348]\n",
      "[D loss: 0.773986] [G loss: 1.493294]\n",
      "[D loss: 0.685901] [G loss: 1.841783]\n",
      "[D loss: 0.773606] [G loss: 1.550557]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 1.010355] [G loss: 1.470577]\n",
      "[D loss: 0.679420] [G loss: 1.366257]\n",
      "[D loss: 0.738022] [G loss: 1.308881]\n",
      "[D loss: 0.696032] [G loss: 1.636677]\n",
      "[D loss: 0.822906] [G loss: 1.651485]\n",
      "[D loss: 0.901412] [G loss: 1.452021]\n",
      "[D loss: 0.690694] [G loss: 1.572481]\n",
      "[D loss: 0.743154] [G loss: 1.581554]\n",
      "[D loss: 0.949248] [G loss: 1.327012]\n",
      "[D loss: 0.820003] [G loss: 1.484878]\n",
      "[D loss: 0.855116] [G loss: 1.467026]\n",
      "[D loss: 0.725130] [G loss: 1.581763]\n",
      "[D loss: 0.948952] [G loss: 1.434728]\n",
      "[D loss: 0.766372] [G loss: 1.363589]\n",
      "[D loss: 0.892839] [G loss: 1.342829]\n",
      "[D loss: 0.833808] [G loss: 1.556210]\n",
      "[D loss: 0.693532] [G loss: 1.639411]\n",
      "[D loss: 0.814376] [G loss: 1.492534]\n",
      "[D loss: 0.762201] [G loss: 1.471392]\n",
      "[D loss: 0.869516] [G loss: 1.325059]\n",
      "[D loss: 0.768703] [G loss: 1.510895]\n",
      "[D loss: 0.737371] [G loss: 1.537530]\n",
      "[D loss: 0.731780] [G loss: 1.618550]\n",
      "[D loss: 0.775091] [G loss: 1.502917]\n",
      "[D loss: 0.834252] [G loss: 1.299638]\n",
      "[D loss: 0.866041] [G loss: 1.497886]\n",
      "[D loss: 0.821655] [G loss: 1.455686]\n",
      "[D loss: 0.709989] [G loss: 1.558283]\n",
      "[D loss: 0.839906] [G loss: 1.623708]\n",
      "[D loss: 0.719994] [G loss: 1.502403]\n",
      "[D loss: 0.797959] [G loss: 1.501019]\n",
      "[D loss: 0.913394] [G loss: 1.552946]\n",
      "[D loss: 0.989434] [G loss: 1.422023]\n",
      "[D loss: 0.513373] [G loss: 1.830214]\n",
      "[D loss: 0.809293] [G loss: 1.350637]\n",
      "[D loss: 0.856036] [G loss: 1.289115]\n",
      "[D loss: 0.840575] [G loss: 1.367728]\n",
      "[D loss: 0.922655] [G loss: 1.473626]\n",
      "[D loss: 0.611667] [G loss: 1.537899]\n",
      "[D loss: 0.958352] [G loss: 1.742072]\n",
      "[D loss: 0.804189] [G loss: 1.620350]\n",
      "[D loss: 0.701655] [G loss: 1.709430]\n",
      "[D loss: 0.553242] [G loss: 1.765241]\n",
      "[D loss: 0.900769] [G loss: 1.492355]\n",
      "[D loss: 0.823352] [G loss: 1.471377]\n",
      "[D loss: 0.881975] [G loss: 1.471765]\n",
      "[D loss: 0.838665] [G loss: 1.670794]\n",
      "[D loss: 0.853018] [G loss: 1.784971]\n",
      "[D loss: 0.699723] [G loss: 1.695978]\n",
      "[D loss: 0.839967] [G loss: 1.519151]\n",
      "[D loss: 1.159114] [G loss: 1.373921]\n",
      "[D loss: 0.822792] [G loss: 1.709961]\n",
      "[D loss: 0.761026] [G loss: 1.527113]\n",
      "[D loss: 0.887100] [G loss: 1.363620]\n",
      "[D loss: 0.755103] [G loss: 1.628674]\n",
      "[D loss: 0.890303] [G loss: 1.547436]\n",
      "[D loss: 0.854708] [G loss: 1.386144]\n",
      "[D loss: 0.916355] [G loss: 1.461629]\n",
      "[D loss: 0.737718] [G loss: 1.694917]\n",
      "[D loss: 0.983181] [G loss: 1.386559]\n",
      "[D loss: 0.698197] [G loss: 1.572411]\n",
      "[D loss: 0.687530] [G loss: 1.311685]\n",
      "[D loss: 0.954925] [G loss: 1.253855]\n",
      "[D loss: 0.723817] [G loss: 1.466820]\n",
      "[D loss: 0.693431] [G loss: 1.687015]\n",
      "[D loss: 0.911598] [G loss: 1.352185]\n",
      "[D loss: 0.551138] [G loss: 1.618294]\n",
      "[D loss: 0.874511] [G loss: 1.561100]\n",
      "[D loss: 0.776266] [G loss: 1.578203]\n",
      "[D loss: 0.836936] [G loss: 1.376124]\n",
      "[D loss: 0.454228] [G loss: 1.666885]\n",
      "[D loss: 1.009057] [G loss: 1.480796]\n",
      "[D loss: 0.807862] [G loss: 1.709044]\n",
      "[D loss: 0.892644] [G loss: 1.369076]\n",
      "[D loss: 0.734208] [G loss: 1.672073]\n",
      "[D loss: 0.606639] [G loss: 1.461831]\n",
      "[D loss: 1.157548] [G loss: 1.214704]\n",
      "[D loss: 0.655848] [G loss: 1.458837]\n",
      "[D loss: 0.851910] [G loss: 1.479782]\n",
      "[D loss: 1.056886] [G loss: 1.996068]\n",
      "[D loss: 0.974087] [G loss: 1.521780]\n",
      "[D loss: 0.715581] [G loss: 1.380587]\n",
      "[D loss: 0.729289] [G loss: 1.727768]\n",
      "[D loss: 0.968283] [G loss: 1.228869]\n",
      "[D loss: 0.983829] [G loss: 1.445942]\n",
      "[D loss: 0.378679] [G loss: 1.519129]\n",
      "[D loss: 0.912322] [G loss: 1.280344]\n",
      "[D loss: 0.672964] [G loss: 1.467863]\n",
      "[D loss: 0.827794] [G loss: 1.415431]\n",
      "[D loss: 0.803709] [G loss: 1.546597]\n",
      "[D loss: 0.840593] [G loss: 1.802501]\n",
      "[D loss: 0.858752] [G loss: 1.553653]\n",
      "[D loss: 0.763129] [G loss: 1.568967]\n",
      "[D loss: 0.778426] [G loss: 1.501929]\n",
      "[D loss: 0.731286] [G loss: 1.509193]\n",
      "[D loss: 0.900767] [G loss: 1.489549]\n",
      "[D loss: 0.755411] [G loss: 1.607035]\n",
      "[D loss: 0.907254] [G loss: 1.379588]\n",
      "[D loss: 0.686130] [G loss: 1.481894]\n",
      "[D loss: 0.974995] [G loss: 1.857685]\n",
      "[D loss: 1.007689] [G loss: 1.799444]\n",
      "[D loss: 1.051883] [G loss: 1.322803]\n",
      "[D loss: 0.663283] [G loss: 1.116559]\n",
      "[D loss: 0.779138] [G loss: 1.382872]\n",
      "[D loss: 1.019893] [G loss: 1.396112]\n",
      "[D loss: 0.921677] [G loss: 1.401220]\n",
      "[D loss: 0.855180] [G loss: 1.747787]\n",
      "[D loss: 0.848093] [G loss: 1.552997]\n",
      "[D loss: 0.930207] [G loss: 1.301417]\n",
      "[D loss: 0.778064] [G loss: 1.469173]\n",
      "[D loss: 0.708779] [G loss: 1.525594]\n",
      "[D loss: 0.902488] [G loss: 1.474078]\n",
      "[D loss: 0.995383] [G loss: 1.322313]\n",
      "[D loss: 0.930804] [G loss: 1.414316]\n",
      "[D loss: 0.761762] [G loss: 1.641131]\n",
      "[D loss: 0.604528] [G loss: 1.636567]\n",
      "[D loss: 0.941597] [G loss: 1.349773]\n",
      "[D loss: 0.710452] [G loss: 1.649105]\n",
      "[D loss: 0.858479] [G loss: 1.518848]\n",
      "[D loss: 0.799274] [G loss: 1.675546]\n",
      "[D loss: 0.833372] [G loss: 1.478348]\n",
      "[D loss: 0.682785] [G loss: 1.395101]\n",
      "[D loss: 0.911553] [G loss: 1.532307]\n",
      "[D loss: 0.764049] [G loss: 1.393678]\n",
      "[D loss: 0.774500] [G loss: 1.496029]\n",
      "[D loss: 0.887278] [G loss: 1.392773]\n",
      "[D loss: 0.819579] [G loss: 1.320207]\n",
      "[D loss: 0.608971] [G loss: 1.437926]\n",
      "[D loss: 0.565512] [G loss: 1.841955]\n",
      "[D loss: 0.707883] [G loss: 1.641260]\n",
      "[D loss: 0.858741] [G loss: 1.763419]\n",
      "[D loss: 0.747308] [G loss: 1.573361]\n",
      "[D loss: 0.651211] [G loss: 1.607677]\n",
      "[D loss: 0.720287] [G loss: 1.607710]\n",
      "[D loss: 0.590704] [G loss: 1.536644]\n",
      "[D loss: 0.762158] [G loss: 1.474777]\n",
      "[D loss: 0.805886] [G loss: 1.524154]\n",
      "[D loss: 0.756372] [G loss: 1.612080]\n",
      "[D loss: 0.637153] [G loss: 1.666801]\n",
      "[D loss: 0.653352] [G loss: 1.684025]\n",
      "[D loss: 0.665348] [G loss: 1.551362]\n",
      "[D loss: 0.862896] [G loss: 1.404088]\n",
      "[D loss: 0.771185] [G loss: 1.669872]\n",
      "[D loss: 0.733741] [G loss: 1.472077]\n",
      "[D loss: 0.833661] [G loss: 1.642126]\n",
      "[D loss: 0.964200] [G loss: 1.612979]\n",
      "[D loss: 0.710879] [G loss: 1.548618]\n",
      "[D loss: 0.789038] [G loss: 1.547748]\n",
      "[D loss: 0.672598] [G loss: 1.661345]\n",
      "[D loss: 0.765136] [G loss: 1.512872]\n",
      "[D loss: 0.813544] [G loss: 1.617608]\n",
      "[D loss: 0.785918] [G loss: 1.350985]\n",
      "[D loss: 0.975162] [G loss: 1.603844]\n",
      "[D loss: 0.807879] [G loss: 1.827370]\n",
      "[D loss: 0.913297] [G loss: 1.587847]\n",
      "[D loss: 1.086044] [G loss: 1.498060]\n",
      "[D loss: 0.939193] [G loss: 1.493409]\n",
      "[D loss: 0.606846] [G loss: 1.443151]\n",
      "[D loss: 0.849029] [G loss: 1.521464]\n",
      "[D loss: 0.857819] [G loss: 1.507932]\n",
      "[D loss: 0.847031] [G loss: 1.585773]\n",
      "[D loss: 0.960343] [G loss: 1.275363]\n",
      "[D loss: 0.813076] [G loss: 1.429924]\n",
      "[D loss: 0.880583] [G loss: 1.361340]\n",
      "[D loss: 0.710780] [G loss: 1.507033]\n",
      "[D loss: 0.732654] [G loss: 1.444928]\n",
      "[D loss: 0.661974] [G loss: 1.622218]\n",
      "[D loss: 0.775289] [G loss: 1.692045]\n",
      "[D loss: 0.599018] [G loss: 1.548125]\n",
      "[D loss: 0.804057] [G loss: 1.416696]\n",
      "[D loss: 0.834277] [G loss: 1.513095]\n",
      "[D loss: 0.733074] [G loss: 1.564870]\n",
      "[D loss: 0.908520] [G loss: 1.497093]\n",
      "[D loss: 0.764493] [G loss: 1.768993]\n",
      "[D loss: 1.080814] [G loss: 1.580389]\n",
      "[D loss: 0.866158] [G loss: 1.701494]\n",
      "[D loss: 0.690488] [G loss: 1.620886]\n",
      "[D loss: 0.884511] [G loss: 1.545777]\n",
      "[D loss: 0.750292] [G loss: 1.420137]\n",
      "[D loss: 0.684533] [G loss: 1.533895]\n",
      "[D loss: 1.023582] [G loss: 1.706523]\n",
      "[D loss: 0.974069] [G loss: 1.429918]\n",
      "[D loss: 0.891805] [G loss: 1.419632]\n",
      "[D loss: 0.698323] [G loss: 1.578176]\n",
      "[D loss: 1.003251] [G loss: 1.353849]\n",
      "[D loss: 0.737189] [G loss: 1.365570]\n",
      "[D loss: 0.872531] [G loss: 1.306554]\n",
      "[D loss: 0.702072] [G loss: 1.547090]\n",
      "[D loss: 0.728104] [G loss: 1.576685]\n",
      "[D loss: 0.633296] [G loss: 1.462197]\n",
      "[D loss: 0.915440] [G loss: 1.585405]\n",
      "[D loss: 0.809746] [G loss: 1.744280]\n",
      "[D loss: 0.770740] [G loss: 1.648885]\n",
      "[D loss: 0.728826] [G loss: 1.480476]\n",
      "[D loss: 0.922809] [G loss: 1.397952]\n",
      "[D loss: 1.115943] [G loss: 1.348939]\n",
      "[D loss: 0.879895] [G loss: 1.509160]\n",
      "[D loss: 0.932585] [G loss: 1.743916]\n",
      "[D loss: 0.951230] [G loss: 1.203752]\n",
      "[D loss: 0.741153] [G loss: 1.360000]\n",
      "[D loss: 0.916155] [G loss: 1.328358]\n",
      "[D loss: 0.632153] [G loss: 1.511511]\n",
      "[D loss: 0.831487] [G loss: 1.415725]\n",
      "[D loss: 0.730001] [G loss: 1.639284]\n",
      "[D loss: 0.876552] [G loss: 1.333874]\n",
      "[D loss: 0.569559] [G loss: 1.430224]\n",
      "[D loss: 0.808647] [G loss: 1.395201]\n",
      "[D loss: 1.006545] [G loss: 1.560642]\n",
      "[D loss: 0.846856] [G loss: 1.392565]\n",
      "[D loss: 0.786198] [G loss: 1.511031]\n",
      "[D loss: 0.867438] [G loss: 1.514353]\n",
      "[D loss: 0.929235] [G loss: 1.705936]\n",
      "[D loss: 0.957376] [G loss: 1.492401]\n",
      "[D loss: 0.767546] [G loss: 1.240360]\n",
      "[D loss: 0.764486] [G loss: 1.328167]\n",
      "[D loss: 0.986511] [G loss: 1.395487]\n",
      "[D loss: 0.921807] [G loss: 1.165748]\n",
      "[D loss: 0.879619] [G loss: 1.408402]\n",
      "[D loss: 0.726471] [G loss: 1.429689]\n",
      "[D loss: 0.621927] [G loss: 1.635268]\n",
      "[D loss: 0.930991] [G loss: 1.374958]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.502930] [G loss: 1.530859]\n",
      "[D loss: 0.903068] [G loss: 1.346556]\n",
      "[D loss: 1.341107] [G loss: 1.397220]\n",
      "[D loss: 0.860225] [G loss: 1.530251]\n",
      "[D loss: 1.077135] [G loss: 1.272922]\n",
      "[D loss: 0.846503] [G loss: 1.268718]\n",
      "[D loss: 0.969005] [G loss: 1.448852]\n",
      "[D loss: 0.734892] [G loss: 1.365721]\n",
      "[D loss: 0.741188] [G loss: 1.589150]\n",
      "[D loss: 0.841442] [G loss: 1.624864]\n",
      "[D loss: 0.810965] [G loss: 1.560069]\n",
      "[D loss: 0.748124] [G loss: 1.499053]\n",
      "[D loss: 0.838165] [G loss: 1.378019]\n",
      "[D loss: 0.778201] [G loss: 1.457660]\n",
      "epoch:4, g_loss:2790.754638671875,d_loss:1540.39599609375\n",
      "[D loss: 0.993049] [G loss: 1.316366]\n",
      "[D loss: 0.831748] [G loss: 1.421134]\n",
      "[D loss: 1.141907] [G loss: 1.406425]\n",
      "[D loss: 0.718743] [G loss: 1.600514]\n",
      "[D loss: 0.656872] [G loss: 1.478467]\n",
      "[D loss: 0.966391] [G loss: 1.258916]\n",
      "[D loss: 0.779210] [G loss: 1.432025]\n",
      "[D loss: 0.839766] [G loss: 1.323117]\n",
      "[D loss: 0.804226] [G loss: 1.537234]\n",
      "[D loss: 0.917790] [G loss: 1.608945]\n",
      "[D loss: 0.951592] [G loss: 1.752517]\n",
      "[D loss: 0.906706] [G loss: 1.452459]\n",
      "[D loss: 0.592392] [G loss: 1.592211]\n",
      "[D loss: 0.730438] [G loss: 1.244921]\n",
      "[D loss: 0.710175] [G loss: 1.447564]\n",
      "[D loss: 0.774244] [G loss: 1.647676]\n",
      "[D loss: 0.822539] [G loss: 1.633820]\n",
      "[D loss: 0.828093] [G loss: 1.471829]\n",
      "[D loss: 0.900100] [G loss: 1.505057]\n",
      "[D loss: 0.636942] [G loss: 1.948176]\n",
      "[D loss: 0.833232] [G loss: 1.437016]\n",
      "[D loss: 0.940937] [G loss: 1.611691]\n",
      "[D loss: 0.737683] [G loss: 1.506877]\n",
      "[D loss: 0.743334] [G loss: 1.526946]\n",
      "[D loss: 0.644272] [G loss: 1.546134]\n",
      "[D loss: 0.721153] [G loss: 1.618525]\n",
      "[D loss: 0.815902] [G loss: 1.542286]\n",
      "[D loss: 0.816640] [G loss: 1.246230]\n",
      "[D loss: 1.007311] [G loss: 1.524762]\n",
      "[D loss: 0.989680] [G loss: 1.142728]\n",
      "[D loss: 1.004106] [G loss: 1.527874]\n",
      "[D loss: 0.989672] [G loss: 1.710641]\n",
      "[D loss: 0.759445] [G loss: 1.579987]\n",
      "[D loss: 0.787989] [G loss: 1.575137]\n",
      "[D loss: 0.703224] [G loss: 1.663739]\n",
      "[D loss: 0.829179] [G loss: 1.419013]\n",
      "[D loss: 0.697733] [G loss: 1.479079]\n",
      "[D loss: 0.653242] [G loss: 1.599916]\n",
      "[D loss: 0.953196] [G loss: 1.450454]\n",
      "[D loss: 0.694785] [G loss: 1.526602]\n",
      "[D loss: 0.811772] [G loss: 1.651733]\n",
      "[D loss: 0.843082] [G loss: 1.519281]\n",
      "[D loss: 0.853192] [G loss: 1.264590]\n",
      "[D loss: 0.904385] [G loss: 1.407657]\n",
      "[D loss: 0.810801] [G loss: 1.551063]\n",
      "[D loss: 1.034611] [G loss: 1.335927]\n",
      "[D loss: 0.902034] [G loss: 1.593982]\n",
      "[D loss: 0.672898] [G loss: 1.447848]\n",
      "[D loss: 0.825908] [G loss: 1.625336]\n",
      "[D loss: 0.754427] [G loss: 1.494556]\n",
      "[D loss: 0.738274] [G loss: 1.338048]\n",
      "[D loss: 0.907028] [G loss: 1.302760]\n",
      "[D loss: 0.925655] [G loss: 1.285539]\n",
      "[D loss: 0.754073] [G loss: 1.379830]\n",
      "[D loss: 0.783151] [G loss: 1.498532]\n",
      "[D loss: 0.716346] [G loss: 1.481978]\n",
      "[D loss: 0.827649] [G loss: 1.532580]\n",
      "[D loss: 0.806322] [G loss: 1.544353]\n",
      "[D loss: 0.732047] [G loss: 1.645256]\n",
      "[D loss: 0.793685] [G loss: 1.599669]\n",
      "[D loss: 0.923994] [G loss: 1.318375]\n",
      "[D loss: 0.843648] [G loss: 1.659811]\n",
      "[D loss: 0.697837] [G loss: 1.632942]\n",
      "[D loss: 0.873104] [G loss: 1.416238]\n",
      "[D loss: 0.853436] [G loss: 1.602495]\n",
      "[D loss: 0.911087] [G loss: 1.413313]\n",
      "[D loss: 0.847519] [G loss: 1.630727]\n",
      "[D loss: 0.997773] [G loss: 1.520575]\n",
      "[D loss: 0.734932] [G loss: 1.451865]\n",
      "[D loss: 0.651920] [G loss: 1.608173]\n",
      "[D loss: 0.841404] [G loss: 1.463964]\n",
      "[D loss: 0.848513] [G loss: 1.487469]\n",
      "[D loss: 0.792407] [G loss: 1.254746]\n",
      "[D loss: 0.583817] [G loss: 1.644751]\n",
      "[D loss: 0.648431] [G loss: 1.820851]\n",
      "[D loss: 0.454803] [G loss: 1.763155]\n",
      "[D loss: 0.985858] [G loss: 1.614293]\n",
      "[D loss: 0.787365] [G loss: 1.714888]\n",
      "[D loss: 0.740714] [G loss: 1.552082]\n",
      "[D loss: 1.043787] [G loss: 1.489406]\n",
      "[D loss: 0.970722] [G loss: 1.500070]\n",
      "[D loss: 0.917233] [G loss: 1.389103]\n",
      "[D loss: 0.946167] [G loss: 1.513069]\n",
      "[D loss: 0.750319] [G loss: 1.545514]\n",
      "[D loss: 0.970068] [G loss: 1.400500]\n",
      "[D loss: 0.835818] [G loss: 1.532511]\n",
      "[D loss: 1.026837] [G loss: 1.492277]\n",
      "[D loss: 0.816479] [G loss: 1.473110]\n",
      "[D loss: 0.840177] [G loss: 1.367925]\n",
      "[D loss: 0.633668] [G loss: 1.662041]\n",
      "[D loss: 0.792162] [G loss: 1.307953]\n",
      "[D loss: 0.573191] [G loss: 1.430392]\n",
      "[D loss: 0.815743] [G loss: 1.651643]\n",
      "[D loss: 0.547946] [G loss: 1.682953]\n",
      "[D loss: 0.990328] [G loss: 1.564445]\n",
      "[D loss: 0.748055] [G loss: 1.656932]\n",
      "[D loss: 0.735837] [G loss: 1.400099]\n",
      "[D loss: 0.559169] [G loss: 1.592276]\n",
      "[D loss: 0.763060] [G loss: 1.611045]\n",
      "[D loss: 0.721916] [G loss: 1.429180]\n",
      "[D loss: 0.695367] [G loss: 1.655668]\n",
      "[D loss: 0.789523] [G loss: 1.645117]\n",
      "[D loss: 0.951596] [G loss: 1.513219]\n",
      "[D loss: 0.923907] [G loss: 1.401069]\n",
      "[D loss: 0.727343] [G loss: 1.617330]\n",
      "[D loss: 0.698336] [G loss: 1.605376]\n",
      "[D loss: 0.847758] [G loss: 1.241245]\n",
      "[D loss: 0.946222] [G loss: 1.410214]\n",
      "[D loss: 0.680926] [G loss: 1.426869]\n",
      "[D loss: 0.716618] [G loss: 1.521844]\n",
      "[D loss: 0.649501] [G loss: 1.392246]\n",
      "[D loss: 0.582747] [G loss: 1.598021]\n",
      "[D loss: 0.755420] [G loss: 1.465680]\n",
      "[D loss: 0.812291] [G loss: 1.594790]\n",
      "[D loss: 0.734099] [G loss: 1.459499]\n",
      "[D loss: 0.659586] [G loss: 1.562145]\n",
      "[D loss: 0.928876] [G loss: 1.540739]\n",
      "[D loss: 0.993085] [G loss: 1.347054]\n",
      "[D loss: 0.658961] [G loss: 1.542437]\n",
      "[D loss: 0.723697] [G loss: 1.419207]\n",
      "[D loss: 1.025407] [G loss: 1.450581]\n",
      "[D loss: 0.718647] [G loss: 1.667329]\n",
      "[D loss: 0.913509] [G loss: 1.463025]\n",
      "[D loss: 0.844402] [G loss: 1.452754]\n",
      "[D loss: 0.890350] [G loss: 1.568147]\n",
      "[D loss: 0.803313] [G loss: 1.499543]\n",
      "[D loss: 0.694716] [G loss: 1.378851]\n",
      "[D loss: 0.764257] [G loss: 1.364279]\n",
      "[D loss: 0.670703] [G loss: 1.562020]\n",
      "[D loss: 0.890796] [G loss: 1.525904]\n",
      "[D loss: 0.696638] [G loss: 1.377606]\n",
      "[D loss: 0.754543] [G loss: 1.380152]\n",
      "[D loss: 0.872425] [G loss: 1.681354]\n",
      "[D loss: 0.735686] [G loss: 1.591448]\n",
      "[D loss: 0.754462] [G loss: 1.569506]\n",
      "[D loss: 0.893712] [G loss: 1.716787]\n",
      "[D loss: 0.867851] [G loss: 1.410005]\n",
      "[D loss: 0.582225] [G loss: 1.412981]\n",
      "[D loss: 0.935576] [G loss: 1.617643]\n",
      "[D loss: 0.893918] [G loss: 1.556635]\n",
      "[D loss: 0.933567] [G loss: 1.669260]\n",
      "[D loss: 0.957860] [G loss: 1.549331]\n",
      "[D loss: 0.929650] [G loss: 1.619385]\n",
      "[D loss: 0.685936] [G loss: 1.366487]\n",
      "[D loss: 0.857637] [G loss: 1.394275]\n",
      "[D loss: 0.677194] [G loss: 1.477631]\n",
      "[D loss: 0.827686] [G loss: 1.465871]\n",
      "[D loss: 1.159760] [G loss: 1.579852]\n",
      "[D loss: 0.991974] [G loss: 1.331035]\n",
      "[D loss: 0.781967] [G loss: 1.544707]\n",
      "[D loss: 0.707034] [G loss: 1.384716]\n",
      "[D loss: 0.761189] [G loss: 1.218863]\n",
      "[D loss: 0.818441] [G loss: 1.583097]\n",
      "[D loss: 0.936614] [G loss: 1.605683]\n",
      "[D loss: 0.553416] [G loss: 1.489410]\n",
      "[D loss: 0.876389] [G loss: 1.688253]\n",
      "[D loss: 0.717830] [G loss: 1.311389]\n",
      "[D loss: 0.677456] [G loss: 1.393849]\n",
      "[D loss: 0.789374] [G loss: 1.715038]\n",
      "[D loss: 0.704813] [G loss: 1.687644]\n",
      "[D loss: 0.600447] [G loss: 1.628802]\n",
      "[D loss: 0.919413] [G loss: 1.538369]\n",
      "[D loss: 0.775007] [G loss: 1.483257]\n",
      "[D loss: 0.657394] [G loss: 1.490863]\n",
      "[D loss: 0.643741] [G loss: 1.258488]\n",
      "[D loss: 0.755462] [G loss: 1.523192]\n",
      "[D loss: 0.780570] [G loss: 1.492623]\n",
      "[D loss: 0.663967] [G loss: 1.589162]\n",
      "[D loss: 0.809803] [G loss: 1.780164]\n",
      "[D loss: 0.748176] [G loss: 1.710974]\n",
      "[D loss: 1.045308] [G loss: 1.332285]\n",
      "[D loss: 0.712134] [G loss: 1.509042]\n",
      "[D loss: 0.748908] [G loss: 1.723500]\n",
      "[D loss: 0.972808] [G loss: 1.720487]\n",
      "[D loss: 0.818787] [G loss: 1.588798]\n",
      "[D loss: 0.715965] [G loss: 1.462301]\n",
      "[D loss: 0.750830] [G loss: 1.673887]\n",
      "[D loss: 0.542081] [G loss: 1.692886]\n",
      "[D loss: 0.971427] [G loss: 1.381172]\n",
      "[D loss: 0.663727] [G loss: 1.577140]\n",
      "[D loss: 0.850863] [G loss: 1.716626]\n",
      "[D loss: 0.756757] [G loss: 1.671732]\n",
      "[D loss: 0.608299] [G loss: 1.639305]\n",
      "[D loss: 0.793368] [G loss: 1.673603]\n",
      "[D loss: 0.735922] [G loss: 1.588656]\n",
      "[D loss: 0.893401] [G loss: 1.275606]\n",
      "[D loss: 0.960922] [G loss: 1.444387]\n",
      "[D loss: 0.587910] [G loss: 1.465877]\n",
      "[D loss: 0.986019] [G loss: 1.481910]\n",
      "[D loss: 0.621536] [G loss: 1.761111]\n",
      "[D loss: 0.708890] [G loss: 1.779567]\n",
      "[D loss: 0.649793] [G loss: 1.890116]\n",
      "[D loss: 0.893467] [G loss: 1.326959]\n",
      "[D loss: 0.776381] [G loss: 1.629316]\n",
      "[D loss: 0.625363] [G loss: 1.656169]\n",
      "[D loss: 1.072626] [G loss: 1.248344]\n",
      "[D loss: 0.712953] [G loss: 1.468000]\n",
      "[D loss: 0.622562] [G loss: 1.743124]\n",
      "[D loss: 0.778693] [G loss: 1.493494]\n",
      "[D loss: 0.881833] [G loss: 1.526377]\n",
      "[D loss: 0.903932] [G loss: 1.703776]\n",
      "[D loss: 1.115909] [G loss: 1.455546]\n",
      "[D loss: 0.931160] [G loss: 1.715588]\n",
      "[D loss: 0.895989] [G loss: 1.674334]\n",
      "[D loss: 0.836775] [G loss: 1.812129]\n",
      "[D loss: 1.075418] [G loss: 1.232957]\n",
      "[D loss: 0.804824] [G loss: 1.235535]\n",
      "[D loss: 0.666255] [G loss: 1.628025]\n",
      "[D loss: 0.662056] [G loss: 1.636270]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.712450] [G loss: 1.532850]\n",
      "[D loss: 0.774252] [G loss: 1.547328]\n",
      "[D loss: 0.811720] [G loss: 1.600775]\n",
      "[D loss: 0.781087] [G loss: 1.738413]\n",
      "[D loss: 0.828797] [G loss: 1.576520]\n",
      "[D loss: 0.508377] [G loss: 1.523671]\n",
      "[D loss: 0.797266] [G loss: 1.284990]\n",
      "[D loss: 0.872677] [G loss: 1.524570]\n",
      "[D loss: 0.647689] [G loss: 1.560675]\n",
      "[D loss: 0.743089] [G loss: 1.579125]\n",
      "[D loss: 0.617450] [G loss: 1.601795]\n",
      "[D loss: 0.712570] [G loss: 1.811326]\n",
      "[D loss: 0.751125] [G loss: 1.489222]\n",
      "[D loss: 0.738009] [G loss: 1.585552]\n",
      "[D loss: 0.630046] [G loss: 1.692394]\n",
      "[D loss: 0.831690] [G loss: 1.813982]\n",
      "[D loss: 0.789107] [G loss: 1.440407]\n",
      "[D loss: 0.960887] [G loss: 1.398737]\n",
      "[D loss: 0.730809] [G loss: 1.569865]\n",
      "[D loss: 0.866364] [G loss: 1.655085]\n",
      "[D loss: 0.856608] [G loss: 1.598295]\n",
      "[D loss: 0.915533] [G loss: 1.557336]\n",
      "[D loss: 0.576936] [G loss: 1.667838]\n",
      "[D loss: 0.887128] [G loss: 1.430054]\n",
      "[D loss: 0.707838] [G loss: 1.423964]\n",
      "[D loss: 0.951725] [G loss: 1.259815]\n",
      "[D loss: 0.903480] [G loss: 1.543441]\n",
      "[D loss: 0.758512] [G loss: 1.376704]\n",
      "[D loss: 0.741676] [G loss: 1.470189]\n",
      "[D loss: 0.884051] [G loss: 1.412887]\n",
      "[D loss: 0.640499] [G loss: 1.597871]\n",
      "[D loss: 0.854187] [G loss: 1.333915]\n",
      "[D loss: 0.676827] [G loss: 1.598217]\n",
      "[D loss: 0.641020] [G loss: 1.539162]\n",
      "[D loss: 0.715398] [G loss: 1.503307]\n",
      "[D loss: 0.578562] [G loss: 1.756613]\n",
      "[D loss: 0.680623] [G loss: 1.586594]\n",
      "[D loss: 0.655215] [G loss: 1.837739]\n",
      "[D loss: 1.194994] [G loss: 1.833563]\n",
      "[D loss: 0.748200] [G loss: 1.608653]\n",
      "[D loss: 0.740368] [G loss: 1.670472]\n",
      "[D loss: 0.920801] [G loss: 1.412002]\n",
      "[D loss: 0.968853] [G loss: 1.386713]\n",
      "[D loss: 0.788842] [G loss: 1.498150]\n",
      "[D loss: 0.795599] [G loss: 1.589139]\n",
      "[D loss: 0.612519] [G loss: 1.282755]\n",
      "[D loss: 0.608743] [G loss: 1.561501]\n",
      "[D loss: 0.869294] [G loss: 1.392267]\n",
      "[D loss: 0.884494] [G loss: 1.634139]\n",
      "[D loss: 0.771551] [G loss: 1.588705]\n",
      "[D loss: 0.527813] [G loss: 1.713245]\n",
      "[D loss: 0.936273] [G loss: 1.718005]\n",
      "[D loss: 1.139369] [G loss: 1.559987]\n",
      "[D loss: 0.676403] [G loss: 1.573754]\n",
      "[D loss: 0.727778] [G loss: 1.604333]\n",
      "[D loss: 0.712400] [G loss: 1.564101]\n",
      "[D loss: 0.711146] [G loss: 1.598630]\n",
      "[D loss: 0.833442] [G loss: 1.708930]\n",
      "[D loss: 0.875057] [G loss: 1.446096]\n",
      "[D loss: 0.793794] [G loss: 1.488269]\n",
      "[D loss: 0.749194] [G loss: 1.497450]\n",
      "[D loss: 1.003826] [G loss: 1.474211]\n",
      "[D loss: 0.666250] [G loss: 1.476416]\n",
      "[D loss: 1.027907] [G loss: 1.411323]\n",
      "[D loss: 0.950224] [G loss: 1.493901]\n",
      "[D loss: 0.816794] [G loss: 1.380263]\n",
      "[D loss: 0.865265] [G loss: 1.189901]\n",
      "[D loss: 0.926283] [G loss: 1.266739]\n",
      "[D loss: 0.790447] [G loss: 1.367543]\n",
      "[D loss: 0.981776] [G loss: 1.451783]\n",
      "[D loss: 0.762278] [G loss: 1.335885]\n",
      "[D loss: 0.654636] [G loss: 1.582881]\n",
      "[D loss: 0.972713] [G loss: 1.405290]\n",
      "[D loss: 0.958555] [G loss: 1.514593]\n",
      "[D loss: 0.917562] [G loss: 1.418190]\n",
      "[D loss: 0.723563] [G loss: 1.373374]\n",
      "[D loss: 0.747138] [G loss: 1.436178]\n",
      "[D loss: 0.781404] [G loss: 1.472364]\n",
      "[D loss: 0.888467] [G loss: 1.342360]\n",
      "[D loss: 0.716138] [G loss: 1.539901]\n",
      "[D loss: 0.871768] [G loss: 1.443363]\n",
      "[D loss: 0.989561] [G loss: 1.507325]\n",
      "[D loss: 0.787033] [G loss: 1.331484]\n",
      "[D loss: 0.909605] [G loss: 1.473379]\n",
      "[D loss: 0.920614] [G loss: 1.387652]\n",
      "[D loss: 0.981491] [G loss: 1.670261]\n",
      "[D loss: 0.883513] [G loss: 1.451368]\n",
      "[D loss: 0.712554] [G loss: 1.612798]\n",
      "[D loss: 0.654169] [G loss: 1.761528]\n",
      "[D loss: 0.837238] [G loss: 1.505840]\n",
      "[D loss: 1.072020] [G loss: 1.403859]\n",
      "[D loss: 0.892993] [G loss: 1.461909]\n",
      "[D loss: 0.843555] [G loss: 1.634892]\n",
      "[D loss: 0.806610] [G loss: 1.498644]\n",
      "[D loss: 0.964527] [G loss: 1.469474]\n",
      "[D loss: 1.027724] [G loss: 1.480326]\n",
      "[D loss: 0.583596] [G loss: 1.379473]\n",
      "[D loss: 0.909539] [G loss: 1.405439]\n",
      "[D loss: 0.973800] [G loss: 1.362820]\n",
      "[D loss: 0.804666] [G loss: 1.442245]\n",
      "[D loss: 0.615976] [G loss: 1.602547]\n",
      "[D loss: 0.859727] [G loss: 1.518735]\n",
      "[D loss: 1.088363] [G loss: 1.306869]\n",
      "[D loss: 0.677215] [G loss: 1.412475]\n",
      "[D loss: 0.788903] [G loss: 1.357704]\n",
      "[D loss: 0.845108] [G loss: 1.461271]\n",
      "[D loss: 0.962177] [G loss: 1.421122]\n",
      "[D loss: 0.866008] [G loss: 1.385570]\n",
      "[D loss: 0.979078] [G loss: 1.305461]\n",
      "[D loss: 0.702397] [G loss: 1.402514]\n",
      "[D loss: 0.669882] [G loss: 1.356269]\n",
      "[D loss: 0.851850] [G loss: 1.495437]\n",
      "[D loss: 1.021730] [G loss: 1.400281]\n",
      "[D loss: 0.738276] [G loss: 1.343536]\n",
      "[D loss: 0.831898] [G loss: 1.388967]\n",
      "[D loss: 0.820796] [G loss: 1.465124]\n",
      "[D loss: 0.866108] [G loss: 1.407572]\n",
      "[D loss: 1.037460] [G loss: 1.584587]\n",
      "[D loss: 0.719520] [G loss: 1.497645]\n",
      "[D loss: 0.742581] [G loss: 1.455096]\n",
      "[D loss: 0.945024] [G loss: 1.371598]\n",
      "[D loss: 0.740780] [G loss: 1.454521]\n",
      "[D loss: 0.925164] [G loss: 1.250189]\n",
      "[D loss: 0.845786] [G loss: 1.494882]\n",
      "[D loss: 0.839963] [G loss: 1.499534]\n",
      "[D loss: 0.661191] [G loss: 1.711202]\n",
      "[D loss: 0.801046] [G loss: 1.693477]\n",
      "[D loss: 0.727929] [G loss: 1.556714]\n",
      "[D loss: 0.920926] [G loss: 1.447794]\n",
      "[D loss: 0.954165] [G loss: 1.233604]\n",
      "[D loss: 0.684055] [G loss: 1.751232]\n",
      "[D loss: 0.765363] [G loss: 1.574195]\n",
      "[D loss: 0.865806] [G loss: 1.484956]\n",
      "[D loss: 0.753571] [G loss: 1.378357]\n",
      "[D loss: 0.695319] [G loss: 1.268561]\n",
      "[D loss: 0.751326] [G loss: 1.549067]\n",
      "[D loss: 0.528439] [G loss: 1.703075]\n",
      "[D loss: 0.721508] [G loss: 1.740059]\n",
      "[D loss: 0.846844] [G loss: 1.280224]\n",
      "[D loss: 0.888503] [G loss: 1.404887]\n",
      "[D loss: 0.789075] [G loss: 1.560040]\n",
      "[D loss: 1.036743] [G loss: 1.594631]\n",
      "[D loss: 1.087065] [G loss: 1.447095]\n",
      "[D loss: 0.850767] [G loss: 1.554383]\n",
      "[D loss: 1.182938] [G loss: 1.195795]\n",
      "[D loss: 1.039658] [G loss: 1.387954]\n",
      "[D loss: 0.769934] [G loss: 1.336919]\n",
      "[D loss: 0.789405] [G loss: 1.531696]\n",
      "[D loss: 1.038112] [G loss: 1.323258]\n",
      "[D loss: 0.990247] [G loss: 1.256416]\n",
      "[D loss: 0.758405] [G loss: 1.447754]\n",
      "[D loss: 1.004630] [G loss: 1.319435]\n",
      "[D loss: 1.021648] [G loss: 1.280189]\n",
      "[D loss: 0.914571] [G loss: 1.434123]\n",
      "[D loss: 0.793109] [G loss: 1.523783]\n",
      "[D loss: 0.834434] [G loss: 1.370734]\n",
      "[D loss: 0.740319] [G loss: 1.695242]\n",
      "[D loss: 0.814496] [G loss: 1.434332]\n",
      "[D loss: 0.951370] [G loss: 1.292397]\n",
      "[D loss: 0.739704] [G loss: 1.820531]\n",
      "[D loss: 0.993526] [G loss: 1.403150]\n",
      "[D loss: 0.879322] [G loss: 1.543790]\n",
      "[D loss: 0.760936] [G loss: 1.525213]\n",
      "[D loss: 0.816654] [G loss: 1.300949]\n",
      "[D loss: 0.832697] [G loss: 1.564216]\n",
      "[D loss: 0.758419] [G loss: 1.417809]\n",
      "[D loss: 1.027515] [G loss: 1.416794]\n",
      "[D loss: 0.729976] [G loss: 1.319806]\n",
      "[D loss: 0.698538] [G loss: 1.434741]\n",
      "[D loss: 0.902968] [G loss: 1.428275]\n",
      "[D loss: 0.941729] [G loss: 1.562303]\n",
      "[D loss: 0.561759] [G loss: 1.459181]\n",
      "[D loss: 0.647231] [G loss: 1.395608]\n",
      "[D loss: 0.685120] [G loss: 1.432766]\n",
      "[D loss: 0.847140] [G loss: 1.543285]\n",
      "[D loss: 0.750447] [G loss: 1.609953]\n",
      "[D loss: 0.966634] [G loss: 1.593506]\n",
      "[D loss: 0.894132] [G loss: 1.531093]\n",
      "[D loss: 0.805602] [G loss: 1.450181]\n",
      "[D loss: 0.896667] [G loss: 1.493072]\n",
      "[D loss: 0.790073] [G loss: 1.566065]\n",
      "[D loss: 0.945746] [G loss: 1.370783]\n",
      "[D loss: 0.727718] [G loss: 1.444495]\n",
      "[D loss: 0.912475] [G loss: 1.368940]\n",
      "[D loss: 0.697315] [G loss: 1.610376]\n",
      "[D loss: 0.809097] [G loss: 1.693491]\n",
      "[D loss: 0.538350] [G loss: 1.770844]\n",
      "[D loss: 0.632817] [G loss: 1.615207]\n",
      "[D loss: 0.765126] [G loss: 1.603987]\n",
      "[D loss: 0.488575] [G loss: 1.581001]\n",
      "[D loss: 0.690072] [G loss: 1.631695]\n",
      "[D loss: 0.831428] [G loss: 1.614444]\n",
      "[D loss: 0.924869] [G loss: 1.533961]\n",
      "[D loss: 0.764489] [G loss: 1.635329]\n",
      "[D loss: 0.795810] [G loss: 1.440659]\n",
      "[D loss: 0.862168] [G loss: 1.298170]\n",
      "[D loss: 1.078462] [G loss: 1.535167]\n",
      "[D loss: 0.910362] [G loss: 1.556720]\n",
      "[D loss: 0.723441] [G loss: 1.515324]\n",
      "[D loss: 0.807661] [G loss: 1.606593]\n",
      "[D loss: 0.933037] [G loss: 1.303052]\n",
      "[D loss: 0.734788] [G loss: 1.372063]\n",
      "[D loss: 1.108019] [G loss: 1.344279]\n",
      "[D loss: 0.653474] [G loss: 1.562596]\n",
      "[D loss: 0.738882] [G loss: 1.433420]\n",
      "[D loss: 0.835985] [G loss: 1.332477]\n",
      "[D loss: 0.717239] [G loss: 1.445905]\n",
      "[D loss: 0.801547] [G loss: 1.387375]\n",
      "[D loss: 0.625550] [G loss: 1.658333]\n",
      "[D loss: 0.974408] [G loss: 1.461696]\n",
      "[D loss: 0.734705] [G loss: 1.488744]\n",
      "[D loss: 0.789811] [G loss: 1.559643]\n",
      "[D loss: 1.006555] [G loss: 1.453731]\n",
      "[D loss: 0.578479] [G loss: 1.572966]\n",
      "[D loss: 0.758751] [G loss: 1.539392]\n",
      "[D loss: 0.994563] [G loss: 1.348835]\n",
      "[D loss: 0.627614] [G loss: 1.410926]\n",
      "[D loss: 0.706710] [G loss: 1.341845]\n",
      "[D loss: 0.755069] [G loss: 1.535518]\n",
      "[D loss: 0.793604] [G loss: 1.427087]\n",
      "[D loss: 0.982722] [G loss: 1.420727]\n",
      "[D loss: 0.791112] [G loss: 1.537263]\n",
      "[D loss: 0.688873] [G loss: 1.593268]\n",
      "[D loss: 0.548844] [G loss: 1.748780]\n",
      "[D loss: 0.759827] [G loss: 1.605281]\n",
      "[D loss: 0.812152] [G loss: 1.444416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.821239] [G loss: 1.636037]\n",
      "[D loss: 0.794454] [G loss: 1.513471]\n",
      "[D loss: 0.815717] [G loss: 1.393716]\n",
      "[D loss: 0.761030] [G loss: 1.568807]\n",
      "[D loss: 0.795927] [G loss: 1.376051]\n",
      "[D loss: 0.855288] [G loss: 1.472753]\n",
      "[D loss: 0.676476] [G loss: 1.343808]\n",
      "[D loss: 0.750419] [G loss: 1.456581]\n",
      "[D loss: 0.614609] [G loss: 1.475241]\n",
      "[D loss: 0.703048] [G loss: 1.501690]\n",
      "[D loss: 0.817320] [G loss: 1.586762]\n",
      "[D loss: 0.943116] [G loss: 1.429603]\n",
      "[D loss: 0.810608] [G loss: 1.332337]\n",
      "[D loss: 0.966008] [G loss: 1.514166]\n",
      "[D loss: 0.764550] [G loss: 1.605638]\n",
      "[D loss: 0.693729] [G loss: 2.057627]\n",
      "[D loss: 0.859847] [G loss: 1.906181]\n",
      "[D loss: 0.655990] [G loss: 1.565971]\n",
      "[D loss: 0.850198] [G loss: 1.416502]\n",
      "[D loss: 0.957836] [G loss: 1.272356]\n",
      "[D loss: 0.985121] [G loss: 1.247803]\n",
      "[D loss: 0.904729] [G loss: 1.484126]\n",
      "[D loss: 0.739261] [G loss: 1.577274]\n",
      "[D loss: 0.728743] [G loss: 1.512968]\n",
      "[D loss: 0.673429] [G loss: 1.646008]\n",
      "[D loss: 0.914620] [G loss: 1.553324]\n",
      "[D loss: 0.801630] [G loss: 1.565473]\n",
      "[D loss: 0.736607] [G loss: 1.814428]\n",
      "[D loss: 0.927690] [G loss: 1.288443]\n",
      "[D loss: 0.917242] [G loss: 1.297560]\n",
      "[D loss: 0.759607] [G loss: 1.440853]\n",
      "[D loss: 0.819297] [G loss: 1.572526]\n",
      "[D loss: 0.609646] [G loss: 1.698052]\n",
      "[D loss: 0.679936] [G loss: 2.001457]\n",
      "[D loss: 0.923367] [G loss: 1.422025]\n",
      "[D loss: 0.756182] [G loss: 1.636784]\n",
      "[D loss: 1.228274] [G loss: 1.360004]\n",
      "[D loss: 0.945187] [G loss: 1.412692]\n",
      "[D loss: 0.719338] [G loss: 1.605419]\n",
      "[D loss: 0.793528] [G loss: 1.510288]\n",
      "[D loss: 0.874833] [G loss: 1.406146]\n",
      "[D loss: 0.468177] [G loss: 1.673744]\n",
      "[D loss: 0.796780] [G loss: 1.352059]\n",
      "[D loss: 1.033959] [G loss: 1.196430]\n",
      "[D loss: 0.766822] [G loss: 1.525618]\n",
      "[D loss: 0.888605] [G loss: 1.661928]\n",
      "[D loss: 0.607426] [G loss: 1.485875]\n",
      "[D loss: 0.764398] [G loss: 1.388325]\n",
      "[D loss: 1.053798] [G loss: 1.531437]\n",
      "[D loss: 0.770601] [G loss: 1.372304]\n",
      "[D loss: 0.733352] [G loss: 1.710145]\n",
      "[D loss: 0.634757] [G loss: 1.680229]\n",
      "[D loss: 0.645406] [G loss: 1.707342]\n",
      "[D loss: 0.902323] [G loss: 1.508364]\n",
      "[D loss: 0.713825] [G loss: 1.386232]\n",
      "[D loss: 0.725588] [G loss: 1.580550]\n",
      "[D loss: 0.497351] [G loss: 1.607741]\n",
      "[D loss: 1.064415] [G loss: 1.669586]\n",
      "[D loss: 0.658128] [G loss: 1.852987]\n",
      "[D loss: 0.753320] [G loss: 1.680011]\n",
      "[D loss: 0.998508] [G loss: 1.454212]\n",
      "[D loss: 0.720647] [G loss: 1.568940]\n",
      "[D loss: 0.687502] [G loss: 1.558846]\n",
      "[D loss: 0.948420] [G loss: 1.410176]\n",
      "[D loss: 0.851307] [G loss: 1.373116]\n",
      "[D loss: 0.971323] [G loss: 1.406700]\n",
      "[D loss: 0.842979] [G loss: 1.468425]\n",
      "[D loss: 0.876219] [G loss: 1.396720]\n",
      "[D loss: 0.670760] [G loss: 1.533452]\n",
      "[D loss: 0.690879] [G loss: 1.387584]\n",
      "[D loss: 1.064525] [G loss: 1.516682]\n",
      "[D loss: 1.027056] [G loss: 1.407025]\n",
      "[D loss: 0.669253] [G loss: 1.519645]\n",
      "[D loss: 0.702709] [G loss: 1.612015]\n",
      "[D loss: 0.853709] [G loss: 1.429431]\n",
      "[D loss: 0.580506] [G loss: 1.581707]\n",
      "[D loss: 0.991721] [G loss: 1.514684]\n",
      "[D loss: 0.948385] [G loss: 1.371627]\n",
      "[D loss: 1.029636] [G loss: 1.527357]\n",
      "[D loss: 0.682213] [G loss: 1.410852]\n",
      "[D loss: 1.064872] [G loss: 1.518775]\n",
      "[D loss: 0.813783] [G loss: 1.421693]\n",
      "[D loss: 0.638345] [G loss: 1.390959]\n",
      "[D loss: 0.887068] [G loss: 1.214764]\n",
      "[D loss: 0.993450] [G loss: 1.443180]\n",
      "[D loss: 0.868798] [G loss: 1.536120]\n",
      "[D loss: 0.788423] [G loss: 1.494154]\n",
      "[D loss: 0.759433] [G loss: 1.427289]\n",
      "[D loss: 0.636836] [G loss: 1.400586]\n",
      "[D loss: 0.693241] [G loss: 1.518175]\n",
      "[D loss: 1.022274] [G loss: 1.535361]\n",
      "[D loss: 0.879871] [G loss: 1.423926]\n",
      "[D loss: 0.709736] [G loss: 1.674025]\n",
      "[D loss: 0.865073] [G loss: 1.338017]\n",
      "[D loss: 0.691778] [G loss: 1.563205]\n",
      "[D loss: 0.757130] [G loss: 1.355630]\n",
      "[D loss: 0.753636] [G loss: 1.467732]\n",
      "[D loss: 0.878801] [G loss: 1.613868]\n",
      "[D loss: 0.733540] [G loss: 1.538759]\n",
      "[D loss: 0.607925] [G loss: 1.512958]\n",
      "[D loss: 0.634014] [G loss: 1.570191]\n",
      "[D loss: 0.695124] [G loss: 1.646196]\n",
      "[D loss: 0.858340] [G loss: 1.704983]\n",
      "[D loss: 0.676583] [G loss: 1.615402]\n",
      "[D loss: 0.712741] [G loss: 1.497478]\n",
      "[D loss: 0.777398] [G loss: 1.359580]\n",
      "[D loss: 1.030836] [G loss: 1.242420]\n",
      "[D loss: 0.803507] [G loss: 1.580872]\n",
      "[D loss: 1.072642] [G loss: 1.584659]\n",
      "[D loss: 0.776307] [G loss: 1.445106]\n",
      "[D loss: 0.893979] [G loss: 1.485207]\n",
      "[D loss: 0.595509] [G loss: 1.559625]\n",
      "[D loss: 0.892832] [G loss: 1.736449]\n",
      "[D loss: 1.047353] [G loss: 1.592161]\n",
      "[D loss: 0.683109] [G loss: 1.763512]\n",
      "[D loss: 1.233473] [G loss: 1.379907]\n",
      "[D loss: 0.973518] [G loss: 1.631887]\n",
      "[D loss: 0.709546] [G loss: 1.565144]\n",
      "[D loss: 0.996898] [G loss: 1.244437]\n",
      "[D loss: 0.821719] [G loss: 1.322640]\n",
      "[D loss: 0.783977] [G loss: 1.404569]\n",
      "[D loss: 0.570983] [G loss: 1.517991]\n",
      "[D loss: 0.838011] [G loss: 1.347950]\n",
      "[D loss: 1.009340] [G loss: 1.290080]\n",
      "[D loss: 0.557342] [G loss: 1.601994]\n",
      "[D loss: 1.022937] [G loss: 1.321987]\n",
      "[D loss: 0.773131] [G loss: 1.428664]\n",
      "[D loss: 0.902138] [G loss: 1.365618]\n",
      "[D loss: 0.647177] [G loss: 1.584682]\n",
      "[D loss: 0.649113] [G loss: 1.496974]\n",
      "[D loss: 0.702954] [G loss: 1.452271]\n",
      "[D loss: 0.657978] [G loss: 1.429448]\n",
      "[D loss: 0.751823] [G loss: 1.620101]\n",
      "[D loss: 0.930584] [G loss: 1.383294]\n",
      "[D loss: 0.920366] [G loss: 1.392674]\n",
      "[D loss: 0.739998] [G loss: 1.606674]\n",
      "[D loss: 0.870868] [G loss: 1.504158]\n",
      "[D loss: 0.974813] [G loss: 1.352157]\n",
      "[D loss: 1.034270] [G loss: 1.288106]\n",
      "[D loss: 0.732946] [G loss: 1.549701]\n",
      "[D loss: 0.795513] [G loss: 1.545797]\n",
      "[D loss: 0.809003] [G loss: 1.425856]\n",
      "[D loss: 0.834428] [G loss: 1.513940]\n",
      "[D loss: 0.734043] [G loss: 1.493089]\n",
      "[D loss: 0.732479] [G loss: 1.474053]\n",
      "[D loss: 0.736460] [G loss: 1.508162]\n",
      "[D loss: 1.170986] [G loss: 1.200526]\n",
      "[D loss: 0.712499] [G loss: 1.596165]\n",
      "[D loss: 0.799035] [G loss: 1.391239]\n",
      "[D loss: 0.900427] [G loss: 1.530123]\n",
      "[D loss: 0.840488] [G loss: 1.722230]\n",
      "[D loss: 0.705855] [G loss: 1.900225]\n",
      "[D loss: 0.919443] [G loss: 1.500947]\n",
      "[D loss: 0.705807] [G loss: 1.312387]\n",
      "[D loss: 0.743812] [G loss: 1.322077]\n",
      "[D loss: 0.717235] [G loss: 1.414263]\n",
      "[D loss: 0.776324] [G loss: 1.365547]\n",
      "[D loss: 0.919966] [G loss: 1.526325]\n",
      "[D loss: 0.723553] [G loss: 1.377573]\n",
      "[D loss: 0.751467] [G loss: 1.561011]\n",
      "[D loss: 0.710616] [G loss: 1.351956]\n",
      "[D loss: 0.730553] [G loss: 1.528610]\n",
      "[D loss: 0.831936] [G loss: 1.580887]\n",
      "[D loss: 0.889613] [G loss: 1.465767]\n",
      "[D loss: 0.875134] [G loss: 1.484130]\n",
      "[D loss: 0.732765] [G loss: 1.546671]\n",
      "[D loss: 0.825622] [G loss: 1.462203]\n",
      "[D loss: 0.872697] [G loss: 1.506012]\n",
      "[D loss: 0.851250] [G loss: 1.579082]\n",
      "[D loss: 0.669340] [G loss: 1.651846]\n",
      "[D loss: 0.885475] [G loss: 1.375975]\n",
      "[D loss: 0.897408] [G loss: 1.316757]\n",
      "[D loss: 0.678126] [G loss: 1.441843]\n",
      "[D loss: 0.781722] [G loss: 1.538925]\n",
      "[D loss: 1.235465] [G loss: 1.505966]\n",
      "[D loss: 0.746879] [G loss: 1.342518]\n",
      "[D loss: 0.687447] [G loss: 1.658243]\n",
      "[D loss: 0.904132] [G loss: 1.678597]\n",
      "[D loss: 0.766621] [G loss: 1.293416]\n",
      "[D loss: 0.939666] [G loss: 1.451661]\n",
      "[D loss: 0.907883] [G loss: 1.519188]\n",
      "[D loss: 0.763193] [G loss: 1.323245]\n",
      "[D loss: 0.957003] [G loss: 1.364591]\n",
      "[D loss: 1.127996] [G loss: 1.343416]\n",
      "[D loss: 0.883051] [G loss: 1.355122]\n",
      "[D loss: 0.769899] [G loss: 1.513451]\n",
      "[D loss: 0.654517] [G loss: 1.696621]\n",
      "[D loss: 0.804280] [G loss: 1.518463]\n",
      "[D loss: 0.755091] [G loss: 1.315466]\n",
      "[D loss: 0.867858] [G loss: 1.281841]\n",
      "[D loss: 0.902822] [G loss: 1.390301]\n",
      "[D loss: 0.844784] [G loss: 1.533925]\n",
      "[D loss: 0.450177] [G loss: 1.644495]\n",
      "[D loss: 0.918588] [G loss: 1.358866]\n",
      "[D loss: 0.706376] [G loss: 1.665937]\n",
      "[D loss: 0.766186] [G loss: 1.444597]\n",
      "[D loss: 0.790842] [G loss: 1.356612]\n",
      "[D loss: 0.760104] [G loss: 1.294755]\n",
      "[D loss: 0.905697] [G loss: 1.295119]\n",
      "[D loss: 0.752599] [G loss: 1.341620]\n",
      "[D loss: 0.905477] [G loss: 1.330618]\n",
      "[D loss: 0.782448] [G loss: 1.394194]\n",
      "[D loss: 1.044176] [G loss: 1.423568]\n",
      "[D loss: 0.795469] [G loss: 1.476239]\n",
      "[D loss: 0.717689] [G loss: 1.400712]\n",
      "[D loss: 0.839824] [G loss: 1.486482]\n",
      "[D loss: 0.842193] [G loss: 1.404814]\n",
      "[D loss: 0.748854] [G loss: 1.398310]\n",
      "[D loss: 0.761969] [G loss: 1.453036]\n",
      "[D loss: 0.775754] [G loss: 1.709695]\n",
      "[D loss: 1.020276] [G loss: 1.589731]\n",
      "[D loss: 0.811800] [G loss: 1.489705]\n",
      "[D loss: 0.961947] [G loss: 1.566948]\n",
      "[D loss: 0.726512] [G loss: 1.432891]\n",
      "[D loss: 0.891145] [G loss: 1.387879]\n",
      "[D loss: 0.723240] [G loss: 1.404471]\n",
      "[D loss: 0.759558] [G loss: 1.518328]\n",
      "[D loss: 0.749275] [G loss: 1.374533]\n",
      "[D loss: 0.742753] [G loss: 1.427107]\n",
      "[D loss: 0.967076] [G loss: 1.414908]\n",
      "[D loss: 0.981792] [G loss: 1.396762]\n",
      "[D loss: 0.734790] [G loss: 1.369837]\n",
      "[D loss: 0.809458] [G loss: 1.488930]\n",
      "[D loss: 0.717095] [G loss: 1.499768]\n",
      "[D loss: 0.752764] [G loss: 1.340014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.786988] [G loss: 1.520880]\n",
      "[D loss: 0.766897] [G loss: 1.397874]\n",
      "[D loss: 1.106402] [G loss: 1.446547]\n",
      "[D loss: 0.536632] [G loss: 1.583067]\n",
      "[D loss: 0.878204] [G loss: 1.509737]\n",
      "[D loss: 0.763367] [G loss: 1.481978]\n",
      "[D loss: 0.947264] [G loss: 1.662262]\n",
      "[D loss: 0.570376] [G loss: 1.531589]\n",
      "[D loss: 0.749581] [G loss: 1.550128]\n",
      "[D loss: 0.816436] [G loss: 1.638278]\n",
      "[D loss: 0.838871] [G loss: 1.755373]\n",
      "[D loss: 0.994571] [G loss: 1.358028]\n",
      "[D loss: 0.869525] [G loss: 1.356635]\n",
      "[D loss: 0.738497] [G loss: 1.630893]\n",
      "[D loss: 0.733842] [G loss: 1.603737]\n",
      "[D loss: 0.733801] [G loss: 1.381888]\n",
      "[D loss: 0.913050] [G loss: 1.481978]\n",
      "[D loss: 0.700952] [G loss: 1.569435]\n",
      "[D loss: 0.961935] [G loss: 1.643661]\n",
      "[D loss: 0.768420] [G loss: 1.575812]\n",
      "[D loss: 1.059269] [G loss: 1.551029]\n",
      "[D loss: 0.719771] [G loss: 1.355486]\n",
      "[D loss: 0.883623] [G loss: 1.250645]\n",
      "[D loss: 0.695268] [G loss: 1.364547]\n",
      "[D loss: 0.941332] [G loss: 1.560553]\n",
      "[D loss: 0.912434] [G loss: 1.518047]\n",
      "[D loss: 0.726451] [G loss: 1.699749]\n",
      "[D loss: 0.771421] [G loss: 1.555107]\n",
      "[D loss: 0.888952] [G loss: 1.342098]\n",
      "[D loss: 1.012781] [G loss: 1.283806]\n",
      "[D loss: 0.756267] [G loss: 1.406529]\n",
      "[D loss: 0.798774] [G loss: 1.593966]\n",
      "[D loss: 1.049273] [G loss: 1.254378]\n",
      "[D loss: 0.847385] [G loss: 1.511291]\n",
      "[D loss: 0.741623] [G loss: 1.540351]\n",
      "[D loss: 0.576178] [G loss: 1.484910]\n",
      "[D loss: 0.757904] [G loss: 1.548211]\n",
      "[D loss: 1.000131] [G loss: 1.500374]\n",
      "[D loss: 0.844546] [G loss: 1.383530]\n",
      "[D loss: 0.798495] [G loss: 1.490904]\n",
      "[D loss: 0.839030] [G loss: 1.499819]\n",
      "[D loss: 0.920405] [G loss: 1.497628]\n",
      "[D loss: 0.660201] [G loss: 1.470819]\n",
      "[D loss: 0.905655] [G loss: 1.432999]\n",
      "[D loss: 0.814643] [G loss: 1.178398]\n",
      "[D loss: 0.817462] [G loss: 1.770524]\n",
      "[D loss: 0.791646] [G loss: 1.704598]\n",
      "[D loss: 0.783801] [G loss: 1.364125]\n",
      "[D loss: 0.923569] [G loss: 1.552241]\n",
      "[D loss: 0.725602] [G loss: 1.413681]\n",
      "[D loss: 1.029557] [G loss: 1.316462]\n",
      "[D loss: 0.882811] [G loss: 1.387384]\n",
      "[D loss: 0.770650] [G loss: 1.686965]\n",
      "[D loss: 0.703789] [G loss: 1.370002]\n",
      "[D loss: 0.692086] [G loss: 1.332694]\n",
      "[D loss: 0.932623] [G loss: 1.691460]\n",
      "[D loss: 0.795176] [G loss: 1.473680]\n",
      "[D loss: 1.168621] [G loss: 1.590177]\n",
      "[D loss: 0.735963] [G loss: 1.495750]\n",
      "[D loss: 0.652396] [G loss: 1.523676]\n",
      "[D loss: 0.778393] [G loss: 1.585338]\n",
      "[D loss: 0.694390] [G loss: 1.618013]\n",
      "[D loss: 0.857481] [G loss: 1.475592]\n",
      "[D loss: 0.738990] [G loss: 1.507733]\n",
      "[D loss: 0.542089] [G loss: 1.501438]\n",
      "[D loss: 0.660069] [G loss: 1.574020]\n",
      "[D loss: 0.619776] [G loss: 1.549566]\n",
      "[D loss: 0.680434] [G loss: 1.511152]\n",
      "[D loss: 0.819569] [G loss: 1.408746]\n",
      "[D loss: 0.642653] [G loss: 1.464373]\n",
      "[D loss: 0.804560] [G loss: 1.445315]\n",
      "[D loss: 0.825881] [G loss: 1.554817]\n",
      "[D loss: 0.837630] [G loss: 1.621731]\n",
      "[D loss: 0.805766] [G loss: 1.647255]\n",
      "[D loss: 0.791024] [G loss: 1.363677]\n",
      "[D loss: 0.805533] [G loss: 1.411844]\n",
      "[D loss: 0.671275] [G loss: 1.599501]\n",
      "[D loss: 1.003832] [G loss: 1.639107]\n",
      "[D loss: 0.864713] [G loss: 1.777992]\n",
      "[D loss: 0.878295] [G loss: 1.546671]\n",
      "[D loss: 0.779909] [G loss: 1.331668]\n",
      "[D loss: 0.870586] [G loss: 1.411080]\n",
      "[D loss: 0.952230] [G loss: 1.367038]\n",
      "[D loss: 0.854496] [G loss: 1.584180]\n",
      "[D loss: 0.877551] [G loss: 1.675575]\n",
      "[D loss: 0.952306] [G loss: 1.333592]\n",
      "[D loss: 1.178587] [G loss: 1.201468]\n",
      "[D loss: 0.945138] [G loss: 1.398012]\n",
      "[D loss: 0.916205] [G loss: 1.374558]\n",
      "[D loss: 0.692707] [G loss: 1.320358]\n",
      "[D loss: 0.910966] [G loss: 1.203509]\n",
      "[D loss: 0.812610] [G loss: 1.476810]\n",
      "[D loss: 0.720286] [G loss: 1.422150]\n",
      "[D loss: 0.838132] [G loss: 1.393674]\n",
      "[D loss: 0.771460] [G loss: 1.357783]\n",
      "[D loss: 0.859233] [G loss: 1.505342]\n",
      "[D loss: 0.679911] [G loss: 1.434136]\n",
      "[D loss: 0.621742] [G loss: 1.514410]\n",
      "[D loss: 0.779363] [G loss: 1.262656]\n",
      "[D loss: 0.939284] [G loss: 1.616312]\n",
      "[D loss: 0.898772] [G loss: 1.614229]\n",
      "[D loss: 0.875041] [G loss: 1.654788]\n",
      "[D loss: 0.587505] [G loss: 1.500566]\n",
      "[D loss: 0.981865] [G loss: 1.387093]\n",
      "[D loss: 0.899764] [G loss: 1.434825]\n",
      "[D loss: 1.086807] [G loss: 1.163889]\n",
      "[D loss: 0.739222] [G loss: 1.517737]\n",
      "[D loss: 0.622959] [G loss: 1.487303]\n",
      "[D loss: 1.210292] [G loss: 1.267651]\n",
      "[D loss: 0.857321] [G loss: 1.416675]\n",
      "[D loss: 0.851408] [G loss: 1.252851]\n",
      "[D loss: 0.855373] [G loss: 1.464010]\n",
      "[D loss: 0.718747] [G loss: 1.484854]\n",
      "[D loss: 0.539837] [G loss: 1.659269]\n",
      "[D loss: 0.893329] [G loss: 1.604938]\n",
      "[D loss: 0.744729] [G loss: 1.577923]\n",
      "[D loss: 1.176346] [G loss: 1.365900]\n",
      "[D loss: 0.986203] [G loss: 1.351659]\n",
      "[D loss: 0.804843] [G loss: 1.437370]\n",
      "[D loss: 1.128058] [G loss: 1.651847]\n",
      "[D loss: 0.687719] [G loss: 1.455743]\n",
      "[D loss: 1.156254] [G loss: 1.369972]\n",
      "[D loss: 0.624649] [G loss: 1.502388]\n",
      "[D loss: 0.874241] [G loss: 1.513899]\n",
      "[D loss: 0.835029] [G loss: 1.394796]\n",
      "[D loss: 0.888754] [G loss: 1.415810]\n",
      "[D loss: 0.708703] [G loss: 1.426310]\n",
      "[D loss: 0.638060] [G loss: 1.433337]\n",
      "[D loss: 0.722890] [G loss: 1.600652]\n",
      "[D loss: 0.882700] [G loss: 1.504016]\n",
      "[D loss: 0.922584] [G loss: 1.429401]\n",
      "[D loss: 0.599316] [G loss: 1.538793]\n",
      "[D loss: 0.804925] [G loss: 1.358494]\n",
      "[D loss: 0.640239] [G loss: 1.626282]\n",
      "[D loss: 0.762767] [G loss: 1.523074]\n",
      "[D loss: 0.787559] [G loss: 1.571211]\n",
      "[D loss: 0.963636] [G loss: 1.624193]\n",
      "[D loss: 0.716226] [G loss: 1.350968]\n",
      "[D loss: 0.807813] [G loss: 1.430236]\n",
      "[D loss: 0.872644] [G loss: 1.255825]\n",
      "[D loss: 0.824340] [G loss: 1.577856]\n",
      "[D loss: 0.938611] [G loss: 1.666534]\n",
      "[D loss: 0.686180] [G loss: 1.623659]\n",
      "[D loss: 0.768031] [G loss: 1.502951]\n",
      "[D loss: 0.840954] [G loss: 1.367876]\n",
      "[D loss: 0.694398] [G loss: 1.574739]\n",
      "[D loss: 0.909671] [G loss: 1.532651]\n",
      "[D loss: 0.709604] [G loss: 1.457928]\n",
      "[D loss: 0.886176] [G loss: 1.536790]\n",
      "[D loss: 0.738136] [G loss: 1.695771]\n",
      "[D loss: 0.819047] [G loss: 1.705937]\n",
      "[D loss: 0.840451] [G loss: 1.228530]\n",
      "[D loss: 0.813319] [G loss: 1.375821]\n",
      "[D loss: 0.714605] [G loss: 1.345654]\n",
      "[D loss: 0.802115] [G loss: 1.644923]\n",
      "[D loss: 0.823096] [G loss: 1.692497]\n",
      "[D loss: 0.695666] [G loss: 1.575342]\n",
      "[D loss: 0.728724] [G loss: 1.499447]\n",
      "[D loss: 0.730839] [G loss: 1.555583]\n",
      "[D loss: 0.569862] [G loss: 1.703032]\n",
      "[D loss: 0.828331] [G loss: 1.443824]\n",
      "[D loss: 0.896433] [G loss: 1.838314]\n",
      "[D loss: 0.810455] [G loss: 1.442780]\n",
      "[D loss: 0.549794] [G loss: 1.578731]\n",
      "[D loss: 0.623906] [G loss: 1.468124]\n",
      "[D loss: 0.892824] [G loss: 1.404052]\n",
      "[D loss: 0.608831] [G loss: 1.536268]\n",
      "[D loss: 1.036914] [G loss: 1.648224]\n",
      "[D loss: 0.830681] [G loss: 1.477773]\n",
      "[D loss: 0.729520] [G loss: 1.406756]\n",
      "[D loss: 0.881029] [G loss: 1.422008]\n",
      "[D loss: 1.034279] [G loss: 1.289086]\n",
      "[D loss: 0.913078] [G loss: 1.584887]\n",
      "[D loss: 0.692527] [G loss: 1.531598]\n",
      "[D loss: 0.920847] [G loss: 1.280169]\n",
      "[D loss: 0.662996] [G loss: 1.296106]\n",
      "[D loss: 0.736316] [G loss: 1.640580]\n",
      "[D loss: 0.807802] [G loss: 1.413002]\n",
      "[D loss: 0.694471] [G loss: 1.519575]\n",
      "[D loss: 0.834445] [G loss: 1.361510]\n",
      "[D loss: 0.883110] [G loss: 1.467642]\n",
      "[D loss: 0.620360] [G loss: 1.708614]\n",
      "[D loss: 0.720237] [G loss: 1.516203]\n",
      "[D loss: 0.908918] [G loss: 1.658710]\n",
      "[D loss: 0.705902] [G loss: 1.668368]\n",
      "[D loss: 0.954512] [G loss: 1.255968]\n",
      "[D loss: 0.915229] [G loss: 1.563647]\n",
      "[D loss: 0.869901] [G loss: 1.572592]\n",
      "[D loss: 0.880863] [G loss: 1.707061]\n",
      "[D loss: 0.757466] [G loss: 1.791663]\n",
      "[D loss: 0.970255] [G loss: 1.538206]\n",
      "[D loss: 0.653781] [G loss: 1.540542]\n",
      "[D loss: 0.938869] [G loss: 1.448822]\n",
      "[D loss: 0.594631] [G loss: 1.510730]\n",
      "[D loss: 0.698273] [G loss: 1.646336]\n",
      "[D loss: 0.885157] [G loss: 1.528009]\n",
      "[D loss: 0.675855] [G loss: 1.720892]\n",
      "[D loss: 0.820789] [G loss: 1.541883]\n",
      "[D loss: 0.784355] [G loss: 1.572879]\n",
      "[D loss: 0.973705] [G loss: 1.511224]\n",
      "[D loss: 0.826759] [G loss: 1.310205]\n",
      "[D loss: 0.743609] [G loss: 1.524248]\n",
      "[D loss: 0.767055] [G loss: 1.577338]\n",
      "[D loss: 0.783504] [G loss: 1.641195]\n",
      "[D loss: 0.787440] [G loss: 1.325136]\n",
      "[D loss: 0.712354] [G loss: 1.668630]\n",
      "[D loss: 0.770565] [G loss: 1.325026]\n",
      "[D loss: 1.145485] [G loss: 1.293667]\n",
      "[D loss: 0.685465] [G loss: 1.419292]\n",
      "[D loss: 0.691473] [G loss: 1.391440]\n",
      "[D loss: 0.595790] [G loss: 1.576642]\n",
      "[D loss: 0.901254] [G loss: 1.536648]\n",
      "[D loss: 0.806467] [G loss: 1.457508]\n",
      "[D loss: 0.932857] [G loss: 1.340799]\n",
      "[D loss: 1.134376] [G loss: 1.377497]\n",
      "[D loss: 0.947822] [G loss: 1.644228]\n",
      "[D loss: 1.105960] [G loss: 1.110343]\n",
      "[D loss: 0.728642] [G loss: 1.579139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.898966] [G loss: 1.741618]\n",
      "[D loss: 0.657829] [G loss: 1.647646]\n",
      "[D loss: 0.699473] [G loss: 1.521706]\n",
      "[D loss: 0.740229] [G loss: 1.551123]\n",
      "[D loss: 0.756422] [G loss: 1.393776]\n",
      "[D loss: 0.953335] [G loss: 1.656542]\n",
      "[D loss: 0.677408] [G loss: 1.599282]\n",
      "[D loss: 0.769823] [G loss: 1.239985]\n",
      "[D loss: 0.903274] [G loss: 1.542587]\n",
      "[D loss: 0.759410] [G loss: 1.683990]\n",
      "[D loss: 0.957700] [G loss: 1.351003]\n",
      "[D loss: 0.807329] [G loss: 1.490092]\n",
      "[D loss: 0.811075] [G loss: 1.500938]\n",
      "[D loss: 1.034575] [G loss: 1.484472]\n",
      "[D loss: 1.006683] [G loss: 1.372665]\n",
      "[D loss: 0.690130] [G loss: 1.357849]\n",
      "[D loss: 0.878679] [G loss: 1.402238]\n",
      "[D loss: 0.779086] [G loss: 1.458996]\n",
      "[D loss: 0.861160] [G loss: 1.552738]\n",
      "[D loss: 0.985234] [G loss: 1.758185]\n",
      "[D loss: 0.909372] [G loss: 1.579542]\n",
      "[D loss: 0.836690] [G loss: 1.538917]\n",
      "[D loss: 0.843588] [G loss: 1.313763]\n",
      "[D loss: 0.658787] [G loss: 1.419313]\n",
      "[D loss: 0.887851] [G loss: 1.388556]\n",
      "[D loss: 0.819201] [G loss: 1.374624]\n",
      "[D loss: 0.728441] [G loss: 1.536381]\n",
      "[D loss: 0.953872] [G loss: 1.367408]\n",
      "[D loss: 0.694563] [G loss: 1.388918]\n",
      "[D loss: 1.040644] [G loss: 1.224189]\n",
      "[D loss: 1.031473] [G loss: 1.406880]\n",
      "[D loss: 0.597804] [G loss: 1.655873]\n",
      "[D loss: 0.832728] [G loss: 1.477426]\n",
      "[D loss: 0.880684] [G loss: 1.559601]\n",
      "[D loss: 0.722304] [G loss: 1.571513]\n",
      "[D loss: 1.053768] [G loss: 1.330140]\n",
      "[D loss: 0.741455] [G loss: 1.683376]\n",
      "[D loss: 0.778517] [G loss: 1.509720]\n",
      "[D loss: 0.903724] [G loss: 1.511693]\n",
      "[D loss: 0.929510] [G loss: 1.436390]\n",
      "[D loss: 0.771415] [G loss: 1.383764]\n",
      "[D loss: 0.843018] [G loss: 1.327321]\n",
      "[D loss: 0.982544] [G loss: 1.572817]\n",
      "[D loss: 0.931994] [G loss: 1.653991]\n",
      "[D loss: 0.772999] [G loss: 1.560710]\n",
      "[D loss: 0.999919] [G loss: 1.631433]\n",
      "[D loss: 0.934102] [G loss: 1.424891]\n",
      "[D loss: 0.996183] [G loss: 1.245693]\n",
      "[D loss: 0.777713] [G loss: 1.858050]\n",
      "[D loss: 0.819952] [G loss: 1.459705]\n",
      "[D loss: 0.768425] [G loss: 1.337789]\n",
      "[D loss: 0.925237] [G loss: 1.549143]\n",
      "[D loss: 0.793920] [G loss: 1.493182]\n",
      "[D loss: 0.781655] [G loss: 1.376811]\n",
      "[D loss: 0.806549] [G loss: 1.603415]\n",
      "[D loss: 0.908376] [G loss: 1.441188]\n",
      "[D loss: 0.815408] [G loss: 1.197939]\n",
      "[D loss: 0.796561] [G loss: 1.259111]\n",
      "[D loss: 0.731739] [G loss: 1.378493]\n",
      "[D loss: 0.742072] [G loss: 1.742679]\n",
      "[D loss: 0.759515] [G loss: 1.658644]\n",
      "[D loss: 0.892546] [G loss: 1.668880]\n",
      "[D loss: 0.692570] [G loss: 1.455224]\n",
      "[D loss: 0.803095] [G loss: 1.502676]\n",
      "[D loss: 1.001329] [G loss: 1.128425]\n",
      "[D loss: 0.849393] [G loss: 1.359331]\n",
      "[D loss: 0.928402] [G loss: 1.724449]\n",
      "[D loss: 0.829523] [G loss: 1.483713]\n",
      "[D loss: 0.777676] [G loss: 1.513115]\n",
      "[D loss: 0.697455] [G loss: 1.358817]\n",
      "[D loss: 0.796874] [G loss: 1.401055]\n",
      "[D loss: 0.975192] [G loss: 1.337242]\n",
      "[D loss: 0.882452] [G loss: 1.391274]\n",
      "[D loss: 0.895899] [G loss: 1.401806]\n",
      "[D loss: 0.870818] [G loss: 1.308325]\n",
      "[D loss: 0.887339] [G loss: 1.447984]\n",
      "[D loss: 0.714257] [G loss: 1.532618]\n",
      "[D loss: 0.651216] [G loss: 1.568687]\n",
      "[D loss: 0.686044] [G loss: 1.580446]\n",
      "[D loss: 0.755533] [G loss: 1.717343]\n",
      "[D loss: 0.809886] [G loss: 1.725798]\n",
      "[D loss: 0.846168] [G loss: 1.352815]\n",
      "[D loss: 0.724722] [G loss: 1.413286]\n",
      "[D loss: 0.791734] [G loss: 1.594953]\n",
      "[D loss: 0.842036] [G loss: 1.363760]\n",
      "[D loss: 0.862838] [G loss: 1.488883]\n",
      "[D loss: 0.601297] [G loss: 1.482904]\n",
      "[D loss: 0.660598] [G loss: 1.528484]\n",
      "[D loss: 0.718287] [G loss: 1.660970]\n",
      "[D loss: 0.895401] [G loss: 1.499896]\n",
      "[D loss: 0.976032] [G loss: 1.356191]\n",
      "[D loss: 0.740955] [G loss: 1.656091]\n",
      "[D loss: 0.828674] [G loss: 1.381956]\n",
      "[D loss: 0.810066] [G loss: 1.509466]\n",
      "[D loss: 0.816586] [G loss: 1.231073]\n",
      "[D loss: 1.037153] [G loss: 1.348573]\n",
      "[D loss: 0.841075] [G loss: 1.443708]\n",
      "[D loss: 0.770502] [G loss: 1.460986]\n",
      "[D loss: 0.976032] [G loss: 1.561496]\n",
      "[D loss: 0.737497] [G loss: 1.385255]\n",
      "[D loss: 0.709929] [G loss: 1.633959]\n",
      "[D loss: 0.824739] [G loss: 1.311742]\n",
      "[D loss: 0.753168] [G loss: 1.350329]\n",
      "[D loss: 0.733211] [G loss: 1.487232]\n",
      "[D loss: 0.928316] [G loss: 1.448603]\n",
      "[D loss: 0.941653] [G loss: 1.289304]\n",
      "[D loss: 0.821155] [G loss: 1.601611]\n",
      "[D loss: 0.714895] [G loss: 1.664699]\n",
      "[D loss: 0.803366] [G loss: 1.441315]\n",
      "[D loss: 0.798542] [G loss: 1.210095]\n",
      "[D loss: 0.932404] [G loss: 1.382603]\n",
      "[D loss: 0.860745] [G loss: 1.610782]\n",
      "[D loss: 0.733494] [G loss: 1.419990]\n",
      "[D loss: 0.855564] [G loss: 1.232870]\n",
      "[D loss: 0.915624] [G loss: 1.543356]\n",
      "[D loss: 1.033890] [G loss: 1.221632]\n",
      "[D loss: 0.789912] [G loss: 1.408950]\n",
      "[D loss: 0.744956] [G loss: 1.325125]\n",
      "[D loss: 1.049504] [G loss: 1.456469]\n",
      "[D loss: 0.799241] [G loss: 1.283985]\n",
      "[D loss: 0.642302] [G loss: 1.393843]\n",
      "[D loss: 0.643225] [G loss: 1.525923]\n",
      "[D loss: 0.728784] [G loss: 1.527602]\n",
      "[D loss: 0.860215] [G loss: 1.552375]\n",
      "[D loss: 0.939186] [G loss: 1.371127]\n",
      "[D loss: 0.925157] [G loss: 1.529722]\n",
      "[D loss: 0.830103] [G loss: 1.218234]\n",
      "[D loss: 0.994852] [G loss: 1.557813]\n",
      "[D loss: 0.923400] [G loss: 1.393025]\n",
      "[D loss: 0.943092] [G loss: 1.584006]\n",
      "[D loss: 0.728978] [G loss: 1.406208]\n",
      "[D loss: 0.521804] [G loss: 1.617958]\n",
      "[D loss: 0.740112] [G loss: 1.547488]\n",
      "[D loss: 0.680856] [G loss: 1.485228]\n",
      "[D loss: 0.857895] [G loss: 1.714649]\n",
      "[D loss: 0.763077] [G loss: 1.315747]\n",
      "[D loss: 0.646996] [G loss: 1.377630]\n",
      "[D loss: 0.882548] [G loss: 1.540731]\n",
      "[D loss: 0.676033] [G loss: 1.439614]\n",
      "[D loss: 1.105603] [G loss: 1.688443]\n",
      "[D loss: 0.696113] [G loss: 1.714849]\n",
      "[D loss: 0.779335] [G loss: 1.490317]\n",
      "[D loss: 0.849553] [G loss: 1.463252]\n",
      "[D loss: 0.954293] [G loss: 1.431059]\n",
      "[D loss: 0.788280] [G loss: 1.389511]\n",
      "[D loss: 0.835716] [G loss: 1.379610]\n",
      "[D loss: 0.754543] [G loss: 1.321877]\n",
      "[D loss: 0.937822] [G loss: 1.632024]\n",
      "[D loss: 0.906178] [G loss: 1.258637]\n",
      "[D loss: 0.688640] [G loss: 1.472269]\n",
      "[D loss: 0.868004] [G loss: 1.477831]\n",
      "[D loss: 0.868139] [G loss: 1.477504]\n",
      "[D loss: 0.892059] [G loss: 1.527182]\n",
      "[D loss: 0.897928] [G loss: 1.381418]\n",
      "[D loss: 0.736290] [G loss: 1.465169]\n",
      "[D loss: 0.837769] [G loss: 1.446123]\n",
      "[D loss: 0.583992] [G loss: 1.438274]\n",
      "[D loss: 0.910053] [G loss: 1.705435]\n",
      "[D loss: 1.010344] [G loss: 1.514706]\n",
      "[D loss: 0.879970] [G loss: 1.714284]\n",
      "[D loss: 0.776328] [G loss: 1.499814]\n",
      "[D loss: 0.912373] [G loss: 1.399081]\n",
      "[D loss: 0.934796] [G loss: 1.276578]\n",
      "[D loss: 0.893924] [G loss: 1.388497]\n",
      "[D loss: 0.788238] [G loss: 1.289089]\n",
      "[D loss: 0.837464] [G loss: 1.271313]\n",
      "[D loss: 1.020573] [G loss: 1.090662]\n",
      "[D loss: 0.726389] [G loss: 1.567098]\n",
      "[D loss: 0.727646] [G loss: 1.586368]\n",
      "[D loss: 0.727432] [G loss: 1.532470]\n",
      "[D loss: 0.726080] [G loss: 1.442544]\n",
      "[D loss: 0.898199] [G loss: 1.299766]\n",
      "[D loss: 0.832368] [G loss: 1.676908]\n",
      "[D loss: 0.777811] [G loss: 1.362873]\n",
      "[D loss: 0.761244] [G loss: 1.360787]\n",
      "[D loss: 0.877156] [G loss: 1.392203]\n",
      "[D loss: 0.778932] [G loss: 1.697883]\n",
      "[D loss: 0.659319] [G loss: 1.636379]\n",
      "[D loss: 0.693055] [G loss: 1.639178]\n",
      "[D loss: 0.809384] [G loss: 1.522115]\n",
      "[D loss: 0.676149] [G loss: 1.441235]\n",
      "[D loss: 1.000532] [G loss: 1.289081]\n",
      "[D loss: 1.085551] [G loss: 1.410382]\n",
      "[D loss: 0.653466] [G loss: 1.715825]\n",
      "[D loss: 0.829329] [G loss: 1.439578]\n",
      "[D loss: 0.964789] [G loss: 1.438979]\n",
      "[D loss: 0.917979] [G loss: 1.328769]\n",
      "[D loss: 0.725784] [G loss: 1.336337]\n",
      "[D loss: 0.772776] [G loss: 1.468533]\n",
      "[D loss: 0.897122] [G loss: 1.595614]\n",
      "[D loss: 0.605439] [G loss: 1.495091]\n",
      "[D loss: 1.089149] [G loss: 1.429158]\n",
      "[D loss: 0.940628] [G loss: 1.480212]\n",
      "[D loss: 1.027142] [G loss: 1.213140]\n",
      "[D loss: 0.888415] [G loss: 1.544927]\n",
      "[D loss: 0.515389] [G loss: 1.584558]\n",
      "[D loss: 0.970737] [G loss: 1.697227]\n",
      "[D loss: 0.881288] [G loss: 1.310045]\n",
      "[D loss: 0.976890] [G loss: 1.427386]\n",
      "[D loss: 0.804494] [G loss: 1.248879]\n",
      "[D loss: 0.866334] [G loss: 1.416654]\n",
      "[D loss: 0.816464] [G loss: 1.514690]\n",
      "[D loss: 0.888487] [G loss: 1.340882]\n",
      "[D loss: 1.064581] [G loss: 1.463033]\n",
      "[D loss: 0.743420] [G loss: 1.452501]\n",
      "[D loss: 0.825488] [G loss: 1.514104]\n",
      "[D loss: 0.931273] [G loss: 1.695190]\n",
      "[D loss: 0.686584] [G loss: 1.335745]\n",
      "[D loss: 0.896996] [G loss: 1.547375]\n",
      "[D loss: 0.795105] [G loss: 1.650825]\n",
      "[D loss: 0.875240] [G loss: 1.320817]\n",
      "[D loss: 0.959299] [G loss: 1.351763]\n",
      "[D loss: 0.845395] [G loss: 1.489830]\n",
      "[D loss: 0.552087] [G loss: 1.570896]\n",
      "[D loss: 0.975164] [G loss: 1.347361]\n",
      "[D loss: 0.859173] [G loss: 1.387621]\n",
      "[D loss: 0.834652] [G loss: 1.448622]\n",
      "[D loss: 0.776583] [G loss: 1.542218]\n",
      "[D loss: 0.837273] [G loss: 1.512478]\n",
      "[D loss: 0.665087] [G loss: 1.553713]\n",
      "[D loss: 0.760915] [G loss: 1.465367]\n",
      "[D loss: 0.923885] [G loss: 1.324773]\n",
      "[D loss: 0.941273] [G loss: 1.284943]\n",
      "[D loss: 0.790684] [G loss: 1.773383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.708910] [G loss: 1.237728]\n",
      "[D loss: 0.677894] [G loss: 1.461748]\n",
      "[D loss: 0.905620] [G loss: 1.171282]\n",
      "[D loss: 1.050402] [G loss: 1.376508]\n",
      "[D loss: 0.838718] [G loss: 1.634121]\n",
      "[D loss: 0.873087] [G loss: 1.427270]\n",
      "[D loss: 0.846491] [G loss: 1.385169]\n",
      "[D loss: 1.104718] [G loss: 1.409570]\n",
      "[D loss: 0.854114] [G loss: 1.459660]\n",
      "[D loss: 0.660599] [G loss: 1.450851]\n",
      "[D loss: 0.856850] [G loss: 1.425750]\n",
      "[D loss: 1.005489] [G loss: 1.302678]\n",
      "[D loss: 0.890538] [G loss: 1.273242]\n",
      "[D loss: 0.830536] [G loss: 1.340326]\n",
      "[D loss: 0.704361] [G loss: 1.707777]\n",
      "[D loss: 1.009523] [G loss: 1.478122]\n",
      "[D loss: 0.721811] [G loss: 1.730449]\n",
      "[D loss: 0.796136] [G loss: 1.510271]\n",
      "[D loss: 1.042934] [G loss: 1.330994]\n",
      "[D loss: 0.730865] [G loss: 1.496964]\n",
      "[D loss: 0.833409] [G loss: 1.283195]\n",
      "[D loss: 0.667509] [G loss: 1.389027]\n",
      "[D loss: 0.791647] [G loss: 1.470319]\n",
      "[D loss: 0.693006] [G loss: 1.678600]\n",
      "[D loss: 0.769352] [G loss: 1.408829]\n",
      "[D loss: 0.697236] [G loss: 1.411988]\n",
      "[D loss: 0.907763] [G loss: 1.420658]\n",
      "[D loss: 0.909347] [G loss: 1.256571]\n",
      "[D loss: 0.872150] [G loss: 1.292741]\n",
      "[D loss: 0.764745] [G loss: 1.427075]\n",
      "[D loss: 0.688312] [G loss: 1.622460]\n",
      "[D loss: 0.710492] [G loss: 1.498852]\n",
      "[D loss: 0.695067] [G loss: 1.551261]\n",
      "[D loss: 0.572754] [G loss: 1.828338]\n",
      "[D loss: 0.908209] [G loss: 1.344329]\n",
      "[D loss: 0.708737] [G loss: 1.620225]\n",
      "[D loss: 0.695228] [G loss: 1.571026]\n",
      "[D loss: 0.795245] [G loss: 1.624962]\n",
      "[D loss: 0.734761] [G loss: 1.549610]\n",
      "[D loss: 0.906660] [G loss: 1.645289]\n",
      "[D loss: 0.613061] [G loss: 1.617526]\n",
      "[D loss: 0.964873] [G loss: 1.465220]\n",
      "[D loss: 0.730926] [G loss: 1.463456]\n",
      "[D loss: 0.938498] [G loss: 1.195073]\n",
      "[D loss: 0.660623] [G loss: 1.545285]\n",
      "[D loss: 0.644024] [G loss: 1.489468]\n",
      "[D loss: 0.723564] [G loss: 1.382672]\n",
      "[D loss: 0.566991] [G loss: 1.761032]\n",
      "[D loss: 0.964857] [G loss: 1.686425]\n",
      "[D loss: 0.844983] [G loss: 1.578062]\n",
      "[D loss: 1.123920] [G loss: 1.428619]\n",
      "[D loss: 0.605209] [G loss: 1.640713]\n",
      "[D loss: 0.817535] [G loss: 1.644973]\n",
      "[D loss: 0.673574] [G loss: 1.530253]\n",
      "[D loss: 0.523465] [G loss: 1.678968]\n",
      "[D loss: 0.877995] [G loss: 1.470587]\n",
      "[D loss: 0.927796] [G loss: 1.463676]\n",
      "[D loss: 0.773478] [G loss: 1.549858]\n",
      "[D loss: 0.982495] [G loss: 1.433620]\n",
      "[D loss: 0.841449] [G loss: 1.570836]\n",
      "[D loss: 0.757759] [G loss: 1.394155]\n",
      "[D loss: 0.650958] [G loss: 1.643592]\n",
      "[D loss: 1.001533] [G loss: 1.546778]\n",
      "[D loss: 1.034738] [G loss: 1.436815]\n",
      "[D loss: 0.779371] [G loss: 1.543402]\n",
      "[D loss: 0.939590] [G loss: 1.447675]\n",
      "[D loss: 1.175107] [G loss: 1.358061]\n",
      "[D loss: 1.030713] [G loss: 1.452144]\n",
      "[D loss: 0.972079] [G loss: 1.460449]\n",
      "[D loss: 0.974633] [G loss: 1.350126]\n",
      "[D loss: 0.677856] [G loss: 1.367729]\n",
      "[D loss: 0.680540] [G loss: 1.247322]\n",
      "[D loss: 0.810209] [G loss: 1.406802]\n",
      "[D loss: 0.669523] [G loss: 1.691484]\n",
      "[D loss: 0.743909] [G loss: 1.530117]\n",
      "[D loss: 0.853789] [G loss: 1.647858]\n",
      "[D loss: 0.705607] [G loss: 1.581791]\n",
      "[D loss: 0.678679] [G loss: 1.612539]\n",
      "[D loss: 0.801966] [G loss: 1.351437]\n",
      "[D loss: 0.964531] [G loss: 1.429262]\n",
      "[D loss: 0.895925] [G loss: 1.497572]\n",
      "[D loss: 0.666215] [G loss: 1.710485]\n",
      "[D loss: 0.927136] [G loss: 1.630908]\n",
      "[D loss: 1.184513] [G loss: 1.595811]\n",
      "[D loss: 0.827785] [G loss: 1.352851]\n",
      "[D loss: 1.037769] [G loss: 1.368587]\n",
      "[D loss: 0.958110] [G loss: 1.467562]\n",
      "[D loss: 0.842520] [G loss: 1.362436]\n",
      "[D loss: 0.870407] [G loss: 1.496712]\n",
      "[D loss: 0.820038] [G loss: 1.416608]\n",
      "[D loss: 0.990324] [G loss: 1.537714]\n",
      "[D loss: 1.065315] [G loss: 1.482604]\n",
      "[D loss: 0.764097] [G loss: 1.411671]\n",
      "[D loss: 0.970551] [G loss: 1.472367]\n",
      "[D loss: 0.922795] [G loss: 1.414017]\n",
      "[D loss: 0.818491] [G loss: 1.410716]\n",
      "[D loss: 1.084661] [G loss: 1.346007]\n",
      "[D loss: 0.656055] [G loss: 1.736755]\n",
      "[D loss: 0.528555] [G loss: 1.615040]\n",
      "[D loss: 0.914564] [G loss: 1.362072]\n",
      "[D loss: 0.921756] [G loss: 1.500449]\n",
      "[D loss: 0.959192] [G loss: 1.510208]\n",
      "[D loss: 0.762193] [G loss: 1.416823]\n",
      "[D loss: 0.762739] [G loss: 1.216383]\n",
      "[D loss: 0.829998] [G loss: 1.491163]\n",
      "[D loss: 0.783359] [G loss: 1.447273]\n",
      "[D loss: 0.892664] [G loss: 1.510799]\n",
      "[D loss: 0.724696] [G loss: 1.587915]\n",
      "[D loss: 0.772587] [G loss: 1.467789]\n",
      "[D loss: 0.739140] [G loss: 1.425104]\n",
      "[D loss: 0.866000] [G loss: 1.706613]\n",
      "[D loss: 0.687214] [G loss: 1.400220]\n",
      "[D loss: 0.901742] [G loss: 1.374724]\n",
      "[D loss: 0.665861] [G loss: 1.334179]\n",
      "[D loss: 0.698053] [G loss: 1.480632]\n",
      "[D loss: 0.845857] [G loss: 1.551676]\n",
      "[D loss: 1.124398] [G loss: 1.553860]\n",
      "[D loss: 0.766290] [G loss: 1.371269]\n",
      "[D loss: 0.919453] [G loss: 1.446585]\n",
      "[D loss: 0.735465] [G loss: 1.449063]\n",
      "[D loss: 0.576710] [G loss: 1.611010]\n",
      "[D loss: 0.810456] [G loss: 1.475895]\n",
      "[D loss: 0.998465] [G loss: 1.330104]\n",
      "[D loss: 0.901454] [G loss: 1.530818]\n",
      "[D loss: 0.666624] [G loss: 1.882871]\n",
      "[D loss: 0.693331] [G loss: 1.674489]\n",
      "[D loss: 1.003790] [G loss: 1.854322]\n",
      "[D loss: 1.090044] [G loss: 1.345318]\n",
      "[D loss: 0.752665] [G loss: 1.386422]\n",
      "[D loss: 0.971828] [G loss: 1.452552]\n",
      "[D loss: 0.698559] [G loss: 1.556628]\n",
      "[D loss: 0.919760] [G loss: 1.459487]\n",
      "[D loss: 1.006779] [G loss: 1.364043]\n",
      "[D loss: 0.705840] [G loss: 1.504573]\n",
      "[D loss: 0.908397] [G loss: 1.658244]\n",
      "[D loss: 0.797873] [G loss: 1.432639]\n",
      "[D loss: 0.636530] [G loss: 1.443529]\n",
      "[D loss: 0.819460] [G loss: 1.460244]\n",
      "[D loss: 0.847151] [G loss: 1.355124]\n",
      "[D loss: 0.769592] [G loss: 1.349530]\n",
      "[D loss: 0.946505] [G loss: 1.549942]\n",
      "[D loss: 0.776795] [G loss: 1.588871]\n",
      "[D loss: 0.892288] [G loss: 1.422332]\n",
      "[D loss: 0.922665] [G loss: 1.310206]\n",
      "[D loss: 0.997732] [G loss: 1.398494]\n",
      "[D loss: 0.805497] [G loss: 1.579665]\n",
      "[D loss: 0.751733] [G loss: 1.343576]\n",
      "[D loss: 1.008651] [G loss: 1.199179]\n",
      "[D loss: 0.750344] [G loss: 1.508049]\n",
      "[D loss: 0.619234] [G loss: 1.454783]\n",
      "[D loss: 1.007289] [G loss: 1.588780]\n",
      "[D loss: 0.750308] [G loss: 1.412014]\n",
      "[D loss: 0.987971] [G loss: 1.447241]\n",
      "[D loss: 0.769577] [G loss: 1.397207]\n",
      "[D loss: 0.703742] [G loss: 1.566729]\n",
      "[D loss: 0.783079] [G loss: 1.495162]\n",
      "[D loss: 0.832266] [G loss: 1.354006]\n",
      "[D loss: 0.660233] [G loss: 1.752390]\n",
      "[D loss: 0.625957] [G loss: 1.656469]\n",
      "[D loss: 0.695714] [G loss: 1.510307]\n",
      "[D loss: 0.766739] [G loss: 1.425409]\n",
      "[D loss: 0.736036] [G loss: 1.649854]\n",
      "[D loss: 0.809901] [G loss: 1.353812]\n",
      "[D loss: 0.823717] [G loss: 1.596870]\n",
      "[D loss: 0.633873] [G loss: 1.585570]\n",
      "[D loss: 0.764218] [G loss: 1.504565]\n",
      "[D loss: 0.695942] [G loss: 1.767381]\n",
      "[D loss: 0.885094] [G loss: 1.480212]\n",
      "[D loss: 1.005936] [G loss: 1.457116]\n",
      "[D loss: 0.742423] [G loss: 1.477135]\n",
      "[D loss: 0.855957] [G loss: 1.663126]\n",
      "[D loss: 0.899197] [G loss: 1.531342]\n",
      "[D loss: 0.913241] [G loss: 1.549492]\n",
      "[D loss: 0.596764] [G loss: 1.447584]\n",
      "[D loss: 0.882424] [G loss: 1.442205]\n",
      "[D loss: 0.979810] [G loss: 1.394087]\n",
      "[D loss: 0.936871] [G loss: 1.334287]\n",
      "[D loss: 0.727123] [G loss: 1.490920]\n",
      "[D loss: 0.780762] [G loss: 1.648628]\n",
      "[D loss: 0.748894] [G loss: 1.779643]\n",
      "[D loss: 0.675903] [G loss: 1.681328]\n",
      "[D loss: 0.728682] [G loss: 1.371618]\n",
      "[D loss: 0.716233] [G loss: 1.570607]\n",
      "[D loss: 0.930436] [G loss: 1.334849]\n",
      "[D loss: 0.913259] [G loss: 1.393001]\n",
      "[D loss: 0.672117] [G loss: 1.509718]\n",
      "[D loss: 0.856107] [G loss: 1.349876]\n",
      "[D loss: 0.815358] [G loss: 1.282013]\n",
      "[D loss: 1.137573] [G loss: 1.238401]\n",
      "[D loss: 0.584712] [G loss: 1.354259]\n",
      "[D loss: 0.833700] [G loss: 1.388467]\n",
      "[D loss: 0.846985] [G loss: 1.495192]\n",
      "[D loss: 0.739914] [G loss: 1.790050]\n",
      "[D loss: 0.974367] [G loss: 1.486050]\n",
      "[D loss: 0.727522] [G loss: 1.488118]\n",
      "[D loss: 0.767029] [G loss: 1.376084]\n",
      "[D loss: 0.855907] [G loss: 1.355785]\n",
      "[D loss: 0.781730] [G loss: 1.255204]\n",
      "[D loss: 0.868629] [G loss: 1.624425]\n",
      "[D loss: 0.851079] [G loss: 1.328200]\n",
      "[D loss: 0.974872] [G loss: 1.450109]\n",
      "[D loss: 0.946882] [G loss: 1.392374]\n",
      "[D loss: 0.939564] [G loss: 1.160963]\n",
      "[D loss: 0.947722] [G loss: 1.477940]\n",
      "[D loss: 0.909527] [G loss: 1.517404]\n",
      "[D loss: 1.187998] [G loss: 1.337353]\n",
      "[D loss: 0.892456] [G loss: 1.669112]\n",
      "[D loss: 0.875397] [G loss: 1.582286]\n",
      "[D loss: 0.739425] [G loss: 1.431288]\n",
      "[D loss: 0.926331] [G loss: 1.393488]\n",
      "[D loss: 1.030590] [G loss: 1.090045]\n",
      "[D loss: 0.781730] [G loss: 1.774161]\n",
      "[D loss: 0.787240] [G loss: 1.390082]\n",
      "[D loss: 0.623009] [G loss: 1.580174]\n",
      "[D loss: 0.830992] [G loss: 1.481243]\n",
      "[D loss: 0.986602] [G loss: 1.405547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.643350] [G loss: 1.669079]\n",
      "[D loss: 0.888214] [G loss: 1.601024]\n",
      "[D loss: 0.743205] [G loss: 1.570986]\n",
      "[D loss: 0.906198] [G loss: 1.458534]\n",
      "[D loss: 0.741634] [G loss: 1.334602]\n",
      "[D loss: 0.921729] [G loss: 1.440753]\n",
      "[D loss: 1.013931] [G loss: 1.788489]\n",
      "[D loss: 0.966778] [G loss: 1.549544]\n",
      "[D loss: 0.925257] [G loss: 1.583134]\n",
      "[D loss: 0.944942] [G loss: 1.467834]\n",
      "[D loss: 0.956984] [G loss: 1.404254]\n",
      "[D loss: 0.987205] [G loss: 1.361276]\n",
      "[D loss: 0.991150] [G loss: 1.120843]\n",
      "[D loss: 0.804939] [G loss: 1.573331]\n",
      "[D loss: 0.920510] [G loss: 1.658345]\n",
      "[D loss: 0.476152] [G loss: 1.697118]\n",
      "[D loss: 0.829299] [G loss: 1.590437]\n",
      "[D loss: 0.762434] [G loss: 1.589734]\n",
      "[D loss: 0.969118] [G loss: 1.502356]\n",
      "[D loss: 0.693208] [G loss: 1.486022]\n",
      "[D loss: 0.727194] [G loss: 1.427585]\n",
      "[D loss: 0.697117] [G loss: 1.704662]\n",
      "[D loss: 1.005769] [G loss: 1.550009]\n",
      "[D loss: 0.798838] [G loss: 1.694580]\n",
      "[D loss: 0.845873] [G loss: 1.431850]\n",
      "[D loss: 0.932312] [G loss: 1.559255]\n",
      "[D loss: 0.766689] [G loss: 1.281855]\n",
      "[D loss: 1.012791] [G loss: 1.325370]\n",
      "[D loss: 0.840037] [G loss: 1.416302]\n",
      "[D loss: 0.884533] [G loss: 1.482231]\n",
      "[D loss: 0.892478] [G loss: 1.408167]\n",
      "[D loss: 0.734856] [G loss: 1.654114]\n",
      "[D loss: 0.759751] [G loss: 1.420751]\n",
      "[D loss: 0.709880] [G loss: 1.315829]\n",
      "[D loss: 0.790689] [G loss: 1.641750]\n",
      "[D loss: 0.866209] [G loss: 1.486391]\n",
      "[D loss: 0.692391] [G loss: 1.606274]\n",
      "[D loss: 0.922213] [G loss: 1.501564]\n",
      "[D loss: 0.664043] [G loss: 1.493134]\n",
      "[D loss: 0.850668] [G loss: 1.452738]\n",
      "[D loss: 0.931313] [G loss: 1.306768]\n",
      "[D loss: 0.930112] [G loss: 1.642339]\n",
      "[D loss: 0.832080] [G loss: 1.524399]\n",
      "[D loss: 0.832034] [G loss: 1.397384]\n",
      "[D loss: 0.897943] [G loss: 1.463079]\n",
      "[D loss: 0.993965] [G loss: 1.279792]\n",
      "[D loss: 0.557901] [G loss: 1.460113]\n",
      "[D loss: 0.872084] [G loss: 1.762281]\n",
      "[D loss: 0.803830] [G loss: 1.377111]\n",
      "[D loss: 0.843666] [G loss: 1.328799]\n",
      "[D loss: 0.816800] [G loss: 1.441635]\n",
      "[D loss: 0.828226] [G loss: 1.171721]\n",
      "[D loss: 0.699973] [G loss: 1.670902]\n",
      "[D loss: 0.936630] [G loss: 1.697916]\n",
      "[D loss: 1.080883] [G loss: 1.305792]\n",
      "[D loss: 0.814552] [G loss: 1.544212]\n",
      "[D loss: 0.814510] [G loss: 1.410639]\n",
      "[D loss: 0.920762] [G loss: 1.385676]\n",
      "[D loss: 0.899285] [G loss: 1.441003]\n",
      "[D loss: 0.753709] [G loss: 1.691651]\n",
      "[D loss: 1.062214] [G loss: 1.274938]\n",
      "[D loss: 0.980741] [G loss: 1.623386]\n",
      "[D loss: 0.820199] [G loss: 1.738464]\n",
      "[D loss: 0.820638] [G loss: 1.400337]\n",
      "[D loss: 0.901972] [G loss: 1.304330]\n",
      "[D loss: 1.056754] [G loss: 1.278166]\n",
      "[D loss: 0.758251] [G loss: 1.363046]\n",
      "[D loss: 0.877771] [G loss: 1.298717]\n",
      "[D loss: 0.808606] [G loss: 1.527702]\n",
      "[D loss: 1.010362] [G loss: 1.401063]\n",
      "[D loss: 0.692893] [G loss: 1.538191]\n",
      "[D loss: 0.945234] [G loss: 1.352932]\n",
      "[D loss: 0.750643] [G loss: 1.395716]\n",
      "[D loss: 0.682560] [G loss: 1.426654]\n",
      "[D loss: 0.725771] [G loss: 1.457482]\n",
      "[D loss: 0.838892] [G loss: 1.527313]\n",
      "[D loss: 0.747542] [G loss: 1.417257]\n",
      "[D loss: 0.903080] [G loss: 1.306504]\n",
      "[D loss: 0.893127] [G loss: 1.337546]\n",
      "[D loss: 0.606457] [G loss: 1.525334]\n",
      "[D loss: 0.834207] [G loss: 1.474865]\n",
      "[D loss: 0.759563] [G loss: 1.419462]\n",
      "[D loss: 0.838531] [G loss: 1.516673]\n",
      "[D loss: 0.725452] [G loss: 1.624180]\n",
      "[D loss: 0.908105] [G loss: 1.516127]\n",
      "[D loss: 0.935022] [G loss: 1.487498]\n",
      "[D loss: 0.803921] [G loss: 1.338341]\n",
      "[D loss: 0.779136] [G loss: 1.570523]\n",
      "[D loss: 0.833179] [G loss: 1.298920]\n",
      "[D loss: 0.956882] [G loss: 1.353077]\n",
      "[D loss: 1.010799] [G loss: 1.570749]\n",
      "[D loss: 0.952754] [G loss: 1.477681]\n",
      "[D loss: 0.742661] [G loss: 1.448840]\n",
      "[D loss: 1.017293] [G loss: 1.315389]\n",
      "[D loss: 0.726018] [G loss: 1.349570]\n",
      "[D loss: 0.837478] [G loss: 1.322238]\n",
      "[D loss: 1.038126] [G loss: 1.582794]\n",
      "[D loss: 0.696060] [G loss: 1.522275]\n",
      "[D loss: 0.761984] [G loss: 1.485331]\n",
      "[D loss: 0.679981] [G loss: 1.377295]\n",
      "[D loss: 0.893538] [G loss: 1.453661]\n",
      "[D loss: 0.729098] [G loss: 1.408041]\n",
      "[D loss: 0.699742] [G loss: 1.459110]\n",
      "[D loss: 0.737733] [G loss: 1.515815]\n",
      "[D loss: 0.744272] [G loss: 1.523251]\n",
      "[D loss: 0.709502] [G loss: 1.445560]\n",
      "[D loss: 0.875815] [G loss: 1.522943]\n",
      "[D loss: 0.752259] [G loss: 1.496324]\n",
      "[D loss: 1.030345] [G loss: 1.437773]\n",
      "[D loss: 0.826928] [G loss: 1.316625]\n",
      "[D loss: 0.650113] [G loss: 1.639486]\n",
      "[D loss: 0.877698] [G loss: 1.313366]\n",
      "[D loss: 0.726934] [G loss: 1.580603]\n",
      "[D loss: 0.606649] [G loss: 1.659863]\n",
      "[D loss: 1.010155] [G loss: 1.444656]\n",
      "[D loss: 0.747323] [G loss: 1.544253]\n",
      "[D loss: 1.030542] [G loss: 1.286040]\n",
      "[D loss: 0.722408] [G loss: 1.511642]\n",
      "[D loss: 1.091128] [G loss: 1.657047]\n",
      "[D loss: 0.893525] [G loss: 1.499030]\n",
      "[D loss: 0.946614] [G loss: 1.561194]\n",
      "[D loss: 0.799030] [G loss: 1.485855]\n",
      "[D loss: 0.851127] [G loss: 1.658263]\n",
      "[D loss: 0.799965] [G loss: 1.488450]\n",
      "[D loss: 1.229863] [G loss: 1.458123]\n",
      "[D loss: 0.809715] [G loss: 1.339488]\n",
      "[D loss: 0.905205] [G loss: 1.176169]\n",
      "[D loss: 1.020837] [G loss: 1.361231]\n",
      "[D loss: 0.748495] [G loss: 1.295607]\n",
      "[D loss: 0.801855] [G loss: 1.333577]\n",
      "[D loss: 0.903484] [G loss: 1.504672]\n",
      "[D loss: 0.875859] [G loss: 1.520339]\n",
      "[D loss: 0.802440] [G loss: 1.505681]\n",
      "[D loss: 0.874430] [G loss: 1.177646]\n",
      "[D loss: 0.881280] [G loss: 1.408148]\n",
      "[D loss: 0.734834] [G loss: 1.394525]\n",
      "[D loss: 0.793693] [G loss: 1.382620]\n",
      "[D loss: 0.926371] [G loss: 1.394218]\n",
      "[D loss: 0.899643] [G loss: 1.516264]\n",
      "[D loss: 1.170929] [G loss: 1.350099]\n",
      "[D loss: 0.847763] [G loss: 1.412043]\n",
      "[D loss: 0.825648] [G loss: 1.454649]\n",
      "[D loss: 0.802128] [G loss: 1.337381]\n",
      "[D loss: 0.742622] [G loss: 1.430657]\n",
      "[D loss: 0.831529] [G loss: 1.536077]\n",
      "[D loss: 0.973057] [G loss: 1.254702]\n",
      "[D loss: 1.011380] [G loss: 1.310988]\n",
      "[D loss: 0.830835] [G loss: 1.513007]\n",
      "[D loss: 0.963564] [G loss: 1.375388]\n",
      "[D loss: 0.637065] [G loss: 1.472194]\n",
      "[D loss: 0.682334] [G loss: 1.332721]\n",
      "[D loss: 0.802001] [G loss: 1.404341]\n",
      "[D loss: 0.946459] [G loss: 1.186608]\n",
      "[D loss: 0.913466] [G loss: 1.291548]\n",
      "[D loss: 0.763337] [G loss: 1.538834]\n",
      "[D loss: 1.068294] [G loss: 1.571867]\n",
      "[D loss: 0.990393] [G loss: 1.566010]\n",
      "[D loss: 0.778380] [G loss: 1.503713]\n",
      "[D loss: 0.797618] [G loss: 1.623387]\n",
      "[D loss: 0.932436] [G loss: 1.479545]\n",
      "[D loss: 0.649385] [G loss: 1.424995]\n",
      "[D loss: 0.889931] [G loss: 1.424042]\n",
      "[D loss: 0.803830] [G loss: 1.562843]\n",
      "[D loss: 0.887794] [G loss: 1.431785]\n",
      "[D loss: 0.893447] [G loss: 1.549769]\n",
      "[D loss: 1.243265] [G loss: 1.307885]\n",
      "[D loss: 0.889741] [G loss: 1.203783]\n",
      "[D loss: 0.929856] [G loss: 1.232364]\n",
      "[D loss: 0.982037] [G loss: 1.238090]\n",
      "[D loss: 0.931293] [G loss: 1.294194]\n",
      "[D loss: 1.016991] [G loss: 1.156850]\n",
      "[D loss: 0.921481] [G loss: 1.170432]\n",
      "[D loss: 0.825164] [G loss: 1.474826]\n",
      "[D loss: 0.770900] [G loss: 1.520487]\n",
      "[D loss: 0.745198] [G loss: 1.578056]\n",
      "[D loss: 1.109448] [G loss: 1.357657]\n",
      "[D loss: 0.787712] [G loss: 1.460429]\n",
      "[D loss: 0.658618] [G loss: 1.647921]\n",
      "[D loss: 0.938065] [G loss: 1.509302]\n",
      "[D loss: 1.038567] [G loss: 1.152985]\n",
      "[D loss: 0.768537] [G loss: 1.379495]\n",
      "[D loss: 0.940312] [G loss: 1.320905]\n",
      "[D loss: 0.696814] [G loss: 1.538660]\n",
      "[D loss: 0.914685] [G loss: 1.376384]\n",
      "[D loss: 0.790465] [G loss: 1.370971]\n",
      "[D loss: 0.690446] [G loss: 1.405193]\n",
      "[D loss: 0.857806] [G loss: 1.449503]\n",
      "[D loss: 0.731803] [G loss: 1.586267]\n",
      "[D loss: 0.785939] [G loss: 1.567933]\n",
      "[D loss: 0.688115] [G loss: 1.758874]\n",
      "[D loss: 0.638745] [G loss: 1.648610]\n",
      "[D loss: 0.576426] [G loss: 1.706686]\n",
      "[D loss: 0.850923] [G loss: 1.508418]\n",
      "[D loss: 0.789275] [G loss: 1.499794]\n",
      "[D loss: 0.922999] [G loss: 1.797384]\n",
      "[D loss: 0.842673] [G loss: 1.345716]\n",
      "[D loss: 0.852013] [G loss: 1.555178]\n",
      "[D loss: 0.868444] [G loss: 1.710279]\n",
      "[D loss: 0.578272] [G loss: 1.667421]\n",
      "[D loss: 0.764670] [G loss: 1.512569]\n",
      "[D loss: 0.989937] [G loss: 1.643301]\n",
      "[D loss: 0.813374] [G loss: 1.713475]\n",
      "[D loss: 0.782098] [G loss: 1.388205]\n",
      "[D loss: 0.929138] [G loss: 1.733678]\n",
      "[D loss: 1.141661] [G loss: 1.326906]\n",
      "[D loss: 0.755378] [G loss: 1.612519]\n",
      "[D loss: 0.752292] [G loss: 1.494586]\n",
      "[D loss: 0.739198] [G loss: 1.544132]\n",
      "[D loss: 0.771831] [G loss: 1.303905]\n",
      "[D loss: 0.877253] [G loss: 1.470314]\n",
      "[D loss: 1.027114] [G loss: 1.400893]\n",
      "[D loss: 0.748829] [G loss: 1.460638]\n",
      "[D loss: 0.875720] [G loss: 1.399939]\n",
      "[D loss: 0.681920] [G loss: 1.429431]\n",
      "[D loss: 0.982959] [G loss: 1.381355]\n",
      "[D loss: 0.929231] [G loss: 1.295467]\n",
      "[D loss: 0.849865] [G loss: 1.516836]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.886054] [G loss: 1.393336]\n",
      "[D loss: 0.784759] [G loss: 1.430594]\n",
      "[D loss: 0.989804] [G loss: 1.413313]\n",
      "[D loss: 0.874257] [G loss: 1.269818]\n",
      "[D loss: 0.701888] [G loss: 1.403460]\n",
      "[D loss: 0.879005] [G loss: 1.739744]\n",
      "[D loss: 0.923365] [G loss: 1.259873]\n",
      "[D loss: 0.783424] [G loss: 1.243211]\n",
      "[D loss: 0.896876] [G loss: 1.227391]\n",
      "[D loss: 1.089416] [G loss: 1.180568]\n",
      "[D loss: 0.962649] [G loss: 1.220806]\n",
      "[D loss: 0.622350] [G loss: 1.441170]\n",
      "[D loss: 0.814171] [G loss: 1.443055]\n",
      "[D loss: 0.697830] [G loss: 1.344542]\n",
      "[D loss: 0.851416] [G loss: 1.519646]\n",
      "[D loss: 0.984855] [G loss: 1.234088]\n",
      "[D loss: 0.878629] [G loss: 1.419054]\n",
      "[D loss: 0.779644] [G loss: 1.386611]\n",
      "[D loss: 0.808929] [G loss: 1.573274]\n",
      "[D loss: 0.773286] [G loss: 1.458687]\n",
      "[D loss: 0.832942] [G loss: 1.496421]\n",
      "[D loss: 0.839445] [G loss: 1.372732]\n",
      "[D loss: 0.865110] [G loss: 1.561471]\n",
      "[D loss: 0.811561] [G loss: 1.425852]\n",
      "[D loss: 0.643179] [G loss: 1.551081]\n",
      "[D loss: 0.869866] [G loss: 1.246426]\n",
      "[D loss: 0.724535] [G loss: 1.519992]\n",
      "[D loss: 0.854405] [G loss: 1.356586]\n",
      "[D loss: 0.628524] [G loss: 1.453266]\n",
      "[D loss: 0.917555] [G loss: 1.436728]\n",
      "[D loss: 0.821726] [G loss: 1.340193]\n",
      "[D loss: 1.166900] [G loss: 1.374573]\n",
      "[D loss: 0.946896] [G loss: 1.393476]\n",
      "[D loss: 0.949018] [G loss: 1.260685]\n",
      "[D loss: 0.991607] [G loss: 1.228969]\n",
      "[D loss: 0.784180] [G loss: 1.458220]\n",
      "[D loss: 0.869895] [G loss: 1.337278]\n",
      "[D loss: 0.621765] [G loss: 1.516296]\n",
      "[D loss: 0.969638] [G loss: 1.199433]\n",
      "[D loss: 0.889638] [G loss: 1.548812]\n",
      "[D loss: 0.937617] [G loss: 1.482612]\n",
      "[D loss: 0.810502] [G loss: 1.398364]\n",
      "[D loss: 0.885794] [G loss: 1.505823]\n",
      "[D loss: 0.622850] [G loss: 1.525451]\n",
      "[D loss: 0.918717] [G loss: 1.413097]\n",
      "[D loss: 0.929887] [G loss: 1.310266]\n",
      "[D loss: 0.949426] [G loss: 1.466489]\n",
      "[D loss: 0.926918] [G loss: 1.283760]\n",
      "[D loss: 0.775883] [G loss: 1.405859]\n",
      "[D loss: 0.881168] [G loss: 1.388635]\n",
      "[D loss: 0.927531] [G loss: 1.315626]\n",
      "[D loss: 0.741385] [G loss: 1.679680]\n",
      "[D loss: 0.854233] [G loss: 1.439053]\n",
      "[D loss: 0.576675] [G loss: 1.490297]\n",
      "[D loss: 0.880023] [G loss: 1.209756]\n",
      "[D loss: 0.776631] [G loss: 1.484229]\n",
      "[D loss: 0.844763] [G loss: 1.568099]\n",
      "[D loss: 0.831995] [G loss: 1.481278]\n",
      "[D loss: 1.017499] [G loss: 1.387466]\n",
      "[D loss: 0.642448] [G loss: 1.500690]\n",
      "[D loss: 1.122576] [G loss: 1.586380]\n",
      "[D loss: 0.975784] [G loss: 1.225786]\n",
      "[D loss: 1.016385] [G loss: 1.410969]\n",
      "[D loss: 0.670943] [G loss: 1.376750]\n",
      "[D loss: 0.791297] [G loss: 1.203820]\n",
      "[D loss: 0.764415] [G loss: 1.300104]\n",
      "[D loss: 0.894093] [G loss: 1.439681]\n",
      "[D loss: 0.774619] [G loss: 1.440694]\n",
      "[D loss: 0.749692] [G loss: 1.353227]\n",
      "[D loss: 0.951790] [G loss: 1.705388]\n",
      "[D loss: 0.894272] [G loss: 1.532285]\n",
      "[D loss: 0.805492] [G loss: 1.395776]\n",
      "[D loss: 0.805395] [G loss: 1.342704]\n",
      "[D loss: 0.699075] [G loss: 1.573510]\n",
      "[D loss: 0.810440] [G loss: 1.293627]\n",
      "[D loss: 0.967835] [G loss: 1.276089]\n",
      "[D loss: 0.772812] [G loss: 1.545235]\n",
      "[D loss: 0.936075] [G loss: 1.583103]\n",
      "[D loss: 0.872845] [G loss: 1.521884]\n",
      "[D loss: 0.918154] [G loss: 1.537569]\n",
      "[D loss: 0.870431] [G loss: 1.634005]\n",
      "[D loss: 0.954382] [G loss: 1.333558]\n",
      "[D loss: 0.843374] [G loss: 1.714235]\n",
      "[D loss: 0.817752] [G loss: 1.406379]\n",
      "[D loss: 0.787790] [G loss: 1.490384]\n",
      "[D loss: 0.893007] [G loss: 1.379908]\n",
      "[D loss: 0.958065] [G loss: 1.259339]\n",
      "[D loss: 0.861549] [G loss: 1.254528]\n",
      "[D loss: 0.824208] [G loss: 1.409484]\n",
      "[D loss: 0.819493] [G loss: 1.637447]\n",
      "[D loss: 0.812166] [G loss: 1.540079]\n",
      "[D loss: 0.722774] [G loss: 1.349805]\n",
      "[D loss: 0.876759] [G loss: 1.348556]\n",
      "[D loss: 0.899993] [G loss: 1.494798]\n",
      "[D loss: 0.714023] [G loss: 1.714010]\n",
      "[D loss: 0.930314] [G loss: 1.674645]\n",
      "[D loss: 1.095502] [G loss: 1.340817]\n",
      "[D loss: 1.175980] [G loss: 1.304106]\n",
      "[D loss: 0.881607] [G loss: 1.339593]\n",
      "[D loss: 0.872170] [G loss: 1.827843]\n",
      "[D loss: 0.780731] [G loss: 1.329792]\n",
      "[D loss: 0.904761] [G loss: 1.160764]\n",
      "[D loss: 0.733089] [G loss: 1.342919]\n",
      "[D loss: 0.721709] [G loss: 1.607548]\n",
      "[D loss: 0.809811] [G loss: 1.308253]\n",
      "[D loss: 0.759624] [G loss: 1.564249]\n",
      "[D loss: 0.681451] [G loss: 1.536405]\n",
      "[D loss: 0.845842] [G loss: 1.475504]\n",
      "[D loss: 0.506773] [G loss: 1.570919]\n",
      "[D loss: 0.809928] [G loss: 1.535701]\n",
      "[D loss: 0.787237] [G loss: 1.648386]\n",
      "[D loss: 0.940759] [G loss: 1.291539]\n",
      "[D loss: 0.587209] [G loss: 1.618414]\n",
      "[D loss: 0.953767] [G loss: 1.425870]\n",
      "[D loss: 0.826989] [G loss: 1.572475]\n",
      "[D loss: 1.011785] [G loss: 1.390466]\n",
      "[D loss: 0.817215] [G loss: 1.509054]\n",
      "[D loss: 0.832982] [G loss: 1.450565]\n",
      "[D loss: 0.779378] [G loss: 1.350799]\n",
      "[D loss: 0.624759] [G loss: 1.396124]\n",
      "[D loss: 0.985045] [G loss: 1.416560]\n",
      "[D loss: 0.846017] [G loss: 1.278330]\n",
      "[D loss: 0.868708] [G loss: 1.363005]\n",
      "[D loss: 0.949288] [G loss: 1.461889]\n",
      "[D loss: 0.874864] [G loss: 1.618656]\n",
      "[D loss: 0.987632] [G loss: 1.630050]\n",
      "[D loss: 1.170560] [G loss: 1.469312]\n",
      "[D loss: 0.821781] [G loss: 1.443294]\n",
      "[D loss: 0.665326] [G loss: 1.668742]\n",
      "[D loss: 1.011441] [G loss: 1.495385]\n",
      "[D loss: 0.923981] [G loss: 1.448687]\n",
      "[D loss: 0.807503] [G loss: 1.487116]\n",
      "[D loss: 0.756250] [G loss: 1.543280]\n",
      "[D loss: 0.852423] [G loss: 1.195766]\n",
      "[D loss: 0.872331] [G loss: 1.455085]\n",
      "[D loss: 0.781038] [G loss: 1.522687]\n",
      "[D loss: 0.718002] [G loss: 1.504985]\n",
      "[D loss: 0.790675] [G loss: 1.442403]\n",
      "[D loss: 0.783473] [G loss: 1.400148]\n",
      "[D loss: 0.732915] [G loss: 1.478327]\n",
      "[D loss: 0.852133] [G loss: 1.674665]\n",
      "[D loss: 0.800331] [G loss: 1.276951]\n",
      "[D loss: 0.866078] [G loss: 1.638499]\n",
      "[D loss: 0.934377] [G loss: 1.460360]\n",
      "[D loss: 0.587801] [G loss: 1.567003]\n",
      "[D loss: 1.071710] [G loss: 1.540088]\n",
      "[D loss: 0.703502] [G loss: 1.515364]\n",
      "[D loss: 0.892630] [G loss: 1.349152]\n",
      "[D loss: 0.889800] [G loss: 1.706012]\n",
      "[D loss: 0.856285] [G loss: 1.689558]\n",
      "[D loss: 0.839624] [G loss: 1.553014]\n",
      "[D loss: 0.682519] [G loss: 1.775231]\n",
      "[D loss: 0.851345] [G loss: 1.397830]\n",
      "[D loss: 0.948726] [G loss: 1.313959]\n",
      "[D loss: 0.991178] [G loss: 1.540805]\n",
      "[D loss: 0.749074] [G loss: 1.392014]\n",
      "[D loss: 1.019475] [G loss: 1.570770]\n",
      "[D loss: 0.823581] [G loss: 1.487784]\n",
      "[D loss: 0.871284] [G loss: 1.437784]\n",
      "[D loss: 0.808051] [G loss: 1.346372]\n",
      "[D loss: 0.734533] [G loss: 1.326368]\n",
      "[D loss: 0.983864] [G loss: 1.561010]\n",
      "[D loss: 0.922451] [G loss: 1.366161]\n",
      "[D loss: 0.925777] [G loss: 1.448009]\n",
      "[D loss: 1.375545] [G loss: 1.211145]\n",
      "[D loss: 0.979038] [G loss: 1.261594]\n",
      "[D loss: 0.881695] [G loss: 1.223481]\n",
      "[D loss: 0.878967] [G loss: 1.466254]\n",
      "[D loss: 0.983801] [G loss: 1.460157]\n",
      "[D loss: 0.838385] [G loss: 1.252633]\n",
      "[D loss: 0.603656] [G loss: 1.266072]\n",
      "[D loss: 0.624078] [G loss: 1.650647]\n",
      "[D loss: 0.935054] [G loss: 1.410724]\n",
      "[D loss: 0.826948] [G loss: 1.565579]\n",
      "[D loss: 1.007534] [G loss: 1.368650]\n",
      "[D loss: 0.767005] [G loss: 1.362751]\n",
      "[D loss: 0.907817] [G loss: 1.177623]\n",
      "[D loss: 0.777128] [G loss: 1.419688]\n",
      "[D loss: 0.783992] [G loss: 1.353646]\n",
      "[D loss: 0.857145] [G loss: 1.446757]\n",
      "[D loss: 0.997544] [G loss: 1.451139]\n",
      "[D loss: 1.143534] [G loss: 1.325038]\n",
      "[D loss: 0.780406] [G loss: 1.699731]\n",
      "[D loss: 0.831212] [G loss: 1.709715]\n",
      "[D loss: 0.732755] [G loss: 1.436007]\n",
      "[D loss: 0.860059] [G loss: 1.480727]\n",
      "[D loss: 0.727495] [G loss: 1.279027]\n",
      "[D loss: 0.943305] [G loss: 1.493138]\n",
      "[D loss: 0.870283] [G loss: 1.457976]\n",
      "[D loss: 0.958569] [G loss: 1.086796]\n",
      "[D loss: 0.823175] [G loss: 1.441869]\n",
      "[D loss: 1.095456] [G loss: 1.293884]\n",
      "[D loss: 0.871595] [G loss: 1.577287]\n",
      "[D loss: 1.012511] [G loss: 1.413002]\n",
      "[D loss: 0.833063] [G loss: 1.475392]\n",
      "[D loss: 1.060881] [G loss: 1.197854]\n",
      "[D loss: 0.821680] [G loss: 1.330603]\n",
      "[D loss: 0.876860] [G loss: 1.402876]\n",
      "[D loss: 0.841028] [G loss: 1.574399]\n",
      "[D loss: 0.977407] [G loss: 1.389035]\n",
      "[D loss: 0.859161] [G loss: 1.126416]\n",
      "[D loss: 0.815883] [G loss: 1.676239]\n",
      "[D loss: 0.834776] [G loss: 1.488252]\n",
      "[D loss: 1.072973] [G loss: 1.571018]\n",
      "[D loss: 0.794893] [G loss: 1.438328]\n",
      "[D loss: 1.055911] [G loss: 1.225882]\n",
      "[D loss: 0.875009] [G loss: 1.378526]\n",
      "[D loss: 0.725058] [G loss: 1.431830]\n",
      "[D loss: 1.018373] [G loss: 1.407855]\n",
      "[D loss: 0.950757] [G loss: 1.239741]\n",
      "[D loss: 0.830711] [G loss: 1.295317]\n",
      "[D loss: 0.959960] [G loss: 1.236972]\n",
      "[D loss: 0.806099] [G loss: 1.448562]\n",
      "[D loss: 0.780567] [G loss: 1.464205]\n",
      "[D loss: 1.073131] [G loss: 1.364006]\n",
      "[D loss: 0.980747] [G loss: 1.356673]\n",
      "[D loss: 0.614727] [G loss: 1.540786]\n",
      "[D loss: 0.703732] [G loss: 1.482756]\n",
      "[D loss: 0.814860] [G loss: 1.481574]\n",
      "[D loss: 0.829404] [G loss: 1.505149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.707735] [G loss: 1.338179]\n",
      "[D loss: 0.740275] [G loss: 1.373622]\n",
      "[D loss: 0.796083] [G loss: 1.386020]\n",
      "[D loss: 0.927815] [G loss: 1.452904]\n",
      "[D loss: 0.780964] [G loss: 1.813613]\n",
      "[D loss: 0.965513] [G loss: 1.671745]\n",
      "[D loss: 0.690516] [G loss: 1.526201]\n",
      "[D loss: 0.762344] [G loss: 1.447895]\n",
      "[D loss: 0.797400] [G loss: 1.566050]\n",
      "[D loss: 0.964154] [G loss: 1.476434]\n",
      "[D loss: 0.674245] [G loss: 1.461692]\n",
      "[D loss: 0.995775] [G loss: 1.267933]\n",
      "[D loss: 0.921961] [G loss: 1.409103]\n",
      "[D loss: 0.949378] [G loss: 1.418623]\n",
      "[D loss: 0.714273] [G loss: 1.420519]\n",
      "[D loss: 0.902377] [G loss: 1.436893]\n",
      "[D loss: 0.856907] [G loss: 1.567670]\n",
      "[D loss: 0.658423] [G loss: 1.484461]\n",
      "[D loss: 1.088127] [G loss: 1.386730]\n",
      "[D loss: 0.980667] [G loss: 1.357267]\n",
      "[D loss: 1.052109] [G loss: 1.430768]\n",
      "[D loss: 1.044542] [G loss: 0.990161]\n",
      "[D loss: 0.976815] [G loss: 1.350066]\n",
      "[D loss: 1.007443] [G loss: 1.569538]\n",
      "[D loss: 0.694502] [G loss: 1.467738]\n",
      "[D loss: 0.571538] [G loss: 1.501609]\n",
      "[D loss: 0.822608] [G loss: 1.458813]\n",
      "[D loss: 0.855099] [G loss: 1.441907]\n",
      "[D loss: 0.670487] [G loss: 1.492679]\n",
      "[D loss: 0.838038] [G loss: 1.400977]\n",
      "[D loss: 0.881724] [G loss: 1.355302]\n",
      "[D loss: 0.963792] [G loss: 1.582692]\n",
      "[D loss: 0.814803] [G loss: 1.229800]\n",
      "[D loss: 0.835985] [G loss: 1.630289]\n",
      "[D loss: 1.096700] [G loss: 1.045004]\n",
      "[D loss: 0.873890] [G loss: 1.366381]\n",
      "[D loss: 0.714261] [G loss: 1.593398]\n",
      "[D loss: 0.860685] [G loss: 1.391759]\n",
      "[D loss: 1.056248] [G loss: 1.453499]\n",
      "[D loss: 0.655459] [G loss: 1.551895]\n",
      "[D loss: 0.894213] [G loss: 1.323114]\n",
      "[D loss: 0.821917] [G loss: 1.320156]\n",
      "[D loss: 0.844506] [G loss: 1.460881]\n",
      "[D loss: 1.101284] [G loss: 1.203727]\n",
      "[D loss: 0.775078] [G loss: 1.473841]\n",
      "[D loss: 0.992693] [G loss: 1.333606]\n",
      "[D loss: 0.801355] [G loss: 1.576320]\n",
      "[D loss: 0.985963] [G loss: 1.265278]\n",
      "[D loss: 0.848744] [G loss: 1.303666]\n",
      "[D loss: 0.947044] [G loss: 1.327302]\n",
      "[D loss: 0.826763] [G loss: 1.299668]\n",
      "[D loss: 0.960917] [G loss: 1.632656]\n",
      "[D loss: 0.830913] [G loss: 1.275893]\n",
      "[D loss: 0.725672] [G loss: 1.463702]\n",
      "[D loss: 0.785364] [G loss: 1.286442]\n",
      "[D loss: 0.627375] [G loss: 1.631378]\n",
      "[D loss: 0.637444] [G loss: 1.514970]\n",
      "[D loss: 0.565704] [G loss: 1.694811]\n",
      "[D loss: 0.916872] [G loss: 1.543400]\n",
      "[D loss: 0.764540] [G loss: 1.523084]\n",
      "[D loss: 1.022338] [G loss: 1.555915]\n",
      "[D loss: 0.725104] [G loss: 1.530537]\n",
      "[D loss: 1.073569] [G loss: 1.412059]\n",
      "[D loss: 0.765559] [G loss: 1.594338]\n",
      "[D loss: 0.602195] [G loss: 1.604065]\n",
      "[D loss: 1.240206] [G loss: 1.462015]\n",
      "[D loss: 0.653511] [G loss: 1.476915]\n",
      "[D loss: 1.000685] [G loss: 1.636343]\n",
      "[D loss: 1.175613] [G loss: 1.129825]\n",
      "[D loss: 0.912827] [G loss: 1.262068]\n",
      "[D loss: 0.882060] [G loss: 1.528572]\n",
      "[D loss: 0.641104] [G loss: 1.758173]\n",
      "[D loss: 0.794787] [G loss: 1.620431]\n",
      "[D loss: 0.890291] [G loss: 1.316850]\n",
      "[D loss: 0.869069] [G loss: 1.369729]\n",
      "[D loss: 0.764952] [G loss: 1.413162]\n",
      "[D loss: 0.880737] [G loss: 1.257091]\n",
      "[D loss: 1.066989] [G loss: 1.108150]\n",
      "[D loss: 0.520841] [G loss: 1.448049]\n",
      "[D loss: 0.803439] [G loss: 1.625427]\n",
      "[D loss: 0.855451] [G loss: 1.540331]\n",
      "[D loss: 0.779451] [G loss: 1.579598]\n",
      "[D loss: 0.667548] [G loss: 1.425981]\n",
      "[D loss: 0.885900] [G loss: 1.535373]\n",
      "[D loss: 0.999258] [G loss: 1.337867]\n",
      "[D loss: 0.629182] [G loss: 1.434527]\n",
      "[D loss: 0.720540] [G loss: 1.431717]\n",
      "[D loss: 0.894931] [G loss: 1.609735]\n",
      "[D loss: 0.692707] [G loss: 1.576084]\n",
      "[D loss: 0.936542] [G loss: 1.300591]\n",
      "[D loss: 0.755686] [G loss: 1.492239]\n",
      "[D loss: 1.014912] [G loss: 1.244374]\n",
      "[D loss: 0.748745] [G loss: 1.382951]\n",
      "[D loss: 1.013696] [G loss: 1.528280]\n",
      "[D loss: 0.838584] [G loss: 1.605261]\n",
      "[D loss: 0.886305] [G loss: 1.629372]\n",
      "[D loss: 0.837700] [G loss: 1.610064]\n",
      "[D loss: 0.893284] [G loss: 1.213279]\n",
      "[D loss: 0.929447] [G loss: 1.401503]\n",
      "[D loss: 0.691598] [G loss: 1.482762]\n",
      "[D loss: 0.896688] [G loss: 1.156936]\n",
      "[D loss: 0.808350] [G loss: 1.351618]\n",
      "[D loss: 0.767071] [G loss: 1.438056]\n",
      "[D loss: 0.841856] [G loss: 1.399678]\n",
      "[D loss: 0.768210] [G loss: 1.432143]\n",
      "[D loss: 0.694237] [G loss: 1.398269]\n",
      "[D loss: 0.679359] [G loss: 1.545267]\n",
      "[D loss: 0.955551] [G loss: 1.328925]\n",
      "[D loss: 0.818460] [G loss: 1.426512]\n",
      "[D loss: 0.837831] [G loss: 1.580147]\n",
      "[D loss: 1.108501] [G loss: 1.392108]\n",
      "[D loss: 1.013830] [G loss: 1.436014]\n",
      "[D loss: 0.781012] [G loss: 1.251560]\n",
      "[D loss: 0.929060] [G loss: 1.377920]\n",
      "[D loss: 0.689418] [G loss: 1.474161]\n",
      "[D loss: 0.879162] [G loss: 1.604266]\n",
      "[D loss: 1.123192] [G loss: 1.488515]\n",
      "[D loss: 1.121088] [G loss: 1.484870]\n",
      "[D loss: 0.799347] [G loss: 1.180558]\n",
      "[D loss: 0.952444] [G loss: 1.298794]\n",
      "epoch:5, g_loss:2769.28173828125,d_loss:1552.3658447265625\n",
      "[D loss: 0.976705] [G loss: 1.295115]\n",
      "[D loss: 0.771717] [G loss: 1.642103]\n",
      "[D loss: 0.843347] [G loss: 1.615745]\n",
      "[D loss: 0.890346] [G loss: 1.344198]\n",
      "[D loss: 0.991117] [G loss: 1.417795]\n",
      "[D loss: 0.825562] [G loss: 1.555012]\n",
      "[D loss: 0.890378] [G loss: 1.585414]\n",
      "[D loss: 0.810057] [G loss: 1.603316]\n",
      "[D loss: 0.796801] [G loss: 1.330613]\n",
      "[D loss: 0.843379] [G loss: 1.490600]\n",
      "[D loss: 0.600899] [G loss: 1.524582]\n",
      "[D loss: 0.890414] [G loss: 1.204784]\n",
      "[D loss: 0.804584] [G loss: 1.434792]\n",
      "[D loss: 0.752194] [G loss: 1.559822]\n",
      "[D loss: 0.637949] [G loss: 1.584636]\n",
      "[D loss: 0.735413] [G loss: 1.476789]\n",
      "[D loss: 1.098757] [G loss: 1.424369]\n",
      "[D loss: 0.956157] [G loss: 1.565275]\n",
      "[D loss: 0.785462] [G loss: 1.516137]\n",
      "[D loss: 0.932963] [G loss: 1.462890]\n",
      "[D loss: 1.150288] [G loss: 1.371761]\n",
      "[D loss: 0.743464] [G loss: 1.451653]\n",
      "[D loss: 0.617960] [G loss: 1.450287]\n",
      "[D loss: 0.665277] [G loss: 1.369071]\n",
      "[D loss: 0.917198] [G loss: 1.333117]\n",
      "[D loss: 1.079540] [G loss: 1.442793]\n",
      "[D loss: 0.704363] [G loss: 1.427769]\n",
      "[D loss: 0.811422] [G loss: 1.495506]\n",
      "[D loss: 0.832403] [G loss: 1.479616]\n",
      "[D loss: 0.724908] [G loss: 1.616736]\n",
      "[D loss: 0.904939] [G loss: 1.431736]\n",
      "[D loss: 0.960112] [G loss: 1.566071]\n",
      "[D loss: 1.111878] [G loss: 1.184257]\n",
      "[D loss: 0.656489] [G loss: 1.461168]\n",
      "[D loss: 0.853830] [G loss: 1.366287]\n",
      "[D loss: 0.836512] [G loss: 1.184748]\n",
      "[D loss: 0.747996] [G loss: 1.469833]\n",
      "[D loss: 0.860532] [G loss: 1.369587]\n",
      "[D loss: 0.876494] [G loss: 1.337447]\n",
      "[D loss: 0.989507] [G loss: 1.512194]\n",
      "[D loss: 0.916687] [G loss: 1.536049]\n",
      "[D loss: 0.939893] [G loss: 1.423648]\n",
      "[D loss: 0.814871] [G loss: 1.231875]\n",
      "[D loss: 0.829222] [G loss: 1.405710]\n",
      "[D loss: 0.744075] [G loss: 1.381617]\n",
      "[D loss: 0.742429] [G loss: 1.287994]\n",
      "[D loss: 0.668911] [G loss: 1.505514]\n",
      "[D loss: 0.899318] [G loss: 1.312080]\n",
      "[D loss: 0.910163] [G loss: 1.532160]\n",
      "[D loss: 0.912781] [G loss: 1.558802]\n",
      "[D loss: 0.502317] [G loss: 1.740565]\n",
      "[D loss: 0.678018] [G loss: 1.475598]\n",
      "[D loss: 0.710119] [G loss: 1.377290]\n",
      "[D loss: 0.782405] [G loss: 1.478052]\n",
      "[D loss: 0.921103] [G loss: 1.475121]\n",
      "[D loss: 0.792490] [G loss: 1.457372]\n",
      "[D loss: 0.589840] [G loss: 1.417856]\n",
      "[D loss: 0.898919] [G loss: 1.288899]\n",
      "[D loss: 0.651887] [G loss: 1.691027]\n",
      "[D loss: 1.081972] [G loss: 1.537463]\n",
      "[D loss: 0.886310] [G loss: 1.662353]\n",
      "[D loss: 0.991721] [G loss: 1.470492]\n",
      "[D loss: 0.794651] [G loss: 1.581307]\n",
      "[D loss: 0.824660] [G loss: 1.556087]\n",
      "[D loss: 0.623429] [G loss: 1.587311]\n",
      "[D loss: 0.841122] [G loss: 1.431514]\n",
      "[D loss: 0.589411] [G loss: 1.569684]\n",
      "[D loss: 0.863893] [G loss: 1.432559]\n",
      "[D loss: 0.711228] [G loss: 1.428907]\n",
      "[D loss: 0.795417] [G loss: 1.541065]\n",
      "[D loss: 0.772181] [G loss: 1.448034]\n",
      "[D loss: 0.773125] [G loss: 1.657935]\n",
      "[D loss: 0.839886] [G loss: 1.225276]\n",
      "[D loss: 0.853203] [G loss: 1.402208]\n",
      "[D loss: 0.644117] [G loss: 1.808270]\n",
      "[D loss: 1.031626] [G loss: 1.414258]\n",
      "[D loss: 1.110218] [G loss: 1.471350]\n",
      "[D loss: 0.922431] [G loss: 1.427522]\n",
      "[D loss: 0.902892] [G loss: 1.255950]\n",
      "[D loss: 0.676001] [G loss: 1.379226]\n",
      "[D loss: 0.799042] [G loss: 1.557405]\n",
      "[D loss: 0.828131] [G loss: 1.244162]\n",
      "[D loss: 0.604730] [G loss: 1.528433]\n",
      "[D loss: 0.752266] [G loss: 1.511427]\n",
      "[D loss: 0.792775] [G loss: 1.434151]\n",
      "[D loss: 0.856142] [G loss: 1.523021]\n",
      "[D loss: 1.022553] [G loss: 1.611082]\n",
      "[D loss: 0.967237] [G loss: 1.420984]\n",
      "[D loss: 1.061149] [G loss: 1.481416]\n",
      "[D loss: 0.869475] [G loss: 1.535458]\n",
      "[D loss: 0.641456] [G loss: 1.425412]\n",
      "[D loss: 0.834069] [G loss: 1.296353]\n",
      "[D loss: 1.050031] [G loss: 1.193498]\n",
      "[D loss: 0.859501] [G loss: 1.476216]\n",
      "[D loss: 0.683774] [G loss: 1.639533]\n",
      "[D loss: 0.746374] [G loss: 1.367960]\n",
      "[D loss: 1.089188] [G loss: 1.291580]\n",
      "[D loss: 0.681309] [G loss: 1.405077]\n",
      "[D loss: 0.941476] [G loss: 1.330049]\n",
      "[D loss: 1.020278] [G loss: 1.238106]\n",
      "[D loss: 1.039496] [G loss: 1.763804]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.973033] [G loss: 1.322256]\n",
      "[D loss: 0.718038] [G loss: 1.383124]\n",
      "[D loss: 0.974098] [G loss: 1.496317]\n",
      "[D loss: 0.775806] [G loss: 1.322430]\n",
      "[D loss: 0.878816] [G loss: 1.585878]\n",
      "[D loss: 0.882102] [G loss: 1.484204]\n",
      "[D loss: 0.822817] [G loss: 1.197706]\n",
      "[D loss: 0.784820] [G loss: 1.371243]\n",
      "[D loss: 0.884760] [G loss: 1.618843]\n",
      "[D loss: 0.682424] [G loss: 1.654297]\n",
      "[D loss: 0.769585] [G loss: 1.480133]\n",
      "[D loss: 0.894904] [G loss: 1.453875]\n",
      "[D loss: 0.697888] [G loss: 1.541837]\n",
      "[D loss: 0.791134] [G loss: 1.451295]\n",
      "[D loss: 0.716929] [G loss: 1.384819]\n",
      "[D loss: 0.833116] [G loss: 1.408438]\n",
      "[D loss: 0.686675] [G loss: 1.608372]\n",
      "[D loss: 0.767743] [G loss: 1.529180]\n",
      "[D loss: 0.757133] [G loss: 1.460553]\n",
      "[D loss: 0.666189] [G loss: 1.390744]\n",
      "[D loss: 0.855312] [G loss: 1.729333]\n",
      "[D loss: 0.894396] [G loss: 1.444435]\n",
      "[D loss: 0.749730] [G loss: 1.627673]\n",
      "[D loss: 1.034158] [G loss: 1.777272]\n",
      "[D loss: 0.944155] [G loss: 1.311262]\n",
      "[D loss: 0.728620] [G loss: 1.363542]\n",
      "[D loss: 0.803680] [G loss: 1.349770]\n",
      "[D loss: 0.685718] [G loss: 1.523703]\n",
      "[D loss: 0.893773] [G loss: 1.457940]\n",
      "[D loss: 0.915007] [G loss: 1.434001]\n",
      "[D loss: 0.831088] [G loss: 1.375106]\n",
      "[D loss: 0.749922] [G loss: 1.140466]\n",
      "[D loss: 0.756764] [G loss: 1.616914]\n",
      "[D loss: 0.743360] [G loss: 1.617672]\n",
      "[D loss: 0.773078] [G loss: 1.446973]\n",
      "[D loss: 0.972295] [G loss: 1.361084]\n",
      "[D loss: 0.880910] [G loss: 1.657265]\n",
      "[D loss: 0.741601] [G loss: 1.621774]\n",
      "[D loss: 0.908071] [G loss: 1.343511]\n",
      "[D loss: 0.830214] [G loss: 1.476166]\n",
      "[D loss: 0.747272] [G loss: 1.382815]\n",
      "[D loss: 0.967439] [G loss: 1.280291]\n",
      "[D loss: 0.748339] [G loss: 1.498153]\n",
      "[D loss: 0.874807] [G loss: 1.518591]\n",
      "[D loss: 0.838193] [G loss: 1.592656]\n",
      "[D loss: 0.637199] [G loss: 1.565762]\n",
      "[D loss: 0.931688] [G loss: 1.538202]\n",
      "[D loss: 0.748507] [G loss: 1.600131]\n",
      "[D loss: 0.594307] [G loss: 1.547219]\n",
      "[D loss: 0.821387] [G loss: 1.497840]\n",
      "[D loss: 0.794371] [G loss: 1.549052]\n",
      "[D loss: 0.803618] [G loss: 1.719789]\n",
      "[D loss: 0.837366] [G loss: 1.629539]\n",
      "[D loss: 0.936592] [G loss: 1.455250]\n",
      "[D loss: 0.934240] [G loss: 1.397147]\n",
      "[D loss: 1.098995] [G loss: 1.511109]\n",
      "[D loss: 0.782738] [G loss: 1.521387]\n",
      "[D loss: 1.064790] [G loss: 1.424226]\n",
      "[D loss: 0.824007] [G loss: 1.711820]\n",
      "[D loss: 0.847676] [G loss: 1.370763]\n",
      "[D loss: 0.655272] [G loss: 1.612820]\n",
      "[D loss: 0.826158] [G loss: 1.352505]\n",
      "[D loss: 0.833202] [G loss: 1.387238]\n",
      "[D loss: 0.660288] [G loss: 1.537723]\n",
      "[D loss: 0.903013] [G loss: 1.568353]\n",
      "[D loss: 0.745408] [G loss: 1.601601]\n",
      "[D loss: 0.762701] [G loss: 1.471686]\n",
      "[D loss: 0.834293] [G loss: 1.513116]\n",
      "[D loss: 0.555988] [G loss: 1.599048]\n",
      "[D loss: 0.873431] [G loss: 1.448528]\n",
      "[D loss: 0.625429] [G loss: 1.459952]\n",
      "[D loss: 0.771987] [G loss: 1.293325]\n",
      "[D loss: 0.845157] [G loss: 1.677468]\n",
      "[D loss: 0.798449] [G loss: 1.446711]\n",
      "[D loss: 0.806084] [G loss: 1.594036]\n",
      "[D loss: 0.914953] [G loss: 1.620800]\n",
      "[D loss: 0.851743] [G loss: 1.226813]\n",
      "[D loss: 0.855117] [G loss: 1.649702]\n",
      "[D loss: 0.834075] [G loss: 1.416489]\n",
      "[D loss: 0.685281] [G loss: 1.335828]\n",
      "[D loss: 0.933849] [G loss: 1.215322]\n",
      "[D loss: 0.577149] [G loss: 1.719435]\n",
      "[D loss: 0.815921] [G loss: 1.690615]\n",
      "[D loss: 0.868049] [G loss: 1.569622]\n",
      "[D loss: 0.843848] [G loss: 1.497143]\n",
      "[D loss: 0.820230] [G loss: 1.509384]\n",
      "[D loss: 1.150633] [G loss: 1.103736]\n",
      "[D loss: 0.879668] [G loss: 1.327261]\n",
      "[D loss: 0.673903] [G loss: 1.582005]\n",
      "[D loss: 0.596352] [G loss: 1.668930]\n",
      "[D loss: 0.831233] [G loss: 1.649212]\n",
      "[D loss: 0.788935] [G loss: 1.618140]\n",
      "[D loss: 1.190177] [G loss: 1.322863]\n",
      "[D loss: 0.774813] [G loss: 1.398681]\n",
      "[D loss: 0.804971] [G loss: 1.373469]\n",
      "[D loss: 0.575939] [G loss: 1.738333]\n",
      "[D loss: 0.977753] [G loss: 1.570719]\n",
      "[D loss: 0.681831] [G loss: 1.640296]\n",
      "[D loss: 0.656924] [G loss: 1.657085]\n",
      "[D loss: 0.955244] [G loss: 1.302026]\n",
      "[D loss: 0.931054] [G loss: 1.502303]\n",
      "[D loss: 0.681315] [G loss: 1.553013]\n",
      "[D loss: 0.868520] [G loss: 1.389770]\n",
      "[D loss: 0.750230] [G loss: 1.441593]\n",
      "[D loss: 0.906073] [G loss: 1.832176]\n",
      "[D loss: 0.644164] [G loss: 1.541127]\n",
      "[D loss: 1.000174] [G loss: 1.479112]\n",
      "[D loss: 0.544806] [G loss: 1.574354]\n",
      "[D loss: 0.843691] [G loss: 1.379046]\n",
      "[D loss: 0.630520] [G loss: 1.549607]\n",
      "[D loss: 0.836521] [G loss: 1.557294]\n",
      "[D loss: 0.651708] [G loss: 1.651071]\n",
      "[D loss: 0.934506] [G loss: 1.435234]\n",
      "[D loss: 0.959160] [G loss: 1.535527]\n",
      "[D loss: 0.946855] [G loss: 1.263407]\n",
      "[D loss: 0.796096] [G loss: 1.408762]\n",
      "[D loss: 0.868361] [G loss: 1.538671]\n",
      "[D loss: 0.770435] [G loss: 1.398862]\n",
      "[D loss: 0.674302] [G loss: 1.465495]\n",
      "[D loss: 1.047284] [G loss: 1.539810]\n",
      "[D loss: 1.117806] [G loss: 1.454503]\n",
      "[D loss: 0.747871] [G loss: 1.513188]\n",
      "[D loss: 0.731955] [G loss: 1.311489]\n",
      "[D loss: 0.967360] [G loss: 1.137991]\n",
      "[D loss: 0.968092] [G loss: 1.260768]\n",
      "[D loss: 0.751871] [G loss: 1.383976]\n",
      "[D loss: 0.614800] [G loss: 1.562905]\n",
      "[D loss: 0.860690] [G loss: 1.463899]\n",
      "[D loss: 0.874607] [G loss: 1.427730]\n",
      "[D loss: 0.691959] [G loss: 1.417691]\n",
      "[D loss: 0.915467] [G loss: 1.211639]\n",
      "[D loss: 0.820508] [G loss: 1.397776]\n",
      "[D loss: 0.871044] [G loss: 1.374211]\n",
      "[D loss: 0.625647] [G loss: 1.441554]\n",
      "[D loss: 0.856811] [G loss: 1.532924]\n",
      "[D loss: 0.809633] [G loss: 1.599671]\n",
      "[D loss: 0.868075] [G loss: 1.538415]\n",
      "[D loss: 0.526720] [G loss: 1.507530]\n",
      "[D loss: 0.918551] [G loss: 1.436412]\n",
      "[D loss: 0.871410] [G loss: 1.493708]\n",
      "[D loss: 0.808338] [G loss: 1.550812]\n",
      "[D loss: 0.882815] [G loss: 1.341051]\n",
      "[D loss: 0.793216] [G loss: 1.424600]\n",
      "[D loss: 0.923513] [G loss: 1.487315]\n",
      "[D loss: 0.867921] [G loss: 1.437523]\n",
      "[D loss: 0.698838] [G loss: 1.539131]\n",
      "[D loss: 0.711287] [G loss: 1.359995]\n",
      "[D loss: 0.670378] [G loss: 1.550825]\n",
      "[D loss: 1.031907] [G loss: 1.510634]\n",
      "[D loss: 1.109690] [G loss: 1.368369]\n",
      "[D loss: 0.965003] [G loss: 1.194275]\n",
      "[D loss: 1.027754] [G loss: 1.299242]\n",
      "[D loss: 0.972433] [G loss: 1.356328]\n",
      "[D loss: 0.825464] [G loss: 1.541097]\n",
      "[D loss: 0.862028] [G loss: 1.530136]\n",
      "[D loss: 0.848552] [G loss: 1.658680]\n",
      "[D loss: 0.729609] [G loss: 1.502038]\n",
      "[D loss: 0.899922] [G loss: 1.257338]\n",
      "[D loss: 0.769625] [G loss: 1.603021]\n",
      "[D loss: 0.813443] [G loss: 1.523607]\n",
      "[D loss: 0.803345] [G loss: 1.628338]\n",
      "[D loss: 0.724219] [G loss: 1.488314]\n",
      "[D loss: 0.611884] [G loss: 1.637557]\n",
      "[D loss: 0.941908] [G loss: 1.515392]\n",
      "[D loss: 1.082370] [G loss: 1.447765]\n",
      "[D loss: 0.679286] [G loss: 1.485202]\n",
      "[D loss: 0.846105] [G loss: 1.372805]\n",
      "[D loss: 0.755953] [G loss: 1.576346]\n",
      "[D loss: 0.895431] [G loss: 1.449682]\n",
      "[D loss: 0.857127] [G loss: 1.268692]\n",
      "[D loss: 0.526200] [G loss: 1.418925]\n",
      "[D loss: 0.978269] [G loss: 1.561309]\n",
      "[D loss: 0.838942] [G loss: 1.534319]\n",
      "[D loss: 0.758961] [G loss: 1.559656]\n",
      "[D loss: 0.910523] [G loss: 1.426856]\n",
      "[D loss: 0.938161] [G loss: 1.686654]\n",
      "[D loss: 0.864030] [G loss: 1.629232]\n",
      "[D loss: 0.722516] [G loss: 1.649456]\n",
      "[D loss: 0.821357] [G loss: 1.457952]\n",
      "[D loss: 0.834617] [G loss: 1.521941]\n",
      "[D loss: 0.954067] [G loss: 1.460278]\n",
      "[D loss: 0.989809] [G loss: 1.446243]\n",
      "[D loss: 0.803172] [G loss: 1.483470]\n",
      "[D loss: 0.812179] [G loss: 1.407506]\n",
      "[D loss: 0.673132] [G loss: 1.433030]\n",
      "[D loss: 1.037494] [G loss: 1.274226]\n",
      "[D loss: 0.737199] [G loss: 1.535843]\n",
      "[D loss: 1.204431] [G loss: 1.382118]\n",
      "[D loss: 0.924934] [G loss: 1.438739]\n",
      "[D loss: 0.654256] [G loss: 1.624914]\n",
      "[D loss: 0.779730] [G loss: 1.481136]\n",
      "[D loss: 0.647860] [G loss: 1.490116]\n",
      "[D loss: 0.876021] [G loss: 1.345681]\n",
      "[D loss: 0.668324] [G loss: 1.550807]\n",
      "[D loss: 0.776798] [G loss: 1.388052]\n",
      "[D loss: 0.782925] [G loss: 1.366500]\n",
      "[D loss: 0.812895] [G loss: 1.524376]\n",
      "[D loss: 0.777933] [G loss: 1.494216]\n",
      "[D loss: 0.887719] [G loss: 1.514811]\n",
      "[D loss: 0.813277] [G loss: 1.262803]\n",
      "[D loss: 0.982022] [G loss: 1.465436]\n",
      "[D loss: 0.919228] [G loss: 1.621475]\n",
      "[D loss: 0.716233] [G loss: 1.807606]\n",
      "[D loss: 0.853666] [G loss: 1.825819]\n",
      "[D loss: 0.874043] [G loss: 1.595022]\n",
      "[D loss: 1.169098] [G loss: 1.212237]\n",
      "[D loss: 0.842752] [G loss: 1.254198]\n",
      "[D loss: 1.013348] [G loss: 1.391362]\n",
      "[D loss: 0.711260] [G loss: 1.632556]\n",
      "[D loss: 1.054268] [G loss: 1.690457]\n",
      "[D loss: 0.963841] [G loss: 1.317011]\n",
      "[D loss: 1.083771] [G loss: 1.179592]\n",
      "[D loss: 0.735651] [G loss: 1.232556]\n",
      "[D loss: 0.687488] [G loss: 1.432247]\n",
      "[D loss: 1.044211] [G loss: 1.273597]\n",
      "[D loss: 0.733224] [G loss: 1.264334]\n",
      "[D loss: 0.875636] [G loss: 1.575937]\n",
      "[D loss: 0.697591] [G loss: 1.419719]\n",
      "[D loss: 0.904309] [G loss: 1.435220]\n",
      "[D loss: 0.715336] [G loss: 1.548496]\n",
      "[D loss: 0.844015] [G loss: 1.503218]\n",
      "[D loss: 0.792334] [G loss: 1.252355]\n",
      "[D loss: 0.835255] [G loss: 1.292437]\n",
      "[D loss: 0.763620] [G loss: 1.555979]\n",
      "[D loss: 0.826965] [G loss: 1.729580]\n",
      "[D loss: 0.828085] [G loss: 1.625376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.721394] [G loss: 1.269617]\n",
      "[D loss: 0.823630] [G loss: 1.362950]\n",
      "[D loss: 0.815953] [G loss: 1.531024]\n",
      "[D loss: 0.762591] [G loss: 1.603981]\n",
      "[D loss: 0.734275] [G loss: 1.522734]\n",
      "[D loss: 1.021155] [G loss: 1.389312]\n",
      "[D loss: 0.707389] [G loss: 1.568059]\n",
      "[D loss: 0.873823] [G loss: 1.507132]\n",
      "[D loss: 0.711108] [G loss: 1.583645]\n",
      "[D loss: 0.793821] [G loss: 1.656942]\n",
      "[D loss: 0.725478] [G loss: 1.382215]\n",
      "[D loss: 0.804135] [G loss: 1.464440]\n",
      "[D loss: 0.888224] [G loss: 1.381674]\n",
      "[D loss: 0.832468] [G loss: 1.391651]\n",
      "[D loss: 0.970886] [G loss: 1.402567]\n",
      "[D loss: 1.001773] [G loss: 1.506618]\n",
      "[D loss: 0.854911] [G loss: 1.572357]\n",
      "[D loss: 0.875783] [G loss: 1.665189]\n",
      "[D loss: 0.825051] [G loss: 1.432866]\n",
      "[D loss: 0.778156] [G loss: 1.379157]\n",
      "[D loss: 0.796321] [G loss: 1.317710]\n",
      "[D loss: 0.797101] [G loss: 1.446668]\n",
      "[D loss: 0.739718] [G loss: 1.489555]\n",
      "[D loss: 0.699292] [G loss: 1.460978]\n",
      "[D loss: 0.819749] [G loss: 1.317608]\n",
      "[D loss: 0.990389] [G loss: 1.257586]\n",
      "[D loss: 0.735863] [G loss: 1.528169]\n",
      "[D loss: 0.909293] [G loss: 1.285000]\n",
      "[D loss: 0.795611] [G loss: 1.551017]\n",
      "[D loss: 0.599787] [G loss: 1.631531]\n",
      "[D loss: 0.889755] [G loss: 1.603069]\n",
      "[D loss: 0.757063] [G loss: 1.625969]\n",
      "[D loss: 0.976586] [G loss: 1.642790]\n",
      "[D loss: 0.728963] [G loss: 1.420626]\n",
      "[D loss: 0.793400] [G loss: 1.529204]\n",
      "[D loss: 0.701260] [G loss: 1.579787]\n",
      "[D loss: 1.088201] [G loss: 1.345031]\n",
      "[D loss: 0.738430] [G loss: 1.373775]\n",
      "[D loss: 0.666141] [G loss: 1.646262]\n",
      "[D loss: 0.781394] [G loss: 1.526412]\n",
      "[D loss: 0.979273] [G loss: 1.397649]\n",
      "[D loss: 0.932727] [G loss: 1.506710]\n",
      "[D loss: 0.667482] [G loss: 1.521450]\n",
      "[D loss: 0.806645] [G loss: 1.465678]\n",
      "[D loss: 0.819949] [G loss: 1.391218]\n",
      "[D loss: 0.729987] [G loss: 1.549569]\n",
      "[D loss: 1.026545] [G loss: 1.377041]\n",
      "[D loss: 0.973964] [G loss: 1.464772]\n",
      "[D loss: 0.737464] [G loss: 1.549380]\n",
      "[D loss: 0.644560] [G loss: 1.513479]\n",
      "[D loss: 0.826433] [G loss: 1.397914]\n",
      "[D loss: 0.804278] [G loss: 1.482209]\n",
      "[D loss: 0.861902] [G loss: 1.492524]\n",
      "[D loss: 0.778470] [G loss: 1.501595]\n",
      "[D loss: 0.869548] [G loss: 1.528518]\n",
      "[D loss: 0.746790] [G loss: 1.424170]\n",
      "[D loss: 0.793373] [G loss: 1.325895]\n",
      "[D loss: 0.695201] [G loss: 1.301688]\n",
      "[D loss: 0.908519] [G loss: 1.407622]\n",
      "[D loss: 0.818239] [G loss: 1.477473]\n",
      "[D loss: 1.075535] [G loss: 1.640127]\n",
      "[D loss: 0.622026] [G loss: 1.452198]\n",
      "[D loss: 0.676763] [G loss: 1.654797]\n",
      "[D loss: 0.622765] [G loss: 1.365583]\n",
      "[D loss: 0.842011] [G loss: 1.463394]\n",
      "[D loss: 0.907326] [G loss: 1.552508]\n",
      "[D loss: 0.977830] [G loss: 1.537344]\n",
      "[D loss: 0.851531] [G loss: 1.826347]\n",
      "[D loss: 0.889158] [G loss: 1.438848]\n",
      "[D loss: 0.796307] [G loss: 1.509501]\n",
      "[D loss: 0.965875] [G loss: 1.202997]\n",
      "[D loss: 0.950886] [G loss: 1.195199]\n",
      "[D loss: 0.654758] [G loss: 1.710257]\n",
      "[D loss: 0.818179] [G loss: 1.743961]\n",
      "[D loss: 0.671817] [G loss: 1.412360]\n",
      "[D loss: 0.889497] [G loss: 1.510259]\n",
      "[D loss: 0.766480] [G loss: 1.486212]\n",
      "[D loss: 0.919648] [G loss: 1.579145]\n",
      "[D loss: 0.824912] [G loss: 1.390264]\n",
      "[D loss: 0.912155] [G loss: 1.075611]\n",
      "[D loss: 0.663287] [G loss: 1.380800]\n",
      "[D loss: 0.851785] [G loss: 1.544987]\n",
      "[D loss: 0.967554] [G loss: 1.433569]\n",
      "[D loss: 0.655116] [G loss: 1.392720]\n",
      "[D loss: 0.784110] [G loss: 1.467204]\n",
      "[D loss: 0.829316] [G loss: 1.452603]\n",
      "[D loss: 0.748430] [G loss: 1.540309]\n",
      "[D loss: 0.857865] [G loss: 1.423572]\n",
      "[D loss: 0.814467] [G loss: 1.476284]\n",
      "[D loss: 0.786824] [G loss: 1.403502]\n",
      "[D loss: 0.916078] [G loss: 1.446209]\n",
      "[D loss: 0.553928] [G loss: 1.754394]\n",
      "[D loss: 0.931315] [G loss: 1.510503]\n",
      "[D loss: 0.645844] [G loss: 1.551328]\n",
      "[D loss: 0.896580] [G loss: 1.494246]\n",
      "[D loss: 0.903565] [G loss: 1.432886]\n",
      "[D loss: 0.946520] [G loss: 1.607252]\n",
      "[D loss: 0.682662] [G loss: 1.533499]\n",
      "[D loss: 0.588217] [G loss: 1.630187]\n",
      "[D loss: 0.782240] [G loss: 1.492212]\n",
      "[D loss: 0.777172] [G loss: 1.637728]\n",
      "[D loss: 0.690880] [G loss: 1.467630]\n",
      "[D loss: 0.712247] [G loss: 1.529289]\n",
      "[D loss: 1.173735] [G loss: 1.550505]\n",
      "[D loss: 0.966879] [G loss: 1.483248]\n",
      "[D loss: 0.819015] [G loss: 1.254374]\n",
      "[D loss: 0.921641] [G loss: 1.438097]\n",
      "[D loss: 0.693145] [G loss: 1.743511]\n",
      "[D loss: 0.817830] [G loss: 1.448107]\n",
      "[D loss: 0.673612] [G loss: 1.665136]\n",
      "[D loss: 0.877809] [G loss: 1.615076]\n",
      "[D loss: 0.685480] [G loss: 1.549674]\n",
      "[D loss: 1.050404] [G loss: 1.406794]\n",
      "[D loss: 0.992304] [G loss: 1.305416]\n",
      "[D loss: 0.715119] [G loss: 1.400257]\n",
      "[D loss: 0.983242] [G loss: 1.622703]\n",
      "[D loss: 0.664467] [G loss: 1.455840]\n",
      "[D loss: 0.821851] [G loss: 1.785337]\n",
      "[D loss: 0.915241] [G loss: 1.571389]\n",
      "[D loss: 0.818470] [G loss: 1.653049]\n",
      "[D loss: 0.790394] [G loss: 1.355155]\n",
      "[D loss: 0.736750] [G loss: 1.380792]\n",
      "[D loss: 0.925283] [G loss: 1.582145]\n",
      "[D loss: 0.812322] [G loss: 1.482620]\n",
      "[D loss: 0.651882] [G loss: 1.412664]\n",
      "[D loss: 0.906931] [G loss: 1.463129]\n",
      "[D loss: 0.778382] [G loss: 1.263441]\n",
      "[D loss: 1.066210] [G loss: 1.486893]\n",
      "[D loss: 0.842906] [G loss: 1.638190]\n",
      "[D loss: 0.897553] [G loss: 1.494334]\n",
      "[D loss: 0.900948] [G loss: 1.274534]\n",
      "[D loss: 0.804640] [G loss: 1.505311]\n",
      "[D loss: 0.807974] [G loss: 1.506337]\n",
      "[D loss: 0.692137] [G loss: 1.404182]\n",
      "[D loss: 0.650844] [G loss: 1.275511]\n",
      "[D loss: 0.631083] [G loss: 1.542488]\n",
      "[D loss: 0.652369] [G loss: 1.431616]\n",
      "[D loss: 0.911072] [G loss: 1.590447]\n",
      "[D loss: 0.904607] [G loss: 1.549567]\n",
      "[D loss: 0.854941] [G loss: 1.448455]\n",
      "[D loss: 0.775197] [G loss: 1.419932]\n",
      "[D loss: 0.884422] [G loss: 1.472813]\n",
      "[D loss: 0.999048] [G loss: 1.233501]\n",
      "[D loss: 1.121951] [G loss: 1.573431]\n",
      "[D loss: 0.723987] [G loss: 1.451649]\n",
      "[D loss: 0.969377] [G loss: 1.748725]\n",
      "[D loss: 0.795772] [G loss: 1.181792]\n",
      "[D loss: 0.867884] [G loss: 1.272912]\n",
      "[D loss: 0.918068] [G loss: 1.345633]\n",
      "[D loss: 0.881958] [G loss: 1.586558]\n",
      "[D loss: 0.977215] [G loss: 1.764199]\n",
      "[D loss: 0.727932] [G loss: 1.444739]\n",
      "[D loss: 1.154505] [G loss: 1.290193]\n",
      "[D loss: 0.984967] [G loss: 1.204108]\n",
      "[D loss: 0.660600] [G loss: 1.471220]\n",
      "[D loss: 0.980157] [G loss: 1.326607]\n",
      "[D loss: 0.873954] [G loss: 1.336591]\n",
      "[D loss: 1.008727] [G loss: 1.433765]\n",
      "[D loss: 0.920761] [G loss: 1.314950]\n",
      "[D loss: 0.846201] [G loss: 1.318869]\n",
      "[D loss: 0.715830] [G loss: 1.229950]\n",
      "[D loss: 0.860221] [G loss: 1.595017]\n",
      "[D loss: 0.754798] [G loss: 1.386639]\n",
      "[D loss: 0.761557] [G loss: 1.351983]\n",
      "[D loss: 0.807845] [G loss: 1.417094]\n",
      "[D loss: 0.913047] [G loss: 1.658133]\n",
      "[D loss: 0.779994] [G loss: 1.446489]\n",
      "[D loss: 0.771884] [G loss: 1.701225]\n",
      "[D loss: 0.859424] [G loss: 1.554685]\n",
      "[D loss: 0.642819] [G loss: 1.326566]\n",
      "[D loss: 0.974198] [G loss: 1.403940]\n",
      "[D loss: 0.882679] [G loss: 1.418809]\n",
      "[D loss: 0.998406] [G loss: 1.475076]\n",
      "[D loss: 0.952716] [G loss: 1.313510]\n",
      "[D loss: 0.714991] [G loss: 1.227705]\n",
      "[D loss: 0.830047] [G loss: 1.266724]\n",
      "[D loss: 0.872483] [G loss: 1.491625]\n",
      "[D loss: 0.931305] [G loss: 1.649514]\n",
      "[D loss: 0.703357] [G loss: 1.376528]\n",
      "[D loss: 0.943495] [G loss: 1.322420]\n",
      "[D loss: 0.757356] [G loss: 1.478244]\n",
      "[D loss: 0.931500] [G loss: 1.470253]\n",
      "[D loss: 0.737983] [G loss: 1.342281]\n",
      "[D loss: 0.757788] [G loss: 1.379012]\n",
      "[D loss: 0.751921] [G loss: 1.677776]\n",
      "[D loss: 0.875408] [G loss: 1.463841]\n",
      "[D loss: 0.852846] [G loss: 1.618148]\n",
      "[D loss: 0.874670] [G loss: 1.466311]\n",
      "[D loss: 0.801376] [G loss: 1.366760]\n",
      "[D loss: 0.654371] [G loss: 1.716197]\n",
      "[D loss: 0.974340] [G loss: 1.524718]\n",
      "[D loss: 0.908282] [G loss: 1.204835]\n",
      "[D loss: 0.854714] [G loss: 1.505171]\n",
      "[D loss: 0.782640] [G loss: 1.486353]\n",
      "[D loss: 0.806639] [G loss: 1.588236]\n",
      "[D loss: 0.896981] [G loss: 1.591720]\n",
      "[D loss: 0.830497] [G loss: 1.589446]\n",
      "[D loss: 0.674260] [G loss: 1.425954]\n",
      "[D loss: 0.924522] [G loss: 1.515318]\n",
      "[D loss: 0.567933] [G loss: 1.582028]\n",
      "[D loss: 0.881559] [G loss: 1.607030]\n",
      "[D loss: 0.711034] [G loss: 1.455123]\n",
      "[D loss: 0.850648] [G loss: 1.442934]\n",
      "[D loss: 0.625146] [G loss: 1.475998]\n",
      "[D loss: 0.719773] [G loss: 1.424380]\n",
      "[D loss: 0.623248] [G loss: 1.793239]\n",
      "[D loss: 1.007526] [G loss: 1.680990]\n",
      "[D loss: 0.791134] [G loss: 1.551730]\n",
      "[D loss: 0.690126] [G loss: 1.627210]\n",
      "[D loss: 0.762052] [G loss: 1.435389]\n",
      "[D loss: 0.771727] [G loss: 1.454322]\n",
      "[D loss: 0.667607] [G loss: 1.608000]\n",
      "[D loss: 0.778357] [G loss: 1.726419]\n",
      "[D loss: 0.709164] [G loss: 1.641559]\n",
      "[D loss: 0.766837] [G loss: 1.791502]\n",
      "[D loss: 0.990223] [G loss: 1.624270]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.637916] [G loss: 1.605707]\n",
      "[D loss: 0.831458] [G loss: 1.485492]\n",
      "[D loss: 0.989010] [G loss: 1.495962]\n",
      "[D loss: 0.743006] [G loss: 1.571141]\n",
      "[D loss: 0.728190] [G loss: 1.639176]\n",
      "[D loss: 0.644745] [G loss: 1.673696]\n",
      "[D loss: 1.225340] [G loss: 1.459177]\n",
      "[D loss: 1.013420] [G loss: 1.561471]\n",
      "[D loss: 0.793763] [G loss: 1.475322]\n",
      "[D loss: 0.824363] [G loss: 1.486661]\n",
      "[D loss: 0.746938] [G loss: 1.366657]\n",
      "[D loss: 0.966655] [G loss: 1.240790]\n",
      "[D loss: 0.761551] [G loss: 1.536677]\n",
      "[D loss: 0.683535] [G loss: 1.460068]\n",
      "[D loss: 0.568886] [G loss: 1.710742]\n",
      "[D loss: 0.836529] [G loss: 1.435346]\n",
      "[D loss: 0.888873] [G loss: 1.333773]\n",
      "[D loss: 0.855509] [G loss: 1.479316]\n",
      "[D loss: 0.685113] [G loss: 1.604492]\n",
      "[D loss: 0.697513] [G loss: 1.650005]\n",
      "[D loss: 0.866236] [G loss: 1.495722]\n",
      "[D loss: 0.738262] [G loss: 1.662735]\n",
      "[D loss: 0.742826] [G loss: 1.685139]\n",
      "[D loss: 0.893287] [G loss: 1.530043]\n",
      "[D loss: 0.933438] [G loss: 1.420190]\n",
      "[D loss: 0.641056] [G loss: 1.525905]\n",
      "[D loss: 0.638038] [G loss: 1.816132]\n",
      "[D loss: 0.941457] [G loss: 1.390366]\n",
      "[D loss: 0.845812] [G loss: 1.609631]\n",
      "[D loss: 1.118097] [G loss: 1.538943]\n",
      "[D loss: 1.000956] [G loss: 1.407682]\n",
      "[D loss: 0.795822] [G loss: 1.684427]\n",
      "[D loss: 0.969832] [G loss: 1.427996]\n",
      "[D loss: 0.939547] [G loss: 1.530889]\n",
      "[D loss: 0.893323] [G loss: 1.346292]\n",
      "[D loss: 0.762927] [G loss: 1.348938]\n",
      "[D loss: 1.047804] [G loss: 1.392466]\n",
      "[D loss: 0.734087] [G loss: 1.417731]\n",
      "[D loss: 0.935116] [G loss: 1.314515]\n",
      "[D loss: 0.803792] [G loss: 1.454611]\n",
      "[D loss: 0.860231] [G loss: 1.523861]\n",
      "[D loss: 0.835845] [G loss: 1.302165]\n",
      "[D loss: 1.064365] [G loss: 1.329623]\n",
      "[D loss: 0.818322] [G loss: 1.533209]\n",
      "[D loss: 0.829039] [G loss: 1.710982]\n",
      "[D loss: 0.773385] [G loss: 1.506069]\n",
      "[D loss: 0.681685] [G loss: 1.525823]\n",
      "[D loss: 0.658892] [G loss: 1.370621]\n",
      "[D loss: 0.886791] [G loss: 1.381748]\n",
      "[D loss: 0.930185] [G loss: 1.455747]\n",
      "[D loss: 0.707704] [G loss: 1.620941]\n",
      "[D loss: 0.830794] [G loss: 1.614323]\n",
      "[D loss: 0.869765] [G loss: 1.545430]\n",
      "[D loss: 0.840065] [G loss: 1.584511]\n",
      "[D loss: 1.029686] [G loss: 1.438040]\n",
      "[D loss: 0.899752] [G loss: 1.401962]\n",
      "[D loss: 0.649065] [G loss: 1.595980]\n",
      "[D loss: 0.875108] [G loss: 1.285333]\n",
      "[D loss: 0.720332] [G loss: 1.379252]\n",
      "[D loss: 0.899216] [G loss: 1.481549]\n",
      "[D loss: 0.687584] [G loss: 1.687431]\n",
      "[D loss: 0.784193] [G loss: 1.466226]\n",
      "[D loss: 0.812755] [G loss: 1.439213]\n",
      "[D loss: 1.180686] [G loss: 1.393682]\n",
      "[D loss: 0.941479] [G loss: 1.677744]\n",
      "[D loss: 1.033712] [G loss: 1.381740]\n",
      "[D loss: 0.912007] [G loss: 1.386106]\n",
      "[D loss: 0.866579] [G loss: 1.625877]\n",
      "[D loss: 0.723040] [G loss: 1.605774]\n",
      "[D loss: 0.879328] [G loss: 1.314789]\n",
      "[D loss: 0.801973] [G loss: 1.365227]\n",
      "[D loss: 0.815874] [G loss: 1.604347]\n",
      "[D loss: 0.758716] [G loss: 1.620391]\n",
      "[D loss: 0.627209] [G loss: 1.631431]\n",
      "[D loss: 0.972979] [G loss: 1.574769]\n",
      "[D loss: 0.929101] [G loss: 1.475186]\n",
      "[D loss: 0.848000] [G loss: 1.544448]\n",
      "[D loss: 0.836487] [G loss: 1.432724]\n",
      "[D loss: 0.711094] [G loss: 1.346244]\n",
      "[D loss: 0.837058] [G loss: 1.180520]\n",
      "[D loss: 0.675988] [G loss: 1.394300]\n",
      "[D loss: 0.810754] [G loss: 1.355229]\n",
      "[D loss: 0.670154] [G loss: 1.423205]\n",
      "[D loss: 0.898629] [G loss: 1.490201]\n",
      "[D loss: 0.883181] [G loss: 1.500919]\n",
      "[D loss: 0.883574] [G loss: 1.366108]\n",
      "[D loss: 0.880324] [G loss: 1.352416]\n",
      "[D loss: 0.738478] [G loss: 1.686894]\n",
      "[D loss: 1.040318] [G loss: 1.364656]\n",
      "[D loss: 0.894641] [G loss: 1.620675]\n",
      "[D loss: 0.713930] [G loss: 1.554628]\n",
      "[D loss: 0.784149] [G loss: 1.531129]\n",
      "[D loss: 0.605162] [G loss: 1.601898]\n",
      "[D loss: 0.867367] [G loss: 1.539639]\n",
      "[D loss: 0.834534] [G loss: 1.264971]\n",
      "[D loss: 0.839180] [G loss: 1.483786]\n",
      "[D loss: 0.867219] [G loss: 1.526086]\n",
      "[D loss: 0.778837] [G loss: 1.560620]\n",
      "[D loss: 0.814942] [G loss: 1.496933]\n",
      "[D loss: 0.774491] [G loss: 1.646722]\n",
      "[D loss: 0.586415] [G loss: 1.615128]\n",
      "[D loss: 0.909866] [G loss: 1.559910]\n",
      "[D loss: 0.882238] [G loss: 1.421557]\n",
      "[D loss: 0.733432] [G loss: 1.496841]\n",
      "[D loss: 0.831535] [G loss: 1.497898]\n",
      "[D loss: 0.748308] [G loss: 1.454701]\n",
      "[D loss: 0.821295] [G loss: 1.416771]\n",
      "[D loss: 0.854097] [G loss: 1.471915]\n",
      "[D loss: 0.896622] [G loss: 1.577191]\n",
      "[D loss: 0.766391] [G loss: 1.455565]\n",
      "[D loss: 0.960191] [G loss: 1.461003]\n",
      "[D loss: 0.708224] [G loss: 1.596423]\n",
      "[D loss: 0.999963] [G loss: 1.438473]\n",
      "[D loss: 0.955369] [G loss: 1.277158]\n",
      "[D loss: 0.856538] [G loss: 1.426182]\n",
      "[D loss: 0.663158] [G loss: 1.430559]\n",
      "[D loss: 1.019300] [G loss: 1.362231]\n",
      "[D loss: 0.786319] [G loss: 1.579991]\n",
      "[D loss: 0.871495] [G loss: 1.519819]\n",
      "[D loss: 0.946852] [G loss: 1.278104]\n",
      "[D loss: 0.652554] [G loss: 1.369694]\n",
      "[D loss: 0.671234] [G loss: 1.510635]\n",
      "[D loss: 0.812649] [G loss: 1.319781]\n",
      "[D loss: 0.827586] [G loss: 1.687520]\n",
      "[D loss: 0.634026] [G loss: 1.635302]\n",
      "[D loss: 0.850415] [G loss: 1.439235]\n",
      "[D loss: 0.729843] [G loss: 1.381433]\n",
      "[D loss: 0.952804] [G loss: 1.411550]\n",
      "[D loss: 0.803067] [G loss: 1.377156]\n",
      "[D loss: 0.780810] [G loss: 1.632315]\n",
      "[D loss: 0.741824] [G loss: 1.475814]\n",
      "[D loss: 0.800349] [G loss: 1.428408]\n",
      "[D loss: 0.631001] [G loss: 1.754317]\n",
      "[D loss: 0.741595] [G loss: 1.350814]\n",
      "[D loss: 0.654321] [G loss: 1.574358]\n",
      "[D loss: 0.848239] [G loss: 1.677858]\n",
      "[D loss: 0.795858] [G loss: 1.496971]\n",
      "[D loss: 0.670686] [G loss: 1.483558]\n",
      "[D loss: 0.762297] [G loss: 1.587411]\n",
      "[D loss: 0.681173] [G loss: 1.921551]\n",
      "[D loss: 0.920890] [G loss: 1.500888]\n",
      "[D loss: 0.947556] [G loss: 1.498106]\n",
      "[D loss: 0.717084] [G loss: 1.689932]\n",
      "[D loss: 0.971417] [G loss: 1.458247]\n",
      "[D loss: 0.830233] [G loss: 1.493572]\n",
      "[D loss: 0.743255] [G loss: 1.519223]\n",
      "[D loss: 1.014076] [G loss: 1.556175]\n",
      "[D loss: 0.993730] [G loss: 1.317125]\n",
      "[D loss: 0.760053] [G loss: 1.404364]\n",
      "[D loss: 0.909411] [G loss: 1.426147]\n",
      "[D loss: 0.697608] [G loss: 1.465657]\n",
      "[D loss: 1.004891] [G loss: 1.393238]\n",
      "[D loss: 0.855339] [G loss: 1.483152]\n",
      "[D loss: 0.959621] [G loss: 1.187666]\n",
      "[D loss: 0.679407] [G loss: 1.339514]\n",
      "[D loss: 0.737833] [G loss: 1.551573]\n",
      "[D loss: 0.768944] [G loss: 1.587456]\n",
      "[D loss: 0.887198] [G loss: 1.387934]\n",
      "[D loss: 0.515545] [G loss: 1.490470]\n",
      "[D loss: 0.836967] [G loss: 1.454049]\n",
      "[D loss: 0.720570] [G loss: 1.525297]\n",
      "[D loss: 0.742733] [G loss: 1.768288]\n",
      "[D loss: 0.909359] [G loss: 1.541141]\n",
      "[D loss: 0.987421] [G loss: 1.388548]\n",
      "[D loss: 0.774196] [G loss: 1.621316]\n",
      "[D loss: 0.532140] [G loss: 1.805950]\n",
      "[D loss: 0.886228] [G loss: 1.625197]\n",
      "[D loss: 0.751437] [G loss: 1.683860]\n",
      "[D loss: 0.958475] [G loss: 1.525447]\n",
      "[D loss: 0.997396] [G loss: 1.469307]\n",
      "[D loss: 0.859835] [G loss: 1.354909]\n",
      "[D loss: 0.722461] [G loss: 1.505527]\n",
      "[D loss: 0.607082] [G loss: 1.531977]\n",
      "[D loss: 0.785925] [G loss: 1.725509]\n",
      "[D loss: 0.741975] [G loss: 1.544451]\n",
      "[D loss: 1.140625] [G loss: 1.241519]\n",
      "[D loss: 0.910167] [G loss: 1.548600]\n",
      "[D loss: 0.865793] [G loss: 1.739265]\n",
      "[D loss: 0.690381] [G loss: 1.681634]\n",
      "[D loss: 0.754105] [G loss: 1.601658]\n",
      "[D loss: 0.837671] [G loss: 1.509302]\n",
      "[D loss: 0.754712] [G loss: 1.272534]\n",
      "[D loss: 0.772488] [G loss: 1.283204]\n",
      "[D loss: 0.635499] [G loss: 1.539501]\n",
      "[D loss: 0.727023] [G loss: 1.636084]\n",
      "[D loss: 0.753030] [G loss: 1.700566]\n",
      "[D loss: 0.592405] [G loss: 1.639342]\n",
      "[D loss: 0.836924] [G loss: 1.449284]\n",
      "[D loss: 0.731990] [G loss: 1.363532]\n",
      "[D loss: 0.743114] [G loss: 1.358398]\n",
      "[D loss: 0.853442] [G loss: 1.406822]\n",
      "[D loss: 0.827572] [G loss: 1.588704]\n",
      "[D loss: 0.720919] [G loss: 1.797510]\n",
      "[D loss: 0.956166] [G loss: 1.869439]\n",
      "[D loss: 0.784257] [G loss: 1.612961]\n",
      "[D loss: 0.885645] [G loss: 1.426633]\n",
      "[D loss: 0.845667] [G loss: 1.491489]\n",
      "[D loss: 0.874130] [G loss: 1.318332]\n",
      "[D loss: 0.917836] [G loss: 1.529980]\n",
      "[D loss: 0.829380] [G loss: 1.440308]\n",
      "[D loss: 0.950757] [G loss: 1.375085]\n",
      "[D loss: 0.604844] [G loss: 1.763790]\n",
      "[D loss: 1.005225] [G loss: 1.682526]\n",
      "[D loss: 1.107898] [G loss: 1.558473]\n",
      "[D loss: 0.866559] [G loss: 1.632342]\n",
      "[D loss: 1.209452] [G loss: 1.261373]\n",
      "[D loss: 0.658163] [G loss: 1.515328]\n",
      "[D loss: 0.975932] [G loss: 1.371530]\n",
      "[D loss: 0.934513] [G loss: 1.357429]\n",
      "[D loss: 0.714955] [G loss: 1.604905]\n",
      "[D loss: 0.892304] [G loss: 1.650394]\n",
      "[D loss: 0.804687] [G loss: 1.340868]\n",
      "[D loss: 0.944218] [G loss: 1.188793]\n",
      "[D loss: 0.887510] [G loss: 1.230550]\n",
      "[D loss: 0.994180] [G loss: 1.494978]\n",
      "[D loss: 0.802012] [G loss: 1.484753]\n",
      "[D loss: 0.809341] [G loss: 1.398820]\n",
      "[D loss: 0.900791] [G loss: 1.270793]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.882131] [G loss: 1.350269]\n",
      "[D loss: 0.578661] [G loss: 1.354682]\n",
      "[D loss: 0.843155] [G loss: 1.177165]\n",
      "[D loss: 0.880585] [G loss: 1.450794]\n",
      "[D loss: 0.707117] [G loss: 1.468475]\n",
      "[D loss: 0.972617] [G loss: 1.393005]\n",
      "[D loss: 0.585024] [G loss: 1.537542]\n",
      "[D loss: 0.987812] [G loss: 1.442192]\n",
      "[D loss: 0.735486] [G loss: 1.435675]\n",
      "[D loss: 0.588407] [G loss: 1.443535]\n",
      "[D loss: 0.815342] [G loss: 1.564431]\n",
      "[D loss: 0.675213] [G loss: 1.651260]\n",
      "[D loss: 0.775571] [G loss: 1.766812]\n",
      "[D loss: 0.730033] [G loss: 1.491336]\n",
      "[D loss: 0.694044] [G loss: 1.383519]\n",
      "[D loss: 0.740558] [G loss: 1.537681]\n",
      "[D loss: 0.782168] [G loss: 1.606340]\n",
      "[D loss: 0.614579] [G loss: 1.684976]\n",
      "[D loss: 0.866059] [G loss: 1.524970]\n",
      "[D loss: 0.918222] [G loss: 1.638890]\n",
      "[D loss: 0.654557] [G loss: 1.352815]\n",
      "[D loss: 0.969193] [G loss: 1.243654]\n",
      "[D loss: 0.917984] [G loss: 1.448228]\n",
      "[D loss: 0.961152] [G loss: 1.549339]\n",
      "[D loss: 0.834698] [G loss: 1.481046]\n",
      "[D loss: 0.695873] [G loss: 1.667071]\n",
      "[D loss: 0.826735] [G loss: 1.497574]\n",
      "[D loss: 0.792867] [G loss: 1.814027]\n",
      "[D loss: 0.759338] [G loss: 1.437944]\n",
      "[D loss: 1.013977] [G loss: 1.385579]\n",
      "[D loss: 0.951066] [G loss: 1.454159]\n",
      "[D loss: 0.734888] [G loss: 1.216458]\n",
      "[D loss: 0.801639] [G loss: 1.585345]\n",
      "[D loss: 1.087204] [G loss: 1.526608]\n",
      "[D loss: 0.948884] [G loss: 1.621794]\n",
      "[D loss: 0.928835] [G loss: 1.626704]\n",
      "[D loss: 0.965687] [G loss: 1.310457]\n",
      "[D loss: 0.837293] [G loss: 1.371888]\n",
      "[D loss: 0.906512] [G loss: 1.363239]\n",
      "[D loss: 0.864081] [G loss: 1.381098]\n",
      "[D loss: 0.957090] [G loss: 1.427909]\n",
      "[D loss: 0.850411] [G loss: 1.419457]\n",
      "[D loss: 0.868494] [G loss: 1.464505]\n",
      "[D loss: 0.650557] [G loss: 1.547159]\n",
      "[D loss: 0.767953] [G loss: 1.649510]\n",
      "[D loss: 1.103647] [G loss: 1.377315]\n",
      "[D loss: 0.810136] [G loss: 1.346920]\n",
      "[D loss: 0.917674] [G loss: 1.212504]\n",
      "[D loss: 0.830207] [G loss: 1.661124]\n",
      "[D loss: 0.794518] [G loss: 1.663831]\n",
      "[D loss: 0.457945] [G loss: 1.660622]\n",
      "[D loss: 0.895938] [G loss: 1.492083]\n",
      "[D loss: 0.839643] [G loss: 1.492653]\n",
      "[D loss: 0.921729] [G loss: 1.477337]\n",
      "[D loss: 0.744558] [G loss: 1.321679]\n",
      "[D loss: 0.823766] [G loss: 1.560387]\n",
      "[D loss: 0.758501] [G loss: 1.518571]\n",
      "[D loss: 0.680173] [G loss: 1.503328]\n",
      "[D loss: 0.780947] [G loss: 1.530005]\n",
      "[D loss: 0.845830] [G loss: 1.575080]\n",
      "[D loss: 0.779924] [G loss: 1.452065]\n",
      "[D loss: 0.775528] [G loss: 1.488928]\n",
      "[D loss: 1.036519] [G loss: 1.240067]\n",
      "[D loss: 0.687683] [G loss: 1.631358]\n",
      "[D loss: 0.717976] [G loss: 1.851327]\n",
      "[D loss: 0.863868] [G loss: 1.656561]\n",
      "[D loss: 0.727871] [G loss: 1.350635]\n",
      "[D loss: 0.900900] [G loss: 1.544671]\n",
      "[D loss: 0.969616] [G loss: 1.215462]\n",
      "[D loss: 0.801971] [G loss: 1.372803]\n",
      "[D loss: 0.836069] [G loss: 1.603341]\n",
      "[D loss: 0.954982] [G loss: 1.345957]\n",
      "[D loss: 0.959038] [G loss: 1.418925]\n",
      "[D loss: 0.680725] [G loss: 1.783053]\n",
      "[D loss: 0.852242] [G loss: 1.627351]\n",
      "[D loss: 0.931129] [G loss: 1.318842]\n",
      "[D loss: 0.751401] [G loss: 1.458061]\n",
      "[D loss: 0.610296] [G loss: 1.343088]\n",
      "[D loss: 0.711757] [G loss: 1.509345]\n",
      "[D loss: 0.775350] [G loss: 1.360303]\n",
      "[D loss: 0.731252] [G loss: 1.485702]\n",
      "[D loss: 0.672980] [G loss: 1.500975]\n",
      "[D loss: 0.779571] [G loss: 1.605625]\n",
      "[D loss: 0.839426] [G loss: 1.649943]\n",
      "[D loss: 0.812056] [G loss: 1.678041]\n",
      "[D loss: 0.630848] [G loss: 1.507513]\n",
      "[D loss: 0.715951] [G loss: 1.733285]\n",
      "[D loss: 0.720013] [G loss: 1.496198]\n",
      "[D loss: 0.552168] [G loss: 1.413700]\n",
      "[D loss: 0.891012] [G loss: 1.599302]\n",
      "[D loss: 0.810124] [G loss: 1.453892]\n",
      "[D loss: 0.611313] [G loss: 1.691245]\n",
      "[D loss: 0.930153] [G loss: 1.513681]\n",
      "[D loss: 0.980735] [G loss: 1.727096]\n",
      "[D loss: 0.692151] [G loss: 1.568363]\n",
      "[D loss: 0.930631] [G loss: 1.411214]\n",
      "[D loss: 0.563682] [G loss: 1.559044]\n",
      "[D loss: 0.913483] [G loss: 1.448896]\n",
      "[D loss: 0.903806] [G loss: 1.583947]\n",
      "[D loss: 0.982740] [G loss: 1.234196]\n",
      "[D loss: 1.020550] [G loss: 1.352769]\n",
      "[D loss: 0.887821] [G loss: 1.434224]\n",
      "[D loss: 0.789155] [G loss: 1.339099]\n",
      "[D loss: 0.833411] [G loss: 1.248171]\n",
      "[D loss: 0.678730] [G loss: 1.447110]\n",
      "[D loss: 0.812180] [G loss: 1.543759]\n",
      "[D loss: 0.703761] [G loss: 1.378160]\n",
      "[D loss: 0.737540] [G loss: 1.572188]\n",
      "[D loss: 0.780456] [G loss: 1.564122]\n",
      "[D loss: 0.639845] [G loss: 1.498545]\n",
      "[D loss: 0.726233] [G loss: 1.578783]\n",
      "[D loss: 0.624431] [G loss: 1.558547]\n",
      "[D loss: 0.624030] [G loss: 1.657166]\n",
      "[D loss: 0.927778] [G loss: 1.579934]\n",
      "[D loss: 0.862021] [G loss: 1.646235]\n",
      "[D loss: 0.966057] [G loss: 1.513382]\n",
      "[D loss: 0.989566] [G loss: 1.434234]\n",
      "[D loss: 0.895217] [G loss: 1.333436]\n",
      "[D loss: 0.845651] [G loss: 1.345780]\n",
      "[D loss: 0.909082] [G loss: 1.401498]\n",
      "[D loss: 0.619734] [G loss: 1.506700]\n",
      "[D loss: 0.879759] [G loss: 1.580696]\n",
      "[D loss: 0.908887] [G loss: 1.603470]\n",
      "[D loss: 0.838550] [G loss: 1.526168]\n",
      "[D loss: 0.814125] [G loss: 1.737903]\n",
      "[D loss: 1.036105] [G loss: 1.457869]\n",
      "[D loss: 0.463834] [G loss: 1.688142]\n",
      "[D loss: 0.890152] [G loss: 1.754122]\n",
      "[D loss: 0.827536] [G loss: 1.631108]\n",
      "[D loss: 0.733755] [G loss: 1.593954]\n",
      "[D loss: 0.932141] [G loss: 1.417885]\n",
      "[D loss: 0.990530] [G loss: 1.480415]\n",
      "[D loss: 0.783630] [G loss: 1.528635]\n",
      "[D loss: 1.195619] [G loss: 1.468902]\n",
      "[D loss: 0.920986] [G loss: 1.895925]\n",
      "[D loss: 0.892179] [G loss: 1.652179]\n",
      "[D loss: 0.813478] [G loss: 1.413057]\n",
      "[D loss: 0.875124] [G loss: 1.363792]\n",
      "[D loss: 0.723812] [G loss: 1.470056]\n",
      "[D loss: 0.973448] [G loss: 1.263496]\n",
      "[D loss: 0.796245] [G loss: 1.323318]\n",
      "[D loss: 0.959849] [G loss: 1.344461]\n",
      "[D loss: 0.824029] [G loss: 1.498744]\n",
      "[D loss: 0.776881] [G loss: 1.426543]\n",
      "[D loss: 0.999648] [G loss: 1.467448]\n",
      "[D loss: 0.909929] [G loss: 1.312214]\n",
      "[D loss: 0.882892] [G loss: 1.320744]\n",
      "[D loss: 0.609450] [G loss: 1.348538]\n",
      "[D loss: 0.839560] [G loss: 1.518034]\n",
      "[D loss: 0.621943] [G loss: 1.378747]\n",
      "[D loss: 0.881583] [G loss: 1.384681]\n",
      "[D loss: 1.063792] [G loss: 1.361135]\n",
      "[D loss: 0.989231] [G loss: 1.746725]\n",
      "[D loss: 0.940937] [G loss: 1.218653]\n",
      "[D loss: 0.645461] [G loss: 1.424471]\n",
      "[D loss: 1.034879] [G loss: 1.166979]\n",
      "[D loss: 0.748434] [G loss: 1.427507]\n",
      "[D loss: 0.862865] [G loss: 1.454606]\n",
      "[D loss: 0.815032] [G loss: 1.287116]\n",
      "[D loss: 0.878978] [G loss: 1.498651]\n",
      "[D loss: 0.697684] [G loss: 1.708361]\n",
      "[D loss: 0.700695] [G loss: 1.636889]\n",
      "[D loss: 0.639896] [G loss: 1.568752]\n",
      "[D loss: 0.731345] [G loss: 1.445762]\n",
      "[D loss: 0.859538] [G loss: 1.679276]\n",
      "[D loss: 1.075197] [G loss: 1.136100]\n",
      "[D loss: 0.860193] [G loss: 1.275421]\n",
      "[D loss: 0.781855] [G loss: 1.404189]\n",
      "[D loss: 0.754890] [G loss: 1.514287]\n",
      "[D loss: 1.004560] [G loss: 1.461477]\n",
      "[D loss: 0.870144] [G loss: 1.593057]\n",
      "[D loss: 0.663582] [G loss: 1.343425]\n",
      "[D loss: 0.987837] [G loss: 1.539354]\n",
      "[D loss: 0.707268] [G loss: 1.559303]\n",
      "[D loss: 0.775912] [G loss: 1.570229]\n",
      "[D loss: 0.821129] [G loss: 1.476422]\n",
      "[D loss: 0.511753] [G loss: 1.559899]\n",
      "[D loss: 0.619995] [G loss: 1.402703]\n",
      "[D loss: 0.743367] [G loss: 1.659083]\n",
      "[D loss: 1.042939] [G loss: 1.583513]\n",
      "[D loss: 0.870950] [G loss: 1.777536]\n",
      "[D loss: 0.790065] [G loss: 1.761350]\n",
      "[D loss: 0.899379] [G loss: 1.566400]\n",
      "[D loss: 0.875942] [G loss: 1.216764]\n",
      "[D loss: 1.044997] [G loss: 1.554586]\n",
      "[D loss: 0.727350] [G loss: 1.526832]\n",
      "[D loss: 0.955159] [G loss: 1.449566]\n",
      "[D loss: 0.825619] [G loss: 1.398506]\n",
      "[D loss: 0.863290] [G loss: 1.351526]\n",
      "[D loss: 0.914776] [G loss: 1.360645]\n",
      "[D loss: 0.872018] [G loss: 1.508620]\n",
      "[D loss: 0.832882] [G loss: 1.618649]\n",
      "[D loss: 0.662920] [G loss: 1.509886]\n",
      "[D loss: 0.910077] [G loss: 1.544607]\n",
      "[D loss: 0.864770] [G loss: 1.556661]\n",
      "[D loss: 0.872911] [G loss: 1.285915]\n",
      "[D loss: 0.595891] [G loss: 1.266577]\n",
      "[D loss: 0.967381] [G loss: 1.453889]\n",
      "[D loss: 0.745735] [G loss: 1.530310]\n",
      "[D loss: 0.782457] [G loss: 1.267583]\n",
      "[D loss: 0.814298] [G loss: 1.432891]\n",
      "[D loss: 0.955840] [G loss: 1.363646]\n",
      "[D loss: 0.775139] [G loss: 1.543266]\n",
      "[D loss: 0.802059] [G loss: 1.625778]\n",
      "[D loss: 0.835896] [G loss: 1.410826]\n",
      "[D loss: 0.753774] [G loss: 1.452201]\n",
      "[D loss: 0.819611] [G loss: 1.663897]\n",
      "[D loss: 0.712048] [G loss: 1.340050]\n",
      "[D loss: 0.808689] [G loss: 1.467776]\n",
      "[D loss: 0.801938] [G loss: 1.421503]\n",
      "[D loss: 0.693196] [G loss: 1.433331]\n",
      "[D loss: 0.859935] [G loss: 1.473438]\n",
      "[D loss: 0.961041] [G loss: 1.362994]\n",
      "[D loss: 0.724256] [G loss: 1.591952]\n",
      "[D loss: 0.696621] [G loss: 1.482817]\n",
      "[D loss: 0.812409] [G loss: 1.510861]\n",
      "[D loss: 0.784576] [G loss: 1.385583]\n",
      "[D loss: 0.880522] [G loss: 1.530060]\n",
      "[D loss: 0.889055] [G loss: 1.413910]\n",
      "[D loss: 0.957737] [G loss: 1.285934]\n",
      "[D loss: 0.963434] [G loss: 1.437892]\n",
      "[D loss: 0.771820] [G loss: 1.336801]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.786073] [G loss: 1.353925]\n",
      "[D loss: 0.752388] [G loss: 1.537530]\n",
      "[D loss: 0.823724] [G loss: 1.344813]\n",
      "[D loss: 0.821033] [G loss: 1.242949]\n",
      "[D loss: 0.816246] [G loss: 1.220245]\n",
      "[D loss: 0.767964] [G loss: 1.497025]\n",
      "[D loss: 0.697088] [G loss: 1.380741]\n",
      "[D loss: 0.708584] [G loss: 1.286904]\n",
      "[D loss: 0.801053] [G loss: 1.462675]\n",
      "[D loss: 0.705320] [G loss: 1.424884]\n",
      "[D loss: 0.750398] [G loss: 1.533309]\n",
      "[D loss: 0.918682] [G loss: 1.487882]\n",
      "[D loss: 0.891332] [G loss: 1.512151]\n",
      "[D loss: 0.832733] [G loss: 1.496286]\n",
      "[D loss: 0.994320] [G loss: 1.527992]\n",
      "[D loss: 0.865317] [G loss: 1.450874]\n",
      "[D loss: 0.592130] [G loss: 1.508966]\n",
      "[D loss: 0.862552] [G loss: 1.594855]\n",
      "[D loss: 0.750693] [G loss: 1.639472]\n",
      "[D loss: 0.821053] [G loss: 1.378568]\n",
      "[D loss: 0.937197] [G loss: 1.548514]\n",
      "[D loss: 0.797519] [G loss: 1.548691]\n",
      "[D loss: 0.724563] [G loss: 1.515606]\n",
      "[D loss: 0.801094] [G loss: 1.470282]\n",
      "[D loss: 0.938150] [G loss: 1.342678]\n",
      "[D loss: 0.773176] [G loss: 1.455523]\n",
      "[D loss: 0.514512] [G loss: 1.413949]\n",
      "[D loss: 0.878502] [G loss: 1.371732]\n",
      "[D loss: 0.770357] [G loss: 1.398892]\n",
      "[D loss: 0.862452] [G loss: 1.501217]\n",
      "[D loss: 0.724309] [G loss: 1.825576]\n",
      "[D loss: 0.657228] [G loss: 1.623765]\n",
      "[D loss: 0.702336] [G loss: 1.446345]\n",
      "[D loss: 0.628594] [G loss: 1.537889]\n",
      "[D loss: 0.953879] [G loss: 1.411600]\n",
      "[D loss: 0.532499] [G loss: 1.475110]\n",
      "[D loss: 0.784797] [G loss: 1.630617]\n",
      "[D loss: 0.649640] [G loss: 1.718491]\n",
      "[D loss: 1.096204] [G loss: 1.527121]\n",
      "[D loss: 0.875004] [G loss: 1.295488]\n",
      "[D loss: 0.757561] [G loss: 1.695330]\n",
      "[D loss: 0.634940] [G loss: 1.686398]\n",
      "[D loss: 0.932977] [G loss: 1.391824]\n",
      "[D loss: 0.772799] [G loss: 1.462407]\n",
      "[D loss: 0.990451] [G loss: 1.708699]\n",
      "[D loss: 0.842919] [G loss: 1.787846]\n",
      "[D loss: 0.917120] [G loss: 1.507503]\n",
      "[D loss: 0.814610] [G loss: 1.427060]\n",
      "[D loss: 1.043876] [G loss: 1.481890]\n",
      "[D loss: 0.889591] [G loss: 1.435202]\n",
      "[D loss: 0.651124] [G loss: 1.413518]\n",
      "[D loss: 0.743572] [G loss: 1.367890]\n",
      "[D loss: 0.848381] [G loss: 1.402347]\n",
      "[D loss: 0.912771] [G loss: 1.388379]\n",
      "[D loss: 0.753580] [G loss: 1.335926]\n",
      "[D loss: 0.601729] [G loss: 1.726490]\n",
      "[D loss: 1.090830] [G loss: 1.263418]\n",
      "[D loss: 0.655027] [G loss: 1.451848]\n",
      "[D loss: 0.767966] [G loss: 1.388480]\n",
      "[D loss: 0.765151] [G loss: 1.379059]\n",
      "[D loss: 0.737201] [G loss: 1.483228]\n",
      "[D loss: 1.000292] [G loss: 1.428699]\n",
      "[D loss: 0.703033] [G loss: 1.310811]\n",
      "[D loss: 0.678121] [G loss: 1.456104]\n",
      "[D loss: 0.808489] [G loss: 1.501939]\n",
      "[D loss: 0.898343] [G loss: 1.370009]\n",
      "[D loss: 1.136815] [G loss: 1.355486]\n",
      "[D loss: 0.788927] [G loss: 1.614632]\n",
      "[D loss: 0.871420] [G loss: 1.361384]\n",
      "[D loss: 0.733581] [G loss: 1.560196]\n",
      "[D loss: 0.875582] [G loss: 1.305364]\n",
      "[D loss: 0.947529] [G loss: 1.346355]\n",
      "[D loss: 0.761833] [G loss: 1.522991]\n",
      "[D loss: 0.812537] [G loss: 1.647563]\n",
      "[D loss: 0.829503] [G loss: 1.647605]\n",
      "[D loss: 0.961853] [G loss: 1.426170]\n",
      "[D loss: 0.869244] [G loss: 1.278756]\n",
      "[D loss: 0.729956] [G loss: 1.549480]\n",
      "[D loss: 0.739189] [G loss: 1.403250]\n",
      "[D loss: 1.301488] [G loss: 1.350483]\n",
      "[D loss: 0.795960] [G loss: 1.529759]\n",
      "[D loss: 0.750487] [G loss: 1.739928]\n",
      "[D loss: 0.650561] [G loss: 1.635703]\n",
      "[D loss: 0.839532] [G loss: 1.477125]\n",
      "[D loss: 0.736853] [G loss: 1.551728]\n",
      "[D loss: 0.769619] [G loss: 1.502257]\n",
      "[D loss: 0.808437] [G loss: 1.295060]\n",
      "[D loss: 0.839562] [G loss: 1.697633]\n",
      "[D loss: 0.744326] [G loss: 1.403379]\n",
      "[D loss: 0.946437] [G loss: 1.509490]\n",
      "[D loss: 1.037659] [G loss: 1.427181]\n",
      "[D loss: 0.929889] [G loss: 1.471033]\n",
      "[D loss: 0.919645] [G loss: 1.575584]\n",
      "[D loss: 1.035513] [G loss: 1.386145]\n",
      "[D loss: 0.744387] [G loss: 1.315441]\n",
      "[D loss: 0.710816] [G loss: 1.367099]\n",
      "[D loss: 0.925018] [G loss: 1.446715]\n",
      "[D loss: 0.942517] [G loss: 1.246850]\n",
      "[D loss: 0.927359] [G loss: 1.542024]\n",
      "[D loss: 1.008365] [G loss: 1.421274]\n",
      "[D loss: 0.742068] [G loss: 1.475322]\n",
      "[D loss: 0.867572] [G loss: 1.341264]\n",
      "[D loss: 0.824253] [G loss: 1.429976]\n",
      "[D loss: 0.800262] [G loss: 1.310902]\n",
      "[D loss: 0.806188] [G loss: 1.431280]\n",
      "[D loss: 1.019426] [G loss: 1.435567]\n",
      "[D loss: 0.966268] [G loss: 1.211770]\n",
      "[D loss: 0.849476] [G loss: 1.342904]\n",
      "[D loss: 0.681264] [G loss: 1.514099]\n",
      "[D loss: 0.726281] [G loss: 1.231438]\n",
      "[D loss: 0.852877] [G loss: 1.601913]\n",
      "[D loss: 0.808339] [G loss: 1.447472]\n",
      "[D loss: 0.842801] [G loss: 1.473447]\n",
      "[D loss: 0.577628] [G loss: 1.408310]\n",
      "[D loss: 0.772106] [G loss: 1.348066]\n",
      "[D loss: 0.583661] [G loss: 1.544231]\n",
      "[D loss: 0.884145] [G loss: 1.343318]\n",
      "[D loss: 0.943568] [G loss: 1.348808]\n",
      "[D loss: 0.677255] [G loss: 1.404877]\n",
      "[D loss: 0.812372] [G loss: 1.586053]\n",
      "[D loss: 1.140588] [G loss: 1.535004]\n",
      "[D loss: 0.890717] [G loss: 1.580600]\n",
      "[D loss: 0.744947] [G loss: 1.418816]\n",
      "[D loss: 0.904802] [G loss: 1.366374]\n",
      "[D loss: 0.755407] [G loss: 1.276011]\n",
      "[D loss: 0.742840] [G loss: 1.384809]\n",
      "[D loss: 0.807711] [G loss: 1.364978]\n",
      "[D loss: 0.905640] [G loss: 1.408173]\n",
      "[D loss: 0.778283] [G loss: 1.381485]\n",
      "[D loss: 0.857543] [G loss: 1.339778]\n",
      "[D loss: 0.791748] [G loss: 1.495083]\n",
      "[D loss: 0.736308] [G loss: 1.495475]\n",
      "[D loss: 0.707929] [G loss: 1.590486]\n",
      "[D loss: 0.757607] [G loss: 1.603511]\n",
      "[D loss: 0.656425] [G loss: 1.538848]\n",
      "[D loss: 0.929803] [G loss: 1.320470]\n",
      "[D loss: 0.989557] [G loss: 1.492187]\n",
      "[D loss: 0.574006] [G loss: 1.390362]\n",
      "[D loss: 0.666378] [G loss: 1.590039]\n",
      "[D loss: 0.965321] [G loss: 1.465674]\n",
      "[D loss: 0.908860] [G loss: 1.515292]\n",
      "[D loss: 1.069243] [G loss: 1.540786]\n",
      "[D loss: 0.819597] [G loss: 1.463125]\n",
      "[D loss: 0.962290] [G loss: 1.496620]\n",
      "[D loss: 0.658829] [G loss: 1.453329]\n",
      "[D loss: 0.845982] [G loss: 1.585668]\n",
      "[D loss: 0.767331] [G loss: 1.498582]\n",
      "[D loss: 1.025766] [G loss: 1.317619]\n",
      "[D loss: 0.818244] [G loss: 1.441998]\n",
      "[D loss: 0.633082] [G loss: 1.396876]\n",
      "[D loss: 0.831846] [G loss: 1.258489]\n",
      "[D loss: 0.915313] [G loss: 1.315715]\n",
      "[D loss: 1.050656] [G loss: 1.392170]\n",
      "[D loss: 0.818268] [G loss: 1.388703]\n",
      "[D loss: 1.000416] [G loss: 1.481277]\n",
      "[D loss: 1.009497] [G loss: 1.362770]\n",
      "[D loss: 0.999193] [G loss: 1.539581]\n",
      "[D loss: 0.699414] [G loss: 1.398110]\n",
      "[D loss: 0.885161] [G loss: 1.511102]\n",
      "[D loss: 0.783525] [G loss: 1.226480]\n",
      "[D loss: 0.660937] [G loss: 1.319678]\n",
      "[D loss: 0.670759] [G loss: 1.420563]\n",
      "[D loss: 0.907650] [G loss: 1.333981]\n",
      "[D loss: 0.798954] [G loss: 1.368570]\n",
      "[D loss: 0.898790] [G loss: 1.432627]\n",
      "[D loss: 0.763454] [G loss: 1.492904]\n",
      "[D loss: 1.120312] [G loss: 1.701206]\n",
      "[D loss: 0.723905] [G loss: 1.566735]\n",
      "[D loss: 0.761536] [G loss: 1.364048]\n",
      "[D loss: 0.841437] [G loss: 1.352952]\n",
      "[D loss: 0.861879] [G loss: 1.205832]\n",
      "[D loss: 0.657673] [G loss: 1.353865]\n",
      "[D loss: 0.882157] [G loss: 1.579441]\n",
      "[D loss: 0.878001] [G loss: 1.457304]\n",
      "[D loss: 0.826782] [G loss: 1.307043]\n",
      "[D loss: 1.082478] [G loss: 1.314863]\n",
      "[D loss: 0.832032] [G loss: 1.258686]\n",
      "[D loss: 0.940323] [G loss: 1.311829]\n",
      "[D loss: 0.828115] [G loss: 1.468672]\n",
      "[D loss: 0.708688] [G loss: 1.359015]\n",
      "[D loss: 0.839661] [G loss: 1.495024]\n",
      "[D loss: 0.703998] [G loss: 1.626644]\n",
      "[D loss: 0.767894] [G loss: 1.558063]\n",
      "[D loss: 0.944691] [G loss: 1.343695]\n",
      "[D loss: 0.669345] [G loss: 1.606022]\n",
      "[D loss: 0.903428] [G loss: 1.480175]\n",
      "[D loss: 0.743336] [G loss: 1.515250]\n",
      "[D loss: 0.899019] [G loss: 1.479071]\n",
      "[D loss: 0.948995] [G loss: 1.457031]\n",
      "[D loss: 0.728034] [G loss: 1.367475]\n",
      "[D loss: 0.601793] [G loss: 1.704198]\n",
      "[D loss: 0.779359] [G loss: 1.403237]\n",
      "[D loss: 0.947272] [G loss: 1.500833]\n",
      "[D loss: 0.655572] [G loss: 1.608899]\n",
      "[D loss: 0.821924] [G loss: 1.542519]\n",
      "[D loss: 0.805947] [G loss: 1.572446]\n",
      "[D loss: 0.725826] [G loss: 1.407186]\n",
      "[D loss: 0.701268] [G loss: 1.535743]\n",
      "[D loss: 0.871797] [G loss: 1.447718]\n",
      "[D loss: 0.723547] [G loss: 1.367021]\n",
      "[D loss: 0.846315] [G loss: 1.512344]\n",
      "[D loss: 0.838193] [G loss: 1.510262]\n",
      "[D loss: 0.690719] [G loss: 1.616773]\n",
      "[D loss: 0.668746] [G loss: 1.674326]\n",
      "[D loss: 1.107409] [G loss: 1.310677]\n",
      "[D loss: 0.846940] [G loss: 1.463255]\n",
      "[D loss: 0.611180] [G loss: 1.646447]\n",
      "[D loss: 0.738882] [G loss: 1.646699]\n",
      "[D loss: 0.913365] [G loss: 1.336658]\n",
      "[D loss: 0.981480] [G loss: 1.460814]\n",
      "[D loss: 0.945868] [G loss: 1.398784]\n",
      "[D loss: 0.829749] [G loss: 1.468595]\n",
      "[D loss: 0.799850] [G loss: 1.421012]\n",
      "[D loss: 0.787059] [G loss: 1.376609]\n",
      "[D loss: 0.921124] [G loss: 1.269405]\n",
      "[D loss: 1.029747] [G loss: 1.384518]\n",
      "[D loss: 0.804716] [G loss: 1.466285]\n",
      "[D loss: 0.907854] [G loss: 1.454571]\n",
      "[D loss: 0.934467] [G loss: 1.343156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.875284] [G loss: 1.523440]\n",
      "[D loss: 0.701062] [G loss: 1.462262]\n",
      "[D loss: 0.585352] [G loss: 1.388664]\n",
      "[D loss: 0.974156] [G loss: 1.411154]\n",
      "[D loss: 0.820235] [G loss: 1.477508]\n",
      "[D loss: 0.717515] [G loss: 1.521333]\n",
      "[D loss: 0.763811] [G loss: 1.663990]\n",
      "[D loss: 1.053702] [G loss: 1.388140]\n",
      "[D loss: 0.979786] [G loss: 1.274976]\n",
      "[D loss: 0.819871] [G loss: 1.270689]\n",
      "[D loss: 0.870896] [G loss: 1.393256]\n",
      "[D loss: 0.848641] [G loss: 1.583524]\n",
      "[D loss: 0.996652] [G loss: 1.421608]\n",
      "[D loss: 0.742823] [G loss: 1.502930]\n",
      "[D loss: 0.774021] [G loss: 1.371338]\n",
      "[D loss: 0.794614] [G loss: 1.484742]\n",
      "[D loss: 0.583854] [G loss: 1.674598]\n",
      "[D loss: 0.906726] [G loss: 1.390443]\n",
      "[D loss: 0.577690] [G loss: 1.786281]\n",
      "[D loss: 0.811065] [G loss: 1.498731]\n",
      "[D loss: 0.693233] [G loss: 1.436802]\n",
      "[D loss: 1.028990] [G loss: 1.427726]\n",
      "[D loss: 0.674661] [G loss: 1.470538]\n",
      "[D loss: 0.637412] [G loss: 1.826555]\n",
      "[D loss: 0.849471] [G loss: 1.545246]\n",
      "[D loss: 0.785491] [G loss: 1.549746]\n",
      "[D loss: 0.763180] [G loss: 1.547757]\n",
      "[D loss: 0.906745] [G loss: 1.372456]\n",
      "[D loss: 0.884187] [G loss: 1.598045]\n",
      "[D loss: 0.853570] [G loss: 1.619396]\n",
      "[D loss: 0.766812] [G loss: 1.244078]\n",
      "[D loss: 0.787470] [G loss: 1.551809]\n",
      "[D loss: 1.012713] [G loss: 1.492596]\n",
      "[D loss: 0.878122] [G loss: 1.854864]\n",
      "[D loss: 1.058969] [G loss: 1.767463]\n",
      "[D loss: 0.889623] [G loss: 1.601082]\n",
      "[D loss: 0.678643] [G loss: 1.395393]\n",
      "[D loss: 0.856575] [G loss: 1.277874]\n",
      "[D loss: 1.022498] [G loss: 1.473508]\n",
      "[D loss: 0.704064] [G loss: 1.361717]\n",
      "[D loss: 0.948406] [G loss: 1.282217]\n",
      "[D loss: 0.840048] [G loss: 1.493284]\n",
      "[D loss: 0.700903] [G loss: 1.570602]\n",
      "[D loss: 0.743275] [G loss: 1.381544]\n",
      "[D loss: 0.703377] [G loss: 1.396223]\n",
      "[D loss: 0.899150] [G loss: 1.396519]\n",
      "[D loss: 0.707967] [G loss: 1.459972]\n",
      "[D loss: 0.709787] [G loss: 1.520681]\n",
      "[D loss: 0.591548] [G loss: 1.499354]\n",
      "[D loss: 1.013519] [G loss: 1.299254]\n",
      "[D loss: 0.904911] [G loss: 1.438923]\n",
      "[D loss: 0.911019] [G loss: 1.671433]\n",
      "[D loss: 0.665590] [G loss: 1.563405]\n",
      "[D loss: 0.816596] [G loss: 1.552148]\n",
      "[D loss: 0.898584] [G loss: 1.495316]\n",
      "[D loss: 0.920885] [G loss: 1.469903]\n",
      "[D loss: 0.725968] [G loss: 1.542108]\n",
      "[D loss: 0.708114] [G loss: 1.498754]\n",
      "[D loss: 1.119190] [G loss: 1.308168]\n",
      "[D loss: 0.843852] [G loss: 1.425016]\n",
      "[D loss: 0.744530] [G loss: 1.441833]\n",
      "[D loss: 0.963369] [G loss: 1.534309]\n",
      "[D loss: 0.780707] [G loss: 1.462135]\n",
      "[D loss: 0.961338] [G loss: 1.260924]\n",
      "[D loss: 1.017035] [G loss: 1.290754]\n",
      "[D loss: 0.776809] [G loss: 1.581442]\n",
      "[D loss: 0.778659] [G loss: 1.534564]\n",
      "[D loss: 0.883037] [G loss: 1.469947]\n",
      "[D loss: 0.703933] [G loss: 1.704133]\n",
      "[D loss: 0.765612] [G loss: 1.500457]\n",
      "[D loss: 0.797558] [G loss: 1.407688]\n",
      "[D loss: 0.838500] [G loss: 1.539526]\n",
      "[D loss: 0.838217] [G loss: 1.591582]\n",
      "[D loss: 1.082759] [G loss: 1.220376]\n",
      "[D loss: 0.893374] [G loss: 1.217450]\n",
      "[D loss: 0.698212] [G loss: 1.614594]\n",
      "[D loss: 0.757358] [G loss: 1.384235]\n",
      "[D loss: 0.676153] [G loss: 1.624042]\n",
      "[D loss: 0.740117] [G loss: 1.426646]\n",
      "[D loss: 0.795624] [G loss: 1.436663]\n",
      "[D loss: 1.068851] [G loss: 1.236500]\n",
      "[D loss: 0.756589] [G loss: 1.375465]\n",
      "[D loss: 0.838288] [G loss: 1.453795]\n",
      "[D loss: 0.719675] [G loss: 1.446873]\n",
      "[D loss: 0.642947] [G loss: 1.685431]\n",
      "[D loss: 0.820649] [G loss: 1.549343]\n",
      "[D loss: 0.757806] [G loss: 1.473190]\n",
      "[D loss: 0.902252] [G loss: 1.378526]\n",
      "[D loss: 0.952415] [G loss: 1.489988]\n",
      "[D loss: 0.642495] [G loss: 1.380257]\n",
      "[D loss: 0.779868] [G loss: 1.534062]\n",
      "[D loss: 0.788406] [G loss: 1.447818]\n",
      "[D loss: 0.777680] [G loss: 1.341384]\n",
      "[D loss: 0.859513] [G loss: 1.391745]\n",
      "[D loss: 0.884226] [G loss: 1.503460]\n",
      "[D loss: 0.825244] [G loss: 1.460475]\n",
      "[D loss: 0.652954] [G loss: 1.743704]\n",
      "[D loss: 0.891886] [G loss: 1.560185]\n",
      "[D loss: 0.829176] [G loss: 1.461189]\n",
      "[D loss: 0.887482] [G loss: 1.569139]\n",
      "[D loss: 0.732255] [G loss: 1.231018]\n",
      "[D loss: 0.934528] [G loss: 1.352294]\n",
      "[D loss: 0.715362] [G loss: 1.296417]\n",
      "[D loss: 0.702110] [G loss: 1.497169]\n",
      "[D loss: 0.669411] [G loss: 1.524461]\n",
      "[D loss: 0.788020] [G loss: 1.525294]\n",
      "[D loss: 0.732112] [G loss: 1.462618]\n",
      "[D loss: 0.759662] [G loss: 1.626185]\n",
      "[D loss: 0.831668] [G loss: 1.772240]\n",
      "[D loss: 0.953539] [G loss: 1.413551]\n",
      "[D loss: 0.747625] [G loss: 1.458810]\n",
      "[D loss: 0.851361] [G loss: 1.431010]\n",
      "[D loss: 0.772529] [G loss: 1.271562]\n",
      "[D loss: 0.704466] [G loss: 1.589531]\n",
      "[D loss: 0.645750] [G loss: 1.452363]\n",
      "[D loss: 0.546168] [G loss: 1.675092]\n",
      "[D loss: 0.905943] [G loss: 1.371592]\n",
      "[D loss: 0.751374] [G loss: 1.871932]\n",
      "[D loss: 0.755123] [G loss: 1.727498]\n",
      "[D loss: 0.830856] [G loss: 1.520424]\n",
      "[D loss: 0.653757] [G loss: 1.396658]\n",
      "[D loss: 0.801136] [G loss: 1.207481]\n",
      "[D loss: 0.785353] [G loss: 1.877663]\n",
      "[D loss: 0.742592] [G loss: 1.528395]\n",
      "[D loss: 0.805101] [G loss: 1.664030]\n",
      "[D loss: 0.839227] [G loss: 1.582764]\n",
      "[D loss: 0.694471] [G loss: 1.636273]\n",
      "[D loss: 0.753366] [G loss: 1.425866]\n",
      "[D loss: 1.078800] [G loss: 1.133686]\n",
      "[D loss: 0.742006] [G loss: 1.427084]\n",
      "[D loss: 0.871020] [G loss: 1.606337]\n",
      "[D loss: 0.890709] [G loss: 1.491526]\n",
      "[D loss: 0.802552] [G loss: 1.561197]\n",
      "[D loss: 0.450247] [G loss: 1.616256]\n",
      "[D loss: 0.664786] [G loss: 1.440566]\n",
      "[D loss: 0.830828] [G loss: 1.397615]\n",
      "[D loss: 0.748096] [G loss: 1.552093]\n",
      "[D loss: 1.218448] [G loss: 1.320838]\n",
      "[D loss: 0.927269] [G loss: 1.505175]\n",
      "[D loss: 0.589831] [G loss: 1.762194]\n",
      "[D loss: 0.812454] [G loss: 1.504587]\n",
      "[D loss: 0.927826] [G loss: 1.525693]\n",
      "[D loss: 0.704588] [G loss: 1.593264]\n",
      "[D loss: 0.876200] [G loss: 1.516521]\n",
      "[D loss: 0.880041] [G loss: 1.499678]\n",
      "[D loss: 0.929866] [G loss: 1.553764]\n",
      "[D loss: 0.784428] [G loss: 1.513771]\n",
      "[D loss: 0.689721] [G loss: 1.511338]\n",
      "[D loss: 0.920085] [G loss: 1.314985]\n",
      "[D loss: 0.721484] [G loss: 1.503392]\n",
      "[D loss: 0.691302] [G loss: 1.510987]\n",
      "[D loss: 0.715576] [G loss: 1.716339]\n",
      "[D loss: 0.990062] [G loss: 1.493474]\n",
      "[D loss: 0.969163] [G loss: 1.604705]\n",
      "[D loss: 0.920376] [G loss: 1.565763]\n",
      "[D loss: 0.716110] [G loss: 1.575753]\n",
      "[D loss: 0.989361] [G loss: 1.380697]\n",
      "[D loss: 0.631170] [G loss: 1.401699]\n",
      "[D loss: 0.841154] [G loss: 1.566339]\n",
      "[D loss: 0.769002] [G loss: 1.480689]\n",
      "[D loss: 0.800550] [G loss: 1.378490]\n",
      "[D loss: 0.680256] [G loss: 1.600363]\n",
      "[D loss: 0.868558] [G loss: 1.451896]\n",
      "[D loss: 0.961662] [G loss: 1.508812]\n",
      "[D loss: 0.757062] [G loss: 1.456783]\n",
      "[D loss: 0.844318] [G loss: 1.225430]\n",
      "[D loss: 0.817858] [G loss: 1.367299]\n",
      "[D loss: 0.865516] [G loss: 1.498230]\n",
      "[D loss: 0.545944] [G loss: 1.555539]\n",
      "[D loss: 1.242599] [G loss: 1.485247]\n",
      "[D loss: 0.892691] [G loss: 1.596496]\n",
      "[D loss: 0.754097] [G loss: 1.514076]\n",
      "[D loss: 0.888840] [G loss: 1.441419]\n",
      "[D loss: 0.825104] [G loss: 1.328068]\n",
      "[D loss: 0.730377] [G loss: 1.407649]\n",
      "[D loss: 1.100580] [G loss: 1.405718]\n",
      "[D loss: 0.788085] [G loss: 1.459640]\n",
      "[D loss: 0.712387] [G loss: 1.372513]\n",
      "[D loss: 0.809517] [G loss: 1.465806]\n",
      "[D loss: 0.789314] [G loss: 1.304228]\n",
      "[D loss: 0.806203] [G loss: 1.647622]\n",
      "[D loss: 1.091189] [G loss: 1.364351]\n",
      "[D loss: 0.926536] [G loss: 1.517163]\n",
      "[D loss: 0.713511] [G loss: 1.669535]\n",
      "[D loss: 0.685324] [G loss: 1.622671]\n",
      "[D loss: 0.931115] [G loss: 1.261987]\n",
      "[D loss: 0.957979] [G loss: 1.289984]\n",
      "[D loss: 0.802130] [G loss: 1.365025]\n",
      "[D loss: 0.799577] [G loss: 1.497300]\n",
      "[D loss: 0.840986] [G loss: 1.409221]\n",
      "[D loss: 0.967396] [G loss: 1.580960]\n",
      "[D loss: 0.671356] [G loss: 1.761857]\n",
      "[D loss: 0.799382] [G loss: 1.603426]\n",
      "[D loss: 0.832076] [G loss: 1.652364]\n",
      "[D loss: 0.882787] [G loss: 1.422097]\n",
      "[D loss: 0.899259] [G loss: 1.361423]\n",
      "[D loss: 1.067338] [G loss: 1.276429]\n",
      "[D loss: 0.676339] [G loss: 1.357998]\n",
      "[D loss: 0.855082] [G loss: 1.573144]\n",
      "[D loss: 0.684140] [G loss: 1.338950]\n",
      "[D loss: 0.715045] [G loss: 1.429411]\n",
      "[D loss: 0.648791] [G loss: 1.658723]\n",
      "[D loss: 0.851735] [G loss: 1.624414]\n",
      "[D loss: 0.909382] [G loss: 1.072487]\n",
      "[D loss: 0.872180] [G loss: 1.368459]\n",
      "[D loss: 0.719309] [G loss: 1.249594]\n",
      "[D loss: 0.790038] [G loss: 1.822121]\n",
      "[D loss: 1.014906] [G loss: 1.670794]\n",
      "[D loss: 0.875303] [G loss: 1.422597]\n",
      "[D loss: 0.869806] [G loss: 1.608641]\n",
      "[D loss: 0.811294] [G loss: 1.352765]\n",
      "[D loss: 0.615593] [G loss: 1.312469]\n",
      "[D loss: 0.868659] [G loss: 1.233744]\n",
      "[D loss: 0.751662] [G loss: 1.577023]\n",
      "[D loss: 0.724393] [G loss: 1.577167]\n",
      "[D loss: 1.136221] [G loss: 1.750753]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.697026] [G loss: 1.610071]\n",
      "[D loss: 0.882602] [G loss: 1.662038]\n",
      "[D loss: 0.848786] [G loss: 1.357025]\n",
      "[D loss: 0.804329] [G loss: 1.530277]\n",
      "[D loss: 0.830887] [G loss: 1.277920]\n",
      "[D loss: 0.680603] [G loss: 1.363343]\n",
      "[D loss: 1.058637] [G loss: 1.487198]\n",
      "[D loss: 0.637886] [G loss: 1.880396]\n",
      "[D loss: 0.933990] [G loss: 1.507925]\n",
      "[D loss: 0.809215] [G loss: 1.607472]\n",
      "[D loss: 0.863346] [G loss: 1.583495]\n",
      "[D loss: 0.720579] [G loss: 1.451369]\n",
      "[D loss: 0.949966] [G loss: 1.240456]\n",
      "[D loss: 0.899518] [G loss: 1.393603]\n",
      "[D loss: 0.903031] [G loss: 1.365990]\n",
      "[D loss: 0.744567] [G loss: 1.454638]\n",
      "[D loss: 0.913874] [G loss: 1.502548]\n",
      "[D loss: 0.845224] [G loss: 1.538018]\n",
      "[D loss: 0.664148] [G loss: 1.434842]\n",
      "[D loss: 0.779339] [G loss: 1.504338]\n",
      "[D loss: 0.743212] [G loss: 1.639189]\n",
      "[D loss: 0.912274] [G loss: 1.645072]\n",
      "[D loss: 0.864490] [G loss: 1.704676]\n",
      "[D loss: 0.997675] [G loss: 1.268666]\n",
      "[D loss: 0.684580] [G loss: 1.333480]\n",
      "[D loss: 0.620380] [G loss: 1.496721]\n",
      "[D loss: 0.955137] [G loss: 1.619544]\n",
      "[D loss: 0.787011] [G loss: 1.383902]\n",
      "[D loss: 0.929847] [G loss: 1.344278]\n",
      "[D loss: 0.841091] [G loss: 1.518860]\n",
      "[D loss: 0.741177] [G loss: 1.410346]\n",
      "[D loss: 0.786676] [G loss: 1.510539]\n",
      "[D loss: 0.777473] [G loss: 1.432011]\n",
      "[D loss: 0.838714] [G loss: 1.475656]\n",
      "[D loss: 1.045482] [G loss: 1.280734]\n",
      "[D loss: 0.827574] [G loss: 1.577980]\n",
      "[D loss: 0.799580] [G loss: 1.368204]\n",
      "[D loss: 0.677679] [G loss: 1.372354]\n",
      "[D loss: 0.936415] [G loss: 1.277645]\n",
      "[D loss: 0.878091] [G loss: 1.448962]\n",
      "[D loss: 0.729342] [G loss: 1.432243]\n",
      "[D loss: 0.865751] [G loss: 1.578190]\n",
      "[D loss: 0.674544] [G loss: 1.478693]\n",
      "[D loss: 0.600404] [G loss: 1.593479]\n",
      "[D loss: 0.669307] [G loss: 1.480027]\n",
      "[D loss: 0.857582] [G loss: 1.587959]\n",
      "[D loss: 0.846221] [G loss: 1.605853]\n",
      "[D loss: 0.823266] [G loss: 1.377731]\n",
      "[D loss: 0.791157] [G loss: 1.252239]\n",
      "[D loss: 0.800868] [G loss: 1.367283]\n",
      "[D loss: 0.888575] [G loss: 1.261274]\n",
      "[D loss: 1.007329] [G loss: 1.564456]\n",
      "[D loss: 0.843485] [G loss: 1.669559]\n",
      "[D loss: 0.622843] [G loss: 1.650643]\n",
      "[D loss: 0.608253] [G loss: 1.667240]\n",
      "[D loss: 0.754578] [G loss: 1.593825]\n",
      "[D loss: 0.781313] [G loss: 1.356330]\n",
      "[D loss: 0.980055] [G loss: 1.522758]\n",
      "[D loss: 0.841185] [G loss: 1.450009]\n",
      "[D loss: 0.750196] [G loss: 1.505549]\n",
      "[D loss: 0.739720] [G loss: 1.813507]\n",
      "[D loss: 0.695413] [G loss: 1.714188]\n",
      "[D loss: 0.726197] [G loss: 1.739732]\n",
      "[D loss: 0.835715] [G loss: 1.308154]\n",
      "[D loss: 0.838458] [G loss: 1.516832]\n",
      "[D loss: 0.725781] [G loss: 1.669679]\n",
      "[D loss: 0.684088] [G loss: 1.563332]\n",
      "[D loss: 0.789252] [G loss: 1.461503]\n",
      "[D loss: 0.719693] [G loss: 1.654750]\n",
      "[D loss: 0.905872] [G loss: 1.681202]\n",
      "[D loss: 0.988561] [G loss: 1.386954]\n",
      "[D loss: 0.747822] [G loss: 1.446644]\n",
      "[D loss: 0.967168] [G loss: 1.362950]\n",
      "[D loss: 0.553269] [G loss: 1.625316]\n",
      "[D loss: 0.715159] [G loss: 1.384140]\n",
      "[D loss: 0.706507] [G loss: 1.572948]\n",
      "[D loss: 0.820196] [G loss: 1.790205]\n",
      "[D loss: 0.889143] [G loss: 1.729658]\n",
      "[D loss: 0.905684] [G loss: 1.611416]\n",
      "[D loss: 0.732754] [G loss: 1.372298]\n",
      "[D loss: 0.794102] [G loss: 1.391913]\n",
      "[D loss: 0.818137] [G loss: 1.700683]\n",
      "[D loss: 0.934009] [G loss: 1.600432]\n",
      "[D loss: 0.847632] [G loss: 1.671909]\n",
      "[D loss: 0.764212] [G loss: 1.543607]\n",
      "[D loss: 1.034769] [G loss: 1.564493]\n",
      "[D loss: 0.535223] [G loss: 1.440796]\n",
      "[D loss: 0.795106] [G loss: 1.560820]\n",
      "[D loss: 0.897632] [G loss: 1.466411]\n",
      "[D loss: 0.738088] [G loss: 1.717440]\n",
      "[D loss: 0.808884] [G loss: 1.543310]\n",
      "[D loss: 0.754986] [G loss: 1.456853]\n",
      "[D loss: 0.613932] [G loss: 1.600559]\n",
      "[D loss: 0.614488] [G loss: 1.653272]\n",
      "[D loss: 0.743492] [G loss: 1.513106]\n",
      "[D loss: 0.795026] [G loss: 1.553986]\n",
      "[D loss: 0.847617] [G loss: 1.371955]\n",
      "[D loss: 0.885213] [G loss: 1.569594]\n",
      "[D loss: 1.011509] [G loss: 1.419698]\n",
      "[D loss: 1.094047] [G loss: 1.482665]\n",
      "[D loss: 0.768732] [G loss: 1.518806]\n",
      "[D loss: 0.897391] [G loss: 1.636571]\n",
      "[D loss: 0.720648] [G loss: 1.386240]\n",
      "[D loss: 0.770715] [G loss: 1.654629]\n",
      "[D loss: 0.848127] [G loss: 1.303228]\n",
      "[D loss: 0.779411] [G loss: 1.430019]\n",
      "[D loss: 0.786278] [G loss: 1.464057]\n",
      "[D loss: 0.753560] [G loss: 1.412075]\n",
      "[D loss: 0.969701] [G loss: 1.291056]\n",
      "[D loss: 0.880041] [G loss: 1.384693]\n",
      "[D loss: 0.764596] [G loss: 1.553510]\n",
      "[D loss: 1.018412] [G loss: 1.466466]\n",
      "[D loss: 0.896283] [G loss: 1.848413]\n",
      "[D loss: 1.005748] [G loss: 1.454178]\n",
      "[D loss: 0.974249] [G loss: 1.385993]\n",
      "[D loss: 0.910294] [G loss: 1.267650]\n",
      "[D loss: 0.805199] [G loss: 1.378978]\n",
      "[D loss: 0.929034] [G loss: 1.297236]\n",
      "[D loss: 0.725034] [G loss: 1.307079]\n",
      "[D loss: 0.981697] [G loss: 1.446991]\n",
      "[D loss: 0.963402] [G loss: 1.545188]\n",
      "[D loss: 0.848344] [G loss: 1.231837]\n",
      "[D loss: 0.668440] [G loss: 1.358756]\n",
      "[D loss: 0.712323] [G loss: 1.381582]\n",
      "[D loss: 0.617981] [G loss: 1.436657]\n",
      "[D loss: 0.827081] [G loss: 1.542176]\n",
      "[D loss: 1.067936] [G loss: 1.443507]\n",
      "[D loss: 0.910213] [G loss: 1.499797]\n",
      "[D loss: 0.751672] [G loss: 1.341722]\n",
      "[D loss: 0.963320] [G loss: 1.391057]\n",
      "[D loss: 0.600798] [G loss: 1.666023]\n",
      "[D loss: 0.829868] [G loss: 1.601007]\n",
      "[D loss: 0.711550] [G loss: 1.529684]\n",
      "[D loss: 0.608900] [G loss: 1.525052]\n",
      "[D loss: 0.930948] [G loss: 1.272645]\n",
      "[D loss: 0.814730] [G loss: 1.490369]\n",
      "[D loss: 0.904351] [G loss: 1.477608]\n",
      "[D loss: 1.064476] [G loss: 1.401778]\n",
      "[D loss: 0.859495] [G loss: 1.481010]\n",
      "[D loss: 0.910217] [G loss: 1.556134]\n",
      "[D loss: 0.653402] [G loss: 1.408447]\n",
      "[D loss: 0.967972] [G loss: 1.351650]\n",
      "[D loss: 0.940659] [G loss: 1.423829]\n",
      "[D loss: 0.673106] [G loss: 1.506594]\n",
      "[D loss: 0.662455] [G loss: 1.568079]\n",
      "[D loss: 0.843865] [G loss: 1.698376]\n",
      "[D loss: 0.996882] [G loss: 1.133222]\n",
      "[D loss: 0.947972] [G loss: 1.309907]\n",
      "[D loss: 0.941178] [G loss: 1.432697]\n",
      "[D loss: 0.844939] [G loss: 1.448367]\n",
      "[D loss: 0.856377] [G loss: 1.391966]\n",
      "[D loss: 0.763127] [G loss: 1.549896]\n",
      "[D loss: 0.820136] [G loss: 1.494834]\n",
      "[D loss: 0.661622] [G loss: 1.291889]\n",
      "[D loss: 0.765106] [G loss: 1.318755]\n",
      "[D loss: 0.891652] [G loss: 1.508604]\n",
      "[D loss: 0.799388] [G loss: 1.282006]\n",
      "[D loss: 0.776122] [G loss: 1.523946]\n",
      "[D loss: 0.758093] [G loss: 1.393422]\n",
      "[D loss: 0.839449] [G loss: 1.390299]\n",
      "[D loss: 0.887708] [G loss: 1.560866]\n",
      "[D loss: 0.764507] [G loss: 1.402046]\n",
      "[D loss: 0.929873] [G loss: 1.394200]\n",
      "[D loss: 1.116159] [G loss: 1.260737]\n",
      "[D loss: 0.763220] [G loss: 1.572275]\n",
      "[D loss: 0.709013] [G loss: 1.629294]\n",
      "[D loss: 0.628921] [G loss: 1.535879]\n",
      "[D loss: 0.783359] [G loss: 1.467677]\n",
      "[D loss: 0.751853] [G loss: 1.335115]\n",
      "[D loss: 0.888917] [G loss: 1.601021]\n",
      "[D loss: 0.984027] [G loss: 1.313278]\n",
      "[D loss: 0.710545] [G loss: 1.511012]\n",
      "[D loss: 0.705727] [G loss: 1.535424]\n",
      "[D loss: 0.849988] [G loss: 1.348980]\n",
      "[D loss: 0.750019] [G loss: 1.549356]\n",
      "[D loss: 0.646178] [G loss: 1.524794]\n",
      "[D loss: 0.882030] [G loss: 1.268780]\n",
      "[D loss: 0.670758] [G loss: 1.509185]\n",
      "[D loss: 0.848955] [G loss: 1.651144]\n",
      "[D loss: 0.934074] [G loss: 1.942274]\n",
      "[D loss: 0.767808] [G loss: 1.595805]\n",
      "[D loss: 0.804068] [G loss: 1.542446]\n",
      "[D loss: 0.841638] [G loss: 1.612437]\n",
      "[D loss: 0.964873] [G loss: 1.509214]\n",
      "[D loss: 0.834096] [G loss: 1.719753]\n",
      "[D loss: 0.749671] [G loss: 1.467890]\n",
      "[D loss: 0.777467] [G loss: 1.353981]\n",
      "[D loss: 1.071637] [G loss: 1.258225]\n",
      "[D loss: 0.820321] [G loss: 1.415242]\n",
      "[D loss: 1.072252] [G loss: 1.441656]\n",
      "[D loss: 1.006673] [G loss: 1.466724]\n",
      "[D loss: 0.773473] [G loss: 1.462404]\n",
      "[D loss: 0.893697] [G loss: 1.252809]\n",
      "[D loss: 0.712761] [G loss: 1.437013]\n",
      "[D loss: 0.802160] [G loss: 1.429339]\n",
      "[D loss: 0.716096] [G loss: 1.443435]\n",
      "[D loss: 0.600927] [G loss: 1.467704]\n",
      "[D loss: 0.654106] [G loss: 1.744231]\n",
      "[D loss: 0.724345] [G loss: 1.586541]\n",
      "[D loss: 0.880780] [G loss: 1.431444]\n",
      "[D loss: 0.921442] [G loss: 1.601652]\n",
      "[D loss: 0.899973] [G loss: 1.435967]\n",
      "[D loss: 0.746810] [G loss: 1.629561]\n",
      "[D loss: 1.050494] [G loss: 1.507199]\n",
      "[D loss: 0.684533] [G loss: 1.598141]\n",
      "[D loss: 0.690899] [G loss: 1.407196]\n",
      "[D loss: 0.795841] [G loss: 1.501521]\n",
      "[D loss: 0.819332] [G loss: 1.536240]\n",
      "[D loss: 0.764046] [G loss: 1.902887]\n",
      "[D loss: 0.752346] [G loss: 1.467728]\n",
      "[D loss: 0.737305] [G loss: 1.353269]\n",
      "[D loss: 0.849318] [G loss: 1.375584]\n",
      "[D loss: 0.945667] [G loss: 1.396377]\n",
      "[D loss: 0.861525] [G loss: 1.385319]\n",
      "[D loss: 0.946522] [G loss: 1.307935]\n",
      "[D loss: 0.726848] [G loss: 1.591428]\n",
      "[D loss: 1.104447] [G loss: 1.202759]\n",
      "[D loss: 0.988695] [G loss: 1.453000]\n",
      "[D loss: 0.635527] [G loss: 1.616167]\n",
      "[D loss: 0.947692] [G loss: 1.567783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.880032] [G loss: 1.368866]\n",
      "[D loss: 0.993871] [G loss: 1.597642]\n",
      "[D loss: 0.974360] [G loss: 1.307313]\n",
      "[D loss: 1.048609] [G loss: 1.428432]\n",
      "[D loss: 0.773593] [G loss: 1.394630]\n",
      "[D loss: 0.816447] [G loss: 1.612332]\n",
      "[D loss: 0.823863] [G loss: 1.321786]\n",
      "[D loss: 0.736601] [G loss: 1.404990]\n",
      "[D loss: 0.666440] [G loss: 1.257966]\n",
      "[D loss: 0.940907] [G loss: 1.329733]\n",
      "[D loss: 0.866290] [G loss: 1.409997]\n",
      "[D loss: 0.742386] [G loss: 1.430465]\n",
      "[D loss: 0.853098] [G loss: 1.522533]\n",
      "[D loss: 0.854832] [G loss: 1.416991]\n",
      "[D loss: 0.952934] [G loss: 1.366944]\n",
      "[D loss: 0.871767] [G loss: 1.223133]\n",
      "[D loss: 0.877524] [G loss: 1.286819]\n",
      "[D loss: 0.901267] [G loss: 1.359241]\n",
      "[D loss: 0.797791] [G loss: 1.495410]\n",
      "[D loss: 0.970061] [G loss: 1.539713]\n",
      "[D loss: 0.781692] [G loss: 1.483844]\n",
      "[D loss: 0.974364] [G loss: 1.306784]\n",
      "[D loss: 0.777954] [G loss: 1.396165]\n",
      "[D loss: 0.937111] [G loss: 1.247398]\n",
      "[D loss: 0.974998] [G loss: 1.539591]\n",
      "[D loss: 0.583309] [G loss: 1.511189]\n",
      "[D loss: 0.599734] [G loss: 1.414011]\n",
      "[D loss: 0.952421] [G loss: 1.357662]\n",
      "[D loss: 0.941480] [G loss: 1.525620]\n",
      "[D loss: 0.767955] [G loss: 1.336236]\n",
      "[D loss: 0.729972] [G loss: 1.574234]\n",
      "[D loss: 0.937361] [G loss: 1.413106]\n",
      "[D loss: 0.675732] [G loss: 1.559317]\n",
      "[D loss: 0.994761] [G loss: 1.169484]\n",
      "[D loss: 0.766966] [G loss: 1.476975]\n",
      "[D loss: 0.939439] [G loss: 1.418638]\n",
      "[D loss: 0.604195] [G loss: 1.401285]\n",
      "[D loss: 0.877634] [G loss: 1.394098]\n",
      "[D loss: 0.821442] [G loss: 1.655175]\n",
      "[D loss: 0.664812] [G loss: 1.553794]\n",
      "[D loss: 0.833978] [G loss: 1.457430]\n",
      "[D loss: 0.783870] [G loss: 1.438752]\n",
      "[D loss: 0.613309] [G loss: 1.663415]\n",
      "[D loss: 0.787330] [G loss: 1.539194]\n",
      "[D loss: 1.055876] [G loss: 1.287032]\n",
      "[D loss: 0.850073] [G loss: 1.599428]\n",
      "[D loss: 0.871725] [G loss: 1.534877]\n",
      "[D loss: 0.895358] [G loss: 1.588051]\n",
      "[D loss: 0.849961] [G loss: 1.495832]\n",
      "[D loss: 0.893440] [G loss: 1.463715]\n",
      "[D loss: 0.778670] [G loss: 1.367516]\n",
      "[D loss: 1.034805] [G loss: 1.427184]\n",
      "[D loss: 0.882641] [G loss: 1.491568]\n",
      "[D loss: 0.778085] [G loss: 1.481096]\n",
      "[D loss: 0.747373] [G loss: 1.444027]\n",
      "[D loss: 0.724801] [G loss: 1.394333]\n",
      "[D loss: 0.525912] [G loss: 1.648509]\n",
      "[D loss: 0.436654] [G loss: 1.582650]\n",
      "[D loss: 1.079668] [G loss: 1.702460]\n",
      "[D loss: 1.016033] [G loss: 1.348184]\n",
      "[D loss: 1.119947] [G loss: 1.180528]\n",
      "[D loss: 0.972119] [G loss: 1.400089]\n",
      "[D loss: 1.149132] [G loss: 1.371218]\n",
      "[D loss: 1.074299] [G loss: 1.241379]\n",
      "[D loss: 0.825973] [G loss: 1.474596]\n",
      "[D loss: 1.064005] [G loss: 1.064415]\n",
      "[D loss: 1.039738] [G loss: 1.231576]\n",
      "[D loss: 0.755995] [G loss: 1.592649]\n",
      "[D loss: 0.643106] [G loss: 1.520177]\n",
      "[D loss: 0.858304] [G loss: 1.445547]\n",
      "[D loss: 0.733279] [G loss: 1.445051]\n",
      "[D loss: 0.863855] [G loss: 1.539368]\n",
      "[D loss: 0.757880] [G loss: 1.574996]\n",
      "[D loss: 0.653111] [G loss: 1.417638]\n",
      "[D loss: 0.700652] [G loss: 1.461809]\n",
      "[D loss: 0.996059] [G loss: 1.472024]\n",
      "[D loss: 0.727477] [G loss: 1.467315]\n",
      "[D loss: 0.688899] [G loss: 1.306691]\n",
      "[D loss: 0.875899] [G loss: 1.363471]\n",
      "[D loss: 0.778052] [G loss: 1.557476]\n",
      "[D loss: 1.063817] [G loss: 1.310049]\n",
      "[D loss: 0.735960] [G loss: 1.589902]\n",
      "[D loss: 0.792121] [G loss: 1.400585]\n",
      "[D loss: 0.722485] [G loss: 1.443829]\n",
      "[D loss: 1.047803] [G loss: 1.164028]\n",
      "[D loss: 0.820785] [G loss: 1.565065]\n",
      "[D loss: 1.106740] [G loss: 1.561066]\n",
      "[D loss: 0.843072] [G loss: 1.433056]\n",
      "[D loss: 0.794876] [G loss: 1.395848]\n",
      "[D loss: 0.836986] [G loss: 1.378849]\n",
      "[D loss: 0.729502] [G loss: 1.356720]\n",
      "[D loss: 0.752432] [G loss: 1.376061]\n",
      "[D loss: 1.153216] [G loss: 1.282761]\n",
      "[D loss: 0.783420] [G loss: 1.668420]\n",
      "[D loss: 0.848801] [G loss: 1.449110]\n",
      "[D loss: 0.514074] [G loss: 1.656879]\n",
      "[D loss: 0.927363] [G loss: 1.424256]\n",
      "[D loss: 0.773909] [G loss: 1.537548]\n",
      "[D loss: 1.115389] [G loss: 1.437370]\n",
      "[D loss: 0.931107] [G loss: 1.473934]\n",
      "[D loss: 0.963035] [G loss: 1.218339]\n",
      "[D loss: 0.798003] [G loss: 1.373776]\n",
      "[D loss: 1.147976] [G loss: 1.266120]\n",
      "[D loss: 0.907095] [G loss: 1.462402]\n",
      "[D loss: 0.870405] [G loss: 1.545818]\n",
      "[D loss: 0.696288] [G loss: 1.341175]\n",
      "[D loss: 0.862916] [G loss: 1.259747]\n",
      "[D loss: 0.948328] [G loss: 1.521040]\n",
      "[D loss: 0.803099] [G loss: 1.641278]\n",
      "[D loss: 0.905309] [G loss: 1.374841]\n",
      "[D loss: 0.717128] [G loss: 1.497116]\n",
      "[D loss: 1.040177] [G loss: 1.460970]\n",
      "[D loss: 0.856501] [G loss: 1.746678]\n",
      "[D loss: 0.880064] [G loss: 1.515809]\n",
      "[D loss: 0.841660] [G loss: 1.398574]\n",
      "[D loss: 0.804214] [G loss: 1.343101]\n",
      "[D loss: 0.825623] [G loss: 1.278496]\n",
      "[D loss: 0.688698] [G loss: 1.254871]\n",
      "[D loss: 0.924987] [G loss: 1.552639]\n",
      "[D loss: 0.784737] [G loss: 1.716761]\n",
      "[D loss: 0.776869] [G loss: 1.579659]\n",
      "[D loss: 0.738112] [G loss: 1.659128]\n",
      "[D loss: 0.882986] [G loss: 1.499100]\n",
      "[D loss: 1.007781] [G loss: 1.288143]\n",
      "[D loss: 0.612461] [G loss: 1.312865]\n",
      "[D loss: 0.884788] [G loss: 1.527650]\n",
      "[D loss: 1.115067] [G loss: 1.201551]\n",
      "[D loss: 0.789690] [G loss: 1.469198]\n",
      "[D loss: 0.873901] [G loss: 1.259727]\n",
      "[D loss: 0.677647] [G loss: 1.615178]\n",
      "[D loss: 0.955803] [G loss: 1.486668]\n",
      "[D loss: 0.816457] [G loss: 1.384593]\n",
      "[D loss: 1.097780] [G loss: 1.217049]\n",
      "[D loss: 1.150583] [G loss: 1.165650]\n",
      "[D loss: 0.955895] [G loss: 1.434127]\n",
      "[D loss: 0.740986] [G loss: 1.311759]\n",
      "[D loss: 0.916319] [G loss: 1.198139]\n",
      "[D loss: 0.735042] [G loss: 1.503541]\n",
      "[D loss: 0.856055] [G loss: 1.430421]\n",
      "[D loss: 0.905840] [G loss: 1.327400]\n",
      "[D loss: 0.802555] [G loss: 1.468162]\n",
      "[D loss: 0.864594] [G loss: 1.449820]\n",
      "[D loss: 0.761780] [G loss: 1.465928]\n",
      "[D loss: 0.854860] [G loss: 1.301431]\n",
      "[D loss: 0.852026] [G loss: 1.492067]\n",
      "[D loss: 0.924953] [G loss: 1.393927]\n",
      "[D loss: 0.891738] [G loss: 1.484140]\n",
      "[D loss: 0.651224] [G loss: 1.464306]\n",
      "[D loss: 1.037383] [G loss: 1.314692]\n",
      "[D loss: 0.904103] [G loss: 1.488701]\n",
      "[D loss: 0.730981] [G loss: 1.091931]\n",
      "[D loss: 0.800895] [G loss: 1.123725]\n",
      "[D loss: 0.906040] [G loss: 1.360647]\n",
      "[D loss: 0.951318] [G loss: 1.662475]\n",
      "[D loss: 0.839424] [G loss: 1.381667]\n",
      "[D loss: 0.835809] [G loss: 1.532460]\n",
      "[D loss: 1.150748] [G loss: 1.252729]\n",
      "[D loss: 0.940448] [G loss: 1.306944]\n",
      "[D loss: 0.975932] [G loss: 1.283943]\n",
      "[D loss: 0.825150] [G loss: 1.493479]\n",
      "[D loss: 0.814861] [G loss: 1.415474]\n",
      "[D loss: 0.661897] [G loss: 1.372283]\n",
      "[D loss: 0.896335] [G loss: 1.540857]\n",
      "[D loss: 0.722545] [G loss: 1.483458]\n",
      "[D loss: 0.778031] [G loss: 1.549875]\n",
      "[D loss: 0.838881] [G loss: 1.248908]\n",
      "[D loss: 0.940444] [G loss: 1.323734]\n",
      "[D loss: 0.759226] [G loss: 1.387434]\n",
      "[D loss: 0.759428] [G loss: 1.503428]\n",
      "[D loss: 1.086235] [G loss: 1.393498]\n",
      "[D loss: 0.914048] [G loss: 1.678146]\n",
      "[D loss: 0.508103] [G loss: 1.564255]\n",
      "[D loss: 0.773982] [G loss: 1.286981]\n",
      "[D loss: 0.937713] [G loss: 1.440117]\n",
      "[D loss: 0.590659] [G loss: 1.444550]\n",
      "[D loss: 0.720105] [G loss: 1.358160]\n",
      "[D loss: 0.593487] [G loss: 1.422753]\n",
      "[D loss: 0.949393] [G loss: 1.215560]\n",
      "[D loss: 1.174455] [G loss: 1.429809]\n",
      "[D loss: 0.681953] [G loss: 1.590219]\n",
      "[D loss: 0.960569] [G loss: 1.437220]\n",
      "[D loss: 0.907563] [G loss: 1.400384]\n",
      "[D loss: 0.974394] [G loss: 1.197611]\n",
      "[D loss: 0.765194] [G loss: 1.175588]\n",
      "[D loss: 0.752172] [G loss: 1.341408]\n",
      "[D loss: 0.888987] [G loss: 1.598628]\n",
      "[D loss: 0.756990] [G loss: 1.469568]\n",
      "[D loss: 0.860995] [G loss: 1.204825]\n",
      "[D loss: 0.569592] [G loss: 1.491082]\n",
      "[D loss: 0.894810] [G loss: 1.252510]\n",
      "[D loss: 0.728632] [G loss: 1.611426]\n",
      "[D loss: 0.702888] [G loss: 1.529231]\n",
      "[D loss: 1.080886] [G loss: 1.503148]\n",
      "[D loss: 0.891775] [G loss: 1.441353]\n",
      "[D loss: 0.952181] [G loss: 1.490654]\n",
      "[D loss: 0.695287] [G loss: 1.293501]\n",
      "[D loss: 0.749106] [G loss: 1.275806]\n",
      "[D loss: 1.131902] [G loss: 1.093899]\n",
      "[D loss: 0.745312] [G loss: 1.336194]\n",
      "[D loss: 0.805210] [G loss: 1.466049]\n",
      "[D loss: 0.878247] [G loss: 1.397992]\n",
      "[D loss: 0.990845] [G loss: 1.385572]\n",
      "[D loss: 0.828728] [G loss: 1.418435]\n",
      "[D loss: 0.959962] [G loss: 1.241955]\n",
      "[D loss: 1.012829] [G loss: 1.395615]\n",
      "[D loss: 1.135289] [G loss: 1.484574]\n",
      "[D loss: 0.863147] [G loss: 1.401998]\n",
      "[D loss: 0.639755] [G loss: 1.412257]\n",
      "[D loss: 0.812530] [G loss: 1.457497]\n",
      "[D loss: 0.782310] [G loss: 1.500588]\n",
      "[D loss: 1.025814] [G loss: 1.436701]\n",
      "[D loss: 1.079937] [G loss: 1.258012]\n",
      "[D loss: 0.947148] [G loss: 1.178015]\n",
      "[D loss: 0.695001] [G loss: 1.421060]\n",
      "[D loss: 0.683679] [G loss: 1.415060]\n",
      "[D loss: 0.791935] [G loss: 1.392544]\n",
      "[D loss: 0.769337] [G loss: 1.572839]\n",
      "[D loss: 0.992359] [G loss: 1.259686]\n",
      "[D loss: 0.740758] [G loss: 1.539814]\n",
      "[D loss: 1.033168] [G loss: 1.396683]\n",
      "[D loss: 0.770223] [G loss: 1.511697]\n",
      "[D loss: 0.835767] [G loss: 1.531790]\n",
      "[D loss: 0.711810] [G loss: 1.203888]\n",
      "[D loss: 1.110326] [G loss: 1.458349]\n",
      "[D loss: 0.956701] [G loss: 1.429438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.732160] [G loss: 1.639200]\n",
      "[D loss: 0.723402] [G loss: 1.530612]\n",
      "[D loss: 0.766955] [G loss: 1.424763]\n",
      "[D loss: 0.858086] [G loss: 1.265112]\n",
      "[D loss: 0.867174] [G loss: 1.285824]\n",
      "[D loss: 0.916347] [G loss: 1.146769]\n",
      "[D loss: 0.997525] [G loss: 1.348076]\n",
      "[D loss: 0.762659] [G loss: 1.696334]\n",
      "[D loss: 0.916032] [G loss: 1.577868]\n",
      "[D loss: 0.875033] [G loss: 1.761942]\n",
      "[D loss: 0.999599] [G loss: 1.490358]\n",
      "[D loss: 1.000757] [G loss: 1.549261]\n",
      "epoch:6, g_loss:2760.22314453125,d_loss:1551.5540771484375\n",
      "[D loss: 0.991605] [G loss: 1.308500]\n",
      "[D loss: 1.135488] [G loss: 1.351039]\n",
      "[D loss: 0.719270] [G loss: 1.302677]\n",
      "[D loss: 0.872293] [G loss: 1.461078]\n",
      "[D loss: 0.781759] [G loss: 1.624160]\n",
      "[D loss: 1.064780] [G loss: 1.465168]\n",
      "[D loss: 0.973972] [G loss: 1.218517]\n",
      "[D loss: 0.832074] [G loss: 1.165528]\n",
      "[D loss: 0.983194] [G loss: 1.147917]\n",
      "[D loss: 0.838529] [G loss: 1.261163]\n",
      "[D loss: 0.883702] [G loss: 1.399756]\n",
      "[D loss: 0.977738] [G loss: 1.343130]\n",
      "[D loss: 0.964863] [G loss: 1.397196]\n",
      "[D loss: 0.779495] [G loss: 1.396721]\n",
      "[D loss: 0.759873] [G loss: 1.480622]\n",
      "[D loss: 0.789876] [G loss: 1.579773]\n",
      "[D loss: 0.867312] [G loss: 1.419126]\n",
      "[D loss: 0.869118] [G loss: 1.260013]\n",
      "[D loss: 0.980478] [G loss: 1.254778]\n",
      "[D loss: 0.862208] [G loss: 1.384427]\n",
      "[D loss: 0.762555] [G loss: 1.329507]\n",
      "[D loss: 0.720662] [G loss: 1.461798]\n",
      "[D loss: 0.813074] [G loss: 1.430629]\n",
      "[D loss: 0.744003] [G loss: 1.538177]\n",
      "[D loss: 0.804689] [G loss: 1.471631]\n",
      "[D loss: 1.119644] [G loss: 1.192090]\n",
      "[D loss: 1.027911] [G loss: 1.393104]\n",
      "[D loss: 0.680373] [G loss: 1.328122]\n",
      "[D loss: 0.894605] [G loss: 1.347530]\n",
      "[D loss: 1.172046] [G loss: 1.207741]\n",
      "[D loss: 0.786566] [G loss: 1.456069]\n",
      "[D loss: 0.740281] [G loss: 1.507060]\n",
      "[D loss: 0.808107] [G loss: 1.343347]\n",
      "[D loss: 0.951656] [G loss: 1.331594]\n",
      "[D loss: 0.957982] [G loss: 1.189104]\n",
      "[D loss: 0.911144] [G loss: 1.330172]\n",
      "[D loss: 0.974007] [G loss: 1.403451]\n",
      "[D loss: 0.722292] [G loss: 1.538278]\n",
      "[D loss: 1.219565] [G loss: 1.268051]\n",
      "[D loss: 0.986894] [G loss: 1.509994]\n",
      "[D loss: 0.845030] [G loss: 1.433229]\n",
      "[D loss: 0.637821] [G loss: 1.524352]\n",
      "[D loss: 0.781259] [G loss: 1.373847]\n",
      "[D loss: 0.739694] [G loss: 1.330603]\n",
      "[D loss: 0.914340] [G loss: 1.394155]\n",
      "[D loss: 1.034558] [G loss: 1.381002]\n",
      "[D loss: 0.942318] [G loss: 1.158658]\n",
      "[D loss: 0.711803] [G loss: 1.496960]\n",
      "[D loss: 0.681934] [G loss: 1.434643]\n",
      "[D loss: 0.975707] [G loss: 1.230364]\n",
      "[D loss: 0.664629] [G loss: 1.352043]\n",
      "[D loss: 0.806324] [G loss: 1.581300]\n",
      "[D loss: 0.773666] [G loss: 1.504404]\n",
      "[D loss: 0.698305] [G loss: 1.373534]\n",
      "[D loss: 0.832760] [G loss: 1.634249]\n",
      "[D loss: 0.910735] [G loss: 1.393708]\n",
      "[D loss: 0.797801] [G loss: 1.299560]\n",
      "[D loss: 0.610488] [G loss: 1.503747]\n",
      "[D loss: 0.907628] [G loss: 1.203519]\n",
      "[D loss: 0.674347] [G loss: 1.433642]\n",
      "[D loss: 0.923726] [G loss: 1.375435]\n",
      "[D loss: 0.872601] [G loss: 1.475187]\n",
      "[D loss: 1.015024] [G loss: 1.255129]\n",
      "[D loss: 0.656997] [G loss: 1.529335]\n",
      "[D loss: 1.100927] [G loss: 1.253465]\n",
      "[D loss: 0.964463] [G loss: 1.207662]\n",
      "[D loss: 1.136193] [G loss: 1.186706]\n",
      "[D loss: 1.080980] [G loss: 1.638906]\n",
      "[D loss: 0.824405] [G loss: 1.456489]\n",
      "[D loss: 0.933224] [G loss: 1.555505]\n",
      "[D loss: 0.672799] [G loss: 1.463321]\n",
      "[D loss: 0.743444] [G loss: 1.423634]\n",
      "[D loss: 0.910704] [G loss: 1.247319]\n",
      "[D loss: 0.759760] [G loss: 1.368320]\n",
      "[D loss: 0.774860] [G loss: 1.442441]\n",
      "[D loss: 0.774055] [G loss: 1.699592]\n",
      "[D loss: 1.038769] [G loss: 1.540546]\n",
      "[D loss: 0.787229] [G loss: 1.415016]\n",
      "[D loss: 0.756587] [G loss: 1.448283]\n",
      "[D loss: 0.632516] [G loss: 1.651426]\n",
      "[D loss: 0.758728] [G loss: 1.400467]\n",
      "[D loss: 1.037556] [G loss: 1.391213]\n",
      "[D loss: 1.048645] [G loss: 1.349097]\n",
      "[D loss: 1.085229] [G loss: 1.260307]\n",
      "[D loss: 0.865929] [G loss: 1.517843]\n",
      "[D loss: 0.881849] [G loss: 1.289449]\n",
      "[D loss: 0.710835] [G loss: 1.588277]\n",
      "[D loss: 1.148876] [G loss: 1.372373]\n",
      "[D loss: 0.794522] [G loss: 1.387091]\n",
      "[D loss: 0.837363] [G loss: 1.202819]\n",
      "[D loss: 0.830640] [G loss: 1.302546]\n",
      "[D loss: 0.780455] [G loss: 1.429222]\n",
      "[D loss: 0.616666] [G loss: 1.518407]\n",
      "[D loss: 0.916782] [G loss: 1.292112]\n",
      "[D loss: 0.869754] [G loss: 1.554693]\n",
      "[D loss: 0.789791] [G loss: 1.432614]\n",
      "[D loss: 0.648661] [G loss: 1.710717]\n",
      "[D loss: 0.917202] [G loss: 1.412287]\n",
      "[D loss: 0.773888] [G loss: 1.578094]\n",
      "[D loss: 0.587762] [G loss: 1.498721]\n",
      "[D loss: 0.808514] [G loss: 1.313630]\n",
      "[D loss: 1.025148] [G loss: 1.403061]\n",
      "[D loss: 1.011663] [G loss: 1.383715]\n",
      "[D loss: 0.698481] [G loss: 1.741808]\n",
      "[D loss: 0.853221] [G loss: 1.697887]\n",
      "[D loss: 0.762527] [G loss: 1.482688]\n",
      "[D loss: 0.799586] [G loss: 1.455325]\n",
      "[D loss: 0.910310] [G loss: 1.234316]\n",
      "[D loss: 0.919052] [G loss: 1.367266]\n",
      "[D loss: 0.784639] [G loss: 1.464282]\n",
      "[D loss: 0.847708] [G loss: 1.557087]\n",
      "[D loss: 0.725712] [G loss: 1.591101]\n",
      "[D loss: 0.993251] [G loss: 1.363232]\n",
      "[D loss: 0.763376] [G loss: 1.512007]\n",
      "[D loss: 0.771070] [G loss: 1.456353]\n",
      "[D loss: 1.057765] [G loss: 1.360335]\n",
      "[D loss: 0.820575] [G loss: 1.427207]\n",
      "[D loss: 0.689234] [G loss: 1.388604]\n",
      "[D loss: 0.981028] [G loss: 1.355139]\n",
      "[D loss: 0.642334] [G loss: 1.382798]\n",
      "[D loss: 0.929625] [G loss: 1.343943]\n",
      "[D loss: 0.751846] [G loss: 1.391010]\n",
      "[D loss: 0.662700] [G loss: 1.387032]\n",
      "[D loss: 0.748851] [G loss: 1.638433]\n",
      "[D loss: 0.811503] [G loss: 1.668404]\n",
      "[D loss: 0.698075] [G loss: 1.562706]\n",
      "[D loss: 0.801046] [G loss: 1.491370]\n",
      "[D loss: 0.987868] [G loss: 1.251028]\n",
      "[D loss: 1.010137] [G loss: 1.216081]\n",
      "[D loss: 0.739747] [G loss: 1.571096]\n",
      "[D loss: 0.834077] [G loss: 1.411375]\n",
      "[D loss: 0.795376] [G loss: 1.431348]\n",
      "[D loss: 0.810664] [G loss: 1.361082]\n",
      "[D loss: 0.609266] [G loss: 1.361910]\n",
      "[D loss: 0.889745] [G loss: 1.363202]\n",
      "[D loss: 0.818919] [G loss: 1.330411]\n",
      "[D loss: 0.786924] [G loss: 1.657408]\n",
      "[D loss: 1.141436] [G loss: 1.628240]\n",
      "[D loss: 0.839585] [G loss: 1.560708]\n",
      "[D loss: 0.865836] [G loss: 1.576981]\n",
      "[D loss: 0.668248] [G loss: 1.367284]\n",
      "[D loss: 0.828755] [G loss: 1.309108]\n",
      "[D loss: 0.739399] [G loss: 1.513122]\n",
      "[D loss: 0.859319] [G loss: 1.302309]\n",
      "[D loss: 0.686641] [G loss: 1.408665]\n",
      "[D loss: 0.760786] [G loss: 1.514758]\n",
      "[D loss: 0.874356] [G loss: 1.398227]\n",
      "[D loss: 0.655899] [G loss: 1.385105]\n",
      "[D loss: 0.757590] [G loss: 1.445524]\n",
      "[D loss: 0.767008] [G loss: 1.474144]\n",
      "[D loss: 0.729015] [G loss: 1.443290]\n",
      "[D loss: 0.738962] [G loss: 1.445894]\n",
      "[D loss: 0.824883] [G loss: 1.299979]\n",
      "[D loss: 0.792004] [G loss: 1.364543]\n",
      "[D loss: 0.742704] [G loss: 1.397338]\n",
      "[D loss: 0.844205] [G loss: 1.812421]\n",
      "[D loss: 0.814815] [G loss: 1.787324]\n",
      "[D loss: 0.885292] [G loss: 1.616895]\n",
      "[D loss: 0.875537] [G loss: 1.495242]\n",
      "[D loss: 0.890354] [G loss: 1.279733]\n",
      "[D loss: 0.912812] [G loss: 1.694660]\n",
      "[D loss: 0.787848] [G loss: 1.543416]\n",
      "[D loss: 0.741985] [G loss: 1.465554]\n",
      "[D loss: 0.913173] [G loss: 1.308210]\n",
      "[D loss: 0.856500] [G loss: 1.512878]\n",
      "[D loss: 0.891552] [G loss: 1.340447]\n",
      "[D loss: 0.834528] [G loss: 1.288460]\n",
      "[D loss: 0.680888] [G loss: 1.390326]\n",
      "[D loss: 0.859849] [G loss: 1.345700]\n",
      "[D loss: 0.937083] [G loss: 1.514057]\n",
      "[D loss: 0.831635] [G loss: 1.422958]\n",
      "[D loss: 0.520313] [G loss: 1.513921]\n",
      "[D loss: 0.655988] [G loss: 1.581270]\n",
      "[D loss: 0.895220] [G loss: 1.658935]\n",
      "[D loss: 0.851340] [G loss: 1.255232]\n",
      "[D loss: 0.942170] [G loss: 1.343764]\n",
      "[D loss: 0.763638] [G loss: 1.659475]\n",
      "[D loss: 0.908943] [G loss: 1.540756]\n",
      "[D loss: 0.969787] [G loss: 1.299085]\n",
      "[D loss: 0.702076] [G loss: 1.379625]\n",
      "[D loss: 0.776890] [G loss: 1.542887]\n",
      "[D loss: 0.806969] [G loss: 1.683646]\n",
      "[D loss: 0.978809] [G loss: 1.398335]\n",
      "[D loss: 0.811275] [G loss: 1.418181]\n",
      "[D loss: 1.103518] [G loss: 1.363124]\n",
      "[D loss: 0.908184] [G loss: 1.469636]\n",
      "[D loss: 0.710969] [G loss: 1.428307]\n",
      "[D loss: 0.847533] [G loss: 1.377370]\n",
      "[D loss: 0.755370] [G loss: 1.429503]\n",
      "[D loss: 0.744112] [G loss: 1.526699]\n",
      "[D loss: 0.761815] [G loss: 1.363622]\n",
      "[D loss: 0.736169] [G loss: 1.596002]\n",
      "[D loss: 0.746919] [G loss: 1.540998]\n",
      "[D loss: 0.740936] [G loss: 1.511987]\n",
      "[D loss: 0.575197] [G loss: 1.780164]\n",
      "[D loss: 0.532811] [G loss: 1.676979]\n",
      "[D loss: 0.908646] [G loss: 1.733640]\n",
      "[D loss: 0.873295] [G loss: 1.394377]\n",
      "[D loss: 0.938028] [G loss: 1.356955]\n",
      "[D loss: 0.922130] [G loss: 1.418335]\n",
      "[D loss: 1.032319] [G loss: 1.630896]\n",
      "[D loss: 0.976091] [G loss: 1.768964]\n",
      "[D loss: 0.832334] [G loss: 1.726536]\n",
      "[D loss: 0.868760] [G loss: 1.338566]\n",
      "[D loss: 0.881256] [G loss: 1.623983]\n",
      "[D loss: 0.828284] [G loss: 1.270878]\n",
      "[D loss: 0.725532] [G loss: 1.187290]\n",
      "[D loss: 0.632245] [G loss: 1.433373]\n",
      "[D loss: 0.842460] [G loss: 1.408994]\n",
      "[D loss: 0.860164] [G loss: 1.542273]\n",
      "[D loss: 0.780170] [G loss: 1.452229]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.593123] [G loss: 1.480881]\n",
      "[D loss: 0.830109] [G loss: 1.506240]\n",
      "[D loss: 0.679716] [G loss: 1.513760]\n",
      "[D loss: 0.887190] [G loss: 1.417774]\n",
      "[D loss: 0.737296] [G loss: 1.806007]\n",
      "[D loss: 0.775106] [G loss: 1.520818]\n",
      "[D loss: 1.042204] [G loss: 1.685958]\n",
      "[D loss: 0.709714] [G loss: 1.413763]\n",
      "[D loss: 0.853629] [G loss: 1.616847]\n",
      "[D loss: 0.686677] [G loss: 1.354959]\n",
      "[D loss: 0.814670] [G loss: 1.375233]\n",
      "[D loss: 0.639489] [G loss: 1.529910]\n",
      "[D loss: 0.999582] [G loss: 1.372295]\n",
      "[D loss: 0.916235] [G loss: 1.538388]\n",
      "[D loss: 0.838146] [G loss: 1.865654]\n",
      "[D loss: 0.889145] [G loss: 1.569729]\n",
      "[D loss: 0.833166] [G loss: 1.327443]\n",
      "[D loss: 0.700226] [G loss: 1.531682]\n",
      "[D loss: 0.886041] [G loss: 1.412573]\n",
      "[D loss: 0.938005] [G loss: 1.492016]\n",
      "[D loss: 0.521561] [G loss: 1.272458]\n",
      "[D loss: 0.675801] [G loss: 1.483181]\n",
      "[D loss: 1.223875] [G loss: 1.273679]\n",
      "[D loss: 0.990726] [G loss: 1.287561]\n",
      "[D loss: 0.687207] [G loss: 1.419522]\n",
      "[D loss: 0.855248] [G loss: 1.465965]\n",
      "[D loss: 0.740347] [G loss: 1.455786]\n",
      "[D loss: 0.707121] [G loss: 1.623700]\n",
      "[D loss: 0.784000] [G loss: 1.746461]\n",
      "[D loss: 0.756930] [G loss: 1.465511]\n",
      "[D loss: 0.546405] [G loss: 1.734249]\n",
      "[D loss: 0.809777] [G loss: 1.552512]\n",
      "[D loss: 0.726634] [G loss: 1.365555]\n",
      "[D loss: 0.771874] [G loss: 1.579991]\n",
      "[D loss: 1.133510] [G loss: 1.315763]\n",
      "[D loss: 0.752477] [G loss: 1.509151]\n",
      "[D loss: 0.840901] [G loss: 1.424293]\n",
      "[D loss: 0.781015] [G loss: 1.635164]\n",
      "[D loss: 0.710456] [G loss: 1.432310]\n",
      "[D loss: 0.685006] [G loss: 1.554888]\n",
      "[D loss: 0.884824] [G loss: 1.484749]\n",
      "[D loss: 0.762466] [G loss: 1.327212]\n",
      "[D loss: 0.846609] [G loss: 1.305413]\n",
      "[D loss: 0.857647] [G loss: 1.668444]\n",
      "[D loss: 0.770242] [G loss: 1.464730]\n",
      "[D loss: 0.925188] [G loss: 1.503356]\n",
      "[D loss: 0.970411] [G loss: 1.267228]\n",
      "[D loss: 0.896000] [G loss: 1.299241]\n",
      "[D loss: 0.835816] [G loss: 1.718382]\n",
      "[D loss: 0.694100] [G loss: 1.531032]\n",
      "[D loss: 0.768279] [G loss: 1.586916]\n",
      "[D loss: 0.755050] [G loss: 1.583664]\n",
      "[D loss: 0.829563] [G loss: 1.743407]\n",
      "[D loss: 0.963006] [G loss: 1.627483]\n",
      "[D loss: 0.780370] [G loss: 1.291648]\n",
      "[D loss: 1.131333] [G loss: 1.328368]\n",
      "[D loss: 0.915892] [G loss: 1.337709]\n",
      "[D loss: 0.933838] [G loss: 1.266810]\n",
      "[D loss: 0.900459] [G loss: 1.390770]\n",
      "[D loss: 0.677822] [G loss: 1.407818]\n",
      "[D loss: 0.837269] [G loss: 1.364627]\n",
      "[D loss: 1.119730] [G loss: 1.411415]\n",
      "[D loss: 0.883962] [G loss: 1.339087]\n",
      "[D loss: 0.741972] [G loss: 1.533463]\n",
      "[D loss: 0.847056] [G loss: 1.455252]\n",
      "[D loss: 0.931512] [G loss: 1.270273]\n",
      "[D loss: 0.772275] [G loss: 1.388425]\n",
      "[D loss: 0.746216] [G loss: 1.462897]\n",
      "[D loss: 0.950795] [G loss: 1.616828]\n",
      "[D loss: 0.996980] [G loss: 1.643504]\n",
      "[D loss: 1.029795] [G loss: 1.338692]\n",
      "[D loss: 0.796933] [G loss: 1.275589]\n",
      "[D loss: 0.657610] [G loss: 1.481422]\n",
      "[D loss: 0.802120] [G loss: 1.609113]\n",
      "[D loss: 0.683434] [G loss: 1.576003]\n",
      "[D loss: 0.808284] [G loss: 1.448107]\n",
      "[D loss: 0.926683] [G loss: 1.233789]\n",
      "[D loss: 0.750701] [G loss: 1.521836]\n",
      "[D loss: 0.850712] [G loss: 1.338403]\n",
      "[D loss: 0.632624] [G loss: 1.458250]\n",
      "[D loss: 0.717738] [G loss: 1.474975]\n",
      "[D loss: 0.935124] [G loss: 1.794472]\n",
      "[D loss: 0.856534] [G loss: 1.527899]\n",
      "[D loss: 0.854815] [G loss: 1.432309]\n",
      "[D loss: 0.543367] [G loss: 1.434851]\n",
      "[D loss: 0.922956] [G loss: 1.556664]\n",
      "[D loss: 0.747185] [G loss: 1.612888]\n",
      "[D loss: 0.736271] [G loss: 1.502183]\n",
      "[D loss: 0.814489] [G loss: 1.508497]\n",
      "[D loss: 0.896078] [G loss: 1.357290]\n",
      "[D loss: 0.581460] [G loss: 1.566023]\n",
      "[D loss: 0.722910] [G loss: 1.493725]\n",
      "[D loss: 0.670922] [G loss: 1.528592]\n",
      "[D loss: 0.925048] [G loss: 1.252987]\n",
      "[D loss: 0.843143] [G loss: 1.512406]\n",
      "[D loss: 0.651547] [G loss: 1.618918]\n",
      "[D loss: 0.799923] [G loss: 1.467315]\n",
      "[D loss: 0.925966] [G loss: 1.273335]\n",
      "[D loss: 0.906299] [G loss: 1.486982]\n",
      "[D loss: 1.065105] [G loss: 1.373794]\n",
      "[D loss: 0.816380] [G loss: 1.523357]\n",
      "[D loss: 0.738331] [G loss: 1.499832]\n",
      "[D loss: 0.988116] [G loss: 1.587727]\n",
      "[D loss: 0.810489] [G loss: 1.579740]\n",
      "[D loss: 0.734048] [G loss: 1.664487]\n",
      "[D loss: 0.767660] [G loss: 1.542059]\n",
      "[D loss: 0.733100] [G loss: 1.431777]\n",
      "[D loss: 0.983396] [G loss: 1.405088]\n",
      "[D loss: 0.813171] [G loss: 1.363485]\n",
      "[D loss: 0.812111] [G loss: 1.447467]\n",
      "[D loss: 0.711101] [G loss: 1.500066]\n",
      "[D loss: 0.843359] [G loss: 1.278073]\n",
      "[D loss: 0.705608] [G loss: 1.418272]\n",
      "[D loss: 1.067859] [G loss: 1.108698]\n",
      "[D loss: 0.792983] [G loss: 1.589051]\n",
      "[D loss: 0.852567] [G loss: 1.616942]\n",
      "[D loss: 0.711724] [G loss: 1.614285]\n",
      "[D loss: 0.869788] [G loss: 1.477909]\n",
      "[D loss: 0.767831] [G loss: 1.593896]\n",
      "[D loss: 0.810504] [G loss: 1.382700]\n",
      "[D loss: 0.981299] [G loss: 1.495268]\n",
      "[D loss: 1.115081] [G loss: 1.383291]\n",
      "[D loss: 0.657518] [G loss: 1.536343]\n",
      "[D loss: 0.871308] [G loss: 1.452732]\n",
      "[D loss: 0.746655] [G loss: 1.286010]\n",
      "[D loss: 0.825215] [G loss: 1.329216]\n",
      "[D loss: 0.933292] [G loss: 1.554819]\n",
      "[D loss: 0.667684] [G loss: 1.725486]\n",
      "[D loss: 0.592316] [G loss: 1.476954]\n",
      "[D loss: 0.814816] [G loss: 1.425322]\n",
      "[D loss: 0.651308] [G loss: 1.420429]\n",
      "[D loss: 0.749446] [G loss: 1.455826]\n",
      "[D loss: 0.691324] [G loss: 1.726376]\n",
      "[D loss: 0.683765] [G loss: 1.723657]\n",
      "[D loss: 1.003922] [G loss: 1.691532]\n",
      "[D loss: 0.996901] [G loss: 1.140910]\n",
      "[D loss: 0.683803] [G loss: 1.564613]\n",
      "[D loss: 0.817008] [G loss: 1.596735]\n",
      "[D loss: 0.721951] [G loss: 1.688572]\n",
      "[D loss: 0.884578] [G loss: 1.502231]\n",
      "[D loss: 0.733952] [G loss: 1.465730]\n",
      "[D loss: 0.612468] [G loss: 1.742326]\n",
      "[D loss: 0.602491] [G loss: 1.610102]\n",
      "[D loss: 0.973588] [G loss: 1.490242]\n",
      "[D loss: 0.664873] [G loss: 1.464009]\n",
      "[D loss: 0.898267] [G loss: 1.262281]\n",
      "[D loss: 0.564900] [G loss: 1.624622]\n",
      "[D loss: 0.762114] [G loss: 1.553312]\n",
      "[D loss: 0.878673] [G loss: 1.448549]\n",
      "[D loss: 0.987935] [G loss: 1.247300]\n",
      "[D loss: 0.855271] [G loss: 1.860096]\n",
      "[D loss: 0.873996] [G loss: 1.685274]\n",
      "[D loss: 1.141618] [G loss: 1.949293]\n",
      "[D loss: 1.045290] [G loss: 1.386475]\n",
      "[D loss: 0.704940] [G loss: 1.473543]\n",
      "[D loss: 0.782447] [G loss: 1.492943]\n",
      "[D loss: 1.075805] [G loss: 1.387796]\n",
      "[D loss: 1.007514] [G loss: 1.348293]\n",
      "[D loss: 1.061001] [G loss: 1.304545]\n",
      "[D loss: 0.932517] [G loss: 1.316479]\n",
      "[D loss: 0.951744] [G loss: 1.369856]\n",
      "[D loss: 0.698380] [G loss: 1.504372]\n",
      "[D loss: 0.894051] [G loss: 1.349405]\n",
      "[D loss: 0.839215] [G loss: 1.207158]\n",
      "[D loss: 0.813543] [G loss: 1.447676]\n",
      "[D loss: 0.774328] [G loss: 1.456167]\n",
      "[D loss: 0.896318] [G loss: 1.240378]\n",
      "[D loss: 0.932138] [G loss: 1.243418]\n",
      "[D loss: 0.791230] [G loss: 1.508837]\n",
      "[D loss: 0.800011] [G loss: 1.402717]\n",
      "[D loss: 1.062951] [G loss: 1.566214]\n",
      "[D loss: 0.755043] [G loss: 1.518786]\n",
      "[D loss: 0.990971] [G loss: 1.401378]\n",
      "[D loss: 0.659181] [G loss: 1.224412]\n",
      "[D loss: 0.764748] [G loss: 1.583678]\n",
      "[D loss: 1.004259] [G loss: 1.491206]\n",
      "[D loss: 0.901603] [G loss: 1.341336]\n",
      "[D loss: 0.810290] [G loss: 1.251190]\n",
      "[D loss: 1.055222] [G loss: 1.301019]\n",
      "[D loss: 0.850407] [G loss: 1.465085]\n",
      "[D loss: 0.938771] [G loss: 1.534587]\n",
      "[D loss: 0.770525] [G loss: 1.397503]\n",
      "[D loss: 0.721468] [G loss: 1.520387]\n",
      "[D loss: 0.705505] [G loss: 1.478600]\n",
      "[D loss: 0.856484] [G loss: 1.542953]\n",
      "[D loss: 0.945726] [G loss: 1.517040]\n",
      "[D loss: 0.980346] [G loss: 1.446450]\n",
      "[D loss: 0.599286] [G loss: 1.550475]\n",
      "[D loss: 0.704446] [G loss: 1.428574]\n",
      "[D loss: 0.997895] [G loss: 1.172693]\n",
      "[D loss: 0.833226] [G loss: 1.408880]\n",
      "[D loss: 0.928506] [G loss: 1.373151]\n",
      "[D loss: 0.772750] [G loss: 1.470581]\n",
      "[D loss: 0.894283] [G loss: 1.450589]\n",
      "[D loss: 0.800176] [G loss: 1.570119]\n",
      "[D loss: 0.956988] [G loss: 1.484987]\n",
      "[D loss: 0.874453] [G loss: 1.444320]\n",
      "[D loss: 0.738649] [G loss: 1.328573]\n",
      "[D loss: 0.845246] [G loss: 1.090132]\n",
      "[D loss: 0.788949] [G loss: 1.346791]\n",
      "[D loss: 0.935278] [G loss: 1.390684]\n",
      "[D loss: 0.515504] [G loss: 1.441243]\n",
      "[D loss: 0.690249] [G loss: 1.685050]\n",
      "[D loss: 0.871720] [G loss: 1.695011]\n",
      "[D loss: 0.724139] [G loss: 1.651717]\n",
      "[D loss: 0.788190] [G loss: 1.408015]\n",
      "[D loss: 0.778662] [G loss: 1.538875]\n",
      "[D loss: 1.062139] [G loss: 1.359783]\n",
      "[D loss: 0.704301] [G loss: 1.705400]\n",
      "[D loss: 0.830332] [G loss: 1.450914]\n",
      "[D loss: 1.108034] [G loss: 1.261019]\n",
      "[D loss: 0.865556] [G loss: 1.461413]\n",
      "[D loss: 0.979099] [G loss: 1.580283]\n",
      "[D loss: 0.932047] [G loss: 1.274180]\n",
      "[D loss: 0.648360] [G loss: 1.372005]\n",
      "[D loss: 0.727710] [G loss: 1.347976]\n",
      "[D loss: 0.728721] [G loss: 1.536351]\n",
      "[D loss: 0.681296] [G loss: 1.464289]\n",
      "[D loss: 0.900281] [G loss: 1.371902]\n",
      "[D loss: 1.036262] [G loss: 1.280876]\n",
      "[D loss: 0.633464] [G loss: 1.649308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.730266] [G loss: 1.447506]\n",
      "[D loss: 1.062401] [G loss: 1.452164]\n",
      "[D loss: 0.867517] [G loss: 1.461790]\n",
      "[D loss: 0.623755] [G loss: 1.461909]\n",
      "[D loss: 0.795120] [G loss: 1.521840]\n",
      "[D loss: 0.920394] [G loss: 1.353797]\n",
      "[D loss: 0.766831] [G loss: 1.344292]\n",
      "[D loss: 0.858200] [G loss: 1.354054]\n",
      "[D loss: 0.843139] [G loss: 1.515065]\n",
      "[D loss: 0.842023] [G loss: 1.380181]\n",
      "[D loss: 0.821981] [G loss: 1.440339]\n",
      "[D loss: 0.772319] [G loss: 1.429410]\n",
      "[D loss: 0.857683] [G loss: 1.638308]\n",
      "[D loss: 0.879712] [G loss: 1.527577]\n",
      "[D loss: 0.774160] [G loss: 1.429971]\n",
      "[D loss: 0.980349] [G loss: 1.350354]\n",
      "[D loss: 0.814829] [G loss: 1.489655]\n",
      "[D loss: 0.834336] [G loss: 1.570561]\n",
      "[D loss: 0.824220] [G loss: 1.342224]\n",
      "[D loss: 0.975365] [G loss: 1.432757]\n",
      "[D loss: 0.851112] [G loss: 1.624135]\n",
      "[D loss: 0.805321] [G loss: 1.664512]\n",
      "[D loss: 0.856641] [G loss: 1.424192]\n",
      "[D loss: 0.928862] [G loss: 1.263573]\n",
      "[D loss: 0.839878] [G loss: 1.632393]\n",
      "[D loss: 1.069646] [G loss: 1.534580]\n",
      "[D loss: 0.866160] [G loss: 1.608588]\n",
      "[D loss: 1.134239] [G loss: 1.289935]\n",
      "[D loss: 0.857523] [G loss: 1.341169]\n",
      "[D loss: 0.780573] [G loss: 1.342417]\n",
      "[D loss: 0.697825] [G loss: 1.379079]\n",
      "[D loss: 0.842080] [G loss: 1.519685]\n",
      "[D loss: 0.678261] [G loss: 1.481503]\n",
      "[D loss: 0.863198] [G loss: 1.458325]\n",
      "[D loss: 0.771744] [G loss: 1.406728]\n",
      "[D loss: 0.871319] [G loss: 1.544857]\n",
      "[D loss: 0.670465] [G loss: 1.409739]\n",
      "[D loss: 0.606079] [G loss: 1.672486]\n",
      "[D loss: 0.949951] [G loss: 1.313342]\n",
      "[D loss: 0.669425] [G loss: 1.496909]\n",
      "[D loss: 0.929881] [G loss: 1.391966]\n",
      "[D loss: 0.779483] [G loss: 1.364579]\n",
      "[D loss: 0.883321] [G loss: 1.186864]\n",
      "[D loss: 0.759622] [G loss: 1.576284]\n",
      "[D loss: 0.769159] [G loss: 1.460201]\n",
      "[D loss: 0.729913] [G loss: 1.556732]\n",
      "[D loss: 0.829715] [G loss: 1.431116]\n",
      "[D loss: 0.658467] [G loss: 1.613535]\n",
      "[D loss: 0.730008] [G loss: 1.416707]\n",
      "[D loss: 0.715482] [G loss: 1.508411]\n",
      "[D loss: 1.178380] [G loss: 1.549585]\n",
      "[D loss: 0.803072] [G loss: 1.420746]\n",
      "[D loss: 0.861031] [G loss: 1.411388]\n",
      "[D loss: 0.499952] [G loss: 1.659817]\n",
      "[D loss: 0.716599] [G loss: 1.442882]\n",
      "[D loss: 0.710701] [G loss: 1.611698]\n",
      "[D loss: 0.971258] [G loss: 1.384429]\n",
      "[D loss: 0.691836] [G loss: 1.508388]\n",
      "[D loss: 0.910940] [G loss: 1.471629]\n",
      "[D loss: 0.903193] [G loss: 1.554713]\n",
      "[D loss: 0.882903] [G loss: 1.541521]\n",
      "[D loss: 0.913402] [G loss: 1.528254]\n",
      "[D loss: 0.795696] [G loss: 1.319848]\n",
      "[D loss: 0.902330] [G loss: 1.424526]\n",
      "[D loss: 0.793655] [G loss: 1.344272]\n",
      "[D loss: 1.031767] [G loss: 1.417259]\n",
      "[D loss: 0.804186] [G loss: 1.541255]\n",
      "[D loss: 1.027220] [G loss: 1.365631]\n",
      "[D loss: 0.815735] [G loss: 1.310935]\n",
      "[D loss: 0.727824] [G loss: 1.596146]\n",
      "[D loss: 0.810955] [G loss: 1.565534]\n",
      "[D loss: 1.074350] [G loss: 1.407191]\n",
      "[D loss: 0.861062] [G loss: 1.447281]\n",
      "[D loss: 0.812282] [G loss: 1.481983]\n",
      "[D loss: 0.696169] [G loss: 1.408485]\n",
      "[D loss: 0.962899] [G loss: 1.174771]\n",
      "[D loss: 0.842652] [G loss: 1.401550]\n",
      "[D loss: 0.710456] [G loss: 1.224638]\n",
      "[D loss: 0.852559] [G loss: 1.357512]\n",
      "[D loss: 0.803236] [G loss: 1.391749]\n",
      "[D loss: 0.819195] [G loss: 1.345011]\n",
      "[D loss: 0.717855] [G loss: 1.422924]\n",
      "[D loss: 1.107087] [G loss: 1.476562]\n",
      "[D loss: 0.835596] [G loss: 1.486148]\n",
      "[D loss: 1.040168] [G loss: 1.288011]\n",
      "[D loss: 0.581354] [G loss: 1.347494]\n",
      "[D loss: 0.711459] [G loss: 1.648089]\n",
      "[D loss: 0.894933] [G loss: 1.629302]\n",
      "[D loss: 0.868892] [G loss: 1.414850]\n",
      "[D loss: 0.804773] [G loss: 1.238274]\n",
      "[D loss: 0.935368] [G loss: 1.603401]\n",
      "[D loss: 0.934022] [G loss: 1.303527]\n",
      "[D loss: 0.896442] [G loss: 1.206194]\n",
      "[D loss: 0.879993] [G loss: 1.333224]\n",
      "[D loss: 1.006936] [G loss: 1.447710]\n",
      "[D loss: 0.661726] [G loss: 1.437232]\n",
      "[D loss: 0.828882] [G loss: 1.351159]\n",
      "[D loss: 0.834919] [G loss: 1.533320]\n",
      "[D loss: 0.584314] [G loss: 1.511997]\n",
      "[D loss: 0.724364] [G loss: 1.383763]\n",
      "[D loss: 0.810299] [G loss: 1.307199]\n",
      "[D loss: 0.800470] [G loss: 1.400128]\n",
      "[D loss: 0.618926] [G loss: 1.544830]\n",
      "[D loss: 1.038347] [G loss: 1.383977]\n",
      "[D loss: 0.910231] [G loss: 1.482499]\n",
      "[D loss: 0.663238] [G loss: 1.281998]\n",
      "[D loss: 0.720504] [G loss: 1.462359]\n",
      "[D loss: 0.900542] [G loss: 1.464870]\n",
      "[D loss: 0.861840] [G loss: 1.424378]\n",
      "[D loss: 0.853083] [G loss: 1.649381]\n",
      "[D loss: 0.743081] [G loss: 1.624060]\n",
      "[D loss: 0.893618] [G loss: 1.614762]\n",
      "[D loss: 0.940477] [G loss: 1.720070]\n",
      "[D loss: 0.640796] [G loss: 1.637954]\n",
      "[D loss: 0.942516] [G loss: 1.405851]\n",
      "[D loss: 0.729037] [G loss: 1.396171]\n",
      "[D loss: 0.809443] [G loss: 1.347290]\n",
      "[D loss: 0.724591] [G loss: 1.548264]\n",
      "[D loss: 0.885231] [G loss: 1.439805]\n",
      "[D loss: 0.598162] [G loss: 1.631565]\n",
      "[D loss: 0.872850] [G loss: 1.340042]\n",
      "[D loss: 0.692416] [G loss: 1.696082]\n",
      "[D loss: 1.057145] [G loss: 1.510160]\n",
      "[D loss: 1.068568] [G loss: 1.456338]\n",
      "[D loss: 0.708728] [G loss: 1.622448]\n",
      "[D loss: 0.758971] [G loss: 1.610983]\n",
      "[D loss: 0.854401] [G loss: 1.514288]\n",
      "[D loss: 0.763178] [G loss: 1.426965]\n",
      "[D loss: 0.629341] [G loss: 1.539365]\n",
      "[D loss: 0.780359] [G loss: 1.235813]\n",
      "[D loss: 0.884300] [G loss: 1.450272]\n",
      "[D loss: 0.737056] [G loss: 1.495061]\n",
      "[D loss: 1.037670] [G loss: 1.392594]\n",
      "[D loss: 0.824740] [G loss: 1.784789]\n",
      "[D loss: 0.645598] [G loss: 1.694396]\n",
      "[D loss: 1.038283] [G loss: 1.457683]\n",
      "[D loss: 0.688509] [G loss: 1.438928]\n",
      "[D loss: 0.601929] [G loss: 1.556250]\n",
      "[D loss: 0.838929] [G loss: 1.488513]\n",
      "[D loss: 0.799777] [G loss: 1.717034]\n",
      "[D loss: 0.803120] [G loss: 1.670157]\n",
      "[D loss: 0.862088] [G loss: 1.656982]\n",
      "[D loss: 0.814063] [G loss: 1.604884]\n",
      "[D loss: 0.890370] [G loss: 1.313486]\n",
      "[D loss: 0.705846] [G loss: 1.555579]\n",
      "[D loss: 1.042030] [G loss: 1.386722]\n",
      "[D loss: 0.891505] [G loss: 1.363973]\n",
      "[D loss: 0.788344] [G loss: 1.710968]\n",
      "[D loss: 0.870266] [G loss: 1.617085]\n",
      "[D loss: 0.380574] [G loss: 1.759326]\n",
      "[D loss: 0.913689] [G loss: 1.689596]\n",
      "[D loss: 0.930693] [G loss: 1.506282]\n",
      "[D loss: 0.662776] [G loss: 1.476851]\n",
      "[D loss: 0.709138] [G loss: 1.692457]\n",
      "[D loss: 0.929157] [G loss: 1.447766]\n",
      "[D loss: 0.996737] [G loss: 1.767155]\n",
      "[D loss: 1.168415] [G loss: 1.364838]\n",
      "[D loss: 1.095269] [G loss: 1.367278]\n",
      "[D loss: 0.836664] [G loss: 1.346059]\n",
      "[D loss: 0.942493] [G loss: 1.404363]\n",
      "[D loss: 0.593606] [G loss: 1.474530]\n",
      "[D loss: 0.852719] [G loss: 1.400033]\n",
      "[D loss: 0.966597] [G loss: 1.213042]\n",
      "[D loss: 0.700504] [G loss: 1.400729]\n",
      "[D loss: 0.923323] [G loss: 1.744173]\n",
      "[D loss: 0.944416] [G loss: 1.483532]\n",
      "[D loss: 0.867581] [G loss: 1.582940]\n",
      "[D loss: 0.789158] [G loss: 1.534795]\n",
      "[D loss: 0.760934] [G loss: 1.498114]\n",
      "[D loss: 0.776187] [G loss: 1.669228]\n",
      "[D loss: 1.050040] [G loss: 1.516025]\n",
      "[D loss: 0.917416] [G loss: 1.500869]\n",
      "[D loss: 1.165442] [G loss: 1.306372]\n",
      "[D loss: 0.610964] [G loss: 1.588394]\n",
      "[D loss: 0.792874] [G loss: 1.349181]\n",
      "[D loss: 0.835509] [G loss: 1.350214]\n",
      "[D loss: 0.821681] [G loss: 1.437207]\n",
      "[D loss: 0.893178] [G loss: 1.443837]\n",
      "[D loss: 0.754761] [G loss: 1.496197]\n",
      "[D loss: 0.841304] [G loss: 1.622618]\n",
      "[D loss: 0.806730] [G loss: 1.355680]\n",
      "[D loss: 1.179400] [G loss: 1.380778]\n",
      "[D loss: 0.719716] [G loss: 1.570541]\n",
      "[D loss: 0.822029] [G loss: 1.481179]\n",
      "[D loss: 0.681538] [G loss: 1.560234]\n",
      "[D loss: 0.836691] [G loss: 1.419200]\n",
      "[D loss: 0.746436] [G loss: 1.337102]\n",
      "[D loss: 0.753220] [G loss: 1.439016]\n",
      "[D loss: 0.743249] [G loss: 1.356661]\n",
      "[D loss: 1.074791] [G loss: 1.185762]\n",
      "[D loss: 1.006592] [G loss: 1.420033]\n",
      "[D loss: 0.797419] [G loss: 1.618293]\n",
      "[D loss: 0.887208] [G loss: 1.352287]\n",
      "[D loss: 0.818387] [G loss: 1.672507]\n",
      "[D loss: 0.857562] [G loss: 1.550543]\n",
      "[D loss: 0.720815] [G loss: 1.533357]\n",
      "[D loss: 0.643902] [G loss: 1.518354]\n",
      "[D loss: 0.865372] [G loss: 1.340872]\n",
      "[D loss: 0.753206] [G loss: 1.510920]\n",
      "[D loss: 0.630958] [G loss: 1.639904]\n",
      "[D loss: 0.599457] [G loss: 1.702749]\n",
      "[D loss: 0.871568] [G loss: 1.526044]\n",
      "[D loss: 0.741056] [G loss: 1.554827]\n",
      "[D loss: 1.113241] [G loss: 1.248465]\n",
      "[D loss: 0.531987] [G loss: 1.474968]\n",
      "[D loss: 0.856617] [G loss: 1.736712]\n",
      "[D loss: 0.731050] [G loss: 1.483629]\n",
      "[D loss: 0.824124] [G loss: 1.287431]\n",
      "[D loss: 0.765454] [G loss: 1.834966]\n",
      "[D loss: 0.929964] [G loss: 1.805533]\n",
      "[D loss: 0.832110] [G loss: 1.434497]\n",
      "[D loss: 0.797993] [G loss: 1.538332]\n",
      "[D loss: 0.812577] [G loss: 1.656552]\n",
      "[D loss: 0.669893] [G loss: 1.543266]\n",
      "[D loss: 0.673860] [G loss: 1.393641]\n",
      "[D loss: 0.815692] [G loss: 1.483935]\n",
      "[D loss: 0.720552] [G loss: 1.454760]\n",
      "[D loss: 0.707765] [G loss: 1.480555]\n",
      "[D loss: 0.886432] [G loss: 1.380165]\n",
      "[D loss: 0.743542] [G loss: 1.643664]\n",
      "[D loss: 0.801966] [G loss: 1.781528]\n",
      "[D loss: 0.933144] [G loss: 1.632181]\n",
      "[D loss: 0.949903] [G loss: 1.555192]\n",
      "[D loss: 0.913691] [G loss: 1.499579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.820633] [G loss: 1.416650]\n",
      "[D loss: 1.013515] [G loss: 1.166718]\n",
      "[D loss: 0.895102] [G loss: 1.625647]\n",
      "[D loss: 0.742772] [G loss: 1.517558]\n",
      "[D loss: 0.668829] [G loss: 1.575763]\n",
      "[D loss: 0.746642] [G loss: 1.464187]\n",
      "[D loss: 0.766892] [G loss: 1.411465]\n",
      "[D loss: 0.972938] [G loss: 1.465560]\n",
      "[D loss: 0.908738] [G loss: 1.772166]\n",
      "[D loss: 0.718746] [G loss: 1.391566]\n",
      "[D loss: 0.824283] [G loss: 1.293922]\n",
      "[D loss: 0.865470] [G loss: 1.356125]\n",
      "[D loss: 0.833821] [G loss: 1.553220]\n",
      "[D loss: 0.810153] [G loss: 1.510373]\n",
      "[D loss: 0.786895] [G loss: 1.357583]\n",
      "[D loss: 0.945797] [G loss: 1.352060]\n",
      "[D loss: 0.862345] [G loss: 1.341508]\n",
      "[D loss: 0.776818] [G loss: 1.397142]\n",
      "[D loss: 0.884316] [G loss: 1.501049]\n",
      "[D loss: 0.905811] [G loss: 1.294458]\n",
      "[D loss: 1.071874] [G loss: 1.364448]\n",
      "[D loss: 0.808860] [G loss: 1.330064]\n",
      "[D loss: 0.933985] [G loss: 1.277784]\n",
      "[D loss: 0.794338] [G loss: 1.536918]\n",
      "[D loss: 0.687128] [G loss: 1.544090]\n",
      "[D loss: 0.733940] [G loss: 1.667733]\n",
      "[D loss: 0.904964] [G loss: 1.218233]\n",
      "[D loss: 0.565980] [G loss: 1.612988]\n",
      "[D loss: 0.624690] [G loss: 1.601782]\n",
      "[D loss: 0.720056] [G loss: 1.567839]\n",
      "[D loss: 0.842622] [G loss: 1.449441]\n",
      "[D loss: 0.810939] [G loss: 1.494425]\n",
      "[D loss: 0.902032] [G loss: 1.395016]\n",
      "[D loss: 0.761634] [G loss: 1.588145]\n",
      "[D loss: 0.702683] [G loss: 1.435174]\n",
      "[D loss: 0.448433] [G loss: 1.503314]\n",
      "[D loss: 0.699913] [G loss: 1.354571]\n",
      "[D loss: 0.885482] [G loss: 1.356292]\n",
      "[D loss: 0.713713] [G loss: 1.511261]\n",
      "[D loss: 1.076873] [G loss: 1.637280]\n",
      "[D loss: 1.036548] [G loss: 1.609462]\n",
      "[D loss: 0.837978] [G loss: 1.466513]\n",
      "[D loss: 0.874320] [G loss: 1.555564]\n",
      "[D loss: 0.864365] [G loss: 1.469945]\n",
      "[D loss: 0.803950] [G loss: 1.461311]\n",
      "[D loss: 1.067241] [G loss: 1.260812]\n",
      "[D loss: 0.719722] [G loss: 1.376805]\n",
      "[D loss: 0.867494] [G loss: 1.390901]\n",
      "[D loss: 0.850595] [G loss: 1.387840]\n",
      "[D loss: 0.692227] [G loss: 1.404676]\n",
      "[D loss: 0.803122] [G loss: 1.557011]\n",
      "[D loss: 0.915348] [G loss: 1.413205]\n",
      "[D loss: 0.766359] [G loss: 1.562468]\n",
      "[D loss: 0.796987] [G loss: 1.638264]\n",
      "[D loss: 0.660857] [G loss: 1.486620]\n",
      "[D loss: 0.703559] [G loss: 1.261084]\n",
      "[D loss: 0.727988] [G loss: 1.558190]\n",
      "[D loss: 0.857231] [G loss: 1.404357]\n",
      "[D loss: 0.967297] [G loss: 1.380952]\n",
      "[D loss: 0.738111] [G loss: 1.479274]\n",
      "[D loss: 0.909092] [G loss: 1.287690]\n",
      "[D loss: 0.885220] [G loss: 1.341847]\n",
      "[D loss: 0.731693] [G loss: 1.428963]\n",
      "[D loss: 0.854527] [G loss: 1.258009]\n",
      "[D loss: 0.774761] [G loss: 1.263585]\n",
      "[D loss: 0.857342] [G loss: 1.368679]\n",
      "[D loss: 0.785074] [G loss: 1.512528]\n",
      "[D loss: 0.812750] [G loss: 1.502409]\n",
      "[D loss: 0.812598] [G loss: 1.584451]\n",
      "[D loss: 0.826546] [G loss: 1.580427]\n",
      "[D loss: 0.818780] [G loss: 1.373346]\n",
      "[D loss: 0.738829] [G loss: 1.406667]\n",
      "[D loss: 0.614731] [G loss: 1.491734]\n",
      "[D loss: 0.930293] [G loss: 1.398673]\n",
      "[D loss: 0.828413] [G loss: 1.498518]\n",
      "[D loss: 0.755598] [G loss: 1.619623]\n",
      "[D loss: 0.765492] [G loss: 1.414947]\n",
      "[D loss: 1.079223] [G loss: 1.567107]\n",
      "[D loss: 0.720081] [G loss: 1.664706]\n",
      "[D loss: 0.593809] [G loss: 1.641694]\n",
      "[D loss: 0.778821] [G loss: 1.511777]\n",
      "[D loss: 0.910682] [G loss: 1.439920]\n",
      "[D loss: 0.960541] [G loss: 1.577399]\n",
      "[D loss: 0.761306] [G loss: 1.376192]\n",
      "[D loss: 0.860354] [G loss: 1.245646]\n",
      "[D loss: 0.749509] [G loss: 1.552850]\n",
      "[D loss: 1.027352] [G loss: 1.326990]\n",
      "[D loss: 0.979423] [G loss: 1.391341]\n",
      "[D loss: 0.733127] [G loss: 1.548577]\n",
      "[D loss: 0.615160] [G loss: 1.447290]\n",
      "[D loss: 1.005405] [G loss: 1.670553]\n",
      "[D loss: 0.690569] [G loss: 1.318562]\n",
      "[D loss: 0.879857] [G loss: 1.327649]\n",
      "[D loss: 0.822660] [G loss: 1.712114]\n",
      "[D loss: 0.691249] [G loss: 1.334640]\n",
      "[D loss: 0.603328] [G loss: 1.518744]\n",
      "[D loss: 0.794648] [G loss: 1.592879]\n",
      "[D loss: 0.557770] [G loss: 1.605015]\n",
      "[D loss: 0.804560] [G loss: 1.612204]\n",
      "[D loss: 1.136105] [G loss: 1.358020]\n",
      "[D loss: 0.686314] [G loss: 1.641288]\n",
      "[D loss: 1.070276] [G loss: 1.588187]\n",
      "[D loss: 0.724064] [G loss: 1.389846]\n",
      "[D loss: 0.962699] [G loss: 1.631640]\n",
      "[D loss: 0.560921] [G loss: 1.906776]\n",
      "[D loss: 0.835209] [G loss: 1.348361]\n",
      "[D loss: 0.770014] [G loss: 1.619101]\n",
      "[D loss: 0.752839] [G loss: 1.516711]\n",
      "[D loss: 0.788474] [G loss: 1.810006]\n",
      "[D loss: 0.803863] [G loss: 1.531114]\n",
      "[D loss: 1.107317] [G loss: 1.219242]\n",
      "[D loss: 1.129021] [G loss: 1.394381]\n",
      "[D loss: 0.739976] [G loss: 1.724239]\n",
      "[D loss: 0.915066] [G loss: 1.536817]\n",
      "[D loss: 0.706128] [G loss: 1.405687]\n",
      "[D loss: 0.848190] [G loss: 1.392471]\n",
      "[D loss: 0.843661] [G loss: 1.592722]\n",
      "[D loss: 1.039789] [G loss: 1.432244]\n",
      "[D loss: 1.069060] [G loss: 1.509050]\n",
      "[D loss: 0.766852] [G loss: 1.640086]\n",
      "[D loss: 1.088468] [G loss: 1.532973]\n",
      "[D loss: 0.843090] [G loss: 1.392080]\n",
      "[D loss: 0.743584] [G loss: 1.645010]\n",
      "[D loss: 0.747761] [G loss: 1.438278]\n",
      "[D loss: 0.743655] [G loss: 1.211450]\n",
      "[D loss: 0.779236] [G loss: 1.572384]\n",
      "[D loss: 0.714500] [G loss: 1.457375]\n",
      "[D loss: 0.794619] [G loss: 1.319971]\n",
      "[D loss: 0.646814] [G loss: 1.657494]\n",
      "[D loss: 0.968346] [G loss: 1.449691]\n",
      "[D loss: 0.913275] [G loss: 1.582101]\n",
      "[D loss: 0.835157] [G loss: 1.522329]\n",
      "[D loss: 1.033229] [G loss: 1.683945]\n",
      "[D loss: 0.888630] [G loss: 1.251425]\n",
      "[D loss: 1.074543] [G loss: 1.210898]\n",
      "[D loss: 0.575148] [G loss: 1.712302]\n",
      "[D loss: 0.747306] [G loss: 1.427447]\n",
      "[D loss: 0.678712] [G loss: 1.271590]\n",
      "[D loss: 0.793705] [G loss: 1.484568]\n",
      "[D loss: 0.943246] [G loss: 1.341401]\n",
      "[D loss: 0.531899] [G loss: 1.656523]\n",
      "[D loss: 0.916417] [G loss: 1.657427]\n",
      "[D loss: 0.959897] [G loss: 1.398658]\n",
      "[D loss: 0.587401] [G loss: 1.448108]\n",
      "[D loss: 1.055158] [G loss: 1.323059]\n",
      "[D loss: 0.886791] [G loss: 1.302200]\n",
      "[D loss: 0.692191] [G loss: 1.409106]\n",
      "[D loss: 0.940828] [G loss: 1.485792]\n",
      "[D loss: 0.918209] [G loss: 1.644640]\n",
      "[D loss: 0.783061] [G loss: 1.670767]\n",
      "[D loss: 0.919101] [G loss: 1.467658]\n",
      "[D loss: 0.577346] [G loss: 1.534280]\n",
      "[D loss: 0.850378] [G loss: 1.444264]\n",
      "[D loss: 0.866967] [G loss: 1.558456]\n",
      "[D loss: 0.877854] [G loss: 1.459895]\n",
      "[D loss: 0.734356] [G loss: 1.312911]\n",
      "[D loss: 0.957682] [G loss: 1.653003]\n",
      "[D loss: 0.765344] [G loss: 1.538710]\n",
      "[D loss: 0.771945] [G loss: 1.381396]\n",
      "[D loss: 0.871736] [G loss: 1.312042]\n",
      "[D loss: 0.612840] [G loss: 1.573046]\n",
      "[D loss: 0.767241] [G loss: 1.373461]\n",
      "[D loss: 0.738528] [G loss: 1.412602]\n",
      "[D loss: 0.757279] [G loss: 1.476155]\n",
      "[D loss: 0.707735] [G loss: 1.613793]\n",
      "[D loss: 0.867809] [G loss: 1.413422]\n",
      "[D loss: 0.747086] [G loss: 1.487374]\n",
      "[D loss: 1.037732] [G loss: 1.393264]\n",
      "[D loss: 0.879742] [G loss: 1.557255]\n",
      "[D loss: 0.946322] [G loss: 1.499756]\n",
      "[D loss: 0.756737] [G loss: 1.674771]\n",
      "[D loss: 0.771585] [G loss: 1.517386]\n",
      "[D loss: 0.615192] [G loss: 1.581356]\n",
      "[D loss: 0.919201] [G loss: 1.620788]\n",
      "[D loss: 0.847601] [G loss: 1.638857]\n",
      "[D loss: 0.869665] [G loss: 1.555950]\n",
      "[D loss: 0.779831] [G loss: 1.389583]\n",
      "[D loss: 0.750537] [G loss: 1.335201]\n",
      "[D loss: 0.821437] [G loss: 1.457583]\n",
      "[D loss: 0.879959] [G loss: 1.305001]\n",
      "[D loss: 0.713738] [G loss: 1.438626]\n",
      "[D loss: 0.909036] [G loss: 1.462027]\n",
      "[D loss: 0.770726] [G loss: 1.495745]\n",
      "[D loss: 0.711775] [G loss: 1.449821]\n",
      "[D loss: 0.573987] [G loss: 1.680561]\n",
      "[D loss: 0.618497] [G loss: 1.619220]\n",
      "[D loss: 0.671930] [G loss: 1.621881]\n",
      "[D loss: 0.976167] [G loss: 1.424137]\n",
      "[D loss: 0.847009] [G loss: 1.346190]\n",
      "[D loss: 0.800695] [G loss: 1.493676]\n",
      "[D loss: 0.988028] [G loss: 1.501028]\n",
      "[D loss: 0.796492] [G loss: 1.319362]\n",
      "[D loss: 0.733311] [G loss: 1.684674]\n",
      "[D loss: 0.913184] [G loss: 1.481115]\n",
      "[D loss: 0.972424] [G loss: 1.545466]\n",
      "[D loss: 0.936328] [G loss: 1.579355]\n",
      "[D loss: 0.845233] [G loss: 1.480280]\n",
      "[D loss: 0.824421] [G loss: 1.443334]\n",
      "[D loss: 0.716109] [G loss: 1.169775]\n",
      "[D loss: 0.866131] [G loss: 1.562604]\n",
      "[D loss: 0.893303] [G loss: 1.639635]\n",
      "[D loss: 0.883233] [G loss: 1.333472]\n",
      "[D loss: 0.669085] [G loss: 1.475050]\n",
      "[D loss: 0.748008] [G loss: 1.436485]\n",
      "[D loss: 0.801389] [G loss: 1.398578]\n",
      "[D loss: 0.687509] [G loss: 1.290202]\n",
      "[D loss: 0.675221] [G loss: 1.589066]\n",
      "[D loss: 0.736755] [G loss: 1.452326]\n",
      "[D loss: 0.713306] [G loss: 1.591666]\n",
      "[D loss: 0.989103] [G loss: 1.484889]\n",
      "[D loss: 0.942522] [G loss: 1.489410]\n",
      "[D loss: 0.911739] [G loss: 1.314766]\n",
      "[D loss: 0.907514] [G loss: 1.267422]\n",
      "[D loss: 0.773759] [G loss: 1.441968]\n",
      "[D loss: 0.717491] [G loss: 1.391969]\n",
      "[D loss: 0.835445] [G loss: 1.643119]\n",
      "[D loss: 0.616847] [G loss: 1.521182]\n",
      "[D loss: 0.804028] [G loss: 1.324088]\n",
      "[D loss: 0.581929] [G loss: 1.514845]\n",
      "[D loss: 1.018426] [G loss: 1.385461]\n",
      "[D loss: 0.884440] [G loss: 1.387662]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.882196] [G loss: 1.348227]\n",
      "[D loss: 0.883800] [G loss: 1.431844]\n",
      "[D loss: 0.863692] [G loss: 1.395596]\n",
      "[D loss: 0.770120] [G loss: 1.307862]\n",
      "[D loss: 0.723066] [G loss: 1.450787]\n",
      "[D loss: 0.629082] [G loss: 1.377476]\n",
      "[D loss: 0.705112] [G loss: 1.468801]\n",
      "[D loss: 0.749108] [G loss: 1.405982]\n",
      "[D loss: 0.969109] [G loss: 1.287634]\n",
      "[D loss: 0.705558] [G loss: 1.295359]\n",
      "[D loss: 1.141539] [G loss: 1.173589]\n",
      "[D loss: 0.679392] [G loss: 1.669164]\n",
      "[D loss: 1.042086] [G loss: 1.467916]\n",
      "[D loss: 0.887955] [G loss: 1.633717]\n",
      "[D loss: 0.903398] [G loss: 1.301417]\n",
      "[D loss: 0.980531] [G loss: 1.352894]\n",
      "[D loss: 0.802410] [G loss: 1.363619]\n",
      "[D loss: 0.776654] [G loss: 1.386162]\n",
      "[D loss: 0.675905] [G loss: 1.431120]\n",
      "[D loss: 0.764256] [G loss: 1.608167]\n",
      "[D loss: 0.630136] [G loss: 1.555209]\n",
      "[D loss: 0.705929] [G loss: 1.459849]\n",
      "[D loss: 0.874415] [G loss: 1.395192]\n",
      "[D loss: 0.897462] [G loss: 1.755135]\n",
      "[D loss: 0.991232] [G loss: 1.536926]\n",
      "[D loss: 0.652632] [G loss: 1.557244]\n",
      "[D loss: 0.835229] [G loss: 1.415509]\n",
      "[D loss: 0.797405] [G loss: 1.253418]\n",
      "[D loss: 0.601604] [G loss: 1.379133]\n",
      "[D loss: 1.064668] [G loss: 1.340974]\n",
      "[D loss: 0.903427] [G loss: 1.526947]\n",
      "[D loss: 0.638284] [G loss: 1.579025]\n",
      "[D loss: 1.011546] [G loss: 1.363151]\n",
      "[D loss: 0.837392] [G loss: 1.343017]\n",
      "[D loss: 0.789382] [G loss: 1.557730]\n",
      "[D loss: 0.808411] [G loss: 1.558162]\n",
      "[D loss: 1.044251] [G loss: 1.583766]\n",
      "[D loss: 0.810395] [G loss: 1.554577]\n",
      "[D loss: 0.727146] [G loss: 1.352346]\n",
      "[D loss: 0.868096] [G loss: 1.392989]\n",
      "[D loss: 0.938984] [G loss: 1.381136]\n",
      "[D loss: 0.837859] [G loss: 1.532510]\n",
      "[D loss: 0.782076] [G loss: 1.395207]\n",
      "[D loss: 0.983379] [G loss: 1.457348]\n",
      "[D loss: 0.924801] [G loss: 1.478572]\n",
      "[D loss: 0.767434] [G loss: 1.270134]\n",
      "[D loss: 0.809198] [G loss: 1.631369]\n",
      "[D loss: 0.788263] [G loss: 1.336153]\n",
      "[D loss: 0.762374] [G loss: 1.420513]\n",
      "[D loss: 0.821680] [G loss: 1.405444]\n",
      "[D loss: 1.165483] [G loss: 1.276819]\n",
      "[D loss: 0.944593] [G loss: 1.534536]\n",
      "[D loss: 0.898930] [G loss: 1.396249]\n",
      "[D loss: 1.001941] [G loss: 1.451402]\n",
      "[D loss: 0.966544] [G loss: 1.490652]\n",
      "[D loss: 0.758213] [G loss: 1.453409]\n",
      "[D loss: 1.078802] [G loss: 1.363721]\n",
      "[D loss: 0.963198] [G loss: 1.292922]\n",
      "[D loss: 0.862939] [G loss: 1.456507]\n",
      "[D loss: 0.855714] [G loss: 1.251480]\n",
      "[D loss: 0.745530] [G loss: 1.297390]\n",
      "[D loss: 0.815977] [G loss: 1.234519]\n",
      "[D loss: 0.670533] [G loss: 1.446169]\n",
      "[D loss: 1.155901] [G loss: 1.249756]\n",
      "[D loss: 0.985381] [G loss: 1.456111]\n",
      "[D loss: 0.752780] [G loss: 1.302002]\n",
      "[D loss: 0.911717] [G loss: 1.463936]\n",
      "[D loss: 0.856524] [G loss: 1.483552]\n",
      "[D loss: 0.901134] [G loss: 1.258815]\n",
      "[D loss: 0.714298] [G loss: 1.454689]\n",
      "[D loss: 0.772046] [G loss: 1.613161]\n",
      "[D loss: 0.692848] [G loss: 1.551367]\n",
      "[D loss: 0.788537] [G loss: 1.298617]\n",
      "[D loss: 0.799582] [G loss: 1.549494]\n",
      "[D loss: 0.760061] [G loss: 1.341335]\n",
      "[D loss: 0.843629] [G loss: 1.312047]\n",
      "[D loss: 0.861663] [G loss: 1.268909]\n",
      "[D loss: 0.732709] [G loss: 1.447760]\n",
      "[D loss: 0.846596] [G loss: 1.464553]\n",
      "[D loss: 0.856793] [G loss: 1.709946]\n",
      "[D loss: 0.942163] [G loss: 1.529659]\n",
      "[D loss: 1.037587] [G loss: 1.786105]\n",
      "[D loss: 0.745050] [G loss: 1.509203]\n",
      "[D loss: 0.881942] [G loss: 1.491837]\n",
      "[D loss: 0.661481] [G loss: 1.368535]\n",
      "[D loss: 0.608309] [G loss: 1.607693]\n",
      "[D loss: 0.790849] [G loss: 1.320542]\n",
      "[D loss: 0.810770] [G loss: 1.436487]\n",
      "[D loss: 0.997214] [G loss: 1.321870]\n",
      "[D loss: 0.999894] [G loss: 1.663387]\n",
      "[D loss: 1.105212] [G loss: 1.478386]\n",
      "[D loss: 0.988763] [G loss: 1.409056]\n",
      "[D loss: 0.825615] [G loss: 1.312218]\n",
      "[D loss: 1.086231] [G loss: 1.255729]\n",
      "[D loss: 0.850602] [G loss: 1.589596]\n",
      "[D loss: 0.861636] [G loss: 1.496008]\n",
      "[D loss: 0.637205] [G loss: 1.227116]\n",
      "[D loss: 0.668123] [G loss: 1.439588]\n",
      "[D loss: 0.823808] [G loss: 1.356263]\n",
      "[D loss: 0.754141] [G loss: 1.454917]\n",
      "[D loss: 0.797280] [G loss: 1.589829]\n",
      "[D loss: 0.745995] [G loss: 1.559831]\n",
      "[D loss: 0.967350] [G loss: 1.550793]\n",
      "[D loss: 0.813296] [G loss: 1.321903]\n",
      "[D loss: 0.910626] [G loss: 1.691321]\n",
      "[D loss: 0.747585] [G loss: 1.526157]\n",
      "[D loss: 0.653448] [G loss: 1.571581]\n",
      "[D loss: 0.793213] [G loss: 1.383402]\n",
      "[D loss: 0.967773] [G loss: 1.200294]\n",
      "[D loss: 0.702427] [G loss: 1.436978]\n",
      "[D loss: 0.742840] [G loss: 1.356097]\n",
      "[D loss: 0.804869] [G loss: 1.271415]\n",
      "[D loss: 0.740779] [G loss: 1.505131]\n",
      "[D loss: 0.856163] [G loss: 1.470431]\n",
      "[D loss: 0.610821] [G loss: 1.765046]\n",
      "[D loss: 0.875164] [G loss: 1.380062]\n",
      "[D loss: 0.854177] [G loss: 1.359442]\n",
      "[D loss: 0.931955] [G loss: 1.439442]\n",
      "[D loss: 0.709519] [G loss: 1.615675]\n",
      "[D loss: 0.699074] [G loss: 1.555629]\n",
      "[D loss: 0.946186] [G loss: 1.454698]\n",
      "[D loss: 0.854567] [G loss: 1.447604]\n",
      "[D loss: 0.974763] [G loss: 1.469465]\n",
      "[D loss: 0.677713] [G loss: 1.481035]\n",
      "[D loss: 0.772722] [G loss: 1.541158]\n",
      "[D loss: 0.939658] [G loss: 1.402614]\n",
      "[D loss: 1.009193] [G loss: 1.340656]\n",
      "[D loss: 1.022047] [G loss: 1.578610]\n",
      "[D loss: 0.854106] [G loss: 1.381047]\n",
      "[D loss: 0.791526] [G loss: 1.434368]\n",
      "[D loss: 0.895904] [G loss: 1.246693]\n",
      "[D loss: 0.929800] [G loss: 1.379057]\n",
      "[D loss: 0.857426] [G loss: 1.404208]\n",
      "[D loss: 1.134782] [G loss: 1.388751]\n",
      "[D loss: 0.653429] [G loss: 1.473902]\n",
      "[D loss: 0.752885] [G loss: 1.400841]\n",
      "[D loss: 1.015175] [G loss: 1.378762]\n",
      "[D loss: 0.690724] [G loss: 1.513237]\n",
      "[D loss: 0.874631] [G loss: 1.423314]\n",
      "[D loss: 0.867074] [G loss: 1.473058]\n",
      "[D loss: 1.337292] [G loss: 1.207070]\n",
      "[D loss: 0.650661] [G loss: 1.396473]\n",
      "[D loss: 0.828542] [G loss: 1.372283]\n",
      "[D loss: 1.121826] [G loss: 1.497566]\n",
      "[D loss: 0.848205] [G loss: 1.338095]\n",
      "[D loss: 0.700422] [G loss: 1.372822]\n",
      "[D loss: 0.879248] [G loss: 1.451877]\n",
      "[D loss: 0.926813] [G loss: 1.198970]\n",
      "[D loss: 0.806872] [G loss: 1.379944]\n",
      "[D loss: 0.912329] [G loss: 1.392478]\n",
      "[D loss: 0.848803] [G loss: 1.264025]\n",
      "[D loss: 0.828651] [G loss: 1.372051]\n",
      "[D loss: 0.761484] [G loss: 1.465612]\n",
      "[D loss: 0.895800] [G loss: 1.441142]\n",
      "[D loss: 0.848351] [G loss: 1.507271]\n",
      "[D loss: 0.966859] [G loss: 1.322436]\n",
      "[D loss: 0.721285] [G loss: 1.544531]\n",
      "[D loss: 0.726471] [G loss: 1.414830]\n",
      "[D loss: 0.756830] [G loss: 1.385505]\n",
      "[D loss: 0.745746] [G loss: 1.414189]\n",
      "[D loss: 0.852804] [G loss: 1.553891]\n",
      "[D loss: 1.013531] [G loss: 1.412331]\n",
      "[D loss: 0.701563] [G loss: 1.497753]\n",
      "[D loss: 0.916449] [G loss: 1.447698]\n",
      "[D loss: 0.785608] [G loss: 1.520018]\n",
      "[D loss: 0.899773] [G loss: 1.576015]\n",
      "[D loss: 0.846021] [G loss: 1.337479]\n",
      "[D loss: 1.089352] [G loss: 1.293386]\n",
      "[D loss: 0.968441] [G loss: 1.430465]\n",
      "[D loss: 0.906931] [G loss: 1.398092]\n",
      "[D loss: 0.866160] [G loss: 1.387195]\n",
      "[D loss: 0.830490] [G loss: 1.363015]\n",
      "[D loss: 0.726714] [G loss: 1.448374]\n",
      "[D loss: 0.927618] [G loss: 1.474208]\n",
      "[D loss: 0.742474] [G loss: 1.690859]\n",
      "[D loss: 0.867702] [G loss: 1.454075]\n",
      "[D loss: 1.039761] [G loss: 1.271809]\n",
      "[D loss: 0.742010] [G loss: 1.281792]\n",
      "[D loss: 0.810321] [G loss: 1.476012]\n",
      "[D loss: 0.703179] [G loss: 1.573600]\n",
      "[D loss: 0.853463] [G loss: 1.468690]\n",
      "[D loss: 1.021961] [G loss: 1.363780]\n",
      "[D loss: 1.072713] [G loss: 1.250862]\n",
      "[D loss: 0.950317] [G loss: 1.341970]\n",
      "[D loss: 0.692154] [G loss: 1.449612]\n",
      "[D loss: 0.811109] [G loss: 1.440099]\n",
      "[D loss: 0.923004] [G loss: 1.475568]\n",
      "[D loss: 0.892974] [G loss: 1.392798]\n",
      "[D loss: 0.591209] [G loss: 1.309348]\n",
      "[D loss: 0.859639] [G loss: 1.323242]\n",
      "[D loss: 0.780112] [G loss: 1.423224]\n",
      "[D loss: 0.784487] [G loss: 1.511802]\n",
      "[D loss: 0.819780] [G loss: 1.561188]\n",
      "[D loss: 0.682513] [G loss: 1.423035]\n",
      "[D loss: 1.050876] [G loss: 1.206901]\n",
      "[D loss: 0.780490] [G loss: 1.711140]\n",
      "[D loss: 0.696713] [G loss: 1.730595]\n",
      "[D loss: 0.821918] [G loss: 1.406904]\n",
      "[D loss: 0.632532] [G loss: 1.739828]\n",
      "[D loss: 0.912796] [G loss: 1.392600]\n",
      "[D loss: 0.757357] [G loss: 1.334486]\n",
      "[D loss: 0.866972] [G loss: 1.443571]\n",
      "[D loss: 1.005852] [G loss: 1.479615]\n",
      "[D loss: 0.934806] [G loss: 1.470959]\n",
      "[D loss: 0.664001] [G loss: 1.507893]\n",
      "[D loss: 0.957412] [G loss: 1.545497]\n",
      "[D loss: 0.835971] [G loss: 1.511162]\n",
      "[D loss: 0.912324] [G loss: 1.510397]\n",
      "[D loss: 0.801846] [G loss: 1.538647]\n",
      "[D loss: 0.848762] [G loss: 1.504297]\n",
      "[D loss: 0.983844] [G loss: 1.351815]\n",
      "[D loss: 0.777053] [G loss: 1.355871]\n",
      "[D loss: 0.964184] [G loss: 1.268976]\n",
      "[D loss: 0.791754] [G loss: 1.494454]\n",
      "[D loss: 0.840098] [G loss: 1.376241]\n",
      "[D loss: 0.770685] [G loss: 1.332181]\n",
      "[D loss: 0.758895] [G loss: 1.254732]\n",
      "[D loss: 0.870830] [G loss: 1.361320]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.828217] [G loss: 1.525885]\n",
      "[D loss: 0.896542] [G loss: 1.498877]\n",
      "[D loss: 0.942389] [G loss: 1.721387]\n",
      "[D loss: 0.697631] [G loss: 1.505643]\n",
      "[D loss: 0.558833] [G loss: 1.403977]\n",
      "[D loss: 0.861945] [G loss: 1.425956]\n",
      "[D loss: 0.799479] [G loss: 1.533042]\n",
      "[D loss: 0.720435] [G loss: 1.669929]\n",
      "[D loss: 0.758792] [G loss: 1.509982]\n",
      "[D loss: 1.056918] [G loss: 1.283736]\n",
      "[D loss: 0.906533] [G loss: 1.324604]\n",
      "[D loss: 0.953142] [G loss: 1.265245]\n",
      "[D loss: 0.844694] [G loss: 1.472389]\n",
      "[D loss: 0.566548] [G loss: 1.649217]\n",
      "[D loss: 0.864062] [G loss: 1.295106]\n",
      "[D loss: 0.834227] [G loss: 1.402385]\n",
      "[D loss: 0.914735] [G loss: 1.327564]\n",
      "[D loss: 0.803901] [G loss: 1.449091]\n",
      "[D loss: 1.108988] [G loss: 1.207301]\n",
      "[D loss: 0.856581] [G loss: 1.771965]\n",
      "[D loss: 0.851408] [G loss: 1.517554]\n",
      "[D loss: 0.947370] [G loss: 1.685222]\n",
      "[D loss: 0.879810] [G loss: 1.225810]\n",
      "[D loss: 0.668644] [G loss: 1.335382]\n",
      "[D loss: 0.679779] [G loss: 1.438206]\n",
      "[D loss: 0.759541] [G loss: 1.501635]\n",
      "[D loss: 0.808612] [G loss: 1.387815]\n",
      "[D loss: 0.735657] [G loss: 1.339397]\n",
      "[D loss: 0.914601] [G loss: 1.202438]\n",
      "[D loss: 0.881748] [G loss: 1.421362]\n",
      "[D loss: 0.911383] [G loss: 1.370819]\n",
      "[D loss: 1.081562] [G loss: 1.457263]\n",
      "[D loss: 0.851636] [G loss: 1.333640]\n",
      "[D loss: 0.791865] [G loss: 1.385429]\n",
      "[D loss: 0.832093] [G loss: 1.428736]\n",
      "[D loss: 0.855385] [G loss: 1.563279]\n",
      "[D loss: 0.738255] [G loss: 1.618169]\n",
      "[D loss: 0.648665] [G loss: 1.624767]\n",
      "[D loss: 0.892689] [G loss: 1.411911]\n",
      "[D loss: 0.957449] [G loss: 1.576691]\n",
      "[D loss: 0.785574] [G loss: 1.283774]\n",
      "[D loss: 0.917740] [G loss: 1.451237]\n",
      "[D loss: 1.001256] [G loss: 1.305664]\n",
      "[D loss: 0.979253] [G loss: 1.351123]\n",
      "[D loss: 0.737964] [G loss: 1.510119]\n",
      "[D loss: 0.802604] [G loss: 1.539422]\n",
      "[D loss: 0.881526] [G loss: 1.362497]\n",
      "[D loss: 0.825647] [G loss: 1.341661]\n",
      "[D loss: 0.963434] [G loss: 1.372745]\n",
      "[D loss: 1.061696] [G loss: 1.366109]\n",
      "[D loss: 0.824900] [G loss: 1.676249]\n",
      "[D loss: 0.839665] [G loss: 1.608789]\n",
      "[D loss: 0.700391] [G loss: 1.504505]\n",
      "[D loss: 0.905209] [G loss: 1.296146]\n",
      "[D loss: 0.874563] [G loss: 1.241685]\n",
      "[D loss: 0.854769] [G loss: 1.287126]\n",
      "[D loss: 0.781161] [G loss: 1.414966]\n",
      "[D loss: 0.908181] [G loss: 1.403412]\n",
      "[D loss: 0.804584] [G loss: 1.403547]\n",
      "[D loss: 0.975660] [G loss: 1.212388]\n",
      "[D loss: 0.746738] [G loss: 1.434675]\n",
      "[D loss: 0.875144] [G loss: 1.497765]\n",
      "[D loss: 0.901688] [G loss: 1.509515]\n",
      "[D loss: 0.881552] [G loss: 1.252521]\n",
      "[D loss: 0.791308] [G loss: 1.432081]\n",
      "[D loss: 0.775194] [G loss: 1.409604]\n",
      "[D loss: 0.661815] [G loss: 1.541997]\n",
      "[D loss: 1.104145] [G loss: 1.529662]\n",
      "[D loss: 0.605869] [G loss: 1.504008]\n",
      "[D loss: 1.097948] [G loss: 1.450888]\n",
      "[D loss: 0.789391] [G loss: 1.472296]\n",
      "[D loss: 0.810948] [G loss: 1.449569]\n",
      "[D loss: 0.605498] [G loss: 1.636404]\n",
      "[D loss: 0.693505] [G loss: 1.542530]\n",
      "[D loss: 0.797538] [G loss: 1.429769]\n",
      "[D loss: 0.800403] [G loss: 1.311163]\n",
      "[D loss: 0.846099] [G loss: 1.468470]\n",
      "[D loss: 0.879950] [G loss: 1.599755]\n",
      "[D loss: 0.829271] [G loss: 1.388077]\n",
      "[D loss: 0.794068] [G loss: 1.412206]\n",
      "[D loss: 0.904416] [G loss: 1.368595]\n",
      "[D loss: 0.902296] [G loss: 1.411667]\n",
      "[D loss: 0.832127] [G loss: 1.495656]\n",
      "[D loss: 1.143238] [G loss: 1.423231]\n",
      "[D loss: 0.664453] [G loss: 1.452807]\n",
      "[D loss: 0.865699] [G loss: 1.378183]\n",
      "[D loss: 1.065644] [G loss: 1.396164]\n",
      "[D loss: 0.962997] [G loss: 1.543691]\n",
      "[D loss: 0.681943] [G loss: 1.426048]\n",
      "[D loss: 0.803563] [G loss: 1.398750]\n",
      "[D loss: 0.840745] [G loss: 1.335716]\n",
      "[D loss: 0.975538] [G loss: 1.240785]\n",
      "[D loss: 0.805015] [G loss: 1.611591]\n",
      "[D loss: 0.945229] [G loss: 1.273166]\n",
      "[D loss: 0.610521] [G loss: 1.642651]\n",
      "[D loss: 0.613315] [G loss: 1.679124]\n",
      "[D loss: 0.815133] [G loss: 1.342301]\n",
      "[D loss: 0.943259] [G loss: 1.326558]\n",
      "[D loss: 0.748514] [G loss: 1.677681]\n",
      "[D loss: 0.844221] [G loss: 1.503790]\n",
      "[D loss: 0.976274] [G loss: 1.479083]\n",
      "[D loss: 0.855698] [G loss: 1.464956]\n",
      "[D loss: 0.719950] [G loss: 1.552574]\n",
      "[D loss: 0.902293] [G loss: 1.550683]\n",
      "[D loss: 0.766102] [G loss: 1.435225]\n",
      "[D loss: 0.811144] [G loss: 1.452267]\n",
      "[D loss: 0.651363] [G loss: 1.691493]\n",
      "[D loss: 0.953010] [G loss: 1.501280]\n",
      "[D loss: 0.770913] [G loss: 1.584110]\n",
      "[D loss: 0.874945] [G loss: 1.335298]\n",
      "[D loss: 0.775035] [G loss: 1.470187]\n",
      "[D loss: 0.825264] [G loss: 1.496855]\n",
      "[D loss: 0.884300] [G loss: 1.401043]\n",
      "[D loss: 0.838377] [G loss: 1.348729]\n",
      "[D loss: 0.844978] [G loss: 1.566551]\n",
      "[D loss: 0.577102] [G loss: 1.504935]\n",
      "[D loss: 0.807146] [G loss: 1.596605]\n",
      "[D loss: 0.726046] [G loss: 1.439505]\n",
      "[D loss: 0.995062] [G loss: 1.320221]\n",
      "[D loss: 0.764888] [G loss: 1.352612]\n",
      "[D loss: 0.802857] [G loss: 1.472811]\n",
      "[D loss: 0.667220] [G loss: 1.667990]\n",
      "[D loss: 0.778282] [G loss: 1.468459]\n",
      "[D loss: 0.890936] [G loss: 1.518320]\n",
      "[D loss: 0.785001] [G loss: 1.600052]\n",
      "[D loss: 0.786355] [G loss: 1.618714]\n",
      "[D loss: 0.875548] [G loss: 1.529575]\n",
      "[D loss: 0.824976] [G loss: 1.561504]\n",
      "[D loss: 0.851373] [G loss: 1.221357]\n",
      "[D loss: 0.610918] [G loss: 1.502745]\n",
      "[D loss: 0.759093] [G loss: 1.686356]\n",
      "[D loss: 0.806672] [G loss: 1.410745]\n",
      "[D loss: 0.935323] [G loss: 1.609878]\n",
      "[D loss: 0.693007] [G loss: 1.541893]\n",
      "[D loss: 0.940818] [G loss: 1.549693]\n",
      "[D loss: 0.862404] [G loss: 1.332566]\n",
      "[D loss: 0.826701] [G loss: 1.483803]\n",
      "[D loss: 0.721359] [G loss: 1.366643]\n",
      "[D loss: 0.714040] [G loss: 1.450878]\n",
      "[D loss: 1.117919] [G loss: 1.329739]\n",
      "[D loss: 0.826687] [G loss: 1.618567]\n",
      "[D loss: 0.839487] [G loss: 1.503080]\n",
      "[D loss: 0.764947] [G loss: 1.488848]\n",
      "[D loss: 0.834846] [G loss: 1.566787]\n",
      "[D loss: 0.996239] [G loss: 1.376716]\n",
      "[D loss: 0.816929] [G loss: 1.443960]\n",
      "[D loss: 0.757190] [G loss: 1.492155]\n",
      "[D loss: 1.024489] [G loss: 1.419589]\n",
      "[D loss: 0.734388] [G loss: 1.477915]\n",
      "[D loss: 0.802297] [G loss: 1.455435]\n",
      "[D loss: 0.644169] [G loss: 1.648127]\n",
      "[D loss: 0.860461] [G loss: 1.480971]\n",
      "[D loss: 1.151891] [G loss: 1.567907]\n",
      "[D loss: 0.814998] [G loss: 1.389793]\n",
      "[D loss: 0.750440] [G loss: 1.169173]\n",
      "[D loss: 0.798871] [G loss: 1.210778]\n",
      "[D loss: 0.965558] [G loss: 1.468741]\n",
      "[D loss: 0.649059] [G loss: 1.492452]\n",
      "[D loss: 1.007261] [G loss: 1.365246]\n",
      "[D loss: 0.666239] [G loss: 1.322048]\n",
      "[D loss: 0.910529] [G loss: 1.450184]\n",
      "[D loss: 0.852079] [G loss: 1.416982]\n",
      "[D loss: 0.650577] [G loss: 1.458432]\n",
      "[D loss: 1.019740] [G loss: 1.336145]\n",
      "[D loss: 0.699308] [G loss: 1.611612]\n",
      "[D loss: 0.710908] [G loss: 1.308038]\n",
      "[D loss: 0.903596] [G loss: 1.507581]\n",
      "[D loss: 0.886569] [G loss: 1.697952]\n",
      "[D loss: 0.882589] [G loss: 1.447751]\n",
      "[D loss: 0.838013] [G loss: 1.533962]\n",
      "[D loss: 0.768558] [G loss: 1.819571]\n",
      "[D loss: 0.738272] [G loss: 1.523761]\n",
      "[D loss: 1.079906] [G loss: 1.444057]\n",
      "[D loss: 0.949874] [G loss: 1.637050]\n",
      "[D loss: 0.711108] [G loss: 1.628205]\n",
      "[D loss: 1.079304] [G loss: 1.361770]\n",
      "[D loss: 0.901592] [G loss: 1.450585]\n",
      "[D loss: 0.832240] [G loss: 1.120300]\n",
      "[D loss: 0.839592] [G loss: 1.289913]\n",
      "[D loss: 0.813031] [G loss: 1.453372]\n",
      "[D loss: 0.809556] [G loss: 1.497698]\n",
      "[D loss: 0.908563] [G loss: 1.667905]\n",
      "[D loss: 0.945413] [G loss: 1.388793]\n",
      "[D loss: 0.729484] [G loss: 1.330014]\n",
      "[D loss: 0.733454] [G loss: 1.468953]\n",
      "[D loss: 0.651528] [G loss: 1.351537]\n",
      "[D loss: 0.777285] [G loss: 1.548056]\n",
      "[D loss: 1.151223] [G loss: 1.533618]\n",
      "[D loss: 0.899215] [G loss: 1.462184]\n",
      "[D loss: 0.655118] [G loss: 1.389715]\n",
      "[D loss: 1.011951] [G loss: 1.453276]\n",
      "[D loss: 1.021740] [G loss: 1.245358]\n",
      "[D loss: 0.881269] [G loss: 1.370070]\n",
      "[D loss: 0.889211] [G loss: 1.638110]\n",
      "[D loss: 0.746063] [G loss: 1.389898]\n",
      "[D loss: 0.589621] [G loss: 1.467663]\n",
      "[D loss: 0.697728] [G loss: 1.384936]\n",
      "[D loss: 0.991573] [G loss: 1.115713]\n",
      "[D loss: 0.867554] [G loss: 1.174888]\n",
      "[D loss: 0.892779] [G loss: 1.426208]\n",
      "[D loss: 0.795941] [G loss: 1.570218]\n",
      "[D loss: 0.719531] [G loss: 1.667617]\n",
      "[D loss: 0.667971] [G loss: 1.913739]\n",
      "[D loss: 0.761903] [G loss: 1.452606]\n",
      "[D loss: 0.934741] [G loss: 1.306826]\n",
      "[D loss: 0.601906] [G loss: 1.611839]\n",
      "[D loss: 0.651910] [G loss: 1.710467]\n",
      "[D loss: 0.929945] [G loss: 1.633068]\n",
      "[D loss: 0.744446] [G loss: 1.477648]\n",
      "[D loss: 0.908989] [G loss: 1.472383]\n",
      "[D loss: 0.879839] [G loss: 1.334934]\n",
      "[D loss: 1.092880] [G loss: 1.432193]\n",
      "[D loss: 0.856691] [G loss: 1.670397]\n",
      "[D loss: 0.952349] [G loss: 1.435082]\n",
      "[D loss: 0.692463] [G loss: 1.619338]\n",
      "[D loss: 1.009308] [G loss: 1.530496]\n",
      "[D loss: 0.738865] [G loss: 1.436209]\n",
      "[D loss: 0.797053] [G loss: 1.404236]\n",
      "[D loss: 0.746156] [G loss: 1.395697]\n",
      "[D loss: 0.992474] [G loss: 1.420266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.952770] [G loss: 1.511906]\n",
      "[D loss: 0.720571] [G loss: 1.391008]\n",
      "[D loss: 0.925654] [G loss: 1.241205]\n",
      "[D loss: 0.838455] [G loss: 1.558420]\n",
      "[D loss: 1.136065] [G loss: 1.393126]\n",
      "[D loss: 1.064084] [G loss: 1.526818]\n",
      "[D loss: 0.859878] [G loss: 1.454441]\n",
      "[D loss: 0.817257] [G loss: 1.404111]\n",
      "[D loss: 0.844770] [G loss: 1.470064]\n",
      "[D loss: 0.692253] [G loss: 1.446216]\n",
      "[D loss: 0.681121] [G loss: 1.560688]\n",
      "[D loss: 0.954113] [G loss: 1.296166]\n",
      "[D loss: 0.721945] [G loss: 1.472775]\n",
      "[D loss: 0.784447] [G loss: 1.613123]\n",
      "[D loss: 0.618506] [G loss: 1.450490]\n",
      "[D loss: 0.786960] [G loss: 1.472479]\n",
      "[D loss: 0.956522] [G loss: 1.507607]\n",
      "[D loss: 0.905582] [G loss: 1.208923]\n",
      "[D loss: 1.021911] [G loss: 1.281742]\n",
      "[D loss: 0.757150] [G loss: 1.470371]\n",
      "[D loss: 0.781495] [G loss: 1.739392]\n",
      "[D loss: 0.791863] [G loss: 1.549766]\n",
      "[D loss: 0.709591] [G loss: 1.378401]\n",
      "[D loss: 0.820408] [G loss: 1.518992]\n",
      "[D loss: 0.741145] [G loss: 1.746163]\n",
      "[D loss: 0.955603] [G loss: 1.394203]\n",
      "[D loss: 0.666200] [G loss: 1.319287]\n",
      "[D loss: 0.670045] [G loss: 1.431766]\n",
      "[D loss: 0.797353] [G loss: 1.387252]\n",
      "[D loss: 0.610457] [G loss: 1.403525]\n",
      "[D loss: 0.892180] [G loss: 1.334404]\n",
      "[D loss: 0.876315] [G loss: 1.519116]\n",
      "[D loss: 0.804801] [G loss: 1.457484]\n",
      "[D loss: 0.817236] [G loss: 1.535518]\n",
      "[D loss: 0.795402] [G loss: 1.333331]\n",
      "[D loss: 0.725676] [G loss: 1.495817]\n",
      "[D loss: 0.823195] [G loss: 1.532155]\n",
      "[D loss: 0.890972] [G loss: 1.562597]\n",
      "[D loss: 0.826023] [G loss: 1.576657]\n",
      "[D loss: 0.968371] [G loss: 1.616058]\n",
      "[D loss: 0.794202] [G loss: 1.389539]\n",
      "[D loss: 0.756786] [G loss: 1.354892]\n",
      "[D loss: 0.848778] [G loss: 1.443596]\n",
      "[D loss: 0.639743] [G loss: 1.370635]\n",
      "[D loss: 0.928842] [G loss: 1.431274]\n",
      "[D loss: 0.817267] [G loss: 1.710651]\n",
      "[D loss: 0.876067] [G loss: 1.530092]\n",
      "[D loss: 0.992765] [G loss: 1.306427]\n",
      "[D loss: 0.740913] [G loss: 1.553168]\n",
      "[D loss: 1.042716] [G loss: 1.620573]\n",
      "[D loss: 0.849791] [G loss: 1.483316]\n",
      "[D loss: 0.722629] [G loss: 1.446305]\n",
      "[D loss: 0.824404] [G loss: 1.396134]\n",
      "[D loss: 0.850801] [G loss: 1.323197]\n",
      "[D loss: 0.853409] [G loss: 1.255097]\n",
      "[D loss: 0.693216] [G loss: 1.742222]\n",
      "[D loss: 0.926247] [G loss: 1.402032]\n",
      "[D loss: 1.116199] [G loss: 1.644921]\n",
      "[D loss: 0.839170] [G loss: 1.506381]\n",
      "[D loss: 0.776117] [G loss: 1.345010]\n",
      "[D loss: 0.837147] [G loss: 1.379911]\n",
      "[D loss: 0.711389] [G loss: 1.447985]\n",
      "[D loss: 0.911777] [G loss: 1.280472]\n",
      "[D loss: 0.872533] [G loss: 1.503088]\n",
      "[D loss: 0.741404] [G loss: 1.395181]\n",
      "[D loss: 1.033579] [G loss: 1.261727]\n",
      "[D loss: 0.792188] [G loss: 1.403844]\n",
      "[D loss: 0.690866] [G loss: 1.726211]\n",
      "[D loss: 1.179898] [G loss: 1.640280]\n",
      "[D loss: 0.921089] [G loss: 1.305098]\n",
      "[D loss: 0.965450] [G loss: 1.202639]\n",
      "[D loss: 0.707044] [G loss: 1.525605]\n",
      "[D loss: 0.881378] [G loss: 1.488734]\n",
      "[D loss: 0.886669] [G loss: 1.713620]\n",
      "[D loss: 0.738576] [G loss: 1.306139]\n",
      "[D loss: 0.679132] [G loss: 1.293779]\n",
      "[D loss: 0.755827] [G loss: 1.552150]\n",
      "[D loss: 0.998192] [G loss: 1.516604]\n",
      "[D loss: 0.909368] [G loss: 1.389879]\n",
      "[D loss: 0.803296] [G loss: 1.298973]\n",
      "[D loss: 0.752919] [G loss: 1.374340]\n",
      "[D loss: 1.008322] [G loss: 1.299982]\n",
      "[D loss: 0.809244] [G loss: 1.474437]\n",
      "[D loss: 0.849746] [G loss: 1.500623]\n",
      "[D loss: 0.759893] [G loss: 1.475812]\n",
      "[D loss: 0.787185] [G loss: 1.657062]\n",
      "[D loss: 1.184172] [G loss: 1.540257]\n",
      "[D loss: 0.863613] [G loss: 1.396599]\n",
      "[D loss: 0.761129] [G loss: 1.277702]\n",
      "[D loss: 0.910751] [G loss: 1.259935]\n",
      "[D loss: 0.741638] [G loss: 1.355668]\n",
      "[D loss: 0.664425] [G loss: 1.452996]\n",
      "[D loss: 0.842106] [G loss: 1.470904]\n",
      "[D loss: 0.882620] [G loss: 1.379938]\n",
      "[D loss: 1.023621] [G loss: 1.245676]\n",
      "[D loss: 0.906259] [G loss: 1.607713]\n",
      "[D loss: 0.902641] [G loss: 1.387906]\n",
      "[D loss: 0.763222] [G loss: 1.425797]\n",
      "[D loss: 0.659726] [G loss: 1.313826]\n",
      "[D loss: 0.835966] [G loss: 1.376205]\n",
      "[D loss: 0.937970] [G loss: 1.510665]\n",
      "[D loss: 0.791972] [G loss: 1.529289]\n",
      "[D loss: 0.978204] [G loss: 1.265917]\n",
      "[D loss: 0.860656] [G loss: 1.227533]\n",
      "[D loss: 0.787868] [G loss: 1.346694]\n",
      "[D loss: 0.702109] [G loss: 1.575426]\n",
      "[D loss: 0.800077] [G loss: 1.530168]\n",
      "[D loss: 0.831488] [G loss: 1.391781]\n",
      "[D loss: 0.840964] [G loss: 1.572154]\n",
      "[D loss: 0.889231] [G loss: 1.570663]\n",
      "[D loss: 1.094738] [G loss: 1.385314]\n",
      "[D loss: 0.947548] [G loss: 1.312653]\n",
      "[D loss: 1.069097] [G loss: 1.185095]\n",
      "[D loss: 0.939844] [G loss: 1.358471]\n",
      "[D loss: 0.925757] [G loss: 1.407963]\n",
      "[D loss: 0.817633] [G loss: 1.521783]\n",
      "[D loss: 0.897987] [G loss: 1.321522]\n",
      "[D loss: 0.821010] [G loss: 1.401164]\n",
      "[D loss: 0.943871] [G loss: 1.389588]\n",
      "[D loss: 0.689087] [G loss: 1.530630]\n",
      "[D loss: 0.804514] [G loss: 1.479596]\n",
      "[D loss: 0.826895] [G loss: 1.308911]\n",
      "[D loss: 0.775070] [G loss: 1.464532]\n",
      "[D loss: 0.692164] [G loss: 1.610269]\n",
      "[D loss: 0.814744] [G loss: 1.555820]\n",
      "[D loss: 0.997916] [G loss: 1.571469]\n",
      "[D loss: 0.695570] [G loss: 1.749657]\n",
      "[D loss: 0.733100] [G loss: 1.327293]\n",
      "[D loss: 0.893254] [G loss: 1.234314]\n",
      "[D loss: 0.736811] [G loss: 1.695450]\n",
      "[D loss: 0.963329] [G loss: 1.410317]\n",
      "[D loss: 0.662685] [G loss: 1.495114]\n",
      "[D loss: 0.844535] [G loss: 1.499453]\n",
      "[D loss: 0.895517] [G loss: 1.524788]\n",
      "[D loss: 0.878674] [G loss: 1.481648]\n",
      "[D loss: 0.871463] [G loss: 1.413035]\n",
      "[D loss: 0.708623] [G loss: 1.455916]\n",
      "[D loss: 0.848099] [G loss: 1.321568]\n",
      "[D loss: 0.965545] [G loss: 1.208608]\n",
      "[D loss: 0.778695] [G loss: 1.505246]\n",
      "[D loss: 0.844886] [G loss: 1.372981]\n",
      "[D loss: 0.775876] [G loss: 1.360251]\n",
      "[D loss: 0.737587] [G loss: 1.472713]\n",
      "[D loss: 0.829165] [G loss: 1.485081]\n",
      "[D loss: 0.976649] [G loss: 1.445063]\n",
      "[D loss: 0.937452] [G loss: 1.605176]\n",
      "[D loss: 1.039289] [G loss: 1.362227]\n",
      "[D loss: 0.849414] [G loss: 1.506593]\n",
      "[D loss: 0.691690] [G loss: 1.674876]\n",
      "[D loss: 1.006748] [G loss: 1.361678]\n",
      "[D loss: 0.657727] [G loss: 1.335058]\n",
      "[D loss: 0.515212] [G loss: 1.333269]\n",
      "[D loss: 0.993350] [G loss: 1.351276]\n",
      "[D loss: 0.824545] [G loss: 1.356318]\n",
      "[D loss: 0.740989] [G loss: 1.545525]\n",
      "[D loss: 0.799935] [G loss: 1.482041]\n",
      "[D loss: 0.822968] [G loss: 1.488761]\n",
      "[D loss: 0.922684] [G loss: 1.504810]\n",
      "[D loss: 0.695254] [G loss: 1.436120]\n",
      "[D loss: 0.741254] [G loss: 1.607298]\n",
      "[D loss: 0.688652] [G loss: 1.506611]\n",
      "[D loss: 0.735408] [G loss: 1.445208]\n",
      "[D loss: 0.726523] [G loss: 1.657755]\n",
      "[D loss: 0.746829] [G loss: 1.344481]\n",
      "[D loss: 0.726132] [G loss: 1.589085]\n",
      "[D loss: 0.854725] [G loss: 1.528921]\n",
      "[D loss: 0.604075] [G loss: 1.742276]\n",
      "[D loss: 0.487198] [G loss: 1.830835]\n",
      "[D loss: 0.854187] [G loss: 1.498285]\n",
      "[D loss: 0.907903] [G loss: 1.361651]\n",
      "[D loss: 0.800747] [G loss: 1.620307]\n",
      "[D loss: 0.620882] [G loss: 1.665225]\n",
      "[D loss: 1.089587] [G loss: 1.287271]\n",
      "[D loss: 0.870222] [G loss: 1.469852]\n",
      "[D loss: 0.846970] [G loss: 1.384853]\n",
      "[D loss: 0.736261] [G loss: 1.688093]\n",
      "[D loss: 0.639485] [G loss: 1.525585]\n",
      "[D loss: 1.015728] [G loss: 1.801278]\n",
      "[D loss: 0.993666] [G loss: 1.712402]\n",
      "[D loss: 0.899153] [G loss: 1.635888]\n",
      "[D loss: 1.028062] [G loss: 1.278044]\n",
      "[D loss: 0.862449] [G loss: 1.429450]\n",
      "[D loss: 0.855114] [G loss: 1.670490]\n",
      "[D loss: 1.064276] [G loss: 1.325915]\n",
      "[D loss: 0.803388] [G loss: 1.582997]\n",
      "[D loss: 1.016364] [G loss: 1.461369]\n",
      "[D loss: 0.920352] [G loss: 1.393619]\n",
      "[D loss: 0.919875] [G loss: 1.503291]\n",
      "[D loss: 0.992586] [G loss: 1.318801]\n",
      "[D loss: 0.970952] [G loss: 1.203123]\n",
      "[D loss: 0.856361] [G loss: 1.339285]\n",
      "[D loss: 0.932976] [G loss: 1.280788]\n",
      "[D loss: 0.991980] [G loss: 1.282669]\n",
      "[D loss: 0.665785] [G loss: 1.405972]\n",
      "[D loss: 0.877499] [G loss: 1.295198]\n",
      "[D loss: 1.025614] [G loss: 1.142066]\n",
      "[D loss: 1.006559] [G loss: 1.303779]\n",
      "[D loss: 0.980120] [G loss: 1.295136]\n",
      "[D loss: 0.971987] [G loss: 1.470052]\n",
      "[D loss: 0.903032] [G loss: 1.682919]\n",
      "[D loss: 0.794725] [G loss: 1.310179]\n",
      "[D loss: 0.782479] [G loss: 1.406720]\n",
      "[D loss: 0.762392] [G loss: 1.211985]\n",
      "[D loss: 0.777936] [G loss: 1.256446]\n",
      "[D loss: 0.890253] [G loss: 1.372047]\n",
      "[D loss: 0.923067] [G loss: 1.295528]\n",
      "[D loss: 0.700080] [G loss: 1.327723]\n",
      "[D loss: 0.917425] [G loss: 1.417475]\n",
      "[D loss: 0.892008] [G loss: 1.278694]\n",
      "[D loss: 0.884349] [G loss: 1.399773]\n",
      "[D loss: 0.743665] [G loss: 1.360008]\n",
      "[D loss: 0.831995] [G loss: 1.298353]\n",
      "[D loss: 1.016382] [G loss: 1.335498]\n",
      "[D loss: 0.620045] [G loss: 1.444201]\n",
      "[D loss: 0.930048] [G loss: 1.371891]\n",
      "[D loss: 0.681098] [G loss: 1.574649]\n",
      "[D loss: 0.836160] [G loss: 1.532229]\n",
      "[D loss: 0.916280] [G loss: 1.240562]\n",
      "[D loss: 0.817361] [G loss: 1.425705]\n",
      "[D loss: 0.862246] [G loss: 1.331437]\n",
      "[D loss: 0.883123] [G loss: 1.277120]\n",
      "[D loss: 0.986815] [G loss: 1.411407]\n",
      "[D loss: 0.940534] [G loss: 1.384997]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.856938] [G loss: 1.416894]\n",
      "[D loss: 0.920778] [G loss: 1.309081]\n",
      "[D loss: 1.004368] [G loss: 1.081541]\n",
      "[D loss: 0.976150] [G loss: 1.341494]\n",
      "[D loss: 0.734576] [G loss: 1.372618]\n",
      "[D loss: 1.004425] [G loss: 1.372937]\n",
      "[D loss: 0.576103] [G loss: 1.410935]\n",
      "[D loss: 0.941326] [G loss: 1.406615]\n",
      "[D loss: 0.801007] [G loss: 1.400169]\n",
      "[D loss: 0.864945] [G loss: 1.582094]\n",
      "[D loss: 0.942550] [G loss: 1.471292]\n",
      "[D loss: 0.833300] [G loss: 1.293184]\n",
      "[D loss: 0.919389] [G loss: 1.392059]\n",
      "[D loss: 0.907899] [G loss: 1.301614]\n",
      "[D loss: 0.856627] [G loss: 1.318027]\n",
      "[D loss: 1.017274] [G loss: 1.397194]\n",
      "[D loss: 0.809573] [G loss: 1.378764]\n",
      "[D loss: 0.921048] [G loss: 1.381552]\n",
      "[D loss: 0.833334] [G loss: 1.509270]\n",
      "[D loss: 0.875128] [G loss: 1.518315]\n",
      "[D loss: 0.760691] [G loss: 1.418713]\n",
      "[D loss: 0.861559] [G loss: 1.455617]\n",
      "[D loss: 0.992508] [G loss: 1.164178]\n",
      "[D loss: 0.736730] [G loss: 1.319277]\n",
      "[D loss: 0.862927] [G loss: 1.327293]\n",
      "[D loss: 0.799763] [G loss: 1.533756]\n",
      "[D loss: 1.085126] [G loss: 2.000037]\n",
      "[D loss: 0.909363] [G loss: 1.497686]\n",
      "[D loss: 0.549316] [G loss: 1.459683]\n",
      "[D loss: 0.870036] [G loss: 1.355075]\n",
      "[D loss: 0.739379] [G loss: 1.459123]\n",
      "[D loss: 1.019222] [G loss: 1.429865]\n",
      "[D loss: 0.970006] [G loss: 1.338180]\n",
      "[D loss: 0.713459] [G loss: 1.291999]\n",
      "[D loss: 1.011596] [G loss: 1.567666]\n",
      "[D loss: 1.005594] [G loss: 1.317018]\n",
      "[D loss: 0.967941] [G loss: 1.467062]\n",
      "[D loss: 0.827845] [G loss: 1.622339]\n",
      "[D loss: 0.831526] [G loss: 1.268015]\n",
      "[D loss: 0.955190] [G loss: 1.257305]\n",
      "[D loss: 0.903186] [G loss: 1.465591]\n",
      "[D loss: 0.713468] [G loss: 1.330483]\n",
      "[D loss: 0.873312] [G loss: 1.157904]\n",
      "[D loss: 0.825227] [G loss: 1.438424]\n",
      "[D loss: 1.143010] [G loss: 1.400955]\n",
      "[D loss: 0.756673] [G loss: 1.418631]\n",
      "[D loss: 0.743048] [G loss: 1.402148]\n",
      "[D loss: 1.024475] [G loss: 1.463824]\n",
      "[D loss: 0.677676] [G loss: 1.619888]\n",
      "[D loss: 0.747929] [G loss: 1.343780]\n",
      "[D loss: 0.907803] [G loss: 1.344762]\n",
      "[D loss: 0.888658] [G loss: 1.335890]\n",
      "[D loss: 0.888639] [G loss: 1.306455]\n",
      "[D loss: 0.981510] [G loss: 1.478106]\n",
      "[D loss: 0.708845] [G loss: 1.448896]\n",
      "[D loss: 0.760876] [G loss: 1.502694]\n",
      "[D loss: 0.878524] [G loss: 1.429742]\n",
      "[D loss: 0.727400] [G loss: 1.422659]\n",
      "[D loss: 0.820556] [G loss: 1.447789]\n",
      "[D loss: 0.772782] [G loss: 1.465068]\n",
      "[D loss: 0.814161] [G loss: 1.479411]\n",
      "[D loss: 0.774763] [G loss: 1.462047]\n",
      "[D loss: 0.808722] [G loss: 1.441574]\n",
      "[D loss: 0.664471] [G loss: 1.500851]\n",
      "[D loss: 0.801495] [G loss: 1.510618]\n",
      "[D loss: 0.879145] [G loss: 1.318712]\n",
      "[D loss: 0.787624] [G loss: 1.359433]\n",
      "[D loss: 1.168753] [G loss: 1.259238]\n",
      "[D loss: 0.922284] [G loss: 1.291937]\n",
      "[D loss: 1.017977] [G loss: 1.515471]\n",
      "[D loss: 0.930401] [G loss: 1.387421]\n",
      "[D loss: 1.003690] [G loss: 1.360628]\n",
      "[D loss: 0.683610] [G loss: 1.483247]\n",
      "[D loss: 0.785613] [G loss: 1.154052]\n",
      "[D loss: 1.115082] [G loss: 1.320830]\n",
      "[D loss: 0.798304] [G loss: 1.563353]\n",
      "[D loss: 0.776295] [G loss: 1.609785]\n",
      "[D loss: 0.658895] [G loss: 1.584593]\n",
      "[D loss: 0.981800] [G loss: 1.576604]\n",
      "[D loss: 0.921664] [G loss: 1.492676]\n",
      "[D loss: 1.166225] [G loss: 1.534071]\n",
      "[D loss: 0.850107] [G loss: 1.382061]\n",
      "[D loss: 0.663500] [G loss: 1.519014]\n",
      "[D loss: 0.634523] [G loss: 1.282662]\n",
      "[D loss: 0.928809] [G loss: 1.435469]\n",
      "[D loss: 1.019422] [G loss: 1.441562]\n",
      "[D loss: 0.948775] [G loss: 1.536394]\n",
      "[D loss: 0.754648] [G loss: 1.583977]\n",
      "[D loss: 0.991052] [G loss: 1.348167]\n",
      "[D loss: 0.835876] [G loss: 1.551025]\n",
      "[D loss: 0.776103] [G loss: 1.491770]\n",
      "[D loss: 1.017647] [G loss: 1.348502]\n",
      "[D loss: 0.861140] [G loss: 1.353865]\n",
      "[D loss: 0.917728] [G loss: 1.366668]\n",
      "[D loss: 0.864986] [G loss: 1.568443]\n",
      "[D loss: 0.962338] [G loss: 1.405054]\n",
      "[D loss: 0.793647] [G loss: 1.554248]\n",
      "[D loss: 0.872886] [G loss: 1.385385]\n",
      "[D loss: 0.791984] [G loss: 1.364525]\n",
      "[D loss: 0.929341] [G loss: 1.396571]\n",
      "[D loss: 0.738325] [G loss: 1.320530]\n",
      "[D loss: 0.688974] [G loss: 1.540087]\n",
      "[D loss: 1.008550] [G loss: 1.273474]\n",
      "[D loss: 0.753995] [G loss: 1.350811]\n",
      "[D loss: 0.863684] [G loss: 1.475949]\n",
      "[D loss: 0.685504] [G loss: 1.353031]\n",
      "[D loss: 0.786403] [G loss: 1.335364]\n",
      "[D loss: 0.998115] [G loss: 1.258883]\n",
      "[D loss: 0.920218] [G loss: 1.382868]\n",
      "[D loss: 0.745879] [G loss: 1.442737]\n",
      "[D loss: 0.705733] [G loss: 1.413385]\n",
      "[D loss: 0.902595] [G loss: 1.579009]\n",
      "[D loss: 0.595123] [G loss: 1.484231]\n",
      "[D loss: 0.826580] [G loss: 1.445625]\n",
      "[D loss: 0.831220] [G loss: 1.323442]\n",
      "[D loss: 0.754672] [G loss: 1.412069]\n",
      "[D loss: 0.688891] [G loss: 1.458612]\n",
      "[D loss: 0.759052] [G loss: 1.519614]\n",
      "[D loss: 0.790213] [G loss: 1.397528]\n",
      "[D loss: 1.152200] [G loss: 1.256745]\n",
      "[D loss: 0.813364] [G loss: 1.370521]\n",
      "[D loss: 0.685387] [G loss: 1.351997]\n",
      "[D loss: 0.843124] [G loss: 1.233034]\n",
      "[D loss: 1.045332] [G loss: 1.552785]\n",
      "[D loss: 1.031120] [G loss: 1.294830]\n",
      "[D loss: 0.761512] [G loss: 1.486800]\n",
      "[D loss: 0.677434] [G loss: 1.378530]\n",
      "[D loss: 0.833954] [G loss: 1.415432]\n",
      "[D loss: 0.799430] [G loss: 1.458803]\n",
      "[D loss: 0.888499] [G loss: 1.506509]\n",
      "[D loss: 0.718248] [G loss: 1.458877]\n",
      "[D loss: 0.963218] [G loss: 1.328205]\n",
      "[D loss: 0.964685] [G loss: 1.179073]\n",
      "[D loss: 0.880201] [G loss: 1.613185]\n",
      "[D loss: 0.800357] [G loss: 1.408319]\n",
      "[D loss: 0.871306] [G loss: 1.494471]\n",
      "[D loss: 0.981099] [G loss: 1.449926]\n",
      "[D loss: 0.859999] [G loss: 1.405926]\n",
      "[D loss: 0.655526] [G loss: 1.404224]\n",
      "[D loss: 0.904956] [G loss: 1.318983]\n",
      "[D loss: 0.966147] [G loss: 1.350594]\n",
      "[D loss: 0.876060] [G loss: 1.401561]\n",
      "[D loss: 0.888411] [G loss: 1.251244]\n",
      "[D loss: 0.868519] [G loss: 1.302005]\n",
      "[D loss: 0.781079] [G loss: 1.599889]\n",
      "[D loss: 0.831454] [G loss: 1.381869]\n",
      "[D loss: 0.957729] [G loss: 1.304772]\n",
      "[D loss: 0.737674] [G loss: 1.487597]\n",
      "[D loss: 0.616487] [G loss: 1.567496]\n",
      "[D loss: 0.891202] [G loss: 1.362805]\n",
      "[D loss: 0.903223] [G loss: 1.527372]\n",
      "[D loss: 0.738130] [G loss: 1.439108]\n",
      "[D loss: 1.139757] [G loss: 1.251240]\n",
      "[D loss: 0.767075] [G loss: 1.531036]\n",
      "[D loss: 0.975851] [G loss: 1.542984]\n",
      "[D loss: 0.821864] [G loss: 1.314928]\n",
      "[D loss: 0.941142] [G loss: 1.202207]\n",
      "[D loss: 0.755245] [G loss: 1.435616]\n",
      "[D loss: 0.876565] [G loss: 1.445055]\n",
      "[D loss: 0.830886] [G loss: 1.433637]\n",
      "[D loss: 1.023378] [G loss: 1.459633]\n",
      "[D loss: 1.085549] [G loss: 1.385891]\n",
      "[D loss: 0.836738] [G loss: 1.380591]\n",
      "[D loss: 0.796359] [G loss: 1.241302]\n",
      "[D loss: 0.973736] [G loss: 1.445916]\n",
      "[D loss: 0.893408] [G loss: 1.298172]\n",
      "[D loss: 1.037599] [G loss: 1.343671]\n",
      "[D loss: 0.783637] [G loss: 1.393707]\n",
      "[D loss: 0.942357] [G loss: 1.272338]\n",
      "[D loss: 1.021697] [G loss: 1.520388]\n",
      "[D loss: 0.909769] [G loss: 1.492500]\n",
      "[D loss: 0.902778] [G loss: 1.173430]\n",
      "[D loss: 0.835905] [G loss: 1.432659]\n",
      "[D loss: 0.996923] [G loss: 1.259575]\n",
      "[D loss: 1.050095] [G loss: 1.312517]\n",
      "[D loss: 0.671161] [G loss: 1.596940]\n",
      "[D loss: 1.090221] [G loss: 1.252262]\n",
      "[D loss: 0.847352] [G loss: 1.210899]\n",
      "[D loss: 1.140083] [G loss: 1.163362]\n",
      "[D loss: 0.777267] [G loss: 1.318577]\n",
      "[D loss: 0.959064] [G loss: 1.237073]\n",
      "[D loss: 0.708139] [G loss: 1.302906]\n",
      "[D loss: 0.862079] [G loss: 1.218934]\n",
      "[D loss: 0.617306] [G loss: 1.339012]\n",
      "[D loss: 1.030405] [G loss: 1.427872]\n",
      "[D loss: 0.614242] [G loss: 1.367170]\n",
      "[D loss: 0.807838] [G loss: 1.241682]\n",
      "[D loss: 0.905773] [G loss: 1.438340]\n",
      "[D loss: 0.983809] [G loss: 1.461984]\n",
      "[D loss: 0.732759] [G loss: 1.470326]\n",
      "[D loss: 0.919182] [G loss: 1.354613]\n",
      "[D loss: 0.945549] [G loss: 1.242622]\n",
      "[D loss: 0.705302] [G loss: 1.446760]\n",
      "[D loss: 0.647939] [G loss: 1.421851]\n",
      "[D loss: 1.009353] [G loss: 1.405511]\n",
      "[D loss: 1.094718] [G loss: 1.299912]\n",
      "[D loss: 0.774175] [G loss: 1.366675]\n",
      "[D loss: 0.897333] [G loss: 1.530895]\n",
      "[D loss: 0.846650] [G loss: 1.274359]\n",
      "[D loss: 1.101677] [G loss: 1.253228]\n",
      "[D loss: 0.850938] [G loss: 1.441862]\n",
      "[D loss: 0.933286] [G loss: 1.351954]\n",
      "[D loss: 0.814151] [G loss: 1.393213]\n",
      "[D loss: 0.786501] [G loss: 1.287300]\n",
      "[D loss: 0.553063] [G loss: 1.602837]\n",
      "[D loss: 0.825085] [G loss: 1.547569]\n",
      "[D loss: 0.871844] [G loss: 1.313715]\n",
      "[D loss: 0.771645] [G loss: 1.362614]\n",
      "[D loss: 0.963892] [G loss: 1.378841]\n",
      "[D loss: 0.748951] [G loss: 1.340094]\n",
      "[D loss: 0.904305] [G loss: 1.218442]\n",
      "[D loss: 0.842423] [G loss: 1.318238]\n",
      "[D loss: 1.007009] [G loss: 1.353302]\n",
      "[D loss: 1.026401] [G loss: 1.378336]\n",
      "[D loss: 0.919644] [G loss: 1.177772]\n",
      "[D loss: 0.848207] [G loss: 1.353258]\n",
      "[D loss: 0.691197] [G loss: 1.344333]\n",
      "[D loss: 0.977026] [G loss: 1.137061]\n",
      "[D loss: 1.040440] [G loss: 1.441109]\n",
      "[D loss: 0.819164] [G loss: 1.571113]\n",
      "[D loss: 0.663600] [G loss: 1.497244]\n",
      "[D loss: 1.018516] [G loss: 1.282837]\n",
      "[D loss: 0.694488] [G loss: 1.450718]\n",
      "[D loss: 0.788071] [G loss: 1.471696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.632393] [G loss: 1.493226]\n",
      "[D loss: 0.934466] [G loss: 1.535395]\n",
      "[D loss: 0.840412] [G loss: 1.580351]\n",
      "[D loss: 1.068968] [G loss: 1.606050]\n",
      "[D loss: 1.066706] [G loss: 1.329818]\n",
      "[D loss: 1.102327] [G loss: 1.442800]\n",
      "[D loss: 0.847739] [G loss: 1.265759]\n",
      "[D loss: 0.884692] [G loss: 1.195012]\n",
      "[D loss: 1.064800] [G loss: 1.433060]\n",
      "[D loss: 0.637841] [G loss: 1.429806]\n",
      "[D loss: 0.837652] [G loss: 1.257689]\n",
      "[D loss: 0.928912] [G loss: 1.603758]\n",
      "[D loss: 0.977977] [G loss: 1.441214]\n",
      "[D loss: 0.875695] [G loss: 1.207069]\n",
      "[D loss: 0.971177] [G loss: 1.452965]\n",
      "[D loss: 0.838931] [G loss: 1.331125]\n",
      "[D loss: 0.798047] [G loss: 1.468132]\n",
      "[D loss: 0.674292] [G loss: 1.398586]\n",
      "[D loss: 0.660863] [G loss: 1.341980]\n",
      "[D loss: 0.940767] [G loss: 1.387363]\n",
      "[D loss: 0.879088] [G loss: 1.420392]\n",
      "[D loss: 0.836435] [G loss: 1.433182]\n",
      "[D loss: 0.894652] [G loss: 1.202035]\n",
      "[D loss: 0.717720] [G loss: 1.492958]\n",
      "[D loss: 0.807175] [G loss: 1.500516]\n",
      "[D loss: 0.649527] [G loss: 1.528438]\n",
      "[D loss: 0.815280] [G loss: 1.389051]\n",
      "[D loss: 0.970110] [G loss: 1.199132]\n",
      "[D loss: 0.729930] [G loss: 1.685616]\n",
      "[D loss: 0.831025] [G loss: 1.460848]\n",
      "[D loss: 0.861670] [G loss: 1.580727]\n",
      "[D loss: 0.783833] [G loss: 1.532419]\n",
      "[D loss: 0.805137] [G loss: 1.657047]\n",
      "[D loss: 0.660678] [G loss: 1.444561]\n",
      "[D loss: 1.036282] [G loss: 1.165384]\n",
      "[D loss: 0.815463] [G loss: 1.489233]\n",
      "[D loss: 0.717117] [G loss: 1.472983]\n",
      "[D loss: 0.791951] [G loss: 1.385274]\n",
      "[D loss: 0.716330] [G loss: 1.350682]\n",
      "[D loss: 0.863340] [G loss: 1.285551]\n",
      "[D loss: 0.851370] [G loss: 1.512358]\n",
      "[D loss: 0.673035] [G loss: 1.576921]\n",
      "[D loss: 0.932619] [G loss: 1.796129]\n",
      "[D loss: 0.785277] [G loss: 1.652788]\n",
      "[D loss: 0.926490] [G loss: 1.557461]\n",
      "[D loss: 0.695669] [G loss: 1.470661]\n",
      "[D loss: 1.131193] [G loss: 1.123142]\n",
      "[D loss: 0.801273] [G loss: 1.599085]\n",
      "[D loss: 1.074046] [G loss: 1.389390]\n",
      "[D loss: 0.807368] [G loss: 1.333827]\n",
      "[D loss: 0.819580] [G loss: 1.407130]\n",
      "[D loss: 0.738660] [G loss: 1.313843]\n",
      "[D loss: 1.164316] [G loss: 1.264720]\n",
      "[D loss: 0.959023] [G loss: 1.389033]\n",
      "[D loss: 0.770201] [G loss: 1.361619]\n",
      "[D loss: 0.834942] [G loss: 1.421320]\n",
      "[D loss: 1.044621] [G loss: 1.358684]\n",
      "[D loss: 0.767951] [G loss: 1.462514]\n",
      "[D loss: 0.890069] [G loss: 1.256107]\n",
      "[D loss: 0.805410] [G loss: 1.561588]\n",
      "[D loss: 0.819438] [G loss: 1.382348]\n",
      "[D loss: 0.962419] [G loss: 1.246554]\n",
      "[D loss: 0.494095] [G loss: 1.591018]\n",
      "[D loss: 0.786999] [G loss: 1.391124]\n",
      "[D loss: 0.728882] [G loss: 1.427184]\n",
      "[D loss: 0.614138] [G loss: 1.815515]\n",
      "[D loss: 0.678702] [G loss: 1.760314]\n",
      "[D loss: 0.866297] [G loss: 1.580197]\n",
      "[D loss: 0.793714] [G loss: 1.439449]\n",
      "[D loss: 0.765537] [G loss: 1.244351]\n",
      "[D loss: 0.892671] [G loss: 1.292214]\n",
      "[D loss: 1.049340] [G loss: 1.462324]\n",
      "[D loss: 0.675029] [G loss: 1.707323]\n",
      "[D loss: 0.852491] [G loss: 1.442157]\n",
      "[D loss: 0.886227] [G loss: 1.655587]\n",
      "[D loss: 0.907194] [G loss: 1.299289]\n",
      "[D loss: 0.819196] [G loss: 1.380796]\n",
      "[D loss: 0.895926] [G loss: 1.356395]\n",
      "[D loss: 0.984581] [G loss: 1.268300]\n",
      "[D loss: 0.909753] [G loss: 1.310947]\n",
      "[D loss: 0.927736] [G loss: 1.569942]\n",
      "[D loss: 0.895951] [G loss: 1.528832]\n",
      "[D loss: 0.862033] [G loss: 1.377535]\n",
      "[D loss: 0.753970] [G loss: 1.536218]\n",
      "[D loss: 0.888328] [G loss: 1.583320]\n",
      "[D loss: 1.088617] [G loss: 1.362968]\n",
      "[D loss: 0.892395] [G loss: 1.421241]\n",
      "[D loss: 1.007605] [G loss: 1.431712]\n",
      "[D loss: 1.055452] [G loss: 1.103793]\n",
      "[D loss: 0.959184] [G loss: 1.397451]\n",
      "[D loss: 0.621831] [G loss: 1.414334]\n",
      "[D loss: 0.795058] [G loss: 1.562853]\n",
      "[D loss: 0.910579] [G loss: 1.345266]\n",
      "[D loss: 0.749345] [G loss: 1.344440]\n",
      "[D loss: 0.866981] [G loss: 1.426358]\n",
      "[D loss: 0.943893] [G loss: 1.525205]\n",
      "[D loss: 1.029658] [G loss: 1.408502]\n",
      "[D loss: 0.713353] [G loss: 1.591090]\n",
      "[D loss: 0.890897] [G loss: 1.465780]\n",
      "[D loss: 0.846215] [G loss: 1.289307]\n",
      "[D loss: 0.958790] [G loss: 1.502740]\n",
      "[D loss: 1.051341] [G loss: 1.345806]\n",
      "[D loss: 0.940537] [G loss: 1.326702]\n",
      "[D loss: 0.828241] [G loss: 1.262973]\n",
      "[D loss: 0.780675] [G loss: 1.329271]\n",
      "[D loss: 0.978132] [G loss: 1.261469]\n",
      "[D loss: 0.986460] [G loss: 1.318875]\n",
      "[D loss: 0.770085] [G loss: 1.374471]\n",
      "[D loss: 0.835469] [G loss: 1.418475]\n",
      "[D loss: 0.926573] [G loss: 1.551241]\n",
      "[D loss: 0.795539] [G loss: 1.355453]\n",
      "[D loss: 0.607523] [G loss: 1.475885]\n",
      "[D loss: 0.757855] [G loss: 1.546960]\n",
      "epoch:7, g_loss:2714.525390625,d_loss:1571.2686767578125\n",
      "[D loss: 0.851041] [G loss: 1.434821]\n",
      "[D loss: 0.749270] [G loss: 1.580172]\n",
      "[D loss: 0.594734] [G loss: 1.502423]\n",
      "[D loss: 0.821388] [G loss: 1.301691]\n",
      "[D loss: 0.550961] [G loss: 1.375276]\n",
      "[D loss: 0.756344] [G loss: 1.377448]\n",
      "[D loss: 1.096378] [G loss: 1.475629]\n",
      "[D loss: 0.842613] [G loss: 1.644706]\n",
      "[D loss: 0.864086] [G loss: 1.413826]\n",
      "[D loss: 0.888662] [G loss: 1.347289]\n",
      "[D loss: 1.066441] [G loss: 1.297663]\n",
      "[D loss: 1.004504] [G loss: 1.442247]\n",
      "[D loss: 1.020171] [G loss: 1.430260]\n",
      "[D loss: 0.950545] [G loss: 1.475842]\n",
      "[D loss: 0.504755] [G loss: 1.489441]\n",
      "[D loss: 0.893288] [G loss: 1.503790]\n",
      "[D loss: 0.947991] [G loss: 1.267785]\n",
      "[D loss: 0.811141] [G loss: 1.631976]\n",
      "[D loss: 0.808231] [G loss: 1.611477]\n",
      "[D loss: 0.778637] [G loss: 1.403539]\n",
      "[D loss: 0.674160] [G loss: 1.445487]\n",
      "[D loss: 0.847787] [G loss: 1.410856]\n",
      "[D loss: 0.686878] [G loss: 1.496749]\n",
      "[D loss: 0.723036] [G loss: 1.217782]\n",
      "[D loss: 0.744788] [G loss: 1.362042]\n",
      "[D loss: 0.645834] [G loss: 1.624406]\n",
      "[D loss: 0.806403] [G loss: 1.436143]\n",
      "[D loss: 0.945872] [G loss: 1.554415]\n",
      "[D loss: 0.824423] [G loss: 1.325412]\n",
      "[D loss: 0.625241] [G loss: 1.757497]\n",
      "[D loss: 0.715071] [G loss: 1.851902]\n",
      "[D loss: 0.912344] [G loss: 1.480930]\n",
      "[D loss: 0.910280] [G loss: 1.331715]\n",
      "[D loss: 0.863653] [G loss: 1.389858]\n",
      "[D loss: 0.725541] [G loss: 1.269275]\n",
      "[D loss: 0.910736] [G loss: 1.399317]\n",
      "[D loss: 0.757116] [G loss: 1.758884]\n",
      "[D loss: 0.823565] [G loss: 1.571000]\n",
      "[D loss: 0.933291] [G loss: 1.419391]\n",
      "[D loss: 0.706515] [G loss: 1.482074]\n",
      "[D loss: 0.803373] [G loss: 1.535149]\n",
      "[D loss: 0.637000] [G loss: 1.504045]\n",
      "[D loss: 0.767810] [G loss: 1.463332]\n",
      "[D loss: 0.915109] [G loss: 1.328990]\n",
      "[D loss: 0.703596] [G loss: 1.424401]\n",
      "[D loss: 0.997084] [G loss: 1.634498]\n",
      "[D loss: 0.964296] [G loss: 1.665359]\n",
      "[D loss: 0.842003] [G loss: 1.528112]\n",
      "[D loss: 0.747539] [G loss: 1.489465]\n",
      "[D loss: 0.689196] [G loss: 1.585811]\n",
      "[D loss: 0.628005] [G loss: 1.671165]\n",
      "[D loss: 0.922660] [G loss: 1.345217]\n",
      "[D loss: 0.654354] [G loss: 1.552198]\n",
      "[D loss: 0.680135] [G loss: 1.440847]\n",
      "[D loss: 0.873299] [G loss: 1.546173]\n",
      "[D loss: 0.769445] [G loss: 1.540578]\n",
      "[D loss: 0.879559] [G loss: 1.539967]\n",
      "[D loss: 0.712087] [G loss: 1.381869]\n",
      "[D loss: 0.693817] [G loss: 1.468286]\n",
      "[D loss: 0.812407] [G loss: 1.732435]\n",
      "[D loss: 0.769854] [G loss: 1.484951]\n",
      "[D loss: 0.862651] [G loss: 1.455991]\n",
      "[D loss: 0.883518] [G loss: 1.349502]\n",
      "[D loss: 0.698624] [G loss: 1.532854]\n",
      "[D loss: 0.946174] [G loss: 1.447369]\n",
      "[D loss: 0.769162] [G loss: 1.622857]\n",
      "[D loss: 0.940345] [G loss: 1.267571]\n",
      "[D loss: 1.070669] [G loss: 1.240741]\n",
      "[D loss: 0.821428] [G loss: 1.396566]\n",
      "[D loss: 0.924059] [G loss: 1.455241]\n",
      "[D loss: 0.805114] [G loss: 1.467365]\n",
      "[D loss: 0.764521] [G loss: 1.383116]\n",
      "[D loss: 0.624484] [G loss: 1.318050]\n",
      "[D loss: 0.715346] [G loss: 1.667578]\n",
      "[D loss: 0.642376] [G loss: 1.549341]\n",
      "[D loss: 0.767584] [G loss: 1.528860]\n",
      "[D loss: 0.747005] [G loss: 1.508887]\n",
      "[D loss: 0.930011] [G loss: 1.369039]\n",
      "[D loss: 1.067993] [G loss: 1.320024]\n",
      "[D loss: 0.639363] [G loss: 1.437387]\n",
      "[D loss: 0.808088] [G loss: 1.568454]\n",
      "[D loss: 0.768992] [G loss: 1.445110]\n",
      "[D loss: 0.651672] [G loss: 1.467797]\n",
      "[D loss: 0.794743] [G loss: 1.368016]\n",
      "[D loss: 1.096135] [G loss: 1.422574]\n",
      "[D loss: 1.000148] [G loss: 1.397027]\n",
      "[D loss: 0.926354] [G loss: 1.435249]\n",
      "[D loss: 0.730049] [G loss: 1.548981]\n",
      "[D loss: 0.666113] [G loss: 1.697883]\n",
      "[D loss: 0.834651] [G loss: 1.581218]\n",
      "[D loss: 0.728479] [G loss: 1.643648]\n",
      "[D loss: 0.960328] [G loss: 1.457487]\n",
      "[D loss: 1.066373] [G loss: 1.432078]\n",
      "[D loss: 1.076393] [G loss: 1.491907]\n",
      "[D loss: 0.870457] [G loss: 1.429832]\n",
      "[D loss: 0.812399] [G loss: 1.451844]\n",
      "[D loss: 0.608617] [G loss: 1.371185]\n",
      "[D loss: 0.654342] [G loss: 1.302971]\n",
      "[D loss: 0.682378] [G loss: 1.530911]\n",
      "[D loss: 0.921068] [G loss: 1.476630]\n",
      "[D loss: 0.997853] [G loss: 1.562157]\n",
      "[D loss: 0.785041] [G loss: 1.498501]\n",
      "[D loss: 0.698029] [G loss: 1.555335]\n",
      "[D loss: 0.949185] [G loss: 1.507283]\n",
      "[D loss: 0.808093] [G loss: 1.499215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 1.067439] [G loss: 1.433224]\n",
      "[D loss: 1.017695] [G loss: 1.414011]\n",
      "[D loss: 0.746394] [G loss: 1.426243]\n",
      "[D loss: 0.604777] [G loss: 1.592985]\n",
      "[D loss: 0.898338] [G loss: 1.477860]\n",
      "[D loss: 0.838820] [G loss: 1.434749]\n",
      "[D loss: 1.128376] [G loss: 1.565385]\n",
      "[D loss: 0.784512] [G loss: 1.374115]\n",
      "[D loss: 0.710816] [G loss: 1.354421]\n",
      "[D loss: 0.699789] [G loss: 1.258267]\n",
      "[D loss: 0.911593] [G loss: 1.564641]\n",
      "[D loss: 1.171303] [G loss: 1.362763]\n",
      "[D loss: 0.749642] [G loss: 1.473901]\n",
      "[D loss: 0.875197] [G loss: 1.516400]\n",
      "[D loss: 0.881538] [G loss: 1.358381]\n",
      "[D loss: 0.872302] [G loss: 1.401113]\n",
      "[D loss: 0.817263] [G loss: 1.440954]\n",
      "[D loss: 0.680259] [G loss: 1.408469]\n",
      "[D loss: 0.909798] [G loss: 1.327514]\n",
      "[D loss: 0.703958] [G loss: 1.581400]\n",
      "[D loss: 0.979493] [G loss: 1.541893]\n",
      "[D loss: 0.778056] [G loss: 1.527906]\n",
      "[D loss: 0.707453] [G loss: 1.403985]\n",
      "[D loss: 0.843898] [G loss: 1.377543]\n",
      "[D loss: 0.866425] [G loss: 1.411684]\n",
      "[D loss: 0.808943] [G loss: 1.636079]\n",
      "[D loss: 1.124192] [G loss: 1.332342]\n",
      "[D loss: 1.075186] [G loss: 1.407662]\n",
      "[D loss: 0.865260] [G loss: 1.283182]\n",
      "[D loss: 1.026630] [G loss: 1.204163]\n",
      "[D loss: 0.642706] [G loss: 1.384026]\n",
      "[D loss: 0.728792] [G loss: 1.517349]\n",
      "[D loss: 0.808018] [G loss: 1.473832]\n",
      "[D loss: 0.776867] [G loss: 1.684267]\n",
      "[D loss: 1.081246] [G loss: 1.459447]\n",
      "[D loss: 0.752961] [G loss: 1.497660]\n",
      "[D loss: 0.862885] [G loss: 1.486997]\n",
      "[D loss: 0.944766] [G loss: 1.403415]\n",
      "[D loss: 0.981561] [G loss: 1.506171]\n",
      "[D loss: 0.872207] [G loss: 1.364171]\n",
      "[D loss: 0.948030] [G loss: 1.189365]\n",
      "[D loss: 1.009173] [G loss: 1.086976]\n",
      "[D loss: 1.004567] [G loss: 1.276491]\n",
      "[D loss: 0.874738] [G loss: 1.435425]\n",
      "[D loss: 0.994017] [G loss: 1.542693]\n",
      "[D loss: 0.644774] [G loss: 1.355489]\n",
      "[D loss: 0.792490] [G loss: 1.381104]\n",
      "[D loss: 0.684012] [G loss: 1.303564]\n",
      "[D loss: 0.825020] [G loss: 1.301379]\n",
      "[D loss: 0.944084] [G loss: 1.191696]\n",
      "[D loss: 0.751766] [G loss: 1.496041]\n",
      "[D loss: 0.757432] [G loss: 1.307168]\n",
      "[D loss: 0.672107] [G loss: 1.412175]\n",
      "[D loss: 0.825429] [G loss: 1.355503]\n",
      "[D loss: 0.855825] [G loss: 1.632104]\n",
      "[D loss: 0.725907] [G loss: 1.455800]\n",
      "[D loss: 0.991112] [G loss: 1.570886]\n",
      "[D loss: 0.822633] [G loss: 1.579674]\n",
      "[D loss: 0.790329] [G loss: 1.572622]\n",
      "[D loss: 0.676830] [G loss: 1.366097]\n",
      "[D loss: 1.060962] [G loss: 1.444472]\n",
      "[D loss: 0.796387] [G loss: 1.482270]\n",
      "[D loss: 0.832483] [G loss: 1.559110]\n",
      "[D loss: 0.798907] [G loss: 1.412301]\n",
      "[D loss: 0.804132] [G loss: 1.421734]\n",
      "[D loss: 0.935814] [G loss: 1.407532]\n",
      "[D loss: 0.906775] [G loss: 1.441324]\n",
      "[D loss: 0.820920] [G loss: 1.429956]\n",
      "[D loss: 0.805639] [G loss: 1.554658]\n",
      "[D loss: 0.644765] [G loss: 1.396444]\n",
      "[D loss: 0.741721] [G loss: 1.476806]\n",
      "[D loss: 0.678543] [G loss: 1.497101]\n",
      "[D loss: 0.983630] [G loss: 1.486394]\n",
      "[D loss: 0.728067] [G loss: 1.405196]\n",
      "[D loss: 0.705792] [G loss: 1.561742]\n",
      "[D loss: 0.772509] [G loss: 1.447976]\n",
      "[D loss: 0.927947] [G loss: 1.562294]\n",
      "[D loss: 0.745775] [G loss: 1.555727]\n",
      "[D loss: 0.936471] [G loss: 1.482629]\n",
      "[D loss: 0.665322] [G loss: 1.534921]\n",
      "[D loss: 1.282350] [G loss: 1.412793]\n",
      "[D loss: 0.535906] [G loss: 1.572595]\n",
      "[D loss: 0.768755] [G loss: 1.542825]\n",
      "[D loss: 0.705423] [G loss: 1.584456]\n",
      "[D loss: 0.942761] [G loss: 1.551601]\n",
      "[D loss: 0.765148] [G loss: 1.508606]\n",
      "[D loss: 0.974047] [G loss: 1.668219]\n",
      "[D loss: 0.864955] [G loss: 1.362221]\n",
      "[D loss: 0.696550] [G loss: 1.325680]\n",
      "[D loss: 0.891654] [G loss: 1.480248]\n",
      "[D loss: 0.768950] [G loss: 1.580415]\n",
      "[D loss: 0.856310] [G loss: 1.355832]\n",
      "[D loss: 0.834005] [G loss: 1.484720]\n",
      "[D loss: 0.945521] [G loss: 1.371401]\n",
      "[D loss: 0.709422] [G loss: 1.409873]\n",
      "[D loss: 0.707887] [G loss: 1.382447]\n",
      "[D loss: 0.775546] [G loss: 1.637365]\n",
      "[D loss: 1.069285] [G loss: 1.368905]\n",
      "[D loss: 0.631598] [G loss: 1.667463]\n",
      "[D loss: 0.654586] [G loss: 1.496864]\n",
      "[D loss: 0.587270] [G loss: 1.628793]\n",
      "[D loss: 0.899381] [G loss: 1.379139]\n",
      "[D loss: 0.920582] [G loss: 1.506960]\n",
      "[D loss: 0.625035] [G loss: 1.610288]\n",
      "[D loss: 0.788740] [G loss: 1.453154]\n",
      "[D loss: 0.827938] [G loss: 1.686113]\n",
      "[D loss: 0.710777] [G loss: 1.485954]\n",
      "[D loss: 0.742276] [G loss: 1.424309]\n",
      "[D loss: 0.785186] [G loss: 1.340055]\n",
      "[D loss: 0.812622] [G loss: 1.386616]\n",
      "[D loss: 0.791848] [G loss: 1.334265]\n",
      "[D loss: 0.601979] [G loss: 1.588960]\n",
      "[D loss: 0.750778] [G loss: 1.515639]\n",
      "[D loss: 0.818668] [G loss: 1.277784]\n",
      "[D loss: 0.889653] [G loss: 1.431659]\n",
      "[D loss: 0.829341] [G loss: 1.498073]\n",
      "[D loss: 0.954292] [G loss: 1.577838]\n",
      "[D loss: 0.785642] [G loss: 1.730185]\n",
      "[D loss: 0.875971] [G loss: 1.546107]\n",
      "[D loss: 0.875739] [G loss: 1.407312]\n",
      "[D loss: 0.697787] [G loss: 1.317120]\n",
      "[D loss: 0.738583] [G loss: 1.366200]\n",
      "[D loss: 0.786076] [G loss: 1.483571]\n",
      "[D loss: 0.872410] [G loss: 1.491907]\n",
      "[D loss: 0.985945] [G loss: 1.552888]\n",
      "[D loss: 0.921950] [G loss: 1.423213]\n",
      "[D loss: 0.705176] [G loss: 1.565203]\n",
      "[D loss: 0.847581] [G loss: 1.395802]\n",
      "[D loss: 0.914548] [G loss: 1.245863]\n",
      "[D loss: 0.820988] [G loss: 1.571078]\n",
      "[D loss: 1.107519] [G loss: 1.345811]\n",
      "[D loss: 0.670470] [G loss: 1.592753]\n",
      "[D loss: 0.926098] [G loss: 1.224335]\n",
      "[D loss: 0.882246] [G loss: 1.434901]\n",
      "[D loss: 1.071064] [G loss: 1.576440]\n",
      "[D loss: 0.847398] [G loss: 1.353017]\n",
      "[D loss: 0.934660] [G loss: 1.330355]\n",
      "[D loss: 0.756214] [G loss: 1.340350]\n",
      "[D loss: 1.001042] [G loss: 1.183980]\n",
      "[D loss: 0.874384] [G loss: 1.373683]\n",
      "[D loss: 0.894275] [G loss: 1.255886]\n",
      "[D loss: 0.554516] [G loss: 1.353383]\n",
      "[D loss: 0.726553] [G loss: 1.512408]\n",
      "[D loss: 0.951133] [G loss: 1.628420]\n",
      "[D loss: 0.679350] [G loss: 1.523332]\n",
      "[D loss: 0.741982] [G loss: 1.342785]\n",
      "[D loss: 0.675467] [G loss: 1.459675]\n",
      "[D loss: 0.807917] [G loss: 1.402664]\n",
      "[D loss: 0.809044] [G loss: 1.576255]\n",
      "[D loss: 0.889158] [G loss: 1.511945]\n",
      "[D loss: 0.787873] [G loss: 1.528198]\n",
      "[D loss: 0.911090] [G loss: 1.516843]\n",
      "[D loss: 0.624887] [G loss: 1.524897]\n",
      "[D loss: 0.876758] [G loss: 1.339660]\n",
      "[D loss: 0.724378] [G loss: 1.394676]\n",
      "[D loss: 0.873226] [G loss: 1.643349]\n",
      "[D loss: 0.799345] [G loss: 1.661209]\n",
      "[D loss: 0.719715] [G loss: 1.546476]\n",
      "[D loss: 0.867459] [G loss: 1.524309]\n",
      "[D loss: 0.752131] [G loss: 1.504895]\n",
      "[D loss: 0.843665] [G loss: 1.430743]\n",
      "[D loss: 0.916674] [G loss: 1.362085]\n",
      "[D loss: 0.631050] [G loss: 1.338521]\n",
      "[D loss: 0.765778] [G loss: 1.559090]\n",
      "[D loss: 0.912485] [G loss: 1.198022]\n",
      "[D loss: 0.924189] [G loss: 1.450354]\n",
      "[D loss: 0.800228] [G loss: 1.491023]\n",
      "[D loss: 0.804356] [G loss: 1.635870]\n",
      "[D loss: 0.897466] [G loss: 1.563767]\n",
      "[D loss: 0.824788] [G loss: 1.662124]\n",
      "[D loss: 0.825791] [G loss: 1.339550]\n",
      "[D loss: 0.860300] [G loss: 1.874042]\n",
      "[D loss: 1.090723] [G loss: 1.406236]\n",
      "[D loss: 1.066492] [G loss: 1.267779]\n",
      "[D loss: 0.780205] [G loss: 1.764974]\n",
      "[D loss: 0.757225] [G loss: 1.681223]\n",
      "[D loss: 0.794409] [G loss: 1.455710]\n",
      "[D loss: 0.922768] [G loss: 1.468291]\n",
      "[D loss: 0.838974] [G loss: 1.229823]\n",
      "[D loss: 0.925050] [G loss: 1.487697]\n",
      "[D loss: 0.999768] [G loss: 1.498231]\n",
      "[D loss: 0.826036] [G loss: 1.308720]\n",
      "[D loss: 0.616150] [G loss: 1.334127]\n",
      "[D loss: 0.840510] [G loss: 1.467310]\n",
      "[D loss: 0.816307] [G loss: 1.206083]\n",
      "[D loss: 0.794626] [G loss: 1.293310]\n",
      "[D loss: 0.711476] [G loss: 1.538320]\n",
      "[D loss: 0.877184] [G loss: 1.513901]\n",
      "[D loss: 0.807467] [G loss: 1.329389]\n",
      "[D loss: 0.934483] [G loss: 1.604832]\n",
      "[D loss: 0.856290] [G loss: 1.305005]\n",
      "[D loss: 0.769687] [G loss: 1.440087]\n",
      "[D loss: 0.885665] [G loss: 1.280769]\n",
      "[D loss: 0.727258] [G loss: 1.558977]\n",
      "[D loss: 1.029756] [G loss: 1.576233]\n",
      "[D loss: 1.043456] [G loss: 1.713380]\n",
      "[D loss: 0.765546] [G loss: 1.515710]\n",
      "[D loss: 0.898438] [G loss: 1.589610]\n",
      "[D loss: 0.788312] [G loss: 1.297034]\n",
      "[D loss: 0.627632] [G loss: 1.452299]\n",
      "[D loss: 0.768763] [G loss: 1.464533]\n",
      "[D loss: 0.670331] [G loss: 1.682409]\n",
      "[D loss: 1.023066] [G loss: 1.313645]\n",
      "[D loss: 0.834687] [G loss: 1.393117]\n",
      "[D loss: 0.831404] [G loss: 1.522637]\n",
      "[D loss: 0.709707] [G loss: 1.292914]\n",
      "[D loss: 0.666021] [G loss: 1.602924]\n",
      "[D loss: 0.727193] [G loss: 1.638499]\n",
      "[D loss: 0.984277] [G loss: 1.645442]\n",
      "[D loss: 0.946495] [G loss: 1.388344]\n",
      "[D loss: 0.741422] [G loss: 1.413527]\n",
      "[D loss: 0.749808] [G loss: 1.524396]\n",
      "[D loss: 0.734061] [G loss: 1.363568]\n",
      "[D loss: 0.922704] [G loss: 1.476689]\n",
      "[D loss: 0.763327] [G loss: 1.459262]\n",
      "[D loss: 0.720673] [G loss: 1.490851]\n",
      "[D loss: 0.985662] [G loss: 1.540612]\n",
      "[D loss: 0.716699] [G loss: 1.490565]\n",
      "[D loss: 0.827127] [G loss: 1.321508]\n",
      "[D loss: 0.822045] [G loss: 1.358354]\n",
      "[D loss: 0.748562] [G loss: 1.498001]\n",
      "[D loss: 0.877873] [G loss: 1.692819]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.948781] [G loss: 1.400321]\n",
      "[D loss: 0.716942] [G loss: 1.540440]\n",
      "[D loss: 0.949359] [G loss: 1.479789]\n",
      "[D loss: 0.657397] [G loss: 1.622259]\n",
      "[D loss: 0.911759] [G loss: 1.533955]\n",
      "[D loss: 0.706544] [G loss: 1.370584]\n",
      "[D loss: 0.834146] [G loss: 1.471331]\n",
      "[D loss: 0.917109] [G loss: 1.600035]\n",
      "[D loss: 0.886414] [G loss: 1.619735]\n",
      "[D loss: 1.004700] [G loss: 1.538789]\n",
      "[D loss: 0.857242] [G loss: 1.451581]\n",
      "[D loss: 0.826749] [G loss: 1.399159]\n",
      "[D loss: 0.943909] [G loss: 1.515612]\n",
      "[D loss: 0.801365] [G loss: 1.375449]\n",
      "[D loss: 1.042724] [G loss: 1.622665]\n",
      "[D loss: 0.702622] [G loss: 1.462073]\n",
      "[D loss: 1.035462] [G loss: 1.050766]\n",
      "[D loss: 0.721222] [G loss: 1.485111]\n",
      "[D loss: 0.774248] [G loss: 1.544397]\n",
      "[D loss: 0.709561] [G loss: 1.563040]\n",
      "[D loss: 0.795687] [G loss: 1.572284]\n",
      "[D loss: 0.803829] [G loss: 1.518912]\n",
      "[D loss: 0.826179] [G loss: 1.488074]\n",
      "[D loss: 0.839563] [G loss: 1.551955]\n",
      "[D loss: 0.785356] [G loss: 1.504254]\n",
      "[D loss: 0.835126] [G loss: 1.339810]\n",
      "[D loss: 0.695630] [G loss: 1.327259]\n",
      "[D loss: 0.725963] [G loss: 1.468931]\n",
      "[D loss: 0.616149] [G loss: 1.460573]\n",
      "[D loss: 0.768650] [G loss: 1.546435]\n",
      "[D loss: 0.818792] [G loss: 1.491514]\n",
      "[D loss: 0.720331] [G loss: 1.577755]\n",
      "[D loss: 0.718584] [G loss: 1.489438]\n",
      "[D loss: 0.824262] [G loss: 1.552025]\n",
      "[D loss: 0.789062] [G loss: 1.607415]\n",
      "[D loss: 0.842535] [G loss: 1.285401]\n",
      "[D loss: 0.791029] [G loss: 1.507806]\n",
      "[D loss: 0.791277] [G loss: 1.698456]\n",
      "[D loss: 0.673959] [G loss: 1.781159]\n",
      "[D loss: 0.748203] [G loss: 1.510458]\n",
      "[D loss: 0.725659] [G loss: 1.556453]\n",
      "[D loss: 0.729842] [G loss: 1.556564]\n",
      "[D loss: 0.808150] [G loss: 1.217564]\n",
      "[D loss: 0.925048] [G loss: 1.309675]\n",
      "[D loss: 0.638029] [G loss: 1.528231]\n",
      "[D loss: 0.898907] [G loss: 1.469168]\n",
      "[D loss: 1.041649] [G loss: 1.398614]\n",
      "[D loss: 0.955185] [G loss: 1.539581]\n",
      "[D loss: 0.968706] [G loss: 1.741045]\n",
      "[D loss: 1.012791] [G loss: 1.319889]\n",
      "[D loss: 0.919903] [G loss: 1.276068]\n",
      "[D loss: 0.799147] [G loss: 1.299245]\n",
      "[D loss: 0.725163] [G loss: 1.436851]\n",
      "[D loss: 0.756064] [G loss: 1.370438]\n",
      "[D loss: 0.501782] [G loss: 1.728989]\n",
      "[D loss: 0.629802] [G loss: 1.509377]\n",
      "[D loss: 1.019112] [G loss: 1.432443]\n",
      "[D loss: 0.711473] [G loss: 1.597787]\n",
      "[D loss: 0.913252] [G loss: 1.447630]\n",
      "[D loss: 1.038684] [G loss: 1.449364]\n",
      "[D loss: 0.799084] [G loss: 1.498912]\n",
      "[D loss: 0.802866] [G loss: 1.597368]\n",
      "[D loss: 0.979308] [G loss: 1.485772]\n",
      "[D loss: 0.878953] [G loss: 1.369395]\n",
      "[D loss: 0.881505] [G loss: 1.421603]\n",
      "[D loss: 0.928331] [G loss: 1.435852]\n",
      "[D loss: 0.822130] [G loss: 1.646190]\n",
      "[D loss: 0.901341] [G loss: 1.525144]\n",
      "[D loss: 0.731005] [G loss: 1.277700]\n",
      "[D loss: 0.748550] [G loss: 1.398240]\n",
      "[D loss: 0.852361] [G loss: 1.637412]\n",
      "[D loss: 0.851149] [G loss: 1.387807]\n",
      "[D loss: 0.824789] [G loss: 1.297943]\n",
      "[D loss: 0.916379] [G loss: 1.392202]\n",
      "[D loss: 0.909177] [G loss: 1.302005]\n",
      "[D loss: 0.743429] [G loss: 1.531583]\n",
      "[D loss: 0.766524] [G loss: 1.569533]\n",
      "[D loss: 0.895967] [G loss: 1.600644]\n",
      "[D loss: 0.857985] [G loss: 1.282543]\n",
      "[D loss: 0.929862] [G loss: 1.301895]\n",
      "[D loss: 0.896206] [G loss: 1.447120]\n",
      "[D loss: 0.622297] [G loss: 1.666210]\n",
      "[D loss: 0.836729] [G loss: 1.364517]\n",
      "[D loss: 0.952226] [G loss: 1.492241]\n",
      "[D loss: 0.946964] [G loss: 1.363053]\n",
      "[D loss: 0.779453] [G loss: 1.313144]\n",
      "[D loss: 0.771725] [G loss: 1.697214]\n",
      "[D loss: 0.670896] [G loss: 1.666524]\n",
      "[D loss: 0.842633] [G loss: 1.330151]\n",
      "[D loss: 0.858765] [G loss: 1.317448]\n",
      "[D loss: 1.039834] [G loss: 1.619122]\n",
      "[D loss: 0.813170] [G loss: 1.358785]\n",
      "[D loss: 0.884673] [G loss: 1.403553]\n",
      "[D loss: 0.910673] [G loss: 1.433473]\n",
      "[D loss: 0.690821] [G loss: 1.354459]\n",
      "[D loss: 0.946686] [G loss: 1.309074]\n",
      "[D loss: 1.075681] [G loss: 1.633511]\n",
      "[D loss: 0.833667] [G loss: 1.524634]\n",
      "[D loss: 0.709313] [G loss: 1.497473]\n",
      "[D loss: 0.932867] [G loss: 1.519629]\n",
      "[D loss: 0.824188] [G loss: 1.388689]\n",
      "[D loss: 0.804463] [G loss: 1.331063]\n",
      "[D loss: 0.663402] [G loss: 1.475785]\n",
      "[D loss: 0.625524] [G loss: 1.333088]\n",
      "[D loss: 0.929545] [G loss: 1.428106]\n",
      "[D loss: 0.823920] [G loss: 1.642602]\n",
      "[D loss: 1.016279] [G loss: 1.607164]\n",
      "[D loss: 0.698881] [G loss: 1.386160]\n",
      "[D loss: 0.689762] [G loss: 1.409668]\n",
      "[D loss: 0.688467] [G loss: 1.473401]\n",
      "[D loss: 0.814831] [G loss: 1.365316]\n",
      "[D loss: 1.045180] [G loss: 1.160699]\n",
      "[D loss: 0.871914] [G loss: 1.308986]\n",
      "[D loss: 0.760410] [G loss: 1.763321]\n",
      "[D loss: 0.820676] [G loss: 1.549877]\n",
      "[D loss: 0.882192] [G loss: 1.536518]\n",
      "[D loss: 0.952761] [G loss: 1.442527]\n",
      "[D loss: 0.910354] [G loss: 1.364508]\n",
      "[D loss: 0.754104] [G loss: 1.457539]\n",
      "[D loss: 0.997324] [G loss: 1.297069]\n",
      "[D loss: 0.920352] [G loss: 1.626823]\n",
      "[D loss: 0.789154] [G loss: 1.424566]\n",
      "[D loss: 0.695453] [G loss: 1.694219]\n",
      "[D loss: 1.085453] [G loss: 1.397325]\n",
      "[D loss: 0.818580] [G loss: 1.512021]\n",
      "[D loss: 0.887941] [G loss: 1.190064]\n",
      "[D loss: 0.731048] [G loss: 1.477687]\n",
      "[D loss: 0.992036] [G loss: 1.616548]\n",
      "[D loss: 0.895983] [G loss: 1.440413]\n",
      "[D loss: 0.856922] [G loss: 1.705376]\n",
      "[D loss: 0.805838] [G loss: 1.445176]\n",
      "[D loss: 0.780460] [G loss: 1.195774]\n",
      "[D loss: 1.015127] [G loss: 1.380494]\n",
      "[D loss: 0.882584] [G loss: 1.508561]\n",
      "[D loss: 0.752390] [G loss: 1.515709]\n",
      "[D loss: 0.785815] [G loss: 1.618093]\n",
      "[D loss: 0.788103] [G loss: 1.336885]\n",
      "[D loss: 0.989135] [G loss: 1.450919]\n",
      "[D loss: 0.894111] [G loss: 1.412553]\n",
      "[D loss: 0.757223] [G loss: 1.409658]\n",
      "[D loss: 0.771922] [G loss: 1.467455]\n",
      "[D loss: 0.581108] [G loss: 1.756232]\n",
      "[D loss: 0.770528] [G loss: 1.462032]\n",
      "[D loss: 0.864632] [G loss: 1.407863]\n",
      "[D loss: 0.657292] [G loss: 1.455066]\n",
      "[D loss: 0.752899] [G loss: 1.414829]\n",
      "[D loss: 0.697510] [G loss: 1.537892]\n",
      "[D loss: 0.885622] [G loss: 1.480000]\n",
      "[D loss: 0.794926] [G loss: 1.601470]\n",
      "[D loss: 0.821009] [G loss: 1.579877]\n",
      "[D loss: 0.889177] [G loss: 1.452624]\n",
      "[D loss: 0.693731] [G loss: 1.527778]\n",
      "[D loss: 1.088137] [G loss: 1.401050]\n",
      "[D loss: 0.788588] [G loss: 1.615321]\n",
      "[D loss: 0.927985] [G loss: 1.619704]\n",
      "[D loss: 0.981687] [G loss: 1.533394]\n",
      "[D loss: 0.646273] [G loss: 1.536129]\n",
      "[D loss: 1.121704] [G loss: 1.405320]\n",
      "[D loss: 0.875304] [G loss: 1.445101]\n",
      "[D loss: 0.706532] [G loss: 1.591808]\n",
      "[D loss: 0.998719] [G loss: 1.355952]\n",
      "[D loss: 0.639969] [G loss: 1.632828]\n",
      "[D loss: 0.603141] [G loss: 1.487210]\n",
      "[D loss: 0.858672] [G loss: 1.604101]\n",
      "[D loss: 0.686784] [G loss: 1.773762]\n",
      "[D loss: 0.785743] [G loss: 1.415077]\n",
      "[D loss: 0.771501] [G loss: 1.444357]\n",
      "[D loss: 0.625925] [G loss: 1.606992]\n",
      "[D loss: 0.693051] [G loss: 1.382822]\n",
      "[D loss: 0.812983] [G loss: 1.323214]\n",
      "[D loss: 0.665805] [G loss: 1.622483]\n",
      "[D loss: 0.714913] [G loss: 1.470829]\n",
      "[D loss: 0.729162] [G loss: 1.469140]\n",
      "[D loss: 0.766493] [G loss: 1.682782]\n",
      "[D loss: 1.054091] [G loss: 1.461053]\n",
      "[D loss: 0.950548] [G loss: 1.312323]\n",
      "[D loss: 0.722945] [G loss: 1.418698]\n",
      "[D loss: 0.660875] [G loss: 1.461266]\n",
      "[D loss: 0.868870] [G loss: 1.465197]\n",
      "[D loss: 0.728977] [G loss: 1.888990]\n",
      "[D loss: 0.861364] [G loss: 1.650325]\n",
      "[D loss: 0.796423] [G loss: 1.605652]\n",
      "[D loss: 0.992542] [G loss: 1.623976]\n",
      "[D loss: 0.703175] [G loss: 1.671085]\n",
      "[D loss: 0.761324] [G loss: 1.502728]\n",
      "[D loss: 0.993150] [G loss: 1.379004]\n",
      "[D loss: 0.968390] [G loss: 1.596432]\n",
      "[D loss: 0.610757] [G loss: 1.678946]\n",
      "[D loss: 0.752141] [G loss: 1.557051]\n",
      "[D loss: 0.797067] [G loss: 1.798474]\n",
      "[D loss: 0.871609] [G loss: 1.697929]\n",
      "[D loss: 0.703027] [G loss: 1.661680]\n",
      "[D loss: 0.873615] [G loss: 1.357610]\n",
      "[D loss: 0.898390] [G loss: 1.379514]\n",
      "[D loss: 0.702255] [G loss: 1.340652]\n",
      "[D loss: 0.712425] [G loss: 1.544512]\n",
      "[D loss: 1.048165] [G loss: 1.476043]\n",
      "[D loss: 0.798707] [G loss: 1.589655]\n",
      "[D loss: 0.944089] [G loss: 1.384740]\n",
      "[D loss: 0.895617] [G loss: 1.385033]\n",
      "[D loss: 0.717674] [G loss: 1.717072]\n",
      "[D loss: 0.976279] [G loss: 1.289720]\n",
      "[D loss: 0.809326] [G loss: 1.433165]\n",
      "[D loss: 0.857282] [G loss: 1.485605]\n",
      "[D loss: 0.892875] [G loss: 1.571511]\n",
      "[D loss: 0.673545] [G loss: 1.426322]\n",
      "[D loss: 0.935236] [G loss: 1.227027]\n",
      "[D loss: 0.818946] [G loss: 1.481260]\n",
      "[D loss: 0.788889] [G loss: 1.360236]\n",
      "[D loss: 0.654737] [G loss: 1.358838]\n",
      "[D loss: 0.687045] [G loss: 1.386737]\n",
      "[D loss: 0.839801] [G loss: 1.479040]\n",
      "[D loss: 0.833598] [G loss: 1.583508]\n",
      "[D loss: 0.732898] [G loss: 1.640497]\n",
      "[D loss: 0.736030] [G loss: 1.674784]\n",
      "[D loss: 0.827059] [G loss: 1.495974]\n",
      "[D loss: 0.764188] [G loss: 1.597922]\n",
      "[D loss: 0.770932] [G loss: 1.409932]\n",
      "[D loss: 0.873448] [G loss: 1.617300]\n",
      "[D loss: 0.910387] [G loss: 1.612792]\n",
      "[D loss: 0.775577] [G loss: 1.544477]\n",
      "[D loss: 0.697620] [G loss: 1.595473]\n",
      "[D loss: 0.696058] [G loss: 1.311966]\n",
      "[D loss: 0.806500] [G loss: 1.327195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.891155] [G loss: 1.296619]\n",
      "[D loss: 0.759911] [G loss: 1.661517]\n",
      "[D loss: 0.842063] [G loss: 1.580462]\n",
      "[D loss: 0.492967] [G loss: 1.801739]\n",
      "[D loss: 0.842818] [G loss: 1.498019]\n",
      "[D loss: 0.996054] [G loss: 1.346158]\n",
      "[D loss: 0.928578] [G loss: 1.829908]\n",
      "[D loss: 0.745161] [G loss: 1.641573]\n",
      "[D loss: 0.628270] [G loss: 1.796233]\n",
      "[D loss: 0.764418] [G loss: 1.571965]\n",
      "[D loss: 0.743436] [G loss: 1.565748]\n",
      "[D loss: 0.790398] [G loss: 1.564123]\n",
      "[D loss: 0.999080] [G loss: 1.342750]\n",
      "[D loss: 0.755955] [G loss: 1.499363]\n",
      "[D loss: 0.605251] [G loss: 1.731498]\n",
      "[D loss: 0.685973] [G loss: 1.577567]\n",
      "[D loss: 0.809993] [G loss: 1.632560]\n",
      "[D loss: 0.716922] [G loss: 1.594392]\n",
      "[D loss: 0.694788] [G loss: 1.589684]\n",
      "[D loss: 0.779476] [G loss: 1.586047]\n",
      "[D loss: 0.775858] [G loss: 1.424414]\n",
      "[D loss: 0.682805] [G loss: 1.467237]\n",
      "[D loss: 0.906870] [G loss: 1.487000]\n",
      "[D loss: 0.797555] [G loss: 1.719448]\n",
      "[D loss: 0.636961] [G loss: 1.524902]\n",
      "[D loss: 0.697947] [G loss: 1.702505]\n",
      "[D loss: 0.877055] [G loss: 1.571823]\n",
      "[D loss: 0.740117] [G loss: 1.661803]\n",
      "[D loss: 1.018623] [G loss: 1.837062]\n",
      "[D loss: 0.822156] [G loss: 1.602264]\n",
      "[D loss: 0.769460] [G loss: 1.718441]\n",
      "[D loss: 0.880154] [G loss: 1.530932]\n",
      "[D loss: 0.838239] [G loss: 1.439936]\n",
      "[D loss: 0.845263] [G loss: 1.339957]\n",
      "[D loss: 0.897426] [G loss: 1.616531]\n",
      "[D loss: 0.652467] [G loss: 1.590929]\n",
      "[D loss: 0.863764] [G loss: 1.601409]\n",
      "[D loss: 0.968730] [G loss: 1.471588]\n",
      "[D loss: 0.814348] [G loss: 1.680376]\n",
      "[D loss: 0.859728] [G loss: 1.345177]\n",
      "[D loss: 0.717753] [G loss: 1.462656]\n",
      "[D loss: 0.622363] [G loss: 1.595248]\n",
      "[D loss: 1.017850] [G loss: 1.452628]\n",
      "[D loss: 0.760406] [G loss: 1.316319]\n",
      "[D loss: 0.960095] [G loss: 1.303012]\n",
      "[D loss: 0.631255] [G loss: 1.395144]\n",
      "[D loss: 1.200308] [G loss: 1.277843]\n",
      "[D loss: 0.718625] [G loss: 1.632825]\n",
      "[D loss: 0.651884] [G loss: 1.690364]\n",
      "[D loss: 0.772300] [G loss: 1.414729]\n",
      "[D loss: 0.813454] [G loss: 1.504698]\n",
      "[D loss: 0.779713] [G loss: 1.572868]\n",
      "[D loss: 0.792820] [G loss: 1.544011]\n",
      "[D loss: 0.623028] [G loss: 1.676354]\n",
      "[D loss: 0.980874] [G loss: 1.508009]\n",
      "[D loss: 0.911643] [G loss: 1.589499]\n",
      "[D loss: 0.839386] [G loss: 1.359957]\n",
      "[D loss: 0.835518] [G loss: 1.607303]\n",
      "[D loss: 0.913069] [G loss: 1.339108]\n",
      "[D loss: 0.845562] [G loss: 1.285003]\n",
      "[D loss: 0.970744] [G loss: 1.287616]\n",
      "[D loss: 0.765262] [G loss: 1.324089]\n",
      "[D loss: 0.775310] [G loss: 1.545138]\n",
      "[D loss: 0.679657] [G loss: 1.594909]\n",
      "[D loss: 0.851476] [G loss: 1.605474]\n",
      "[D loss: 0.967966] [G loss: 1.359716]\n",
      "[D loss: 0.674029] [G loss: 1.731886]\n",
      "[D loss: 0.755777] [G loss: 1.289778]\n",
      "[D loss: 0.829552] [G loss: 1.511948]\n",
      "[D loss: 0.961772] [G loss: 1.529036]\n",
      "[D loss: 0.983984] [G loss: 1.443502]\n",
      "[D loss: 0.712214] [G loss: 1.248164]\n",
      "[D loss: 0.878935] [G loss: 1.522219]\n",
      "[D loss: 0.705671] [G loss: 1.567751]\n",
      "[D loss: 0.936175] [G loss: 1.426021]\n",
      "[D loss: 0.914423] [G loss: 1.385231]\n",
      "[D loss: 0.617737] [G loss: 1.437262]\n",
      "[D loss: 1.088847] [G loss: 1.018156]\n",
      "[D loss: 0.871488] [G loss: 1.281854]\n",
      "[D loss: 0.907526] [G loss: 1.587420]\n",
      "[D loss: 0.699515] [G loss: 1.604307]\n",
      "[D loss: 0.888506] [G loss: 1.366587]\n",
      "[D loss: 0.863336] [G loss: 1.338866]\n",
      "[D loss: 0.705395] [G loss: 1.323760]\n",
      "[D loss: 0.967745] [G loss: 1.514017]\n",
      "[D loss: 0.815425] [G loss: 1.708243]\n",
      "[D loss: 0.888238] [G loss: 1.594606]\n",
      "[D loss: 0.859139] [G loss: 1.386226]\n",
      "[D loss: 0.813032] [G loss: 1.631023]\n",
      "[D loss: 0.770756] [G loss: 1.518464]\n",
      "[D loss: 0.807405] [G loss: 1.485746]\n",
      "[D loss: 0.906537] [G loss: 1.473299]\n",
      "[D loss: 0.791338] [G loss: 1.279544]\n",
      "[D loss: 0.575899] [G loss: 1.644142]\n",
      "[D loss: 0.915273] [G loss: 1.303014]\n",
      "[D loss: 0.998275] [G loss: 1.466777]\n",
      "[D loss: 0.672376] [G loss: 1.541109]\n",
      "[D loss: 1.000753] [G loss: 1.496815]\n",
      "[D loss: 0.708398] [G loss: 1.819923]\n",
      "[D loss: 0.877864] [G loss: 1.253386]\n",
      "[D loss: 0.851379] [G loss: 1.603454]\n",
      "[D loss: 0.748185] [G loss: 1.595948]\n",
      "[D loss: 0.735243] [G loss: 1.471676]\n",
      "[D loss: 0.812203] [G loss: 1.553975]\n",
      "[D loss: 0.918050] [G loss: 1.354925]\n",
      "[D loss: 0.807827] [G loss: 1.584428]\n",
      "[D loss: 0.846349] [G loss: 1.580612]\n",
      "[D loss: 0.853870] [G loss: 1.567108]\n",
      "[D loss: 0.669656] [G loss: 1.626713]\n",
      "[D loss: 0.833674] [G loss: 1.597829]\n",
      "[D loss: 0.779500] [G loss: 1.386513]\n",
      "[D loss: 0.909065] [G loss: 1.552406]\n",
      "[D loss: 0.667123] [G loss: 1.654221]\n",
      "[D loss: 0.721431] [G loss: 1.532385]\n",
      "[D loss: 0.768402] [G loss: 1.484192]\n",
      "[D loss: 1.137129] [G loss: 1.296809]\n",
      "[D loss: 0.926868] [G loss: 1.543916]\n",
      "[D loss: 0.793637] [G loss: 1.420973]\n",
      "[D loss: 0.856195] [G loss: 1.516455]\n",
      "[D loss: 0.936894] [G loss: 1.421456]\n",
      "[D loss: 0.820949] [G loss: 1.421693]\n",
      "[D loss: 0.667726] [G loss: 1.410084]\n",
      "[D loss: 0.702510] [G loss: 1.629108]\n",
      "[D loss: 0.703233] [G loss: 1.543907]\n",
      "[D loss: 1.024567] [G loss: 1.417751]\n",
      "[D loss: 0.903317] [G loss: 1.350831]\n",
      "[D loss: 0.902952] [G loss: 1.374237]\n",
      "[D loss: 0.771045] [G loss: 1.278102]\n",
      "[D loss: 0.780619] [G loss: 1.256222]\n",
      "[D loss: 0.866127] [G loss: 1.393902]\n",
      "[D loss: 0.771956] [G loss: 1.652297]\n",
      "[D loss: 0.842712] [G loss: 1.608079]\n",
      "[D loss: 0.865730] [G loss: 1.507863]\n",
      "[D loss: 0.827909] [G loss: 1.670822]\n",
      "[D loss: 0.723936] [G loss: 1.420484]\n",
      "[D loss: 0.814170] [G loss: 1.637204]\n",
      "[D loss: 0.917953] [G loss: 1.354149]\n",
      "[D loss: 0.884140] [G loss: 1.420820]\n",
      "[D loss: 0.899477] [G loss: 1.368688]\n",
      "[D loss: 0.724121] [G loss: 1.653318]\n",
      "[D loss: 0.673955] [G loss: 1.518834]\n",
      "[D loss: 0.796616] [G loss: 1.575169]\n",
      "[D loss: 0.850841] [G loss: 1.484153]\n",
      "[D loss: 0.803826] [G loss: 1.487741]\n",
      "[D loss: 0.873661] [G loss: 1.422245]\n",
      "[D loss: 0.683035] [G loss: 1.376528]\n",
      "[D loss: 0.764430] [G loss: 1.461422]\n",
      "[D loss: 0.761427] [G loss: 1.560180]\n",
      "[D loss: 1.121891] [G loss: 1.490019]\n",
      "[D loss: 0.981300] [G loss: 1.474160]\n",
      "[D loss: 1.008105] [G loss: 1.554305]\n",
      "[D loss: 0.726105] [G loss: 1.325886]\n",
      "[D loss: 0.824037] [G loss: 1.295230]\n",
      "[D loss: 0.694130] [G loss: 1.389366]\n",
      "[D loss: 0.708791] [G loss: 1.566986]\n",
      "[D loss: 0.977076] [G loss: 1.447392]\n",
      "[D loss: 0.679450] [G loss: 1.379627]\n",
      "[D loss: 0.751751] [G loss: 1.569362]\n",
      "[D loss: 0.915085] [G loss: 1.512918]\n",
      "[D loss: 0.705338] [G loss: 1.589772]\n",
      "[D loss: 0.875134] [G loss: 1.480279]\n",
      "[D loss: 0.835057] [G loss: 1.407496]\n",
      "[D loss: 0.844436] [G loss: 1.482018]\n",
      "[D loss: 0.488671] [G loss: 1.658639]\n",
      "[D loss: 0.767456] [G loss: 1.506395]\n",
      "[D loss: 0.805636] [G loss: 1.483675]\n",
      "[D loss: 0.768776] [G loss: 1.271744]\n",
      "[D loss: 0.738578] [G loss: 1.753843]\n",
      "[D loss: 0.973178] [G loss: 1.551830]\n",
      "[D loss: 0.872943] [G loss: 1.532928]\n",
      "[D loss: 1.042143] [G loss: 1.241921]\n",
      "[D loss: 1.014533] [G loss: 1.431495]\n",
      "[D loss: 0.634785] [G loss: 1.515361]\n",
      "[D loss: 0.767397] [G loss: 1.408304]\n",
      "[D loss: 0.944163] [G loss: 1.263185]\n",
      "[D loss: 1.054702] [G loss: 1.261792]\n",
      "[D loss: 0.820441] [G loss: 1.370583]\n",
      "[D loss: 0.578778] [G loss: 1.486411]\n",
      "[D loss: 0.746320] [G loss: 1.574898]\n",
      "[D loss: 0.724288] [G loss: 1.451509]\n",
      "[D loss: 0.831325] [G loss: 1.701844]\n",
      "[D loss: 0.754819] [G loss: 1.473952]\n",
      "[D loss: 0.766701] [G loss: 1.422116]\n",
      "[D loss: 0.682753] [G loss: 1.799269]\n",
      "[D loss: 0.724958] [G loss: 1.422488]\n",
      "[D loss: 0.741365] [G loss: 1.468773]\n",
      "[D loss: 0.804989] [G loss: 1.495215]\n",
      "[D loss: 0.810680] [G loss: 1.461359]\n",
      "[D loss: 0.748956] [G loss: 1.463676]\n",
      "[D loss: 0.910544] [G loss: 1.475345]\n",
      "[D loss: 0.930364] [G loss: 1.475042]\n",
      "[D loss: 1.057163] [G loss: 1.501025]\n",
      "[D loss: 0.908835] [G loss: 1.694740]\n",
      "[D loss: 0.784570] [G loss: 1.406608]\n",
      "[D loss: 0.849774] [G loss: 1.642322]\n",
      "[D loss: 0.867666] [G loss: 1.366632]\n",
      "[D loss: 0.791617] [G loss: 1.261838]\n",
      "[D loss: 0.649039] [G loss: 1.516920]\n",
      "[D loss: 0.708413] [G loss: 1.172600]\n",
      "[D loss: 0.716658] [G loss: 1.332176]\n",
      "[D loss: 0.854836] [G loss: 1.341020]\n",
      "[D loss: 0.795778] [G loss: 1.492856]\n",
      "[D loss: 0.990783] [G loss: 1.487322]\n",
      "[D loss: 0.848479] [G loss: 1.633324]\n",
      "[D loss: 0.909871] [G loss: 1.397767]\n",
      "[D loss: 0.624089] [G loss: 1.456070]\n",
      "[D loss: 0.849289] [G loss: 1.457301]\n",
      "[D loss: 0.908405] [G loss: 1.272815]\n",
      "[D loss: 1.157844] [G loss: 1.343499]\n",
      "[D loss: 0.981959] [G loss: 1.451271]\n",
      "[D loss: 0.773209] [G loss: 1.699064]\n",
      "[D loss: 0.632052] [G loss: 1.377100]\n",
      "[D loss: 0.893609] [G loss: 1.356790]\n",
      "[D loss: 0.767609] [G loss: 1.300198]\n",
      "[D loss: 0.592251] [G loss: 1.532790]\n",
      "[D loss: 1.005796] [G loss: 1.320666]\n",
      "[D loss: 1.020135] [G loss: 1.577083]\n",
      "[D loss: 0.702321] [G loss: 1.417816]\n",
      "[D loss: 0.616632] [G loss: 1.583171]\n",
      "[D loss: 0.816711] [G loss: 1.309221]\n",
      "[D loss: 0.643185] [G loss: 1.501290]\n",
      "[D loss: 0.694592] [G loss: 1.438285]\n",
      "[D loss: 0.896266] [G loss: 1.581015]\n",
      "[D loss: 0.695520] [G loss: 1.594111]\n",
      "[D loss: 0.764141] [G loss: 1.575016]\n",
      "[D loss: 1.023444] [G loss: 1.292043]\n",
      "[D loss: 0.737077] [G loss: 1.470429]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.967296] [G loss: 1.412318]\n",
      "[D loss: 0.879616] [G loss: 1.405559]\n",
      "[D loss: 0.750548] [G loss: 1.392545]\n",
      "[D loss: 0.568559] [G loss: 1.474903]\n",
      "[D loss: 0.986900] [G loss: 1.611577]\n",
      "[D loss: 0.814272] [G loss: 1.452626]\n",
      "[D loss: 1.139317] [G loss: 1.320363]\n",
      "[D loss: 0.985725] [G loss: 1.338867]\n",
      "[D loss: 0.738246] [G loss: 1.391584]\n",
      "[D loss: 0.709837] [G loss: 1.440595]\n",
      "[D loss: 0.925266] [G loss: 1.431270]\n",
      "[D loss: 1.050871] [G loss: 1.514397]\n",
      "[D loss: 0.731492] [G loss: 1.645665]\n",
      "[D loss: 0.861765] [G loss: 1.643946]\n",
      "[D loss: 0.865154] [G loss: 1.552765]\n",
      "[D loss: 0.974039] [G loss: 1.277235]\n",
      "[D loss: 0.975487] [G loss: 1.379481]\n",
      "[D loss: 0.656347] [G loss: 1.341008]\n",
      "[D loss: 0.863941] [G loss: 1.196116]\n",
      "[D loss: 0.749875] [G loss: 1.264081]\n",
      "[D loss: 0.938730] [G loss: 1.339133]\n",
      "[D loss: 1.050557] [G loss: 1.320176]\n",
      "[D loss: 0.831630] [G loss: 1.430244]\n",
      "[D loss: 0.819793] [G loss: 1.473238]\n",
      "[D loss: 0.767326] [G loss: 1.483434]\n",
      "[D loss: 0.804826] [G loss: 1.505756]\n",
      "[D loss: 1.086939] [G loss: 1.477394]\n",
      "[D loss: 0.944602] [G loss: 1.550328]\n",
      "[D loss: 0.681344] [G loss: 1.526232]\n",
      "[D loss: 0.845788] [G loss: 1.361264]\n",
      "[D loss: 0.850933] [G loss: 1.384201]\n",
      "[D loss: 0.669215] [G loss: 1.345806]\n",
      "[D loss: 0.760146] [G loss: 1.184629]\n",
      "[D loss: 0.995459] [G loss: 1.444039]\n",
      "[D loss: 0.749633] [G loss: 1.484219]\n",
      "[D loss: 0.762177] [G loss: 1.445125]\n",
      "[D loss: 0.786544] [G loss: 1.270207]\n",
      "[D loss: 1.036298] [G loss: 1.336158]\n",
      "[D loss: 0.782037] [G loss: 1.412726]\n",
      "[D loss: 0.938273] [G loss: 1.472653]\n",
      "[D loss: 0.962708] [G loss: 1.286114]\n",
      "[D loss: 0.627306] [G loss: 1.499847]\n",
      "[D loss: 0.984318] [G loss: 1.404406]\n",
      "[D loss: 0.901442] [G loss: 1.404529]\n",
      "[D loss: 0.700360] [G loss: 1.741097]\n",
      "[D loss: 0.649567] [G loss: 1.471225]\n",
      "[D loss: 0.823618] [G loss: 1.575903]\n",
      "[D loss: 0.810086] [G loss: 1.383022]\n",
      "[D loss: 0.779861] [G loss: 1.535297]\n",
      "[D loss: 0.843353] [G loss: 1.298094]\n",
      "[D loss: 1.012412] [G loss: 1.699921]\n",
      "[D loss: 0.847062] [G loss: 1.626796]\n",
      "[D loss: 0.828272] [G loss: 1.686883]\n",
      "[D loss: 0.663840] [G loss: 1.631423]\n",
      "[D loss: 0.961066] [G loss: 1.461010]\n",
      "[D loss: 0.856490] [G loss: 1.396709]\n",
      "[D loss: 0.741443] [G loss: 1.444390]\n",
      "[D loss: 0.744844] [G loss: 1.305047]\n",
      "[D loss: 0.638880] [G loss: 1.422981]\n",
      "[D loss: 1.109773] [G loss: 1.423344]\n",
      "[D loss: 0.777814] [G loss: 1.532578]\n",
      "[D loss: 0.794563] [G loss: 1.350073]\n",
      "[D loss: 0.712208] [G loss: 1.525364]\n",
      "[D loss: 0.730360] [G loss: 1.446700]\n",
      "[D loss: 0.810970] [G loss: 1.412579]\n",
      "[D loss: 1.040030] [G loss: 1.555964]\n",
      "[D loss: 0.802299] [G loss: 1.506292]\n",
      "[D loss: 0.882579] [G loss: 1.581566]\n",
      "[D loss: 0.991649] [G loss: 1.517019]\n",
      "[D loss: 0.664668] [G loss: 1.583262]\n",
      "[D loss: 0.814819] [G loss: 1.528849]\n",
      "[D loss: 0.705520] [G loss: 1.479274]\n",
      "[D loss: 0.867734] [G loss: 1.391357]\n",
      "[D loss: 0.805726] [G loss: 1.427047]\n",
      "[D loss: 0.736210] [G loss: 1.486996]\n",
      "[D loss: 0.731634] [G loss: 1.697751]\n",
      "[D loss: 0.759079] [G loss: 1.445666]\n",
      "[D loss: 0.780577] [G loss: 1.562692]\n",
      "[D loss: 0.981300] [G loss: 1.432025]\n",
      "[D loss: 0.717228] [G loss: 1.581962]\n",
      "[D loss: 0.833917] [G loss: 1.408845]\n",
      "[D loss: 0.667349] [G loss: 1.653263]\n",
      "[D loss: 0.766041] [G loss: 1.577210]\n",
      "[D loss: 0.817689] [G loss: 1.387628]\n",
      "[D loss: 0.634755] [G loss: 1.682179]\n",
      "[D loss: 0.859787] [G loss: 1.518459]\n",
      "[D loss: 0.628762] [G loss: 1.568818]\n",
      "[D loss: 0.700927] [G loss: 1.526506]\n",
      "[D loss: 0.847643] [G loss: 1.471825]\n",
      "[D loss: 0.821109] [G loss: 1.633712]\n",
      "[D loss: 0.602454] [G loss: 1.627594]\n",
      "[D loss: 0.968040] [G loss: 1.493562]\n",
      "[D loss: 0.699112] [G loss: 1.633948]\n",
      "[D loss: 1.098456] [G loss: 1.692860]\n",
      "[D loss: 0.853716] [G loss: 1.453827]\n",
      "[D loss: 0.894009] [G loss: 1.369198]\n",
      "[D loss: 1.053581] [G loss: 1.237709]\n",
      "[D loss: 0.771130] [G loss: 1.497444]\n",
      "[D loss: 0.950779] [G loss: 1.409246]\n",
      "[D loss: 0.898927] [G loss: 1.437062]\n",
      "[D loss: 1.195282] [G loss: 1.560542]\n",
      "[D loss: 0.997949] [G loss: 1.494734]\n",
      "[D loss: 0.822579] [G loss: 1.568027]\n",
      "[D loss: 0.708311] [G loss: 1.786651]\n",
      "[D loss: 0.723113] [G loss: 1.448612]\n",
      "[D loss: 0.831831] [G loss: 1.382053]\n",
      "[D loss: 0.834654] [G loss: 1.392829]\n",
      "[D loss: 0.907266] [G loss: 1.265699]\n",
      "[D loss: 1.022104] [G loss: 1.249197]\n",
      "[D loss: 0.583489] [G loss: 1.625318]\n",
      "[D loss: 0.871863] [G loss: 1.469378]\n",
      "[D loss: 0.712424] [G loss: 1.508935]\n",
      "[D loss: 0.777558] [G loss: 1.473268]\n",
      "[D loss: 0.949374] [G loss: 1.351624]\n",
      "[D loss: 0.701141] [G loss: 1.448885]\n",
      "[D loss: 0.830600] [G loss: 1.334663]\n",
      "[D loss: 0.809554] [G loss: 1.160298]\n",
      "[D loss: 1.000277] [G loss: 1.302704]\n",
      "[D loss: 0.790838] [G loss: 1.529063]\n",
      "[D loss: 0.908057] [G loss: 1.457131]\n",
      "[D loss: 0.795578] [G loss: 1.273268]\n",
      "[D loss: 0.957423] [G loss: 1.299871]\n",
      "[D loss: 0.971172] [G loss: 1.409750]\n",
      "[D loss: 1.033138] [G loss: 1.332988]\n",
      "[D loss: 0.935825] [G loss: 1.623617]\n",
      "[D loss: 0.774924] [G loss: 1.467089]\n",
      "[D loss: 0.705482] [G loss: 1.540018]\n",
      "[D loss: 0.831801] [G loss: 1.284506]\n",
      "[D loss: 1.022709] [G loss: 1.349734]\n",
      "[D loss: 0.615731] [G loss: 1.550184]\n",
      "[D loss: 0.938613] [G loss: 1.437720]\n",
      "[D loss: 0.765702] [G loss: 1.556606]\n",
      "[D loss: 0.780272] [G loss: 1.562004]\n",
      "[D loss: 0.861357] [G loss: 1.414852]\n",
      "[D loss: 0.725039] [G loss: 1.476862]\n",
      "[D loss: 0.926324] [G loss: 1.424856]\n",
      "[D loss: 0.938948] [G loss: 1.568403]\n",
      "[D loss: 0.847480] [G loss: 1.242152]\n",
      "[D loss: 0.797344] [G loss: 1.403992]\n",
      "[D loss: 0.941321] [G loss: 1.320984]\n",
      "[D loss: 1.031718] [G loss: 1.638265]\n",
      "[D loss: 0.863935] [G loss: 1.407626]\n",
      "[D loss: 0.847390] [G loss: 1.680731]\n",
      "[D loss: 0.925289] [G loss: 1.530702]\n",
      "[D loss: 0.728298] [G loss: 1.455651]\n",
      "[D loss: 0.711304] [G loss: 1.259814]\n",
      "[D loss: 0.814756] [G loss: 1.385571]\n",
      "[D loss: 0.719302] [G loss: 1.362202]\n",
      "[D loss: 0.685106] [G loss: 1.717840]\n",
      "[D loss: 0.982567] [G loss: 1.553392]\n",
      "[D loss: 0.979666] [G loss: 1.519478]\n",
      "[D loss: 0.682398] [G loss: 1.645539]\n",
      "[D loss: 0.922996] [G loss: 1.530559]\n",
      "[D loss: 0.819189] [G loss: 1.359688]\n",
      "[D loss: 0.801423] [G loss: 1.425312]\n",
      "[D loss: 0.906071] [G loss: 1.479118]\n",
      "[D loss: 0.898444] [G loss: 1.426408]\n",
      "[D loss: 0.695180] [G loss: 1.591043]\n",
      "[D loss: 0.852694] [G loss: 1.383398]\n",
      "[D loss: 0.822616] [G loss: 1.686496]\n",
      "[D loss: 0.998828] [G loss: 1.612419]\n",
      "[D loss: 0.801647] [G loss: 1.468329]\n",
      "[D loss: 0.697284] [G loss: 1.361617]\n",
      "[D loss: 0.672252] [G loss: 1.450305]\n",
      "[D loss: 0.981739] [G loss: 1.371488]\n",
      "[D loss: 0.806262] [G loss: 1.425679]\n",
      "[D loss: 0.880889] [G loss: 1.598966]\n",
      "[D loss: 0.865630] [G loss: 1.381357]\n",
      "[D loss: 0.670837] [G loss: 1.491923]\n",
      "[D loss: 0.772386] [G loss: 1.483579]\n",
      "[D loss: 0.655673] [G loss: 1.583098]\n",
      "[D loss: 0.811394] [G loss: 1.512797]\n",
      "[D loss: 0.792937] [G loss: 1.374247]\n",
      "[D loss: 1.021266] [G loss: 1.689806]\n",
      "[D loss: 0.921117] [G loss: 1.544514]\n",
      "[D loss: 0.782319] [G loss: 1.315746]\n",
      "[D loss: 0.973125] [G loss: 1.391557]\n",
      "[D loss: 0.801340] [G loss: 1.484448]\n",
      "[D loss: 0.860016] [G loss: 1.915787]\n",
      "[D loss: 1.006042] [G loss: 1.657694]\n",
      "[D loss: 0.935414] [G loss: 1.269018]\n",
      "[D loss: 0.958954] [G loss: 1.695619]\n",
      "[D loss: 0.748612] [G loss: 1.571043]\n",
      "[D loss: 0.944366] [G loss: 1.612254]\n",
      "[D loss: 0.583881] [G loss: 1.803772]\n",
      "[D loss: 0.821150] [G loss: 1.439008]\n",
      "[D loss: 0.707273] [G loss: 1.511613]\n",
      "[D loss: 0.962646] [G loss: 1.335668]\n",
      "[D loss: 0.724023] [G loss: 1.710605]\n",
      "[D loss: 0.835096] [G loss: 1.469452]\n",
      "[D loss: 0.705723] [G loss: 1.443190]\n",
      "[D loss: 0.779682] [G loss: 1.669574]\n",
      "[D loss: 0.843566] [G loss: 1.528711]\n",
      "[D loss: 0.665194] [G loss: 1.638979]\n",
      "[D loss: 1.041074] [G loss: 1.339346]\n",
      "[D loss: 0.694517] [G loss: 1.674960]\n",
      "[D loss: 1.077664] [G loss: 1.544905]\n",
      "[D loss: 0.733274] [G loss: 1.358583]\n",
      "[D loss: 0.856227] [G loss: 1.266343]\n",
      "[D loss: 0.885343] [G loss: 1.335755]\n",
      "[D loss: 0.799451] [G loss: 1.360296]\n",
      "[D loss: 0.800808] [G loss: 1.566897]\n",
      "[D loss: 0.821472] [G loss: 1.531341]\n",
      "[D loss: 0.830346] [G loss: 1.606342]\n",
      "[D loss: 0.740248] [G loss: 1.782831]\n",
      "[D loss: 0.825509] [G loss: 1.386337]\n",
      "[D loss: 0.913134] [G loss: 1.632688]\n",
      "[D loss: 0.925297] [G loss: 1.444016]\n",
      "[D loss: 0.841409] [G loss: 1.618893]\n",
      "[D loss: 0.828033] [G loss: 1.497287]\n",
      "[D loss: 0.561105] [G loss: 1.529133]\n",
      "[D loss: 0.812835] [G loss: 1.444851]\n",
      "[D loss: 1.043630] [G loss: 1.575554]\n",
      "[D loss: 0.869031] [G loss: 1.434487]\n",
      "[D loss: 0.787605] [G loss: 1.382274]\n",
      "[D loss: 0.748001] [G loss: 1.509475]\n",
      "[D loss: 0.808685] [G loss: 1.312612]\n",
      "[D loss: 0.770977] [G loss: 1.552277]\n",
      "[D loss: 0.710135] [G loss: 1.378524]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.717422] [G loss: 1.566002]\n",
      "[D loss: 0.831613] [G loss: 1.501999]\n",
      "[D loss: 0.944249] [G loss: 1.634756]\n",
      "[D loss: 0.706726] [G loss: 1.599135]\n",
      "[D loss: 0.872231] [G loss: 1.614971]\n",
      "[D loss: 0.700079] [G loss: 1.408991]\n",
      "[D loss: 0.818624] [G loss: 1.453097]\n",
      "[D loss: 0.744844] [G loss: 1.497027]\n",
      "[D loss: 0.913044] [G loss: 1.230148]\n",
      "[D loss: 0.677392] [G loss: 1.683308]\n",
      "[D loss: 0.728302] [G loss: 1.485237]\n",
      "[D loss: 0.836682] [G loss: 1.505493]\n",
      "[D loss: 0.790741] [G loss: 1.498168]\n",
      "[D loss: 0.684876] [G loss: 1.477371]\n",
      "[D loss: 0.891964] [G loss: 1.424348]\n",
      "[D loss: 0.857139] [G loss: 1.673919]\n",
      "[D loss: 0.709407] [G loss: 1.733883]\n",
      "[D loss: 0.824724] [G loss: 1.528912]\n",
      "[D loss: 0.770311] [G loss: 1.806013]\n",
      "[D loss: 0.861755] [G loss: 1.395850]\n",
      "[D loss: 0.788324] [G loss: 1.293768]\n",
      "[D loss: 0.992577] [G loss: 1.239251]\n",
      "[D loss: 0.751900] [G loss: 1.767460]\n",
      "[D loss: 0.831377] [G loss: 1.327029]\n",
      "[D loss: 0.930618] [G loss: 1.594782]\n",
      "[D loss: 0.604402] [G loss: 1.459311]\n",
      "[D loss: 0.791126] [G loss: 1.430394]\n",
      "[D loss: 0.846144] [G loss: 1.709688]\n",
      "[D loss: 1.161279] [G loss: 1.473352]\n",
      "[D loss: 0.986266] [G loss: 1.590999]\n",
      "[D loss: 0.614293] [G loss: 1.282557]\n",
      "[D loss: 1.019511] [G loss: 1.469421]\n",
      "[D loss: 0.705636] [G loss: 1.392126]\n",
      "[D loss: 0.740836] [G loss: 1.503689]\n",
      "[D loss: 0.666475] [G loss: 1.401210]\n",
      "[D loss: 0.851317] [G loss: 1.605852]\n",
      "[D loss: 0.736505] [G loss: 1.719334]\n",
      "[D loss: 0.885014] [G loss: 1.572679]\n",
      "[D loss: 0.714027] [G loss: 1.615963]\n",
      "[D loss: 0.673069] [G loss: 1.684298]\n",
      "[D loss: 0.981810] [G loss: 1.348232]\n",
      "[D loss: 0.744957] [G loss: 1.484628]\n",
      "[D loss: 0.819050] [G loss: 1.706958]\n",
      "[D loss: 0.687241] [G loss: 1.446499]\n",
      "[D loss: 0.891706] [G loss: 1.704296]\n",
      "[D loss: 0.812324] [G loss: 1.551676]\n",
      "[D loss: 0.687526] [G loss: 1.722937]\n",
      "[D loss: 0.845364] [G loss: 1.476528]\n",
      "[D loss: 0.645355] [G loss: 1.496926]\n",
      "[D loss: 0.853500] [G loss: 1.396380]\n",
      "[D loss: 0.757216] [G loss: 1.489980]\n",
      "[D loss: 0.867951] [G loss: 1.416424]\n",
      "[D loss: 0.757267] [G loss: 1.489599]\n",
      "[D loss: 0.810471] [G loss: 1.621242]\n",
      "[D loss: 1.001492] [G loss: 1.689321]\n",
      "[D loss: 1.037119] [G loss: 1.125856]\n",
      "[D loss: 0.926732] [G loss: 1.431710]\n",
      "[D loss: 0.510730] [G loss: 1.695415]\n",
      "[D loss: 0.705419] [G loss: 1.515202]\n",
      "[D loss: 0.690811] [G loss: 1.618829]\n",
      "[D loss: 0.901376] [G loss: 1.461521]\n",
      "[D loss: 1.012297] [G loss: 1.523219]\n",
      "[D loss: 0.674671] [G loss: 1.361231]\n",
      "[D loss: 0.727419] [G loss: 1.329226]\n",
      "[D loss: 1.131259] [G loss: 1.609761]\n",
      "[D loss: 0.751471] [G loss: 1.682318]\n",
      "[D loss: 0.796580] [G loss: 1.752271]\n",
      "[D loss: 0.829934] [G loss: 1.517226]\n",
      "[D loss: 0.934939] [G loss: 1.257399]\n",
      "[D loss: 0.968651] [G loss: 1.585494]\n",
      "[D loss: 0.980656] [G loss: 1.465591]\n",
      "[D loss: 0.841037] [G loss: 1.452989]\n",
      "[D loss: 0.802502] [G loss: 1.443807]\n",
      "[D loss: 1.105995] [G loss: 1.679716]\n",
      "[D loss: 0.742483] [G loss: 1.447215]\n",
      "[D loss: 1.054251] [G loss: 1.432367]\n",
      "[D loss: 1.138500] [G loss: 1.201052]\n",
      "[D loss: 0.901474] [G loss: 1.328785]\n",
      "[D loss: 0.650918] [G loss: 1.403419]\n",
      "[D loss: 0.780444] [G loss: 1.507089]\n",
      "[D loss: 0.797734] [G loss: 1.508041]\n",
      "[D loss: 0.703124] [G loss: 1.339457]\n",
      "[D loss: 0.797851] [G loss: 1.366262]\n",
      "[D loss: 0.808469] [G loss: 1.384970]\n",
      "[D loss: 0.580744] [G loss: 1.417983]\n",
      "[D loss: 0.889661] [G loss: 1.588908]\n",
      "[D loss: 0.649913] [G loss: 1.424181]\n",
      "[D loss: 0.760979] [G loss: 1.440047]\n",
      "[D loss: 0.472490] [G loss: 1.700804]\n",
      "[D loss: 0.814420] [G loss: 1.448793]\n",
      "[D loss: 0.736906] [G loss: 1.481612]\n",
      "[D loss: 0.997702] [G loss: 1.299524]\n",
      "[D loss: 0.993181] [G loss: 1.388852]\n",
      "[D loss: 0.763214] [G loss: 1.885665]\n",
      "[D loss: 0.957691] [G loss: 1.539626]\n",
      "[D loss: 0.904459] [G loss: 1.453125]\n",
      "[D loss: 1.042703] [G loss: 1.556956]\n",
      "[D loss: 0.881013] [G loss: 1.648667]\n",
      "[D loss: 0.891765] [G loss: 1.370162]\n",
      "[D loss: 0.880901] [G loss: 1.499451]\n",
      "[D loss: 0.617673] [G loss: 1.418627]\n",
      "[D loss: 1.092320] [G loss: 1.523612]\n",
      "[D loss: 0.578652] [G loss: 1.540224]\n",
      "[D loss: 0.897296] [G loss: 1.440851]\n",
      "[D loss: 0.967744] [G loss: 1.289998]\n",
      "[D loss: 0.782514] [G loss: 1.600458]\n",
      "[D loss: 0.745104] [G loss: 1.418743]\n",
      "[D loss: 0.976654] [G loss: 1.316384]\n",
      "[D loss: 0.822083] [G loss: 1.445603]\n",
      "[D loss: 0.790139] [G loss: 1.449052]\n",
      "[D loss: 0.756637] [G loss: 1.315123]\n",
      "[D loss: 0.752374] [G loss: 1.769718]\n",
      "[D loss: 0.889528] [G loss: 1.665630]\n",
      "[D loss: 0.703752] [G loss: 1.525270]\n",
      "[D loss: 0.926961] [G loss: 1.656218]\n",
      "[D loss: 0.908608] [G loss: 1.227150]\n",
      "[D loss: 0.723154] [G loss: 1.399238]\n",
      "[D loss: 1.041260] [G loss: 1.421256]\n",
      "[D loss: 0.716252] [G loss: 1.544376]\n",
      "[D loss: 0.943521] [G loss: 1.353347]\n",
      "[D loss: 0.776538] [G loss: 1.655119]\n",
      "[D loss: 0.946851] [G loss: 1.552329]\n",
      "[D loss: 0.632289] [G loss: 1.530199]\n",
      "[D loss: 1.008778] [G loss: 1.390834]\n",
      "[D loss: 0.777218] [G loss: 1.328436]\n",
      "[D loss: 0.887698] [G loss: 1.343654]\n",
      "[D loss: 0.717772] [G loss: 1.610581]\n",
      "[D loss: 1.092389] [G loss: 1.623474]\n",
      "[D loss: 0.995452] [G loss: 1.203478]\n",
      "[D loss: 0.960748] [G loss: 1.367288]\n",
      "[D loss: 0.694839] [G loss: 1.903202]\n",
      "[D loss: 0.720986] [G loss: 1.663048]\n",
      "[D loss: 0.993860] [G loss: 1.616576]\n",
      "[D loss: 0.796756] [G loss: 1.412251]\n",
      "[D loss: 0.785550] [G loss: 1.608420]\n",
      "[D loss: 0.750268] [G loss: 1.587166]\n",
      "[D loss: 0.719730] [G loss: 1.369602]\n",
      "[D loss: 0.843139] [G loss: 1.334362]\n",
      "[D loss: 0.833450] [G loss: 1.336758]\n",
      "[D loss: 0.817997] [G loss: 1.678261]\n",
      "[D loss: 0.878536] [G loss: 1.478150]\n",
      "[D loss: 0.777069] [G loss: 1.559837]\n",
      "[D loss: 0.811163] [G loss: 1.422406]\n",
      "[D loss: 0.784766] [G loss: 1.332839]\n",
      "[D loss: 0.738983] [G loss: 1.418486]\n",
      "[D loss: 0.951095] [G loss: 1.097628]\n",
      "[D loss: 0.911242] [G loss: 1.353073]\n",
      "[D loss: 0.781800] [G loss: 1.495589]\n",
      "[D loss: 0.859455] [G loss: 1.585430]\n",
      "[D loss: 0.645070] [G loss: 1.529249]\n",
      "[D loss: 0.771554] [G loss: 1.401288]\n",
      "[D loss: 0.795463] [G loss: 1.441438]\n",
      "[D loss: 1.116776] [G loss: 1.356486]\n",
      "[D loss: 0.663060] [G loss: 1.552322]\n",
      "[D loss: 1.027549] [G loss: 1.436985]\n",
      "[D loss: 0.836753] [G loss: 1.427699]\n",
      "[D loss: 0.933338] [G loss: 1.487112]\n",
      "[D loss: 0.757693] [G loss: 1.438305]\n",
      "[D loss: 0.881427] [G loss: 1.491032]\n",
      "[D loss: 0.911556] [G loss: 1.214377]\n",
      "[D loss: 0.790013] [G loss: 1.562933]\n",
      "[D loss: 0.558825] [G loss: 1.558013]\n",
      "[D loss: 0.818076] [G loss: 1.401514]\n",
      "[D loss: 0.951812] [G loss: 1.616644]\n",
      "[D loss: 0.982405] [G loss: 1.336907]\n",
      "[D loss: 1.052086] [G loss: 1.254622]\n",
      "[D loss: 0.701949] [G loss: 1.617075]\n",
      "[D loss: 0.766539] [G loss: 1.504146]\n",
      "[D loss: 0.961686] [G loss: 1.278891]\n",
      "[D loss: 1.147201] [G loss: 1.155501]\n",
      "[D loss: 0.860997] [G loss: 1.584183]\n",
      "[D loss: 0.817702] [G loss: 1.398974]\n",
      "[D loss: 0.689601] [G loss: 1.601500]\n",
      "[D loss: 0.721938] [G loss: 1.372045]\n",
      "[D loss: 0.820659] [G loss: 1.474236]\n",
      "[D loss: 0.648079] [G loss: 1.625589]\n",
      "[D loss: 0.892076] [G loss: 1.406059]\n",
      "[D loss: 0.764490] [G loss: 1.494556]\n",
      "[D loss: 1.055562] [G loss: 1.486443]\n",
      "[D loss: 0.992341] [G loss: 1.226859]\n",
      "[D loss: 0.721936] [G loss: 1.547143]\n",
      "[D loss: 0.997852] [G loss: 1.226150]\n",
      "[D loss: 0.857265] [G loss: 1.548221]\n",
      "[D loss: 0.823756] [G loss: 1.597730]\n",
      "[D loss: 1.016206] [G loss: 1.349823]\n",
      "[D loss: 1.023426] [G loss: 1.381992]\n",
      "[D loss: 0.654471] [G loss: 1.267115]\n",
      "[D loss: 0.706939] [G loss: 1.342221]\n",
      "[D loss: 0.859019] [G loss: 1.343343]\n",
      "[D loss: 0.746046] [G loss: 1.505145]\n",
      "[D loss: 0.744796] [G loss: 1.591855]\n",
      "[D loss: 0.807149] [G loss: 1.444510]\n",
      "[D loss: 0.683700] [G loss: 1.611326]\n",
      "[D loss: 0.720263] [G loss: 1.389964]\n",
      "[D loss: 0.798679] [G loss: 1.410252]\n",
      "[D loss: 0.807683] [G loss: 1.474471]\n",
      "[D loss: 0.824566] [G loss: 1.637324]\n",
      "[D loss: 0.691709] [G loss: 1.804725]\n",
      "[D loss: 0.745434] [G loss: 1.613972]\n",
      "[D loss: 0.663237] [G loss: 1.666554]\n",
      "[D loss: 0.881510] [G loss: 1.560884]\n",
      "[D loss: 0.876209] [G loss: 1.377459]\n",
      "[D loss: 0.853131] [G loss: 1.222117]\n",
      "[D loss: 1.109516] [G loss: 1.455426]\n",
      "[D loss: 0.717412] [G loss: 1.473757]\n",
      "[D loss: 0.603832] [G loss: 1.381111]\n",
      "[D loss: 0.723572] [G loss: 1.670116]\n",
      "[D loss: 0.750236] [G loss: 1.554591]\n",
      "[D loss: 0.894963] [G loss: 1.472721]\n",
      "[D loss: 0.634470] [G loss: 2.004940]\n",
      "[D loss: 0.922536] [G loss: 1.684341]\n",
      "[D loss: 0.992923] [G loss: 1.690209]\n",
      "[D loss: 0.800529] [G loss: 1.686814]\n",
      "[D loss: 0.820967] [G loss: 1.749331]\n",
      "[D loss: 0.803814] [G loss: 1.534540]\n",
      "[D loss: 1.143937] [G loss: 1.279435]\n",
      "[D loss: 0.771986] [G loss: 1.709064]\n",
      "[D loss: 0.926096] [G loss: 1.382111]\n",
      "[D loss: 0.788883] [G loss: 1.429625]\n",
      "[D loss: 0.838317] [G loss: 1.582742]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.748463] [G loss: 1.376163]\n",
      "[D loss: 0.839656] [G loss: 1.465197]\n",
      "[D loss: 0.914456] [G loss: 1.472527]\n",
      "[D loss: 0.839201] [G loss: 1.341282]\n",
      "[D loss: 0.656322] [G loss: 1.385758]\n",
      "[D loss: 0.750334] [G loss: 1.460224]\n",
      "[D loss: 0.977567] [G loss: 1.509983]\n",
      "[D loss: 0.800706] [G loss: 1.504864]\n",
      "[D loss: 0.879945] [G loss: 1.348646]\n",
      "[D loss: 0.847065] [G loss: 1.518265]\n",
      "[D loss: 0.804562] [G loss: 1.435002]\n",
      "[D loss: 0.888668] [G loss: 1.334248]\n",
      "[D loss: 0.761123] [G loss: 1.281918]\n",
      "[D loss: 0.949647] [G loss: 1.447486]\n",
      "[D loss: 0.735186] [G loss: 1.677585]\n",
      "[D loss: 0.893321] [G loss: 1.255847]\n",
      "[D loss: 1.006616] [G loss: 1.391856]\n",
      "[D loss: 0.680670] [G loss: 1.578528]\n",
      "[D loss: 1.096064] [G loss: 1.454267]\n",
      "[D loss: 0.951766] [G loss: 1.570944]\n",
      "[D loss: 1.093042] [G loss: 1.412064]\n",
      "[D loss: 1.013471] [G loss: 1.336895]\n",
      "[D loss: 0.833911] [G loss: 1.431474]\n",
      "[D loss: 0.915821] [G loss: 1.332077]\n",
      "[D loss: 0.796006] [G loss: 1.259485]\n",
      "[D loss: 0.879322] [G loss: 1.285835]\n",
      "[D loss: 1.031519] [G loss: 1.297690]\n",
      "[D loss: 0.773240] [G loss: 1.514578]\n",
      "[D loss: 0.856073] [G loss: 1.543876]\n",
      "[D loss: 0.725159] [G loss: 1.526814]\n",
      "[D loss: 0.642625] [G loss: 1.285782]\n",
      "[D loss: 0.764888] [G loss: 1.583114]\n",
      "[D loss: 0.808203] [G loss: 1.571913]\n",
      "[D loss: 0.823680] [G loss: 1.272916]\n",
      "[D loss: 0.813792] [G loss: 1.344825]\n",
      "[D loss: 0.820752] [G loss: 1.353901]\n",
      "[D loss: 0.930426] [G loss: 1.454386]\n",
      "[D loss: 0.759876] [G loss: 1.571832]\n",
      "[D loss: 0.847629] [G loss: 1.590033]\n",
      "[D loss: 0.784030] [G loss: 1.437546]\n",
      "[D loss: 1.074241] [G loss: 1.620002]\n",
      "[D loss: 0.951034] [G loss: 1.243262]\n",
      "[D loss: 0.962455] [G loss: 1.360411]\n",
      "[D loss: 0.922331] [G loss: 1.446545]\n",
      "[D loss: 0.835376] [G loss: 1.450115]\n",
      "[D loss: 0.866948] [G loss: 1.459076]\n",
      "[D loss: 0.985466] [G loss: 1.289195]\n",
      "[D loss: 0.809086] [G loss: 1.415523]\n",
      "[D loss: 0.905231] [G loss: 1.520337]\n",
      "[D loss: 0.886546] [G loss: 1.493800]\n",
      "[D loss: 0.871080] [G loss: 1.520468]\n",
      "[D loss: 0.976300] [G loss: 1.224944]\n",
      "[D loss: 0.904205] [G loss: 1.236279]\n",
      "[D loss: 0.724304] [G loss: 1.378923]\n",
      "[D loss: 0.742981] [G loss: 1.353722]\n",
      "[D loss: 1.008917] [G loss: 1.430223]\n",
      "[D loss: 0.901340] [G loss: 1.481694]\n",
      "[D loss: 0.905897] [G loss: 1.393187]\n",
      "[D loss: 0.782405] [G loss: 1.579771]\n",
      "[D loss: 0.988775] [G loss: 1.487839]\n",
      "[D loss: 0.975708] [G loss: 1.395883]\n",
      "[D loss: 0.933627] [G loss: 1.738574]\n",
      "[D loss: 0.694725] [G loss: 1.395955]\n",
      "[D loss: 1.015746] [G loss: 1.251538]\n",
      "[D loss: 0.596007] [G loss: 1.431410]\n",
      "[D loss: 0.895072] [G loss: 1.375297]\n",
      "[D loss: 1.239535] [G loss: 1.187071]\n",
      "[D loss: 0.811343] [G loss: 1.567274]\n",
      "[D loss: 0.930417] [G loss: 1.236863]\n",
      "[D loss: 1.180824] [G loss: 1.160129]\n",
      "[D loss: 0.770719] [G loss: 1.328212]\n",
      "[D loss: 0.956970] [G loss: 1.273686]\n",
      "[D loss: 0.907522] [G loss: 1.493843]\n",
      "[D loss: 0.843662] [G loss: 1.408909]\n",
      "[D loss: 0.948094] [G loss: 1.252022]\n",
      "[D loss: 0.861210] [G loss: 1.482311]\n",
      "[D loss: 0.948730] [G loss: 1.410844]\n",
      "[D loss: 0.864381] [G loss: 1.235211]\n",
      "[D loss: 0.734931] [G loss: 1.304126]\n",
      "[D loss: 0.696337] [G loss: 1.735555]\n",
      "[D loss: 0.615283] [G loss: 1.463996]\n",
      "[D loss: 1.049090] [G loss: 1.406418]\n",
      "[D loss: 0.630511] [G loss: 1.392469]\n",
      "[D loss: 1.072877] [G loss: 1.269186]\n",
      "[D loss: 0.672490] [G loss: 1.543565]\n",
      "[D loss: 0.740590] [G loss: 1.402980]\n",
      "[D loss: 0.952063] [G loss: 1.372557]\n",
      "[D loss: 0.814690] [G loss: 1.401126]\n",
      "[D loss: 0.944105] [G loss: 1.554677]\n",
      "[D loss: 0.793213] [G loss: 1.422874]\n",
      "[D loss: 0.686020] [G loss: 1.301801]\n",
      "[D loss: 0.746670] [G loss: 1.264466]\n",
      "[D loss: 0.748590] [G loss: 1.592056]\n",
      "[D loss: 0.857052] [G loss: 1.331240]\n",
      "[D loss: 1.185743] [G loss: 1.317960]\n",
      "[D loss: 0.917632] [G loss: 1.304068]\n",
      "[D loss: 0.879696] [G loss: 1.423609]\n",
      "[D loss: 0.732622] [G loss: 1.462903]\n",
      "[D loss: 0.886969] [G loss: 1.468227]\n",
      "[D loss: 0.898120] [G loss: 1.132174]\n",
      "[D loss: 1.101081] [G loss: 1.206476]\n",
      "[D loss: 1.015610] [G loss: 1.441603]\n",
      "[D loss: 0.910988] [G loss: 1.602388]\n",
      "[D loss: 0.869320] [G loss: 1.373285]\n",
      "[D loss: 0.795450] [G loss: 1.495202]\n",
      "[D loss: 0.576538] [G loss: 1.544969]\n",
      "[D loss: 0.738991] [G loss: 1.303977]\n",
      "[D loss: 0.884795] [G loss: 1.498025]\n",
      "[D loss: 0.896921] [G loss: 1.350123]\n",
      "[D loss: 0.867195] [G loss: 1.608701]\n",
      "[D loss: 0.794460] [G loss: 1.505313]\n",
      "[D loss: 1.085859] [G loss: 1.258862]\n",
      "[D loss: 0.737862] [G loss: 1.453994]\n",
      "[D loss: 0.830457] [G loss: 1.395431]\n",
      "[D loss: 0.883148] [G loss: 1.451449]\n",
      "[D loss: 0.933641] [G loss: 1.433185]\n",
      "[D loss: 0.783890] [G loss: 1.343472]\n",
      "[D loss: 0.788766] [G loss: 1.495740]\n",
      "[D loss: 0.876439] [G loss: 1.269668]\n",
      "[D loss: 1.097862] [G loss: 1.206568]\n",
      "[D loss: 0.753188] [G loss: 1.462943]\n",
      "[D loss: 0.639926] [G loss: 1.530292]\n",
      "[D loss: 0.884238] [G loss: 1.323845]\n",
      "[D loss: 0.862924] [G loss: 1.305350]\n",
      "[D loss: 0.864748] [G loss: 1.339346]\n",
      "[D loss: 0.884918] [G loss: 1.316811]\n",
      "[D loss: 1.096773] [G loss: 1.177257]\n",
      "[D loss: 0.838246] [G loss: 1.182445]\n",
      "[D loss: 0.893866] [G loss: 1.280080]\n",
      "[D loss: 0.806065] [G loss: 1.623964]\n",
      "[D loss: 0.817509] [G loss: 1.565526]\n",
      "[D loss: 0.952883] [G loss: 1.349440]\n",
      "[D loss: 0.981312] [G loss: 1.304377]\n",
      "[D loss: 0.836621] [G loss: 1.457277]\n",
      "[D loss: 0.871852] [G loss: 1.547271]\n",
      "[D loss: 0.806738] [G loss: 1.207609]\n",
      "[D loss: 1.016962] [G loss: 1.489067]\n",
      "[D loss: 0.857075] [G loss: 1.348859]\n",
      "[D loss: 0.643138] [G loss: 1.347078]\n",
      "[D loss: 0.881093] [G loss: 1.435135]\n",
      "[D loss: 0.602388] [G loss: 1.544586]\n",
      "[D loss: 0.760391] [G loss: 1.515224]\n",
      "[D loss: 0.885161] [G loss: 1.378354]\n",
      "[D loss: 0.796045] [G loss: 1.389379]\n",
      "[D loss: 0.884767] [G loss: 1.586780]\n",
      "[D loss: 0.831923] [G loss: 1.523288]\n",
      "[D loss: 0.830448] [G loss: 1.273085]\n",
      "[D loss: 0.917980] [G loss: 1.348745]\n",
      "[D loss: 0.746583] [G loss: 1.498174]\n",
      "[D loss: 0.959864] [G loss: 1.410823]\n",
      "[D loss: 0.705857] [G loss: 1.458077]\n",
      "[D loss: 0.863425] [G loss: 1.596418]\n",
      "[D loss: 0.915437] [G loss: 1.450799]\n",
      "[D loss: 0.698630] [G loss: 1.481737]\n",
      "[D loss: 0.987750] [G loss: 1.509676]\n",
      "[D loss: 1.060686] [G loss: 1.415356]\n",
      "[D loss: 0.586924] [G loss: 1.536370]\n",
      "[D loss: 0.944044] [G loss: 1.404123]\n",
      "[D loss: 0.780264] [G loss: 1.456491]\n",
      "[D loss: 1.088964] [G loss: 1.286392]\n",
      "[D loss: 0.853076] [G loss: 1.354739]\n",
      "[D loss: 0.752952] [G loss: 1.425231]\n",
      "[D loss: 0.883460] [G loss: 1.320859]\n",
      "[D loss: 0.977150] [G loss: 1.269871]\n",
      "[D loss: 0.975812] [G loss: 1.398129]\n",
      "[D loss: 0.942991] [G loss: 1.401435]\n",
      "[D loss: 0.928339] [G loss: 1.231767]\n",
      "[D loss: 0.803292] [G loss: 1.465333]\n",
      "[D loss: 0.770580] [G loss: 1.394055]\n",
      "[D loss: 1.103338] [G loss: 1.232567]\n",
      "[D loss: 0.860802] [G loss: 1.413071]\n",
      "[D loss: 0.684632] [G loss: 1.475341]\n",
      "[D loss: 0.735111] [G loss: 1.338384]\n",
      "[D loss: 0.810533] [G loss: 1.081276]\n",
      "[D loss: 0.771209] [G loss: 1.375399]\n",
      "[D loss: 0.695796] [G loss: 1.528976]\n",
      "[D loss: 0.792594] [G loss: 1.579007]\n",
      "[D loss: 0.848086] [G loss: 1.599038]\n",
      "[D loss: 0.830821] [G loss: 1.445460]\n",
      "[D loss: 0.801308] [G loss: 1.349833]\n",
      "[D loss: 0.944405] [G loss: 1.322232]\n",
      "[D loss: 0.764199] [G loss: 1.445217]\n",
      "[D loss: 0.757254] [G loss: 1.443559]\n",
      "[D loss: 0.982355] [G loss: 1.229398]\n",
      "[D loss: 0.865932] [G loss: 1.491488]\n",
      "[D loss: 0.903587] [G loss: 1.327480]\n",
      "[D loss: 0.757281] [G loss: 1.549348]\n",
      "[D loss: 0.832773] [G loss: 1.474763]\n",
      "[D loss: 0.704013] [G loss: 1.432003]\n",
      "[D loss: 1.039982] [G loss: 1.389964]\n",
      "[D loss: 0.896570] [G loss: 1.262213]\n",
      "[D loss: 0.729797] [G loss: 1.278302]\n",
      "[D loss: 0.931488] [G loss: 1.384238]\n",
      "[D loss: 0.893019] [G loss: 1.649504]\n",
      "[D loss: 0.764965] [G loss: 1.377324]\n",
      "[D loss: 0.703442] [G loss: 1.485185]\n",
      "[D loss: 0.870604] [G loss: 1.417374]\n",
      "[D loss: 0.757729] [G loss: 1.375632]\n",
      "[D loss: 0.757108] [G loss: 1.471512]\n",
      "[D loss: 0.988042] [G loss: 1.647553]\n",
      "[D loss: 0.788253] [G loss: 1.390038]\n",
      "[D loss: 0.975084] [G loss: 1.452907]\n",
      "[D loss: 0.698024] [G loss: 1.315161]\n",
      "[D loss: 0.831183] [G loss: 1.382291]\n",
      "[D loss: 0.643790] [G loss: 1.353709]\n",
      "[D loss: 0.974549] [G loss: 1.355902]\n",
      "[D loss: 0.903584] [G loss: 1.290073]\n",
      "[D loss: 0.875035] [G loss: 1.397366]\n",
      "[D loss: 0.809449] [G loss: 1.530326]\n",
      "[D loss: 0.864764] [G loss: 1.336048]\n",
      "[D loss: 0.574052] [G loss: 1.617566]\n",
      "[D loss: 0.930934] [G loss: 1.407838]\n",
      "[D loss: 0.840643] [G loss: 1.352772]\n",
      "[D loss: 0.974182] [G loss: 1.497921]\n",
      "[D loss: 0.716246] [G loss: 1.493741]\n",
      "[D loss: 0.725237] [G loss: 1.656761]\n",
      "[D loss: 1.013125] [G loss: 1.727338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 1.165331] [G loss: 1.367125]\n",
      "[D loss: 0.713746] [G loss: 1.530212]\n",
      "[D loss: 0.894904] [G loss: 1.326028]\n",
      "[D loss: 0.607694] [G loss: 1.500736]\n",
      "[D loss: 0.640592] [G loss: 1.525029]\n",
      "[D loss: 0.881569] [G loss: 1.440896]\n",
      "[D loss: 0.923699] [G loss: 1.450020]\n",
      "[D loss: 0.908440] [G loss: 1.223514]\n",
      "[D loss: 0.917033] [G loss: 1.234362]\n",
      "[D loss: 0.770325] [G loss: 1.670002]\n",
      "[D loss: 0.886578] [G loss: 1.541746]\n",
      "[D loss: 0.706768] [G loss: 1.576535]\n",
      "[D loss: 0.711949] [G loss: 1.739989]\n",
      "[D loss: 0.551697] [G loss: 1.764728]\n",
      "[D loss: 0.913007] [G loss: 1.455306]\n",
      "[D loss: 0.913579] [G loss: 1.536423]\n",
      "[D loss: 0.850240] [G loss: 1.279715]\n",
      "[D loss: 0.811349] [G loss: 1.270642]\n",
      "[D loss: 0.856523] [G loss: 1.300667]\n",
      "[D loss: 0.861696] [G loss: 1.477280]\n",
      "[D loss: 0.922298] [G loss: 1.388881]\n",
      "[D loss: 0.727296] [G loss: 1.356183]\n",
      "[D loss: 0.796541] [G loss: 1.392272]\n",
      "[D loss: 0.755028] [G loss: 1.433966]\n",
      "[D loss: 0.640813] [G loss: 1.473421]\n",
      "[D loss: 0.816527] [G loss: 1.394517]\n",
      "[D loss: 0.852698] [G loss: 1.179253]\n",
      "[D loss: 0.882510] [G loss: 1.530735]\n",
      "[D loss: 0.947092] [G loss: 1.534959]\n",
      "[D loss: 0.659564] [G loss: 1.557129]\n",
      "[D loss: 0.725949] [G loss: 1.471881]\n",
      "[D loss: 0.986077] [G loss: 1.279553]\n",
      "[D loss: 0.754225] [G loss: 1.583181]\n",
      "[D loss: 0.871563] [G loss: 1.530040]\n",
      "[D loss: 0.824883] [G loss: 1.659534]\n",
      "[D loss: 0.660777] [G loss: 1.484711]\n",
      "[D loss: 0.933936] [G loss: 1.549960]\n",
      "[D loss: 0.826125] [G loss: 1.734759]\n",
      "[D loss: 0.766110] [G loss: 1.410772]\n",
      "[D loss: 0.649826] [G loss: 1.525025]\n",
      "[D loss: 0.908007] [G loss: 1.434911]\n",
      "[D loss: 0.845933] [G loss: 1.322049]\n",
      "[D loss: 0.951850] [G loss: 1.602792]\n",
      "[D loss: 0.954617] [G loss: 1.568837]\n",
      "[D loss: 0.836223] [G loss: 1.668329]\n",
      "[D loss: 0.841482] [G loss: 1.587471]\n",
      "[D loss: 0.990478] [G loss: 1.265264]\n",
      "[D loss: 0.902384] [G loss: 1.367394]\n",
      "[D loss: 0.602001] [G loss: 1.389858]\n",
      "[D loss: 0.826541] [G loss: 1.354097]\n",
      "[D loss: 0.788119] [G loss: 1.496201]\n",
      "[D loss: 0.732223] [G loss: 1.675830]\n",
      "[D loss: 0.984623] [G loss: 1.413907]\n",
      "[D loss: 0.891808] [G loss: 1.433573]\n",
      "[D loss: 1.180519] [G loss: 1.311303]\n",
      "[D loss: 0.894686] [G loss: 1.591822]\n",
      "[D loss: 1.087222] [G loss: 1.407548]\n",
      "[D loss: 0.819010] [G loss: 1.370292]\n",
      "[D loss: 0.751382] [G loss: 1.407064]\n",
      "[D loss: 0.911787] [G loss: 1.436773]\n",
      "[D loss: 0.705212] [G loss: 1.371663]\n",
      "[D loss: 0.863652] [G loss: 1.473935]\n",
      "[D loss: 0.916896] [G loss: 1.399928]\n",
      "[D loss: 0.731763] [G loss: 1.425997]\n",
      "[D loss: 0.789860] [G loss: 1.522803]\n",
      "[D loss: 0.630656] [G loss: 1.448544]\n",
      "[D loss: 0.849963] [G loss: 1.803616]\n",
      "[D loss: 1.261519] [G loss: 1.678059]\n",
      "[D loss: 0.783359] [G loss: 1.558873]\n",
      "[D loss: 0.756112] [G loss: 1.687284]\n",
      "[D loss: 0.763005] [G loss: 1.562054]\n",
      "[D loss: 0.939424] [G loss: 1.341500]\n",
      "[D loss: 0.701223] [G loss: 1.345853]\n",
      "[D loss: 0.826676] [G loss: 1.393697]\n",
      "[D loss: 0.872437] [G loss: 1.655353]\n",
      "[D loss: 0.827653] [G loss: 1.291299]\n",
      "[D loss: 0.947658] [G loss: 1.134148]\n",
      "[D loss: 0.953407] [G loss: 1.374766]\n",
      "[D loss: 0.741525] [G loss: 1.405142]\n",
      "[D loss: 0.781374] [G loss: 1.594307]\n",
      "[D loss: 0.579936] [G loss: 1.753822]\n",
      "[D loss: 0.832252] [G loss: 1.374444]\n",
      "[D loss: 0.989138] [G loss: 1.286761]\n",
      "[D loss: 1.004438] [G loss: 1.609727]\n",
      "[D loss: 1.102426] [G loss: 1.431658]\n",
      "[D loss: 0.766001] [G loss: 1.283153]\n",
      "[D loss: 0.758266] [G loss: 1.353316]\n",
      "[D loss: 0.782991] [G loss: 1.396672]\n",
      "[D loss: 0.773040] [G loss: 1.564918]\n",
      "[D loss: 0.855321] [G loss: 1.460194]\n",
      "[D loss: 0.870938] [G loss: 1.392507]\n",
      "[D loss: 0.971616] [G loss: 1.268101]\n",
      "[D loss: 0.946568] [G loss: 1.231014]\n",
      "[D loss: 0.785429] [G loss: 1.128731]\n",
      "[D loss: 0.727068] [G loss: 1.448974]\n",
      "[D loss: 0.963916] [G loss: 1.507333]\n",
      "[D loss: 0.905834] [G loss: 1.430370]\n",
      "[D loss: 0.749457] [G loss: 1.476674]\n",
      "[D loss: 1.075171] [G loss: 1.535212]\n",
      "[D loss: 0.711091] [G loss: 1.412441]\n",
      "[D loss: 0.984472] [G loss: 1.029807]\n",
      "[D loss: 0.695147] [G loss: 1.249381]\n",
      "[D loss: 0.755922] [G loss: 1.563551]\n",
      "[D loss: 0.689891] [G loss: 1.378554]\n",
      "[D loss: 0.759567] [G loss: 1.378691]\n",
      "[D loss: 0.831358] [G loss: 1.648205]\n",
      "[D loss: 0.905238] [G loss: 1.482587]\n",
      "[D loss: 0.707130] [G loss: 1.601871]\n",
      "[D loss: 0.877220] [G loss: 1.566622]\n",
      "[D loss: 0.916403] [G loss: 1.346606]\n",
      "[D loss: 0.753792] [G loss: 1.639868]\n",
      "[D loss: 0.783855] [G loss: 1.158272]\n",
      "[D loss: 0.789464] [G loss: 1.406337]\n",
      "[D loss: 0.691598] [G loss: 1.367085]\n",
      "[D loss: 0.990554] [G loss: 1.403206]\n",
      "[D loss: 0.931264] [G loss: 1.364745]\n",
      "[D loss: 0.914166] [G loss: 1.262448]\n",
      "[D loss: 0.933158] [G loss: 1.635055]\n",
      "[D loss: 0.713448] [G loss: 1.702446]\n",
      "[D loss: 1.015358] [G loss: 1.480824]\n",
      "[D loss: 0.896528] [G loss: 1.385674]\n",
      "[D loss: 0.820736] [G loss: 1.448472]\n",
      "[D loss: 0.867984] [G loss: 1.471457]\n",
      "[D loss: 0.681603] [G loss: 1.680476]\n",
      "[D loss: 0.646581] [G loss: 1.492864]\n",
      "[D loss: 0.695037] [G loss: 1.580218]\n",
      "[D loss: 0.765470] [G loss: 1.554924]\n",
      "[D loss: 0.709908] [G loss: 1.356960]\n",
      "[D loss: 0.744045] [G loss: 1.546787]\n",
      "[D loss: 0.729452] [G loss: 1.638648]\n",
      "[D loss: 0.932252] [G loss: 1.629262]\n",
      "[D loss: 0.872230] [G loss: 1.540449]\n",
      "[D loss: 0.708726] [G loss: 1.542672]\n",
      "[D loss: 0.722726] [G loss: 1.679213]\n",
      "[D loss: 0.912018] [G loss: 1.319911]\n",
      "[D loss: 0.782950] [G loss: 1.457649]\n",
      "[D loss: 0.851336] [G loss: 1.607995]\n",
      "[D loss: 0.862475] [G loss: 1.635486]\n",
      "[D loss: 0.769418] [G loss: 1.799449]\n",
      "[D loss: 0.963187] [G loss: 1.747826]\n",
      "[D loss: 0.985250] [G loss: 1.280455]\n",
      "[D loss: 0.793350] [G loss: 1.554688]\n",
      "[D loss: 1.039324] [G loss: 1.366693]\n",
      "[D loss: 1.206347] [G loss: 1.254960]\n",
      "[D loss: 0.988354] [G loss: 1.405952]\n",
      "[D loss: 0.799625] [G loss: 1.354507]\n",
      "[D loss: 0.736768] [G loss: 1.387023]\n",
      "[D loss: 0.909829] [G loss: 1.370484]\n",
      "[D loss: 0.705467] [G loss: 1.397195]\n",
      "[D loss: 1.011812] [G loss: 1.355495]\n",
      "[D loss: 0.699137] [G loss: 1.548915]\n",
      "[D loss: 0.982423] [G loss: 1.608501]\n",
      "[D loss: 1.012732] [G loss: 1.484936]\n",
      "[D loss: 0.898068] [G loss: 1.324404]\n",
      "[D loss: 0.865946] [G loss: 1.257121]\n",
      "[D loss: 0.762851] [G loss: 1.340473]\n",
      "[D loss: 0.744242] [G loss: 1.467933]\n",
      "[D loss: 0.745100] [G loss: 1.617938]\n",
      "[D loss: 0.805369] [G loss: 1.331684]\n",
      "[D loss: 0.876920] [G loss: 1.324304]\n",
      "[D loss: 0.946347] [G loss: 1.437097]\n",
      "[D loss: 0.948863] [G loss: 1.396056]\n",
      "[D loss: 0.884757] [G loss: 1.597292]\n",
      "[D loss: 0.902476] [G loss: 1.500940]\n",
      "[D loss: 1.054868] [G loss: 1.376054]\n",
      "[D loss: 0.855685] [G loss: 1.498088]\n",
      "[D loss: 0.883138] [G loss: 1.413549]\n",
      "[D loss: 1.036578] [G loss: 1.158307]\n",
      "[D loss: 0.773882] [G loss: 1.377428]\n",
      "[D loss: 0.933619] [G loss: 1.352119]\n",
      "[D loss: 0.735847] [G loss: 1.517911]\n",
      "[D loss: 0.753247] [G loss: 1.459677]\n",
      "[D loss: 1.074448] [G loss: 1.166285]\n",
      "[D loss: 0.576174] [G loss: 1.422957]\n",
      "[D loss: 1.175978] [G loss: 1.386189]\n",
      "[D loss: 1.000306] [G loss: 1.201368]\n",
      "[D loss: 0.614086] [G loss: 1.234120]\n",
      "[D loss: 0.807789] [G loss: 1.376750]\n",
      "[D loss: 1.017576] [G loss: 1.414507]\n",
      "[D loss: 0.766313] [G loss: 1.534541]\n",
      "[D loss: 0.840168] [G loss: 1.340830]\n",
      "[D loss: 0.969678] [G loss: 1.327936]\n",
      "[D loss: 1.008814] [G loss: 1.319911]\n",
      "[D loss: 0.868889] [G loss: 1.248421]\n",
      "[D loss: 1.419324] [G loss: 0.953923]\n",
      "[D loss: 1.016241] [G loss: 1.358396]\n",
      "[D loss: 0.769321] [G loss: 1.463509]\n",
      "[D loss: 0.943516] [G loss: 1.368161]\n",
      "[D loss: 0.693378] [G loss: 1.364044]\n",
      "[D loss: 0.737661] [G loss: 1.374591]\n",
      "[D loss: 0.927371] [G loss: 1.295867]\n",
      "[D loss: 1.023546] [G loss: 1.292351]\n",
      "[D loss: 0.929304] [G loss: 1.464442]\n",
      "[D loss: 0.924848] [G loss: 1.476709]\n",
      "[D loss: 0.832658] [G loss: 1.465825]\n",
      "[D loss: 0.806392] [G loss: 1.318810]\n",
      "[D loss: 0.937055] [G loss: 1.361636]\n",
      "[D loss: 0.800519] [G loss: 1.217258]\n",
      "[D loss: 0.824554] [G loss: 1.544943]\n",
      "[D loss: 0.767229] [G loss: 1.511225]\n",
      "[D loss: 0.755434] [G loss: 1.500195]\n",
      "[D loss: 0.975188] [G loss: 1.348469]\n",
      "[D loss: 0.891184] [G loss: 1.416216]\n",
      "[D loss: 1.037409] [G loss: 1.291128]\n",
      "[D loss: 0.934328] [G loss: 1.346018]\n",
      "[D loss: 1.031868] [G loss: 1.460037]\n",
      "[D loss: 0.900760] [G loss: 1.424471]\n",
      "[D loss: 0.775391] [G loss: 1.548987]\n",
      "[D loss: 0.765877] [G loss: 1.390987]\n",
      "[D loss: 0.756552] [G loss: 1.405014]\n",
      "[D loss: 0.894892] [G loss: 1.311568]\n",
      "[D loss: 0.944020] [G loss: 1.208620]\n",
      "[D loss: 0.875746] [G loss: 1.315078]\n",
      "[D loss: 0.800337] [G loss: 1.456661]\n",
      "[D loss: 1.114322] [G loss: 1.259550]\n",
      "[D loss: 0.851947] [G loss: 1.530257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.801943] [G loss: 1.593075]\n",
      "[D loss: 0.639010] [G loss: 1.443768]\n",
      "[D loss: 0.941617] [G loss: 1.484012]\n",
      "[D loss: 0.813085] [G loss: 1.519987]\n",
      "[D loss: 0.850423] [G loss: 1.422583]\n",
      "[D loss: 0.688198] [G loss: 1.415640]\n",
      "[D loss: 0.975760] [G loss: 1.376303]\n",
      "[D loss: 0.890975] [G loss: 1.238915]\n",
      "[D loss: 0.973084] [G loss: 1.469077]\n",
      "[D loss: 0.909657] [G loss: 1.104594]\n",
      "[D loss: 0.774576] [G loss: 1.353360]\n",
      "[D loss: 0.904134] [G loss: 1.564365]\n",
      "[D loss: 0.852134] [G loss: 1.590816]\n",
      "[D loss: 1.063463] [G loss: 1.329198]\n",
      "[D loss: 0.918277] [G loss: 1.141834]\n",
      "[D loss: 0.810789] [G loss: 1.635791]\n",
      "[D loss: 0.773522] [G loss: 1.544364]\n",
      "[D loss: 0.688753] [G loss: 1.387860]\n",
      "[D loss: 0.898720] [G loss: 1.397720]\n",
      "[D loss: 0.903990] [G loss: 1.519576]\n",
      "[D loss: 0.947457] [G loss: 1.394181]\n",
      "[D loss: 0.778802] [G loss: 1.217411]\n",
      "[D loss: 0.913290] [G loss: 1.446895]\n",
      "[D loss: 0.884964] [G loss: 1.525242]\n",
      "[D loss: 0.763370] [G loss: 1.503949]\n",
      "[D loss: 0.810962] [G loss: 1.351770]\n",
      "[D loss: 0.725832] [G loss: 1.369394]\n",
      "[D loss: 0.830820] [G loss: 1.684758]\n",
      "[D loss: 0.770916] [G loss: 1.468145]\n",
      "[D loss: 0.672554] [G loss: 1.609647]\n",
      "[D loss: 1.206261] [G loss: 1.309663]\n",
      "[D loss: 0.933540] [G loss: 1.581432]\n",
      "[D loss: 0.644590] [G loss: 1.433323]\n",
      "[D loss: 0.786631] [G loss: 1.491661]\n",
      "[D loss: 0.810650] [G loss: 1.313335]\n",
      "[D loss: 0.806145] [G loss: 1.415427]\n",
      "[D loss: 0.979238] [G loss: 1.362221]\n",
      "[D loss: 0.768425] [G loss: 1.235527]\n",
      "[D loss: 0.788156] [G loss: 1.459155]\n",
      "[D loss: 0.660369] [G loss: 1.623159]\n",
      "[D loss: 0.996203] [G loss: 1.487173]\n",
      "[D loss: 0.739758] [G loss: 1.610343]\n",
      "[D loss: 0.452531] [G loss: 1.539009]\n",
      "[D loss: 0.777028] [G loss: 1.523092]\n",
      "[D loss: 0.859071] [G loss: 1.495444]\n",
      "[D loss: 0.901698] [G loss: 1.521226]\n",
      "[D loss: 0.794016] [G loss: 1.620961]\n",
      "[D loss: 1.003790] [G loss: 1.393216]\n",
      "[D loss: 0.885856] [G loss: 1.492444]\n",
      "[D loss: 0.752823] [G loss: 1.268945]\n",
      "[D loss: 0.866828] [G loss: 1.470145]\n",
      "[D loss: 0.805241] [G loss: 1.447945]\n",
      "[D loss: 1.310055] [G loss: 1.365361]\n",
      "[D loss: 0.809474] [G loss: 1.497603]\n",
      "[D loss: 0.721827] [G loss: 1.654148]\n",
      "[D loss: 0.924950] [G loss: 1.604841]\n",
      "[D loss: 0.891720] [G loss: 1.384079]\n",
      "[D loss: 1.072492] [G loss: 1.305160]\n",
      "[D loss: 0.778487] [G loss: 1.327101]\n",
      "[D loss: 0.519364] [G loss: 1.475393]\n",
      "[D loss: 0.625279] [G loss: 1.397496]\n",
      "[D loss: 0.864619] [G loss: 1.521359]\n",
      "[D loss: 0.899056] [G loss: 1.506111]\n",
      "[D loss: 0.826458] [G loss: 1.533380]\n",
      "[D loss: 0.639189] [G loss: 1.451477]\n",
      "[D loss: 0.704499] [G loss: 1.468201]\n",
      "[D loss: 0.916675] [G loss: 1.563030]\n",
      "[D loss: 0.619769] [G loss: 1.608224]\n",
      "[D loss: 1.035190] [G loss: 1.716465]\n",
      "[D loss: 0.865444] [G loss: 1.570800]\n",
      "[D loss: 1.059215] [G loss: 1.453604]\n",
      "[D loss: 0.831933] [G loss: 1.517823]\n",
      "[D loss: 1.114879] [G loss: 1.246936]\n",
      "[D loss: 0.693197] [G loss: 1.514142]\n",
      "[D loss: 0.719806] [G loss: 1.636223]\n",
      "[D loss: 0.631139] [G loss: 1.704841]\n",
      "[D loss: 0.788769] [G loss: 1.531042]\n",
      "[D loss: 0.780586] [G loss: 1.473446]\n",
      "[D loss: 0.905372] [G loss: 1.495639]\n",
      "[D loss: 1.039110] [G loss: 1.173265]\n",
      "[D loss: 0.829816] [G loss: 1.326746]\n",
      "[D loss: 0.785628] [G loss: 1.455918]\n",
      "[D loss: 1.024215] [G loss: 1.285195]\n",
      "[D loss: 1.012342] [G loss: 1.317853]\n",
      "[D loss: 0.818998] [G loss: 1.452390]\n",
      "[D loss: 0.920679] [G loss: 1.564005]\n",
      "[D loss: 0.847838] [G loss: 1.366825]\n",
      "[D loss: 0.754029] [G loss: 1.383134]\n",
      "[D loss: 0.655617] [G loss: 1.415934]\n",
      "[D loss: 0.939107] [G loss: 1.286429]\n",
      "[D loss: 0.940872] [G loss: 1.450395]\n",
      "[D loss: 0.707668] [G loss: 1.399673]\n",
      "[D loss: 0.727448] [G loss: 1.564531]\n",
      "[D loss: 1.001760] [G loss: 1.255674]\n",
      "[D loss: 1.038038] [G loss: 1.281578]\n",
      "[D loss: 0.799243] [G loss: 1.508425]\n",
      "[D loss: 0.909965] [G loss: 1.469836]\n",
      "[D loss: 0.865154] [G loss: 1.419307]\n",
      "[D loss: 1.136304] [G loss: 1.573933]\n",
      "[D loss: 0.725645] [G loss: 1.566953]\n",
      "[D loss: 0.691780] [G loss: 1.365086]\n",
      "[D loss: 0.985594] [G loss: 1.204375]\n",
      "[D loss: 0.811740] [G loss: 1.444145]\n",
      "[D loss: 0.789733] [G loss: 1.248218]\n",
      "[D loss: 0.923709] [G loss: 1.379337]\n",
      "[D loss: 1.006030] [G loss: 1.131947]\n",
      "[D loss: 0.796163] [G loss: 1.420697]\n",
      "[D loss: 0.718435] [G loss: 1.412805]\n",
      "[D loss: 1.244772] [G loss: 1.348665]\n",
      "[D loss: 1.016600] [G loss: 1.540259]\n",
      "[D loss: 0.757060] [G loss: 1.531854]\n",
      "[D loss: 0.949874] [G loss: 1.441980]\n",
      "[D loss: 0.779849] [G loss: 1.330427]\n",
      "[D loss: 0.843339] [G loss: 1.293287]\n",
      "[D loss: 0.838800] [G loss: 1.305578]\n",
      "[D loss: 0.770765] [G loss: 1.532588]\n",
      "[D loss: 0.990470] [G loss: 1.385906]\n",
      "[D loss: 0.967613] [G loss: 1.304395]\n",
      "[D loss: 0.853564] [G loss: 1.313920]\n",
      "[D loss: 0.873293] [G loss: 1.307269]\n",
      "[D loss: 0.949350] [G loss: 1.346456]\n",
      "[D loss: 0.927090] [G loss: 1.376507]\n",
      "[D loss: 0.974510] [G loss: 1.266381]\n",
      "[D loss: 0.844107] [G loss: 1.379146]\n",
      "[D loss: 0.858550] [G loss: 1.398357]\n",
      "[D loss: 1.038003] [G loss: 1.249416]\n",
      "[D loss: 0.871738] [G loss: 1.326426]\n",
      "[D loss: 0.913623] [G loss: 1.445711]\n",
      "[D loss: 0.779005] [G loss: 1.163632]\n",
      "[D loss: 0.709658] [G loss: 1.369600]\n",
      "[D loss: 1.037912] [G loss: 1.515873]\n",
      "[D loss: 0.846634] [G loss: 1.214982]\n",
      "[D loss: 0.874432] [G loss: 1.135499]\n",
      "[D loss: 0.858906] [G loss: 1.254283]\n",
      "[D loss: 0.557013] [G loss: 1.520255]\n",
      "[D loss: 0.802484] [G loss: 1.552485]\n",
      "[D loss: 0.575076] [G loss: 1.467519]\n",
      "[D loss: 0.920807] [G loss: 1.170723]\n",
      "[D loss: 0.729633] [G loss: 1.410541]\n",
      "[D loss: 0.828973] [G loss: 1.529112]\n",
      "[D loss: 0.749952] [G loss: 1.456972]\n",
      "[D loss: 0.714277] [G loss: 1.475975]\n",
      "[D loss: 0.619534] [G loss: 1.510893]\n",
      "[D loss: 0.759813] [G loss: 1.521855]\n",
      "[D loss: 0.734730] [G loss: 1.369587]\n",
      "[D loss: 0.788000] [G loss: 1.393547]\n",
      "[D loss: 0.839270] [G loss: 1.487636]\n",
      "[D loss: 0.732563] [G loss: 1.724511]\n",
      "[D loss: 0.579494] [G loss: 1.615450]\n",
      "[D loss: 0.773316] [G loss: 1.554132]\n",
      "[D loss: 0.882058] [G loss: 1.396285]\n",
      "[D loss: 0.902989] [G loss: 1.313315]\n",
      "[D loss: 0.854811] [G loss: 1.473779]\n",
      "[D loss: 0.784203] [G loss: 1.522464]\n",
      "[D loss: 0.984464] [G loss: 1.514345]\n",
      "[D loss: 0.865659] [G loss: 1.239256]\n",
      "[D loss: 0.808752] [G loss: 1.407604]\n",
      "[D loss: 0.675947] [G loss: 1.536863]\n",
      "[D loss: 1.041394] [G loss: 1.242043]\n",
      "[D loss: 0.860889] [G loss: 1.553701]\n",
      "[D loss: 0.830716] [G loss: 1.589262]\n",
      "[D loss: 0.732624] [G loss: 1.503518]\n",
      "[D loss: 0.706580] [G loss: 1.459272]\n",
      "[D loss: 0.811708] [G loss: 1.285036]\n",
      "[D loss: 0.802169] [G loss: 1.469402]\n",
      "[D loss: 1.016443] [G loss: 1.426302]\n",
      "[D loss: 0.762256] [G loss: 1.655899]\n",
      "[D loss: 0.877741] [G loss: 1.387214]\n",
      "[D loss: 0.979727] [G loss: 1.352786]\n",
      "[D loss: 0.828175] [G loss: 1.270924]\n",
      "[D loss: 0.900652] [G loss: 1.442412]\n",
      "[D loss: 1.008953] [G loss: 1.455238]\n",
      "[D loss: 0.760556] [G loss: 1.432171]\n",
      "[D loss: 0.713457] [G loss: 1.398476]\n",
      "[D loss: 0.833073] [G loss: 1.308100]\n",
      "[D loss: 1.020209] [G loss: 1.589086]\n",
      "[D loss: 0.707685] [G loss: 1.364680]\n",
      "[D loss: 0.951439] [G loss: 1.477177]\n",
      "[D loss: 0.680895] [G loss: 1.676286]\n",
      "[D loss: 0.758759] [G loss: 1.517584]\n",
      "[D loss: 0.905160] [G loss: 1.582597]\n",
      "[D loss: 0.917349] [G loss: 1.617942]\n",
      "[D loss: 0.959555] [G loss: 1.572423]\n",
      "[D loss: 0.995648] [G loss: 1.277613]\n",
      "[D loss: 0.783791] [G loss: 1.622075]\n",
      "[D loss: 0.914376] [G loss: 1.335000]\n",
      "[D loss: 0.724478] [G loss: 1.574893]\n",
      "[D loss: 0.727751] [G loss: 1.831970]\n",
      "[D loss: 0.756835] [G loss: 1.563460]\n",
      "[D loss: 0.817021] [G loss: 1.572514]\n",
      "[D loss: 0.822908] [G loss: 1.389337]\n",
      "[D loss: 0.971572] [G loss: 1.432535]\n",
      "[D loss: 0.601251] [G loss: 1.474442]\n",
      "[D loss: 0.804396] [G loss: 1.268035]\n",
      "[D loss: 0.979024] [G loss: 1.534141]\n",
      "[D loss: 0.834898] [G loss: 1.561801]\n",
      "[D loss: 1.006712] [G loss: 1.486478]\n",
      "[D loss: 0.763278] [G loss: 1.579814]\n",
      "[D loss: 0.748999] [G loss: 1.290077]\n",
      "[D loss: 0.905255] [G loss: 1.639344]\n",
      "[D loss: 0.982237] [G loss: 1.293907]\n",
      "[D loss: 0.906069] [G loss: 1.623478]\n",
      "[D loss: 0.781108] [G loss: 1.498250]\n",
      "[D loss: 0.921611] [G loss: 1.425817]\n",
      "[D loss: 0.719452] [G loss: 1.524228]\n",
      "[D loss: 0.986838] [G loss: 1.463642]\n",
      "[D loss: 0.986719] [G loss: 1.506782]\n",
      "[D loss: 0.798801] [G loss: 1.556826]\n",
      "[D loss: 0.921185] [G loss: 1.297452]\n",
      "[D loss: 0.851313] [G loss: 1.549870]\n",
      "[D loss: 0.602527] [G loss: 1.698909]\n",
      "[D loss: 0.774450] [G loss: 1.455534]\n",
      "[D loss: 0.929144] [G loss: 1.403991]\n",
      "[D loss: 1.030222] [G loss: 1.298844]\n",
      "[D loss: 1.004378] [G loss: 1.367842]\n",
      "[D loss: 0.805438] [G loss: 1.320042]\n",
      "[D loss: 0.972898] [G loss: 1.462265]\n",
      "[D loss: 0.759159] [G loss: 1.477502]\n",
      "[D loss: 0.747225] [G loss: 1.384953]\n",
      "[D loss: 0.865408] [G loss: 1.505879]\n",
      "[D loss: 0.823591] [G loss: 1.448483]\n",
      "[D loss: 0.633620] [G loss: 1.552209]\n",
      "[D loss: 0.926551] [G loss: 1.308383]\n",
      "[D loss: 0.952747] [G loss: 1.289018]\n",
      "[D loss: 0.926937] [G loss: 1.523268]\n",
      "epoch:8, g_loss:2741.880615234375,d_loss:1564.7579345703125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 1.045100] [G loss: 1.529510]\n",
      "[D loss: 0.713159] [G loss: 1.419028]\n",
      "[D loss: 0.890608] [G loss: 1.538750]\n",
      "[D loss: 0.802396] [G loss: 1.712804]\n",
      "[D loss: 0.882373] [G loss: 1.442200]\n",
      "[D loss: 1.069221] [G loss: 1.292523]\n",
      "[D loss: 0.844791] [G loss: 1.366894]\n",
      "[D loss: 0.784950] [G loss: 1.325084]\n",
      "[D loss: 0.853491] [G loss: 1.458092]\n",
      "[D loss: 0.884903] [G loss: 1.402716]\n",
      "[D loss: 0.713867] [G loss: 1.466942]\n",
      "[D loss: 0.685882] [G loss: 1.340291]\n",
      "[D loss: 0.874753] [G loss: 1.509253]\n",
      "[D loss: 0.815820] [G loss: 1.525617]\n",
      "[D loss: 1.025879] [G loss: 1.360115]\n",
      "[D loss: 0.713327] [G loss: 1.489306]\n",
      "[D loss: 1.123198] [G loss: 1.557496]\n",
      "[D loss: 1.018211] [G loss: 1.236166]\n",
      "[D loss: 0.898759] [G loss: 1.273272]\n",
      "[D loss: 0.591366] [G loss: 1.254088]\n",
      "[D loss: 0.714328] [G loss: 1.237531]\n",
      "[D loss: 0.697372] [G loss: 1.567492]\n",
      "[D loss: 0.838122] [G loss: 1.503424]\n",
      "[D loss: 0.651618] [G loss: 1.206789]\n",
      "[D loss: 0.776768] [G loss: 1.613623]\n",
      "[D loss: 0.614032] [G loss: 1.537224]\n",
      "[D loss: 0.759871] [G loss: 1.721928]\n",
      "[D loss: 0.943080] [G loss: 1.293271]\n",
      "[D loss: 0.973804] [G loss: 1.499021]\n",
      "[D loss: 0.756518] [G loss: 1.472538]\n",
      "[D loss: 0.884624] [G loss: 1.364816]\n",
      "[D loss: 0.923459] [G loss: 1.634819]\n",
      "[D loss: 0.790581] [G loss: 1.637631]\n",
      "[D loss: 0.944888] [G loss: 1.315731]\n",
      "[D loss: 0.828600] [G loss: 1.494776]\n",
      "[D loss: 0.910829] [G loss: 1.464870]\n",
      "[D loss: 1.112941] [G loss: 1.197500]\n",
      "[D loss: 0.880860] [G loss: 1.444073]\n",
      "[D loss: 0.642538] [G loss: 1.684585]\n",
      "[D loss: 0.960775] [G loss: 1.398019]\n",
      "[D loss: 0.877511] [G loss: 1.427048]\n",
      "[D loss: 0.664037] [G loss: 1.527020]\n",
      "[D loss: 0.757364] [G loss: 1.577931]\n",
      "[D loss: 1.087237] [G loss: 1.377033]\n",
      "[D loss: 0.958001] [G loss: 1.469630]\n",
      "[D loss: 0.844648] [G loss: 1.334027]\n",
      "[D loss: 0.675337] [G loss: 1.515149]\n",
      "[D loss: 0.946983] [G loss: 1.394023]\n",
      "[D loss: 0.867203] [G loss: 1.396396]\n",
      "[D loss: 0.741219] [G loss: 1.342251]\n",
      "[D loss: 0.668702] [G loss: 1.377158]\n",
      "[D loss: 0.836303] [G loss: 1.544178]\n",
      "[D loss: 0.928122] [G loss: 1.471256]\n",
      "[D loss: 0.736349] [G loss: 1.301347]\n",
      "[D loss: 0.650594] [G loss: 1.475361]\n",
      "[D loss: 0.789607] [G loss: 1.510410]\n",
      "[D loss: 0.803867] [G loss: 1.307434]\n",
      "[D loss: 0.807791] [G loss: 1.648751]\n",
      "[D loss: 0.735003] [G loss: 1.498427]\n",
      "[D loss: 0.632296] [G loss: 1.596877]\n",
      "[D loss: 0.873733] [G loss: 1.671269]\n",
      "[D loss: 0.687484] [G loss: 1.582011]\n",
      "[D loss: 0.663365] [G loss: 1.578992]\n",
      "[D loss: 0.712714] [G loss: 1.741788]\n",
      "[D loss: 0.951545] [G loss: 1.426258]\n",
      "[D loss: 0.844626] [G loss: 1.379696]\n",
      "[D loss: 0.944921] [G loss: 1.306189]\n",
      "[D loss: 0.597840] [G loss: 1.445338]\n",
      "[D loss: 0.836099] [G loss: 1.453419]\n",
      "[D loss: 0.869060] [G loss: 1.591136]\n",
      "[D loss: 0.611678] [G loss: 1.646208]\n",
      "[D loss: 1.130888] [G loss: 1.726073]\n",
      "[D loss: 0.848929] [G loss: 1.362297]\n",
      "[D loss: 1.245477] [G loss: 1.242358]\n",
      "[D loss: 0.907993] [G loss: 1.231127]\n",
      "[D loss: 0.918928] [G loss: 1.437531]\n",
      "[D loss: 0.601868] [G loss: 1.583710]\n",
      "[D loss: 1.135708] [G loss: 1.483210]\n",
      "[D loss: 0.887458] [G loss: 1.668228]\n",
      "[D loss: 0.922685] [G loss: 1.372226]\n",
      "[D loss: 1.087792] [G loss: 1.243661]\n",
      "[D loss: 0.927323] [G loss: 1.304034]\n",
      "[D loss: 0.692844] [G loss: 1.280462]\n",
      "[D loss: 0.892979] [G loss: 1.240226]\n",
      "[D loss: 0.676645] [G loss: 1.458706]\n",
      "[D loss: 0.831883] [G loss: 1.509098]\n",
      "[D loss: 0.960755] [G loss: 1.463623]\n",
      "[D loss: 0.692644] [G loss: 1.668341]\n",
      "[D loss: 0.756141] [G loss: 1.382978]\n",
      "[D loss: 0.752446] [G loss: 1.519766]\n",
      "[D loss: 0.981855] [G loss: 1.322837]\n",
      "[D loss: 0.947017] [G loss: 1.303978]\n",
      "[D loss: 0.847751] [G loss: 1.323261]\n",
      "[D loss: 0.803321] [G loss: 1.445497]\n",
      "[D loss: 0.685782] [G loss: 1.270375]\n",
      "[D loss: 0.871037] [G loss: 1.487448]\n",
      "[D loss: 0.830387] [G loss: 1.244788]\n",
      "[D loss: 0.764111] [G loss: 1.531626]\n",
      "[D loss: 0.896075] [G loss: 1.484356]\n",
      "[D loss: 0.615227] [G loss: 1.421484]\n",
      "[D loss: 0.744151] [G loss: 1.431438]\n",
      "[D loss: 0.648952] [G loss: 1.586375]\n",
      "[D loss: 0.707522] [G loss: 1.588518]\n",
      "[D loss: 1.033455] [G loss: 1.535012]\n",
      "[D loss: 0.788431] [G loss: 1.432281]\n",
      "[D loss: 0.743638] [G loss: 1.610019]\n",
      "[D loss: 0.626130] [G loss: 1.576518]\n",
      "[D loss: 1.014588] [G loss: 1.370510]\n",
      "[D loss: 0.790788] [G loss: 1.468060]\n",
      "[D loss: 1.047547] [G loss: 1.300578]\n",
      "[D loss: 0.773487] [G loss: 1.389851]\n",
      "[D loss: 0.747887] [G loss: 1.415240]\n",
      "[D loss: 0.729789] [G loss: 1.665156]\n",
      "[D loss: 0.735011] [G loss: 1.397863]\n",
      "[D loss: 0.699516] [G loss: 1.372745]\n",
      "[D loss: 1.023044] [G loss: 1.362367]\n",
      "[D loss: 0.822895] [G loss: 1.362202]\n",
      "[D loss: 0.821367] [G loss: 1.380787]\n",
      "[D loss: 0.885771] [G loss: 1.356311]\n",
      "[D loss: 0.912814] [G loss: 1.404532]\n",
      "[D loss: 0.878528] [G loss: 1.446294]\n",
      "[D loss: 0.583879] [G loss: 1.480301]\n",
      "[D loss: 0.607403] [G loss: 1.571925]\n",
      "[D loss: 0.800900] [G loss: 1.588448]\n",
      "[D loss: 0.745969] [G loss: 1.558367]\n",
      "[D loss: 0.776029] [G loss: 1.388000]\n",
      "[D loss: 0.642892] [G loss: 1.430759]\n",
      "[D loss: 0.859645] [G loss: 1.647684]\n",
      "[D loss: 0.727018] [G loss: 1.424163]\n",
      "[D loss: 0.961626] [G loss: 1.390750]\n",
      "[D loss: 0.800164] [G loss: 1.497965]\n",
      "[D loss: 0.773945] [G loss: 1.466283]\n",
      "[D loss: 1.077884] [G loss: 1.305950]\n",
      "[D loss: 0.880555] [G loss: 1.363981]\n",
      "[D loss: 0.906170] [G loss: 1.518997]\n",
      "[D loss: 0.961774] [G loss: 1.502447]\n",
      "[D loss: 1.000255] [G loss: 1.341817]\n",
      "[D loss: 0.582948] [G loss: 1.382637]\n",
      "[D loss: 0.706698] [G loss: 1.554665]\n",
      "[D loss: 0.987723] [G loss: 1.460492]\n",
      "[D loss: 0.804745] [G loss: 1.318947]\n",
      "[D loss: 0.766923] [G loss: 1.427592]\n",
      "[D loss: 0.818167] [G loss: 1.489387]\n",
      "[D loss: 1.132067] [G loss: 1.441384]\n",
      "[D loss: 0.777843] [G loss: 1.579386]\n",
      "[D loss: 0.735781] [G loss: 1.662143]\n",
      "[D loss: 0.826198] [G loss: 1.431741]\n",
      "[D loss: 0.744397] [G loss: 1.398869]\n",
      "[D loss: 0.869373] [G loss: 1.384083]\n",
      "[D loss: 0.891540] [G loss: 1.303920]\n",
      "[D loss: 0.653073] [G loss: 1.720270]\n",
      "[D loss: 1.003213] [G loss: 1.432739]\n",
      "[D loss: 0.827373] [G loss: 1.714861]\n",
      "[D loss: 0.743192] [G loss: 1.371178]\n",
      "[D loss: 1.115695] [G loss: 1.365720]\n",
      "[D loss: 0.943531] [G loss: 1.474146]\n",
      "[D loss: 0.947327] [G loss: 1.423357]\n",
      "[D loss: 0.532097] [G loss: 1.597067]\n",
      "[D loss: 0.820855] [G loss: 1.274185]\n",
      "[D loss: 0.718997] [G loss: 1.390310]\n",
      "[D loss: 0.845194] [G loss: 1.447609]\n",
      "[D loss: 0.957051] [G loss: 1.168426]\n",
      "[D loss: 0.994461] [G loss: 1.563784]\n",
      "[D loss: 0.778849] [G loss: 1.474877]\n",
      "[D loss: 0.907862] [G loss: 1.394696]\n",
      "[D loss: 0.944020] [G loss: 1.405837]\n",
      "[D loss: 0.758293] [G loss: 1.779915]\n",
      "[D loss: 0.709686] [G loss: 1.532351]\n",
      "[D loss: 0.856561] [G loss: 1.449588]\n",
      "[D loss: 0.842355] [G loss: 1.322723]\n",
      "[D loss: 0.835194] [G loss: 1.572918]\n",
      "[D loss: 0.953774] [G loss: 1.393495]\n",
      "[D loss: 0.836323] [G loss: 1.388061]\n",
      "[D loss: 0.818147] [G loss: 1.561241]\n",
      "[D loss: 0.836551] [G loss: 1.600957]\n",
      "[D loss: 0.765069] [G loss: 1.378438]\n",
      "[D loss: 0.834461] [G loss: 1.256919]\n",
      "[D loss: 0.771675] [G loss: 1.433003]\n",
      "[D loss: 0.642440] [G loss: 1.444152]\n",
      "[D loss: 0.904923] [G loss: 1.514910]\n",
      "[D loss: 0.726206] [G loss: 1.430935]\n",
      "[D loss: 0.767350] [G loss: 1.391471]\n",
      "[D loss: 0.778531] [G loss: 1.626163]\n",
      "[D loss: 0.946374] [G loss: 1.371995]\n",
      "[D loss: 0.866587] [G loss: 1.537436]\n",
      "[D loss: 0.763500] [G loss: 1.429534]\n",
      "[D loss: 0.828983] [G loss: 1.403614]\n",
      "[D loss: 0.835300] [G loss: 1.212335]\n",
      "[D loss: 0.640428] [G loss: 1.526523]\n",
      "[D loss: 0.734185] [G loss: 1.852257]\n",
      "[D loss: 0.724703] [G loss: 1.676098]\n",
      "[D loss: 0.890900] [G loss: 1.314463]\n",
      "[D loss: 0.733830] [G loss: 1.484332]\n",
      "[D loss: 1.141198] [G loss: 1.573408]\n",
      "[D loss: 0.905100] [G loss: 1.419069]\n",
      "[D loss: 1.014782] [G loss: 1.411272]\n",
      "[D loss: 1.033324] [G loss: 1.529226]\n",
      "[D loss: 0.883153] [G loss: 1.519363]\n",
      "[D loss: 0.778293] [G loss: 1.382509]\n",
      "[D loss: 0.812356] [G loss: 1.307486]\n",
      "[D loss: 0.895454] [G loss: 1.436166]\n",
      "[D loss: 0.912624] [G loss: 1.361231]\n",
      "[D loss: 1.051210] [G loss: 1.163689]\n",
      "[D loss: 0.801851] [G loss: 1.305455]\n",
      "[D loss: 1.052146] [G loss: 1.478528]\n",
      "[D loss: 0.765227] [G loss: 1.495999]\n",
      "[D loss: 0.991827] [G loss: 1.531385]\n",
      "[D loss: 0.717996] [G loss: 1.522670]\n",
      "[D loss: 0.707386] [G loss: 1.631877]\n",
      "[D loss: 0.786474] [G loss: 1.101255]\n",
      "[D loss: 1.023559] [G loss: 1.417095]\n",
      "[D loss: 0.793956] [G loss: 1.372720]\n",
      "[D loss: 0.857884] [G loss: 1.325973]\n",
      "[D loss: 0.838672] [G loss: 1.374452]\n",
      "[D loss: 0.897151] [G loss: 1.392632]\n",
      "[D loss: 0.889774] [G loss: 1.349698]\n",
      "[D loss: 0.719190] [G loss: 1.684026]\n",
      "[D loss: 0.614255] [G loss: 1.575082]\n",
      "[D loss: 0.797737] [G loss: 1.458713]\n",
      "[D loss: 0.695345] [G loss: 1.676021]\n",
      "[D loss: 0.850047] [G loss: 1.264044]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.975831] [G loss: 1.540339]\n",
      "[D loss: 0.879981] [G loss: 1.464724]\n",
      "[D loss: 1.000693] [G loss: 1.312801]\n",
      "[D loss: 1.096346] [G loss: 1.578422]\n",
      "[D loss: 0.897094] [G loss: 1.483631]\n",
      "[D loss: 0.666816] [G loss: 1.624478]\n",
      "[D loss: 0.607566] [G loss: 1.441437]\n",
      "[D loss: 0.962000] [G loss: 1.257360]\n",
      "[D loss: 0.805193] [G loss: 1.325670]\n",
      "[D loss: 0.634227] [G loss: 1.573495]\n",
      "[D loss: 0.913308] [G loss: 1.760068]\n",
      "[D loss: 0.746913] [G loss: 1.664697]\n",
      "[D loss: 0.839153] [G loss: 1.403447]\n",
      "[D loss: 1.031537] [G loss: 1.580477]\n",
      "[D loss: 0.906202] [G loss: 1.539430]\n",
      "[D loss: 0.913223] [G loss: 1.469476]\n",
      "[D loss: 0.850597] [G loss: 1.300637]\n",
      "[D loss: 0.807055] [G loss: 1.467778]\n",
      "[D loss: 0.943608] [G loss: 1.499207]\n",
      "[D loss: 0.664023] [G loss: 1.439648]\n",
      "[D loss: 0.880184] [G loss: 1.276500]\n",
      "[D loss: 0.928537] [G loss: 1.487632]\n",
      "[D loss: 0.953350] [G loss: 1.423746]\n",
      "[D loss: 0.719116] [G loss: 1.597886]\n",
      "[D loss: 0.622439] [G loss: 1.714379]\n",
      "[D loss: 0.976070] [G loss: 1.670808]\n",
      "[D loss: 0.842938] [G loss: 1.524340]\n",
      "[D loss: 0.700154] [G loss: 1.517066]\n",
      "[D loss: 1.047152] [G loss: 1.746684]\n",
      "[D loss: 0.915146] [G loss: 1.399789]\n",
      "[D loss: 0.832641] [G loss: 1.320698]\n",
      "[D loss: 1.175823] [G loss: 1.312191]\n",
      "[D loss: 0.870263] [G loss: 1.531365]\n",
      "[D loss: 0.803016] [G loss: 1.478226]\n",
      "[D loss: 0.905970] [G loss: 1.428575]\n",
      "[D loss: 0.936513] [G loss: 1.408655]\n",
      "[D loss: 0.903567] [G loss: 1.515616]\n",
      "[D loss: 0.661178] [G loss: 1.431243]\n",
      "[D loss: 0.871706] [G loss: 1.299728]\n",
      "[D loss: 0.674191] [G loss: 1.591375]\n",
      "[D loss: 0.824978] [G loss: 1.508766]\n",
      "[D loss: 0.876925] [G loss: 1.640023]\n",
      "[D loss: 0.897871] [G loss: 1.454646]\n",
      "[D loss: 0.798112] [G loss: 1.281848]\n",
      "[D loss: 0.855306] [G loss: 1.279809]\n",
      "[D loss: 1.081706] [G loss: 1.152615]\n",
      "[D loss: 0.712948] [G loss: 1.441507]\n",
      "[D loss: 0.652943] [G loss: 1.624564]\n",
      "[D loss: 0.959846] [G loss: 1.478231]\n",
      "[D loss: 0.721541] [G loss: 1.318969]\n",
      "[D loss: 0.943306] [G loss: 1.204469]\n",
      "[D loss: 0.735931] [G loss: 1.477115]\n",
      "[D loss: 0.853050] [G loss: 1.465258]\n",
      "[D loss: 0.766653] [G loss: 1.530041]\n",
      "[D loss: 0.857763] [G loss: 1.555833]\n",
      "[D loss: 0.832328] [G loss: 1.590271]\n",
      "[D loss: 0.694860] [G loss: 1.396594]\n",
      "[D loss: 0.732904] [G loss: 1.459890]\n",
      "[D loss: 0.984515] [G loss: 1.508960]\n",
      "[D loss: 0.895165] [G loss: 1.427030]\n",
      "[D loss: 0.865926] [G loss: 1.441313]\n",
      "[D loss: 0.948146] [G loss: 1.689387]\n",
      "[D loss: 0.721312] [G loss: 1.326509]\n",
      "[D loss: 0.788939] [G loss: 1.370961]\n",
      "[D loss: 0.748176] [G loss: 1.277430]\n",
      "[D loss: 0.757547] [G loss: 1.347959]\n",
      "[D loss: 0.784133] [G loss: 1.523620]\n",
      "[D loss: 0.729482] [G loss: 1.667465]\n",
      "[D loss: 0.978501] [G loss: 1.244416]\n",
      "[D loss: 0.796145] [G loss: 1.496168]\n",
      "[D loss: 0.868363] [G loss: 1.390143]\n",
      "[D loss: 0.638413] [G loss: 1.604342]\n",
      "[D loss: 0.910392] [G loss: 1.351586]\n",
      "[D loss: 0.639820] [G loss: 1.518958]\n",
      "[D loss: 0.868648] [G loss: 1.728194]\n",
      "[D loss: 0.839232] [G loss: 1.419954]\n",
      "[D loss: 0.788498] [G loss: 1.570883]\n",
      "[D loss: 0.638177] [G loss: 1.577068]\n",
      "[D loss: 0.802908] [G loss: 1.524790]\n",
      "[D loss: 0.869109] [G loss: 1.313205]\n",
      "[D loss: 0.959572] [G loss: 1.488043]\n",
      "[D loss: 0.561796] [G loss: 1.413796]\n",
      "[D loss: 0.969176] [G loss: 1.145542]\n",
      "[D loss: 1.077529] [G loss: 1.513954]\n",
      "[D loss: 0.775865] [G loss: 1.564890]\n",
      "[D loss: 0.771158] [G loss: 1.506916]\n",
      "[D loss: 0.755039] [G loss: 1.491163]\n",
      "[D loss: 0.760583] [G loss: 1.672802]\n",
      "[D loss: 0.774145] [G loss: 1.493375]\n",
      "[D loss: 0.714066] [G loss: 1.456578]\n",
      "[D loss: 0.540089] [G loss: 1.771893]\n",
      "[D loss: 0.700667] [G loss: 1.539337]\n",
      "[D loss: 0.689306] [G loss: 1.720449]\n",
      "[D loss: 0.849876] [G loss: 1.630722]\n",
      "[D loss: 0.615355] [G loss: 1.573098]\n",
      "[D loss: 0.880936] [G loss: 1.413806]\n",
      "[D loss: 0.722953] [G loss: 1.669799]\n",
      "[D loss: 0.855975] [G loss: 1.613119]\n",
      "[D loss: 0.953216] [G loss: 1.638133]\n",
      "[D loss: 0.669187] [G loss: 1.649088]\n",
      "[D loss: 0.907985] [G loss: 1.316193]\n",
      "[D loss: 1.113748] [G loss: 1.301228]\n",
      "[D loss: 0.914765] [G loss: 1.442478]\n",
      "[D loss: 0.712954] [G loss: 1.501364]\n",
      "[D loss: 0.731528] [G loss: 1.548110]\n",
      "[D loss: 0.888978] [G loss: 1.466857]\n",
      "[D loss: 0.772107] [G loss: 1.708911]\n",
      "[D loss: 0.867531] [G loss: 1.580221]\n",
      "[D loss: 0.859217] [G loss: 1.618799]\n",
      "[D loss: 1.175757] [G loss: 1.201941]\n",
      "[D loss: 0.548546] [G loss: 1.924463]\n",
      "[D loss: 0.697656] [G loss: 1.627779]\n",
      "[D loss: 1.095316] [G loss: 1.484859]\n",
      "[D loss: 0.590981] [G loss: 1.868897]\n",
      "[D loss: 0.807103] [G loss: 1.567113]\n",
      "[D loss: 0.675741] [G loss: 1.751122]\n",
      "[D loss: 0.991954] [G loss: 1.348711]\n",
      "[D loss: 0.907527] [G loss: 1.557657]\n",
      "[D loss: 0.847508] [G loss: 1.295151]\n",
      "[D loss: 0.731574] [G loss: 1.604465]\n",
      "[D loss: 0.874314] [G loss: 1.231301]\n",
      "[D loss: 0.901985] [G loss: 1.790997]\n",
      "[D loss: 0.796086] [G loss: 1.680549]\n",
      "[D loss: 0.749492] [G loss: 1.467671]\n",
      "[D loss: 1.094545] [G loss: 1.282954]\n",
      "[D loss: 0.748995] [G loss: 1.471157]\n",
      "[D loss: 0.750101] [G loss: 1.527928]\n",
      "[D loss: 0.695436] [G loss: 1.549479]\n",
      "[D loss: 0.733580] [G loss: 1.436742]\n",
      "[D loss: 0.665668] [G loss: 1.577212]\n",
      "[D loss: 0.930620] [G loss: 1.696146]\n",
      "[D loss: 0.570813] [G loss: 1.632747]\n",
      "[D loss: 0.929727] [G loss: 1.388910]\n",
      "[D loss: 0.774469] [G loss: 1.329788]\n",
      "[D loss: 0.839683] [G loss: 1.562629]\n",
      "[D loss: 0.963066] [G loss: 1.624318]\n",
      "[D loss: 0.923758] [G loss: 1.403766]\n",
      "[D loss: 0.906727] [G loss: 1.212169]\n",
      "[D loss: 0.831810] [G loss: 1.305492]\n",
      "[D loss: 0.617514] [G loss: 1.561670]\n",
      "[D loss: 1.035188] [G loss: 1.318866]\n",
      "[D loss: 0.711542] [G loss: 1.532940]\n",
      "[D loss: 0.714598] [G loss: 1.691536]\n",
      "[D loss: 0.571245] [G loss: 1.814444]\n",
      "[D loss: 0.861263] [G loss: 1.856017]\n",
      "[D loss: 0.885238] [G loss: 1.486042]\n",
      "[D loss: 0.942100] [G loss: 1.432101]\n",
      "[D loss: 0.852771] [G loss: 1.365926]\n",
      "[D loss: 0.533092] [G loss: 1.663365]\n",
      "[D loss: 1.087769] [G loss: 1.167622]\n",
      "[D loss: 0.815680] [G loss: 1.506430]\n",
      "[D loss: 0.784004] [G loss: 1.584455]\n",
      "[D loss: 0.874072] [G loss: 1.406776]\n",
      "[D loss: 0.939406] [G loss: 1.325371]\n",
      "[D loss: 0.811210] [G loss: 1.694271]\n",
      "[D loss: 0.766040] [G loss: 1.469627]\n",
      "[D loss: 0.883939] [G loss: 1.338376]\n",
      "[D loss: 0.818713] [G loss: 1.428193]\n",
      "[D loss: 0.821773] [G loss: 1.502949]\n",
      "[D loss: 0.946520] [G loss: 1.339260]\n",
      "[D loss: 0.819902] [G loss: 1.559103]\n",
      "[D loss: 0.818332] [G loss: 1.462119]\n",
      "[D loss: 0.691218] [G loss: 1.431051]\n",
      "[D loss: 0.848402] [G loss: 1.421968]\n",
      "[D loss: 0.789725] [G loss: 1.627330]\n",
      "[D loss: 0.785596] [G loss: 1.538827]\n",
      "[D loss: 0.830968] [G loss: 1.443044]\n",
      "[D loss: 0.663186] [G loss: 1.442101]\n",
      "[D loss: 0.729691] [G loss: 1.355191]\n",
      "[D loss: 0.916389] [G loss: 1.438442]\n",
      "[D loss: 0.884691] [G loss: 1.554316]\n",
      "[D loss: 1.086982] [G loss: 1.482252]\n",
      "[D loss: 0.751609] [G loss: 1.419107]\n",
      "[D loss: 0.722638] [G loss: 1.372393]\n",
      "[D loss: 0.825099] [G loss: 1.536143]\n",
      "[D loss: 0.741589] [G loss: 1.677475]\n",
      "[D loss: 0.812853] [G loss: 1.529494]\n",
      "[D loss: 0.806771] [G loss: 1.410081]\n",
      "[D loss: 0.769405] [G loss: 1.441482]\n",
      "[D loss: 0.836095] [G loss: 1.301810]\n",
      "[D loss: 0.706492] [G loss: 1.436902]\n",
      "[D loss: 0.734529] [G loss: 1.908740]\n",
      "[D loss: 0.719227] [G loss: 1.675076]\n",
      "[D loss: 0.685634] [G loss: 1.566570]\n",
      "[D loss: 0.845643] [G loss: 1.552790]\n",
      "[D loss: 0.824411] [G loss: 1.531017]\n",
      "[D loss: 0.804574] [G loss: 1.708966]\n",
      "[D loss: 0.859971] [G loss: 1.642140]\n",
      "[D loss: 0.941279] [G loss: 1.594742]\n",
      "[D loss: 0.764579] [G loss: 1.597980]\n",
      "[D loss: 0.893834] [G loss: 1.578733]\n",
      "[D loss: 0.839372] [G loss: 1.532244]\n",
      "[D loss: 0.967529] [G loss: 1.502908]\n",
      "[D loss: 0.814805] [G loss: 1.263495]\n",
      "[D loss: 0.799625] [G loss: 1.500665]\n",
      "[D loss: 0.748196] [G loss: 1.408850]\n",
      "[D loss: 0.896388] [G loss: 1.555964]\n",
      "[D loss: 0.972007] [G loss: 1.374088]\n",
      "[D loss: 0.883388] [G loss: 1.499022]\n",
      "[D loss: 0.887512] [G loss: 1.486526]\n",
      "[D loss: 0.972030] [G loss: 1.335040]\n",
      "[D loss: 0.720849] [G loss: 1.491813]\n",
      "[D loss: 0.892505] [G loss: 1.618940]\n",
      "[D loss: 0.864268] [G loss: 1.495093]\n",
      "[D loss: 0.786308] [G loss: 1.468764]\n",
      "[D loss: 0.944663] [G loss: 1.552617]\n",
      "[D loss: 0.803855] [G loss: 1.611974]\n",
      "[D loss: 0.797402] [G loss: 1.454593]\n",
      "[D loss: 0.757069] [G loss: 1.451312]\n",
      "[D loss: 0.649315] [G loss: 1.384250]\n",
      "[D loss: 0.900063] [G loss: 1.401142]\n",
      "[D loss: 0.850441] [G loss: 1.270871]\n",
      "[D loss: 0.885017] [G loss: 1.468635]\n",
      "[D loss: 0.710599] [G loss: 1.709964]\n",
      "[D loss: 0.917962] [G loss: 1.625289]\n",
      "[D loss: 0.995506] [G loss: 1.400474]\n",
      "[D loss: 0.800703] [G loss: 1.297656]\n",
      "[D loss: 0.806810] [G loss: 1.206246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.731223] [G loss: 1.473958]\n",
      "[D loss: 0.810640] [G loss: 1.572113]\n",
      "[D loss: 0.919540] [G loss: 1.533254]\n",
      "[D loss: 1.011252] [G loss: 1.612400]\n",
      "[D loss: 0.846429] [G loss: 1.278280]\n",
      "[D loss: 0.806929] [G loss: 1.380044]\n",
      "[D loss: 0.857096] [G loss: 1.559839]\n",
      "[D loss: 0.811020] [G loss: 1.570667]\n",
      "[D loss: 0.818609] [G loss: 1.365736]\n",
      "[D loss: 0.709301] [G loss: 1.558369]\n",
      "[D loss: 0.736020] [G loss: 1.344690]\n",
      "[D loss: 0.765878] [G loss: 1.511100]\n",
      "[D loss: 1.273660] [G loss: 1.222512]\n",
      "[D loss: 0.642288] [G loss: 1.656418]\n",
      "[D loss: 1.018421] [G loss: 1.826737]\n",
      "[D loss: 0.924800] [G loss: 1.830836]\n",
      "[D loss: 0.978756] [G loss: 1.493865]\n",
      "[D loss: 0.752239] [G loss: 1.607969]\n",
      "[D loss: 0.838056] [G loss: 1.362532]\n",
      "[D loss: 0.918656] [G loss: 1.383486]\n",
      "[D loss: 0.622951] [G loss: 1.293242]\n",
      "[D loss: 0.540149] [G loss: 1.440912]\n",
      "[D loss: 0.835517] [G loss: 1.433736]\n",
      "[D loss: 0.975413] [G loss: 1.545529]\n",
      "[D loss: 0.703803] [G loss: 1.562750]\n",
      "[D loss: 0.719664] [G loss: 1.506843]\n",
      "[D loss: 0.896609] [G loss: 1.457935]\n",
      "[D loss: 0.779636] [G loss: 1.357898]\n",
      "[D loss: 0.871247] [G loss: 1.465008]\n",
      "[D loss: 0.694594] [G loss: 1.629576]\n",
      "[D loss: 1.206983] [G loss: 1.533391]\n",
      "[D loss: 0.805338] [G loss: 2.016613]\n",
      "[D loss: 0.675775] [G loss: 1.547501]\n",
      "[D loss: 0.893439] [G loss: 1.441999]\n",
      "[D loss: 0.824556] [G loss: 1.370609]\n",
      "[D loss: 0.833551] [G loss: 1.303608]\n",
      "[D loss: 1.074371] [G loss: 1.385756]\n",
      "[D loss: 1.014253] [G loss: 1.662715]\n",
      "[D loss: 0.802939] [G loss: 1.328378]\n",
      "[D loss: 0.781546] [G loss: 1.693727]\n",
      "[D loss: 0.895045] [G loss: 1.442367]\n",
      "[D loss: 0.910427] [G loss: 1.274295]\n",
      "[D loss: 0.920766] [G loss: 1.225251]\n",
      "[D loss: 0.645115] [G loss: 1.548724]\n",
      "[D loss: 1.120277] [G loss: 1.271083]\n",
      "[D loss: 0.711648] [G loss: 1.567535]\n",
      "[D loss: 0.911570] [G loss: 1.506685]\n",
      "[D loss: 0.832833] [G loss: 1.286741]\n",
      "[D loss: 0.655967] [G loss: 1.285743]\n",
      "[D loss: 0.708122] [G loss: 1.730670]\n",
      "[D loss: 0.912901] [G loss: 1.336205]\n",
      "[D loss: 0.750037] [G loss: 1.457921]\n",
      "[D loss: 0.700196] [G loss: 1.701213]\n",
      "[D loss: 0.841481] [G loss: 1.549291]\n",
      "[D loss: 0.864031] [G loss: 1.430537]\n",
      "[D loss: 0.830434] [G loss: 1.370668]\n",
      "[D loss: 0.875440] [G loss: 1.419454]\n",
      "[D loss: 0.939419] [G loss: 1.332770]\n",
      "[D loss: 0.717771] [G loss: 1.556368]\n",
      "[D loss: 0.885807] [G loss: 1.502247]\n",
      "[D loss: 0.982306] [G loss: 1.394277]\n",
      "[D loss: 0.708096] [G loss: 1.460843]\n",
      "[D loss: 0.805188] [G loss: 1.540532]\n",
      "[D loss: 0.889205] [G loss: 1.387266]\n",
      "[D loss: 0.607902] [G loss: 1.503641]\n",
      "[D loss: 0.756707] [G loss: 1.661775]\n",
      "[D loss: 1.050217] [G loss: 1.164346]\n",
      "[D loss: 0.545408] [G loss: 1.565999]\n",
      "[D loss: 0.696212] [G loss: 1.413519]\n",
      "[D loss: 0.796369] [G loss: 1.373032]\n",
      "[D loss: 0.628084] [G loss: 1.777246]\n",
      "[D loss: 0.778032] [G loss: 1.477932]\n",
      "[D loss: 0.774975] [G loss: 1.643211]\n",
      "[D loss: 0.763260] [G loss: 1.743897]\n",
      "[D loss: 1.045400] [G loss: 1.387240]\n",
      "[D loss: 0.824969] [G loss: 1.533953]\n",
      "[D loss: 0.696553] [G loss: 1.377170]\n",
      "[D loss: 1.104376] [G loss: 1.419692]\n",
      "[D loss: 0.872310] [G loss: 1.407691]\n",
      "[D loss: 0.802356] [G loss: 1.447539]\n",
      "[D loss: 0.899849] [G loss: 1.415740]\n",
      "[D loss: 0.820141] [G loss: 1.427583]\n",
      "[D loss: 0.909426] [G loss: 1.324531]\n",
      "[D loss: 1.120173] [G loss: 1.438113]\n",
      "[D loss: 0.832026] [G loss: 1.294292]\n",
      "[D loss: 0.837788] [G loss: 1.316630]\n",
      "[D loss: 0.895233] [G loss: 1.266044]\n",
      "[D loss: 0.819996] [G loss: 1.503294]\n",
      "[D loss: 0.866483] [G loss: 1.428343]\n",
      "[D loss: 0.953617] [G loss: 1.298598]\n",
      "[D loss: 1.193639] [G loss: 1.114972]\n",
      "[D loss: 0.958140] [G loss: 1.335505]\n",
      "[D loss: 0.730176] [G loss: 1.558464]\n",
      "[D loss: 0.789472] [G loss: 1.502155]\n",
      "[D loss: 0.825829] [G loss: 1.593851]\n",
      "[D loss: 0.922222] [G loss: 1.488979]\n",
      "[D loss: 0.803974] [G loss: 1.321255]\n",
      "[D loss: 0.768449] [G loss: 1.224630]\n",
      "[D loss: 0.897532] [G loss: 1.264215]\n",
      "[D loss: 0.966007] [G loss: 1.316449]\n",
      "[D loss: 0.779455] [G loss: 1.517554]\n",
      "[D loss: 0.551500] [G loss: 1.521798]\n",
      "[D loss: 0.897206] [G loss: 1.544937]\n",
      "[D loss: 0.888913] [G loss: 1.547079]\n",
      "[D loss: 0.735749] [G loss: 1.522926]\n",
      "[D loss: 1.079253] [G loss: 1.333833]\n",
      "[D loss: 0.554320] [G loss: 1.679679]\n",
      "[D loss: 0.843041] [G loss: 1.357137]\n",
      "[D loss: 0.842396] [G loss: 1.287823]\n",
      "[D loss: 0.792388] [G loss: 1.424068]\n",
      "[D loss: 0.882660] [G loss: 1.580313]\n",
      "[D loss: 0.785632] [G loss: 1.418399]\n",
      "[D loss: 0.770998] [G loss: 1.482921]\n",
      "[D loss: 0.894216] [G loss: 1.528278]\n",
      "[D loss: 0.848540] [G loss: 1.378873]\n",
      "[D loss: 1.089230] [G loss: 1.344070]\n",
      "[D loss: 0.883923] [G loss: 1.343976]\n",
      "[D loss: 0.801367] [G loss: 1.553948]\n",
      "[D loss: 0.638018] [G loss: 1.265734]\n",
      "[D loss: 0.729654] [G loss: 1.559232]\n",
      "[D loss: 0.967405] [G loss: 1.534057]\n",
      "[D loss: 0.566746] [G loss: 1.456600]\n",
      "[D loss: 0.816359] [G loss: 1.418832]\n",
      "[D loss: 0.866159] [G loss: 1.466270]\n",
      "[D loss: 0.786187] [G loss: 1.353187]\n",
      "[D loss: 1.149754] [G loss: 1.290915]\n",
      "[D loss: 0.765108] [G loss: 1.297100]\n",
      "[D loss: 0.850406] [G loss: 1.557206]\n",
      "[D loss: 0.915935] [G loss: 1.259863]\n",
      "[D loss: 0.797217] [G loss: 1.611035]\n",
      "[D loss: 0.743247] [G loss: 1.526128]\n",
      "[D loss: 0.697275] [G loss: 1.525061]\n",
      "[D loss: 0.643581] [G loss: 1.602908]\n",
      "[D loss: 1.005626] [G loss: 1.459748]\n",
      "[D loss: 0.995216] [G loss: 1.363945]\n",
      "[D loss: 0.970183] [G loss: 1.239114]\n",
      "[D loss: 0.897272] [G loss: 1.295270]\n",
      "[D loss: 0.821503] [G loss: 1.508787]\n",
      "[D loss: 0.935037] [G loss: 1.449860]\n",
      "[D loss: 0.767813] [G loss: 1.284298]\n",
      "[D loss: 0.628317] [G loss: 1.399834]\n",
      "[D loss: 0.701410] [G loss: 1.664206]\n",
      "[D loss: 1.085111] [G loss: 1.606333]\n",
      "[D loss: 0.812654] [G loss: 1.302860]\n",
      "[D loss: 0.880730] [G loss: 1.537791]\n",
      "[D loss: 0.803611] [G loss: 1.634941]\n",
      "[D loss: 0.814091] [G loss: 1.513129]\n",
      "[D loss: 1.132077] [G loss: 1.252072]\n",
      "[D loss: 0.640240] [G loss: 1.790425]\n",
      "[D loss: 0.686272] [G loss: 1.800673]\n",
      "[D loss: 0.889894] [G loss: 1.471786]\n",
      "[D loss: 0.767499] [G loss: 1.584434]\n",
      "[D loss: 0.762786] [G loss: 1.352079]\n",
      "[D loss: 0.885621] [G loss: 1.610323]\n",
      "[D loss: 1.000404] [G loss: 1.558271]\n",
      "[D loss: 0.990653] [G loss: 1.399796]\n",
      "[D loss: 0.767113] [G loss: 1.399696]\n",
      "[D loss: 0.887779] [G loss: 1.512605]\n",
      "[D loss: 0.728316] [G loss: 1.544674]\n",
      "[D loss: 0.856132] [G loss: 1.709338]\n",
      "[D loss: 0.938127] [G loss: 1.876070]\n",
      "[D loss: 0.781479] [G loss: 1.620208]\n",
      "[D loss: 0.827958] [G loss: 1.478676]\n",
      "[D loss: 0.824981] [G loss: 1.383634]\n",
      "[D loss: 0.992582] [G loss: 1.429327]\n",
      "[D loss: 0.709799] [G loss: 1.494289]\n",
      "[D loss: 0.682345] [G loss: 1.467460]\n",
      "[D loss: 1.079023] [G loss: 1.416669]\n",
      "[D loss: 0.648555] [G loss: 1.507141]\n",
      "[D loss: 0.658458] [G loss: 1.430199]\n",
      "[D loss: 0.696240] [G loss: 1.387112]\n",
      "[D loss: 0.591334] [G loss: 1.451401]\n",
      "[D loss: 0.936040] [G loss: 1.475653]\n",
      "[D loss: 0.814594] [G loss: 1.409793]\n",
      "[D loss: 0.691708] [G loss: 1.610954]\n",
      "[D loss: 0.659783] [G loss: 1.609399]\n",
      "[D loss: 0.650904] [G loss: 1.489254]\n",
      "[D loss: 0.849245] [G loss: 1.746908]\n",
      "[D loss: 0.926878] [G loss: 1.359513]\n",
      "[D loss: 0.631913] [G loss: 1.528429]\n",
      "[D loss: 0.833786] [G loss: 1.618976]\n",
      "[D loss: 0.690430] [G loss: 1.711994]\n",
      "[D loss: 0.877710] [G loss: 1.752929]\n",
      "[D loss: 0.920533] [G loss: 1.334913]\n",
      "[D loss: 0.728561] [G loss: 1.607397]\n",
      "[D loss: 0.813815] [G loss: 1.368754]\n",
      "[D loss: 0.941733] [G loss: 1.449469]\n",
      "[D loss: 0.750874] [G loss: 1.607458]\n",
      "[D loss: 0.692278] [G loss: 1.344960]\n",
      "[D loss: 0.884258] [G loss: 1.243274]\n",
      "[D loss: 0.784309] [G loss: 1.522428]\n",
      "[D loss: 0.710197] [G loss: 1.517013]\n",
      "[D loss: 0.875004] [G loss: 1.346392]\n",
      "[D loss: 0.800104] [G loss: 1.580440]\n",
      "[D loss: 0.934869] [G loss: 1.688636]\n",
      "[D loss: 0.560685] [G loss: 1.376228]\n",
      "[D loss: 0.919040] [G loss: 1.359583]\n",
      "[D loss: 0.767214] [G loss: 1.466056]\n",
      "[D loss: 0.606962] [G loss: 1.826140]\n",
      "[D loss: 0.826375] [G loss: 1.651829]\n",
      "[D loss: 0.794246] [G loss: 1.563014]\n",
      "[D loss: 0.846428] [G loss: 1.623388]\n",
      "[D loss: 0.936230] [G loss: 1.537931]\n",
      "[D loss: 0.795767] [G loss: 1.325929]\n",
      "[D loss: 0.744259] [G loss: 1.395465]\n",
      "[D loss: 0.888024] [G loss: 1.614282]\n",
      "[D loss: 0.897797] [G loss: 1.408041]\n",
      "[D loss: 0.766553] [G loss: 1.676663]\n",
      "[D loss: 0.787986] [G loss: 1.467323]\n",
      "[D loss: 0.599658] [G loss: 1.700156]\n",
      "[D loss: 0.882273] [G loss: 1.569967]\n",
      "[D loss: 0.665384] [G loss: 1.540938]\n",
      "[D loss: 0.649948] [G loss: 1.717092]\n",
      "[D loss: 0.901287] [G loss: 1.424711]\n",
      "[D loss: 0.958113] [G loss: 1.432353]\n",
      "[D loss: 0.897343] [G loss: 1.503146]\n",
      "[D loss: 0.726002] [G loss: 1.659794]\n",
      "[D loss: 0.662336] [G loss: 1.493094]\n",
      "[D loss: 0.817643] [G loss: 1.574659]\n",
      "[D loss: 0.854525] [G loss: 1.520469]\n",
      "[D loss: 0.946513] [G loss: 1.406969]\n",
      "[D loss: 0.829340] [G loss: 1.260495]\n",
      "[D loss: 0.742415] [G loss: 1.380633]\n",
      "[D loss: 0.854577] [G loss: 1.348377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.951909] [G loss: 1.312291]\n",
      "[D loss: 1.243778] [G loss: 1.325171]\n",
      "[D loss: 0.869802] [G loss: 1.792691]\n",
      "[D loss: 1.038743] [G loss: 1.359563]\n",
      "[D loss: 0.852178] [G loss: 1.455226]\n",
      "[D loss: 0.714018] [G loss: 1.551219]\n",
      "[D loss: 0.651014] [G loss: 1.603142]\n",
      "[D loss: 1.043095] [G loss: 1.106010]\n",
      "[D loss: 0.769498] [G loss: 1.370511]\n",
      "[D loss: 0.839154] [G loss: 1.309744]\n",
      "[D loss: 0.859996] [G loss: 1.629267]\n",
      "[D loss: 0.870160] [G loss: 1.474740]\n",
      "[D loss: 0.812886] [G loss: 1.709526]\n",
      "[D loss: 0.834363] [G loss: 1.403200]\n",
      "[D loss: 0.944184] [G loss: 1.337762]\n",
      "[D loss: 0.863543] [G loss: 1.673837]\n",
      "[D loss: 0.636440] [G loss: 1.597125]\n",
      "[D loss: 0.905193] [G loss: 1.681979]\n",
      "[D loss: 0.841146] [G loss: 1.469662]\n",
      "[D loss: 0.830540] [G loss: 1.491864]\n",
      "[D loss: 0.881987] [G loss: 1.444652]\n",
      "[D loss: 0.686235] [G loss: 1.417690]\n",
      "[D loss: 1.113704] [G loss: 1.532784]\n",
      "[D loss: 0.796820] [G loss: 1.264912]\n",
      "[D loss: 1.015359] [G loss: 1.531973]\n",
      "[D loss: 1.023536] [G loss: 1.873202]\n",
      "[D loss: 0.872012] [G loss: 1.424305]\n",
      "[D loss: 0.959003] [G loss: 1.571474]\n",
      "[D loss: 0.810031] [G loss: 1.548256]\n",
      "[D loss: 0.749584] [G loss: 1.265035]\n",
      "[D loss: 0.835807] [G loss: 1.402406]\n",
      "[D loss: 0.960594] [G loss: 1.311756]\n",
      "[D loss: 0.622214] [G loss: 1.599151]\n",
      "[D loss: 0.855700] [G loss: 1.539445]\n",
      "[D loss: 0.812580] [G loss: 1.610368]\n",
      "[D loss: 0.711958] [G loss: 1.405545]\n",
      "[D loss: 0.698579] [G loss: 1.523116]\n",
      "[D loss: 1.351995] [G loss: 1.189048]\n",
      "[D loss: 0.760089] [G loss: 1.440836]\n",
      "[D loss: 0.836215] [G loss: 1.608818]\n",
      "[D loss: 0.692720] [G loss: 1.359954]\n",
      "[D loss: 0.721302] [G loss: 1.346688]\n",
      "[D loss: 0.697711] [G loss: 1.501332]\n",
      "[D loss: 0.661640] [G loss: 1.645326]\n",
      "[D loss: 0.927619] [G loss: 1.428827]\n",
      "[D loss: 0.937022] [G loss: 1.492904]\n",
      "[D loss: 0.799193] [G loss: 1.215233]\n",
      "[D loss: 0.765090] [G loss: 1.355365]\n",
      "[D loss: 1.010530] [G loss: 1.306214]\n",
      "[D loss: 0.743475] [G loss: 1.509871]\n",
      "[D loss: 0.837661] [G loss: 1.623915]\n",
      "[D loss: 0.829884] [G loss: 1.551779]\n",
      "[D loss: 0.666980] [G loss: 1.517034]\n",
      "[D loss: 0.533767] [G loss: 1.548383]\n",
      "[D loss: 0.804057] [G loss: 1.360709]\n",
      "[D loss: 0.800735] [G loss: 1.557843]\n",
      "[D loss: 1.034254] [G loss: 1.647049]\n",
      "[D loss: 0.767392] [G loss: 1.499413]\n",
      "[D loss: 1.006747] [G loss: 1.621157]\n",
      "[D loss: 0.699890] [G loss: 1.400246]\n",
      "[D loss: 0.944190] [G loss: 1.290192]\n",
      "[D loss: 0.966754] [G loss: 1.171571]\n",
      "[D loss: 0.716963] [G loss: 1.331063]\n",
      "[D loss: 0.831779] [G loss: 1.384871]\n",
      "[D loss: 0.726549] [G loss: 1.647983]\n",
      "[D loss: 0.639256] [G loss: 1.570744]\n",
      "[D loss: 0.709242] [G loss: 1.511656]\n",
      "[D loss: 0.694014] [G loss: 1.562867]\n",
      "[D loss: 0.677538] [G loss: 1.363544]\n",
      "[D loss: 0.972466] [G loss: 1.364516]\n",
      "[D loss: 0.738959] [G loss: 1.670706]\n",
      "[D loss: 0.950998] [G loss: 1.339238]\n",
      "[D loss: 0.819970] [G loss: 1.583130]\n",
      "[D loss: 0.982355] [G loss: 1.406793]\n",
      "[D loss: 1.052052] [G loss: 1.198368]\n",
      "[D loss: 0.596173] [G loss: 1.600385]\n",
      "[D loss: 0.877036] [G loss: 1.582286]\n",
      "[D loss: 0.746922] [G loss: 1.503087]\n",
      "[D loss: 0.730980] [G loss: 1.758104]\n",
      "[D loss: 0.905645] [G loss: 1.614751]\n",
      "[D loss: 0.900297] [G loss: 1.430440]\n",
      "[D loss: 0.804543] [G loss: 1.487381]\n",
      "[D loss: 0.984743] [G loss: 1.414795]\n",
      "[D loss: 0.797544] [G loss: 1.589850]\n",
      "[D loss: 0.628588] [G loss: 1.470862]\n",
      "[D loss: 0.981855] [G loss: 1.415446]\n",
      "[D loss: 0.700246] [G loss: 1.551379]\n",
      "[D loss: 0.877581] [G loss: 1.745342]\n",
      "[D loss: 0.641426] [G loss: 1.472393]\n",
      "[D loss: 0.883877] [G loss: 1.826301]\n",
      "[D loss: 0.966856] [G loss: 1.376903]\n",
      "[D loss: 1.132801] [G loss: 1.228202]\n",
      "[D loss: 0.816380] [G loss: 1.261073]\n",
      "[D loss: 0.766763] [G loss: 1.698993]\n",
      "[D loss: 0.810912] [G loss: 1.434528]\n",
      "[D loss: 0.689859] [G loss: 1.609839]\n",
      "[D loss: 0.718737] [G loss: 1.459563]\n",
      "[D loss: 0.881037] [G loss: 1.362519]\n",
      "[D loss: 0.824129] [G loss: 1.304814]\n",
      "[D loss: 0.906537] [G loss: 1.401343]\n",
      "[D loss: 0.798889] [G loss: 1.320338]\n",
      "[D loss: 0.849923] [G loss: 1.500991]\n",
      "[D loss: 0.947818] [G loss: 1.376214]\n",
      "[D loss: 0.670382] [G loss: 1.262102]\n",
      "[D loss: 0.838977] [G loss: 1.537382]\n",
      "[D loss: 1.075217] [G loss: 1.406629]\n",
      "[D loss: 0.886665] [G loss: 1.323679]\n",
      "[D loss: 0.796887] [G loss: 1.582139]\n",
      "[D loss: 0.882836] [G loss: 1.536854]\n",
      "[D loss: 0.793517] [G loss: 1.602759]\n",
      "[D loss: 0.602611] [G loss: 1.433098]\n",
      "[D loss: 0.807547] [G loss: 1.442395]\n",
      "[D loss: 0.688409] [G loss: 1.612510]\n",
      "[D loss: 1.002503] [G loss: 1.572226]\n",
      "[D loss: 0.867446] [G loss: 1.544264]\n",
      "[D loss: 0.769543] [G loss: 1.453655]\n",
      "[D loss: 0.841782] [G loss: 1.597762]\n",
      "[D loss: 0.902686] [G loss: 1.779416]\n",
      "[D loss: 0.878257] [G loss: 1.609452]\n",
      "[D loss: 0.923413] [G loss: 1.217523]\n",
      "[D loss: 0.936351] [G loss: 1.437402]\n",
      "[D loss: 0.924142] [G loss: 1.580787]\n",
      "[D loss: 0.873292] [G loss: 1.487574]\n",
      "[D loss: 1.012900] [G loss: 1.396010]\n",
      "[D loss: 0.779210] [G loss: 1.609483]\n",
      "[D loss: 0.825811] [G loss: 1.312174]\n",
      "[D loss: 0.782810] [G loss: 1.560110]\n",
      "[D loss: 0.761549] [G loss: 1.458357]\n",
      "[D loss: 0.901304] [G loss: 1.610208]\n",
      "[D loss: 0.769808] [G loss: 1.304988]\n",
      "[D loss: 0.973902] [G loss: 1.404093]\n",
      "[D loss: 0.758414] [G loss: 1.372297]\n",
      "[D loss: 0.861108] [G loss: 1.401908]\n",
      "[D loss: 0.829292] [G loss: 1.239164]\n",
      "[D loss: 0.760106] [G loss: 1.520280]\n",
      "[D loss: 0.932212] [G loss: 1.346543]\n",
      "[D loss: 0.645489] [G loss: 1.462845]\n",
      "[D loss: 0.664679] [G loss: 1.581094]\n",
      "[D loss: 0.732805] [G loss: 1.428019]\n",
      "[D loss: 0.645713] [G loss: 1.436880]\n",
      "[D loss: 0.827958] [G loss: 1.399765]\n",
      "[D loss: 0.903529] [G loss: 1.541483]\n",
      "[D loss: 0.686996] [G loss: 1.431968]\n",
      "[D loss: 0.984213] [G loss: 1.360237]\n",
      "[D loss: 0.702403] [G loss: 1.458451]\n",
      "[D loss: 0.659575] [G loss: 1.488227]\n",
      "[D loss: 0.922789] [G loss: 1.255202]\n",
      "[D loss: 0.836019] [G loss: 1.394974]\n",
      "[D loss: 1.246651] [G loss: 1.444342]\n",
      "[D loss: 0.897414] [G loss: 1.229562]\n",
      "[D loss: 0.989146] [G loss: 1.297685]\n",
      "[D loss: 0.903965] [G loss: 1.752933]\n",
      "[D loss: 0.741729] [G loss: 1.740431]\n",
      "[D loss: 0.789794] [G loss: 1.616587]\n",
      "[D loss: 0.853166] [G loss: 1.689660]\n",
      "[D loss: 0.913123] [G loss: 1.586930]\n",
      "[D loss: 0.771264] [G loss: 1.448993]\n",
      "[D loss: 0.907782] [G loss: 1.269433]\n",
      "[D loss: 1.104019] [G loss: 1.134031]\n",
      "[D loss: 0.922398] [G loss: 1.282848]\n",
      "[D loss: 0.747992] [G loss: 1.578063]\n",
      "[D loss: 0.898903] [G loss: 1.425988]\n",
      "[D loss: 0.899337] [G loss: 1.665493]\n",
      "[D loss: 0.898194] [G loss: 1.505295]\n",
      "[D loss: 1.038459] [G loss: 1.401624]\n",
      "[D loss: 0.804706] [G loss: 1.488057]\n",
      "[D loss: 0.920340] [G loss: 1.543422]\n",
      "[D loss: 0.864386] [G loss: 1.287280]\n",
      "[D loss: 0.744884] [G loss: 1.310222]\n",
      "[D loss: 0.900600] [G loss: 1.301382]\n",
      "[D loss: 0.841522] [G loss: 1.139139]\n",
      "[D loss: 0.627628] [G loss: 1.314607]\n",
      "[D loss: 1.051329] [G loss: 1.350726]\n",
      "[D loss: 0.559237] [G loss: 1.768405]\n",
      "[D loss: 0.801311] [G loss: 1.488825]\n",
      "[D loss: 0.836331] [G loss: 1.519688]\n",
      "[D loss: 0.767568] [G loss: 1.531976]\n",
      "[D loss: 0.787166] [G loss: 1.363532]\n",
      "[D loss: 0.924968] [G loss: 1.291663]\n",
      "[D loss: 0.889183] [G loss: 1.303839]\n",
      "[D loss: 0.750470] [G loss: 1.365168]\n",
      "[D loss: 0.769542] [G loss: 1.448118]\n",
      "[D loss: 0.859852] [G loss: 1.696958]\n",
      "[D loss: 1.176994] [G loss: 1.313185]\n",
      "[D loss: 0.630314] [G loss: 1.434449]\n",
      "[D loss: 0.578351] [G loss: 1.515715]\n",
      "[D loss: 0.835001] [G loss: 1.725741]\n",
      "[D loss: 0.583962] [G loss: 1.618855]\n",
      "[D loss: 0.776582] [G loss: 1.689317]\n",
      "[D loss: 0.841604] [G loss: 1.404315]\n",
      "[D loss: 1.038302] [G loss: 1.634164]\n",
      "[D loss: 0.956298] [G loss: 1.550081]\n",
      "[D loss: 0.824711] [G loss: 1.629349]\n",
      "[D loss: 0.752518] [G loss: 1.597339]\n",
      "[D loss: 0.805658] [G loss: 1.458976]\n",
      "[D loss: 1.078341] [G loss: 1.385993]\n",
      "[D loss: 0.792129] [G loss: 1.391813]\n",
      "[D loss: 0.694081] [G loss: 1.537743]\n",
      "[D loss: 0.701930] [G loss: 1.466948]\n",
      "[D loss: 0.940110] [G loss: 1.599035]\n",
      "[D loss: 0.850085] [G loss: 1.388373]\n",
      "[D loss: 0.720966] [G loss: 1.258299]\n",
      "[D loss: 1.020863] [G loss: 1.312191]\n",
      "[D loss: 0.565465] [G loss: 1.616495]\n",
      "[D loss: 0.925511] [G loss: 1.467265]\n",
      "[D loss: 0.681925] [G loss: 1.618397]\n",
      "[D loss: 0.844754] [G loss: 1.607468]\n",
      "[D loss: 0.966937] [G loss: 1.512619]\n",
      "[D loss: 0.870869] [G loss: 1.445900]\n",
      "[D loss: 0.897783] [G loss: 1.464229]\n",
      "[D loss: 0.563529] [G loss: 1.586504]\n",
      "[D loss: 0.698461] [G loss: 1.461367]\n",
      "[D loss: 0.791555] [G loss: 1.481300]\n",
      "[D loss: 0.870563] [G loss: 1.522047]\n",
      "[D loss: 1.017285] [G loss: 1.481133]\n",
      "[D loss: 0.947323] [G loss: 1.487425]\n",
      "[D loss: 0.827836] [G loss: 1.423631]\n",
      "[D loss: 0.957545] [G loss: 1.194030]\n",
      "[D loss: 0.606031] [G loss: 1.498952]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.960199] [G loss: 1.345784]\n",
      "[D loss: 0.786863] [G loss: 1.563197]\n",
      "[D loss: 0.780344] [G loss: 1.461814]\n",
      "[D loss: 0.769971] [G loss: 1.640805]\n",
      "[D loss: 1.001520] [G loss: 1.584977]\n",
      "[D loss: 0.774341] [G loss: 1.369542]\n",
      "[D loss: 0.949268] [G loss: 1.473518]\n",
      "[D loss: 0.857549] [G loss: 1.323736]\n",
      "[D loss: 0.853936] [G loss: 1.484133]\n",
      "[D loss: 0.900217] [G loss: 1.513205]\n",
      "[D loss: 0.990894] [G loss: 1.076804]\n",
      "[D loss: 1.052428] [G loss: 1.301349]\n",
      "[D loss: 0.754473] [G loss: 1.845907]\n",
      "[D loss: 1.002503] [G loss: 1.638857]\n",
      "[D loss: 0.929095] [G loss: 1.382180]\n",
      "[D loss: 0.765869] [G loss: 1.743589]\n",
      "[D loss: 1.219342] [G loss: 1.442805]\n",
      "[D loss: 0.786206] [G loss: 1.525027]\n",
      "[D loss: 0.680780] [G loss: 1.519857]\n",
      "[D loss: 0.813367] [G loss: 1.424135]\n",
      "[D loss: 0.758965] [G loss: 1.263799]\n",
      "[D loss: 1.191716] [G loss: 1.399455]\n",
      "[D loss: 0.836390] [G loss: 1.430557]\n",
      "[D loss: 0.747867] [G loss: 1.468521]\n",
      "[D loss: 0.828729] [G loss: 1.296348]\n",
      "[D loss: 0.754489] [G loss: 1.587119]\n",
      "[D loss: 1.065333] [G loss: 1.225096]\n",
      "[D loss: 0.718595] [G loss: 1.450037]\n",
      "[D loss: 0.724560] [G loss: 1.455857]\n",
      "[D loss: 1.002138] [G loss: 1.251501]\n",
      "[D loss: 0.825349] [G loss: 1.546340]\n",
      "[D loss: 0.700658] [G loss: 1.457795]\n",
      "[D loss: 0.785122] [G loss: 1.525598]\n",
      "[D loss: 0.607533] [G loss: 1.442065]\n",
      "[D loss: 0.987332] [G loss: 1.389423]\n",
      "[D loss: 0.937802] [G loss: 1.249825]\n",
      "[D loss: 0.871900] [G loss: 1.518292]\n",
      "[D loss: 0.816455] [G loss: 1.680503]\n",
      "[D loss: 0.960189] [G loss: 1.567772]\n",
      "[D loss: 1.161279] [G loss: 1.433362]\n",
      "[D loss: 0.610533] [G loss: 1.720982]\n",
      "[D loss: 0.772310] [G loss: 1.351762]\n",
      "[D loss: 0.953432] [G loss: 1.434831]\n",
      "[D loss: 1.148364] [G loss: 1.122778]\n",
      "[D loss: 0.757162] [G loss: 1.400726]\n",
      "[D loss: 0.645490] [G loss: 1.466743]\n",
      "[D loss: 0.861617] [G loss: 1.481741]\n",
      "[D loss: 0.741762] [G loss: 1.529288]\n",
      "[D loss: 0.636800] [G loss: 1.578539]\n",
      "[D loss: 0.825450] [G loss: 1.316958]\n",
      "[D loss: 0.872811] [G loss: 1.404635]\n",
      "[D loss: 1.148190] [G loss: 1.537545]\n",
      "[D loss: 1.004094] [G loss: 1.370146]\n",
      "[D loss: 0.784604] [G loss: 1.318003]\n",
      "[D loss: 0.955430] [G loss: 1.348383]\n",
      "[D loss: 0.850149] [G loss: 1.392482]\n",
      "[D loss: 0.776827] [G loss: 1.351836]\n",
      "[D loss: 0.865076] [G loss: 1.431174]\n",
      "[D loss: 0.933813] [G loss: 1.558116]\n",
      "[D loss: 1.150993] [G loss: 1.359664]\n",
      "[D loss: 0.751891] [G loss: 1.424294]\n",
      "[D loss: 0.767212] [G loss: 1.539396]\n",
      "[D loss: 1.143019] [G loss: 1.573543]\n",
      "[D loss: 0.714685] [G loss: 1.404826]\n",
      "[D loss: 0.644820] [G loss: 1.308801]\n",
      "[D loss: 0.791999] [G loss: 1.507443]\n",
      "[D loss: 0.670743] [G loss: 1.528463]\n",
      "[D loss: 0.860954] [G loss: 1.461249]\n",
      "[D loss: 0.841123] [G loss: 1.321406]\n",
      "[D loss: 0.709371] [G loss: 1.328482]\n",
      "[D loss: 0.940490] [G loss: 1.519727]\n",
      "[D loss: 0.842761] [G loss: 1.570969]\n",
      "[D loss: 0.801958] [G loss: 1.397408]\n",
      "[D loss: 0.834790] [G loss: 1.361666]\n",
      "[D loss: 0.819869] [G loss: 1.494025]\n",
      "[D loss: 0.938693] [G loss: 1.509275]\n",
      "[D loss: 0.833655] [G loss: 1.418465]\n",
      "[D loss: 0.832483] [G loss: 1.705008]\n",
      "[D loss: 0.857566] [G loss: 1.569373]\n",
      "[D loss: 0.755643] [G loss: 1.245623]\n",
      "[D loss: 0.911991] [G loss: 1.423169]\n",
      "[D loss: 0.830352] [G loss: 1.273894]\n",
      "[D loss: 0.892937] [G loss: 1.636731]\n",
      "[D loss: 0.961290] [G loss: 1.365370]\n",
      "[D loss: 0.710249] [G loss: 1.447112]\n",
      "[D loss: 0.627721] [G loss: 1.474890]\n",
      "[D loss: 0.873363] [G loss: 1.343516]\n",
      "[D loss: 1.195601] [G loss: 1.610042]\n",
      "[D loss: 0.863222] [G loss: 1.423528]\n",
      "[D loss: 0.838893] [G loss: 1.287724]\n",
      "[D loss: 0.844004] [G loss: 1.304561]\n",
      "[D loss: 0.848494] [G loss: 1.290776]\n",
      "[D loss: 0.779651] [G loss: 1.316813]\n",
      "[D loss: 0.742442] [G loss: 1.427891]\n",
      "[D loss: 0.904890] [G loss: 1.315450]\n",
      "[D loss: 0.818185] [G loss: 1.517899]\n",
      "[D loss: 0.910687] [G loss: 1.664203]\n",
      "[D loss: 0.910541] [G loss: 1.269809]\n",
      "[D loss: 0.797917] [G loss: 1.286261]\n",
      "[D loss: 0.752910] [G loss: 1.400691]\n",
      "[D loss: 0.907344] [G loss: 1.416165]\n",
      "[D loss: 0.848500] [G loss: 1.555210]\n",
      "[D loss: 0.949665] [G loss: 1.346105]\n",
      "[D loss: 0.836232] [G loss: 1.399992]\n",
      "[D loss: 0.756527] [G loss: 1.421368]\n",
      "[D loss: 0.779738] [G loss: 1.384505]\n",
      "[D loss: 0.810600] [G loss: 1.427964]\n",
      "[D loss: 0.845444] [G loss: 1.569436]\n",
      "[D loss: 0.999111] [G loss: 1.364120]\n",
      "[D loss: 0.829987] [G loss: 1.384841]\n",
      "[D loss: 0.837756] [G loss: 1.251048]\n",
      "[D loss: 0.701240] [G loss: 1.470352]\n",
      "[D loss: 0.833502] [G loss: 1.406242]\n",
      "[D loss: 0.736310] [G loss: 1.590120]\n",
      "[D loss: 0.682413] [G loss: 1.790458]\n",
      "[D loss: 0.741257] [G loss: 1.712032]\n",
      "[D loss: 0.722368] [G loss: 1.388801]\n",
      "[D loss: 0.723039] [G loss: 1.621368]\n",
      "[D loss: 0.665692] [G loss: 1.450252]\n",
      "[D loss: 0.614462] [G loss: 1.609370]\n",
      "[D loss: 0.844076] [G loss: 1.481942]\n",
      "[D loss: 0.873167] [G loss: 1.530678]\n",
      "[D loss: 0.664481] [G loss: 1.682859]\n",
      "[D loss: 0.743579] [G loss: 1.758575]\n",
      "[D loss: 0.900692] [G loss: 1.494328]\n",
      "[D loss: 0.791381] [G loss: 1.596793]\n",
      "[D loss: 1.022420] [G loss: 1.182687]\n",
      "[D loss: 0.737144] [G loss: 1.558374]\n",
      "[D loss: 0.866938] [G loss: 1.396992]\n",
      "[D loss: 0.942963] [G loss: 1.382436]\n",
      "[D loss: 0.780325] [G loss: 1.401142]\n",
      "[D loss: 0.754569] [G loss: 1.515675]\n",
      "[D loss: 1.109036] [G loss: 1.215125]\n",
      "[D loss: 0.701776] [G loss: 1.485437]\n",
      "[D loss: 0.576060] [G loss: 1.437640]\n",
      "[D loss: 0.776861] [G loss: 1.548089]\n",
      "[D loss: 0.915329] [G loss: 1.479017]\n",
      "[D loss: 0.845542] [G loss: 1.514715]\n",
      "[D loss: 0.893813] [G loss: 1.551376]\n",
      "[D loss: 0.796513] [G loss: 1.620183]\n",
      "[D loss: 0.843192] [G loss: 1.578263]\n",
      "[D loss: 1.043796] [G loss: 1.528868]\n",
      "[D loss: 0.920537] [G loss: 1.467157]\n",
      "[D loss: 0.960385] [G loss: 1.329812]\n",
      "[D loss: 0.995192] [G loss: 1.184613]\n",
      "[D loss: 0.794519] [G loss: 1.388893]\n",
      "[D loss: 0.873498] [G loss: 1.260494]\n",
      "[D loss: 0.861825] [G loss: 1.349517]\n",
      "[D loss: 0.732171] [G loss: 1.435629]\n",
      "[D loss: 0.775785] [G loss: 1.828759]\n",
      "[D loss: 0.911212] [G loss: 1.440660]\n",
      "[D loss: 0.651191] [G loss: 1.472100]\n",
      "[D loss: 0.812572] [G loss: 1.272619]\n",
      "[D loss: 0.878377] [G loss: 1.059876]\n",
      "[D loss: 0.751040] [G loss: 1.567700]\n",
      "[D loss: 0.843636] [G loss: 1.489096]\n",
      "[D loss: 0.702606] [G loss: 1.624106]\n",
      "[D loss: 0.792096] [G loss: 1.613125]\n",
      "[D loss: 0.878280] [G loss: 1.294436]\n",
      "[D loss: 0.947258] [G loss: 1.423684]\n",
      "[D loss: 0.722441] [G loss: 1.563828]\n",
      "[D loss: 0.856037] [G loss: 1.392649]\n",
      "[D loss: 1.161281] [G loss: 1.567369]\n",
      "[D loss: 0.862908] [G loss: 1.545505]\n",
      "[D loss: 0.810053] [G loss: 1.344349]\n",
      "[D loss: 0.819442] [G loss: 1.492807]\n",
      "[D loss: 0.893989] [G loss: 1.516093]\n",
      "[D loss: 0.828046] [G loss: 1.303425]\n",
      "[D loss: 0.868359] [G loss: 1.415455]\n",
      "[D loss: 0.720764] [G loss: 1.443325]\n",
      "[D loss: 0.643739] [G loss: 1.604558]\n",
      "[D loss: 0.793174] [G loss: 1.466830]\n",
      "[D loss: 0.692879] [G loss: 1.311715]\n",
      "[D loss: 1.065726] [G loss: 1.484179]\n",
      "[D loss: 0.908333] [G loss: 1.469930]\n",
      "[D loss: 0.931505] [G loss: 1.542746]\n",
      "[D loss: 0.839282] [G loss: 1.518980]\n",
      "[D loss: 0.892087] [G loss: 1.282827]\n",
      "[D loss: 1.085670] [G loss: 1.406764]\n",
      "[D loss: 0.754110] [G loss: 1.478812]\n",
      "[D loss: 0.811691] [G loss: 1.351021]\n",
      "[D loss: 0.902709] [G loss: 1.294095]\n",
      "[D loss: 0.754614] [G loss: 1.327041]\n",
      "[D loss: 0.721052] [G loss: 1.380040]\n",
      "[D loss: 0.850533] [G loss: 1.479808]\n",
      "[D loss: 1.089821] [G loss: 1.367661]\n",
      "[D loss: 0.839792] [G loss: 1.487437]\n",
      "[D loss: 0.936491] [G loss: 1.631810]\n",
      "[D loss: 0.891128] [G loss: 1.575590]\n",
      "[D loss: 0.956399] [G loss: 1.384610]\n",
      "[D loss: 0.691334] [G loss: 1.402351]\n",
      "[D loss: 0.782900] [G loss: 1.461121]\n",
      "[D loss: 1.021898] [G loss: 1.271253]\n",
      "[D loss: 0.669213] [G loss: 1.434745]\n",
      "[D loss: 0.557613] [G loss: 1.388745]\n",
      "[D loss: 0.904054] [G loss: 1.331028]\n",
      "[D loss: 0.923002] [G loss: 1.420272]\n",
      "[D loss: 0.884632] [G loss: 1.665950]\n",
      "[D loss: 0.892551] [G loss: 1.318336]\n",
      "[D loss: 0.813787] [G loss: 1.234520]\n",
      "[D loss: 0.831970] [G loss: 1.520468]\n",
      "[D loss: 0.946083] [G loss: 1.452104]\n",
      "[D loss: 0.813184] [G loss: 1.657773]\n",
      "[D loss: 0.720826] [G loss: 1.475108]\n",
      "[D loss: 0.794552] [G loss: 1.442115]\n",
      "[D loss: 0.583020] [G loss: 1.552722]\n",
      "[D loss: 0.806673] [G loss: 1.399940]\n",
      "[D loss: 0.532913] [G loss: 1.600146]\n",
      "[D loss: 0.845688] [G loss: 1.399469]\n",
      "[D loss: 0.988248] [G loss: 1.427367]\n",
      "[D loss: 1.005663] [G loss: 1.404324]\n",
      "[D loss: 0.933114] [G loss: 1.447663]\n",
      "[D loss: 0.827383] [G loss: 1.547455]\n",
      "[D loss: 0.712583] [G loss: 1.479452]\n",
      "[D loss: 0.819152] [G loss: 1.357218]\n",
      "[D loss: 0.756638] [G loss: 1.419777]\n",
      "[D loss: 0.894473] [G loss: 1.317398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.924589] [G loss: 1.269801]\n",
      "[D loss: 0.792197] [G loss: 1.455104]\n",
      "[D loss: 0.812379] [G loss: 1.382206]\n",
      "[D loss: 0.559649] [G loss: 1.535625]\n",
      "[D loss: 0.872818] [G loss: 1.435534]\n",
      "[D loss: 0.781396] [G loss: 1.497716]\n",
      "[D loss: 0.731126] [G loss: 1.412078]\n",
      "[D loss: 1.134957] [G loss: 1.585063]\n",
      "[D loss: 0.853642] [G loss: 1.504045]\n",
      "[D loss: 0.744443] [G loss: 1.141657]\n",
      "[D loss: 0.779847] [G loss: 1.444268]\n",
      "[D loss: 0.837882] [G loss: 1.505044]\n",
      "[D loss: 0.752266] [G loss: 1.635997]\n",
      "[D loss: 0.636388] [G loss: 1.699226]\n",
      "[D loss: 0.829267] [G loss: 1.419428]\n",
      "[D loss: 0.841919] [G loss: 1.392651]\n",
      "[D loss: 0.818086] [G loss: 1.364326]\n",
      "[D loss: 0.780535] [G loss: 1.682570]\n",
      "[D loss: 0.901929] [G loss: 1.653954]\n",
      "[D loss: 1.102508] [G loss: 1.517347]\n",
      "[D loss: 0.843930] [G loss: 1.338295]\n",
      "[D loss: 1.173576] [G loss: 1.062465]\n",
      "[D loss: 0.672341] [G loss: 1.349540]\n",
      "[D loss: 0.692791] [G loss: 1.338660]\n",
      "[D loss: 0.694929] [G loss: 1.428231]\n",
      "[D loss: 0.844259] [G loss: 1.572345]\n",
      "[D loss: 0.654969] [G loss: 1.454557]\n",
      "[D loss: 0.886759] [G loss: 1.291738]\n",
      "[D loss: 0.763899] [G loss: 1.597947]\n",
      "[D loss: 0.817913] [G loss: 1.574761]\n",
      "[D loss: 0.804658] [G loss: 1.405150]\n",
      "[D loss: 1.188100] [G loss: 1.289232]\n",
      "[D loss: 0.920278] [G loss: 1.464628]\n",
      "[D loss: 0.818171] [G loss: 1.658262]\n",
      "[D loss: 0.663031] [G loss: 1.559949]\n",
      "[D loss: 1.025068] [G loss: 1.259051]\n",
      "[D loss: 0.803174] [G loss: 1.450983]\n",
      "[D loss: 0.812121] [G loss: 1.575958]\n",
      "[D loss: 0.876826] [G loss: 1.570963]\n",
      "[D loss: 0.614445] [G loss: 1.449028]\n",
      "[D loss: 1.067069] [G loss: 1.622124]\n",
      "[D loss: 0.764430] [G loss: 1.479574]\n",
      "[D loss: 0.840878] [G loss: 1.646713]\n",
      "[D loss: 0.840438] [G loss: 1.493155]\n",
      "[D loss: 0.935590] [G loss: 1.275199]\n",
      "[D loss: 0.819776] [G loss: 1.417354]\n",
      "[D loss: 0.763469] [G loss: 1.507443]\n",
      "[D loss: 0.664757] [G loss: 1.437383]\n",
      "[D loss: 0.910804] [G loss: 1.516304]\n",
      "[D loss: 0.725665] [G loss: 1.455237]\n",
      "[D loss: 0.903518] [G loss: 1.515043]\n",
      "[D loss: 0.778409] [G loss: 1.341230]\n",
      "[D loss: 0.917704] [G loss: 1.116310]\n",
      "[D loss: 0.590759] [G loss: 1.462018]\n",
      "[D loss: 0.813521] [G loss: 1.328175]\n",
      "[D loss: 1.080204] [G loss: 1.470753]\n",
      "[D loss: 0.697949] [G loss: 1.419516]\n",
      "[D loss: 0.593923] [G loss: 1.530596]\n",
      "[D loss: 0.911887] [G loss: 1.462051]\n",
      "[D loss: 0.866508] [G loss: 1.316551]\n",
      "[D loss: 0.801363] [G loss: 1.305197]\n",
      "[D loss: 0.749318] [G loss: 1.498791]\n",
      "[D loss: 0.721294] [G loss: 1.764577]\n",
      "[D loss: 0.861547] [G loss: 1.457757]\n",
      "[D loss: 0.805596] [G loss: 1.642693]\n",
      "[D loss: 0.963049] [G loss: 1.500480]\n",
      "[D loss: 0.804102] [G loss: 1.603384]\n",
      "[D loss: 0.748586] [G loss: 1.607602]\n",
      "[D loss: 0.696957] [G loss: 1.491493]\n",
      "[D loss: 0.855413] [G loss: 1.504535]\n",
      "[D loss: 0.945239] [G loss: 1.480241]\n",
      "[D loss: 0.995403] [G loss: 1.517380]\n",
      "[D loss: 0.785474] [G loss: 1.536834]\n",
      "[D loss: 0.897503] [G loss: 1.482898]\n",
      "[D loss: 0.806949] [G loss: 1.666704]\n",
      "[D loss: 0.936506] [G loss: 1.847139]\n",
      "[D loss: 0.738592] [G loss: 1.568232]\n",
      "[D loss: 0.965441] [G loss: 1.546354]\n",
      "[D loss: 0.731750] [G loss: 1.664300]\n",
      "[D loss: 0.615953] [G loss: 1.428202]\n",
      "[D loss: 1.091767] [G loss: 1.337930]\n",
      "[D loss: 0.949883] [G loss: 1.384707]\n",
      "[D loss: 0.555445] [G loss: 1.473669]\n",
      "[D loss: 1.052605] [G loss: 1.331871]\n",
      "[D loss: 0.746267] [G loss: 1.551633]\n",
      "[D loss: 0.912983] [G loss: 1.403408]\n",
      "[D loss: 0.899614] [G loss: 1.718212]\n",
      "[D loss: 1.026130] [G loss: 1.245123]\n",
      "[D loss: 0.791129] [G loss: 1.381058]\n",
      "[D loss: 0.945661] [G loss: 1.412382]\n",
      "[D loss: 1.039099] [G loss: 1.447488]\n",
      "[D loss: 0.849941] [G loss: 1.455513]\n",
      "[D loss: 0.788655] [G loss: 1.401207]\n",
      "[D loss: 0.765622] [G loss: 1.390238]\n",
      "[D loss: 0.735941] [G loss: 1.618780]\n",
      "[D loss: 0.779450] [G loss: 1.286961]\n",
      "[D loss: 0.834619] [G loss: 1.294844]\n",
      "[D loss: 0.613754] [G loss: 1.516146]\n",
      "[D loss: 0.726423] [G loss: 1.466002]\n",
      "[D loss: 0.872186] [G loss: 1.505088]\n",
      "[D loss: 0.732049] [G loss: 1.407303]\n",
      "[D loss: 0.614906] [G loss: 1.398440]\n",
      "[D loss: 0.910609] [G loss: 1.512538]\n",
      "[D loss: 0.829825] [G loss: 1.534259]\n",
      "[D loss: 0.947935] [G loss: 1.508597]\n",
      "[D loss: 0.892765] [G loss: 1.601436]\n",
      "[D loss: 0.904715] [G loss: 1.759701]\n",
      "[D loss: 0.625459] [G loss: 1.589608]\n",
      "[D loss: 0.799357] [G loss: 1.508449]\n",
      "[D loss: 0.819661] [G loss: 1.453095]\n",
      "[D loss: 0.564792] [G loss: 1.541901]\n",
      "[D loss: 0.851724] [G loss: 1.452078]\n",
      "[D loss: 0.795843] [G loss: 1.617837]\n",
      "[D loss: 0.784038] [G loss: 1.504481]\n",
      "[D loss: 0.954203] [G loss: 1.457653]\n",
      "[D loss: 1.013760] [G loss: 1.567612]\n",
      "[D loss: 0.786871] [G loss: 1.504633]\n",
      "[D loss: 0.842519] [G loss: 1.348376]\n",
      "[D loss: 0.813791] [G loss: 1.354735]\n",
      "[D loss: 0.936505] [G loss: 1.437523]\n",
      "[D loss: 0.932498] [G loss: 1.365761]\n",
      "[D loss: 0.756120] [G loss: 1.390523]\n",
      "[D loss: 0.939030] [G loss: 1.436319]\n",
      "[D loss: 0.621236] [G loss: 1.434575]\n",
      "[D loss: 0.652425] [G loss: 1.474436]\n",
      "[D loss: 0.885465] [G loss: 1.463932]\n",
      "[D loss: 0.700758] [G loss: 1.627285]\n",
      "[D loss: 0.995950] [G loss: 1.464321]\n",
      "[D loss: 0.760754] [G loss: 1.429590]\n",
      "[D loss: 1.038738] [G loss: 1.493026]\n",
      "[D loss: 0.858936] [G loss: 1.177969]\n",
      "[D loss: 1.063324] [G loss: 1.297455]\n",
      "[D loss: 0.932209] [G loss: 1.557235]\n",
      "[D loss: 0.858405] [G loss: 1.556601]\n",
      "[D loss: 0.911576] [G loss: 1.701040]\n",
      "[D loss: 0.833705] [G loss: 1.363517]\n",
      "[D loss: 0.935746] [G loss: 1.261781]\n",
      "[D loss: 0.826477] [G loss: 1.497310]\n",
      "[D loss: 1.031549] [G loss: 1.169289]\n",
      "[D loss: 0.847055] [G loss: 1.331368]\n",
      "[D loss: 0.905660] [G loss: 1.487989]\n",
      "[D loss: 0.928939] [G loss: 1.283199]\n",
      "[D loss: 0.966507] [G loss: 1.595094]\n",
      "[D loss: 1.142695] [G loss: 1.320392]\n",
      "[D loss: 0.725988] [G loss: 1.557302]\n",
      "[D loss: 1.046514] [G loss: 1.264359]\n",
      "[D loss: 0.838007] [G loss: 1.407322]\n",
      "[D loss: 0.765140] [G loss: 1.335930]\n",
      "[D loss: 0.867749] [G loss: 1.383809]\n",
      "[D loss: 0.873015] [G loss: 1.230373]\n",
      "[D loss: 0.969474] [G loss: 1.273934]\n",
      "[D loss: 0.917433] [G loss: 1.306562]\n",
      "[D loss: 0.674077] [G loss: 1.571387]\n",
      "[D loss: 0.813263] [G loss: 1.344101]\n",
      "[D loss: 0.820885] [G loss: 1.407997]\n",
      "[D loss: 0.868547] [G loss: 1.366449]\n",
      "[D loss: 0.847950] [G loss: 1.370882]\n",
      "[D loss: 0.793546] [G loss: 1.466393]\n",
      "[D loss: 0.785889] [G loss: 1.428734]\n",
      "[D loss: 0.839409] [G loss: 1.404663]\n",
      "[D loss: 0.777412] [G loss: 1.324720]\n",
      "[D loss: 0.866973] [G loss: 1.529951]\n",
      "[D loss: 0.664952] [G loss: 1.792103]\n",
      "[D loss: 1.087468] [G loss: 1.590680]\n",
      "[D loss: 0.671432] [G loss: 1.502997]\n",
      "[D loss: 0.789485] [G loss: 1.569428]\n",
      "[D loss: 0.790464] [G loss: 1.321439]\n",
      "[D loss: 0.944217] [G loss: 1.423804]\n",
      "[D loss: 0.813239] [G loss: 1.475980]\n",
      "[D loss: 1.082824] [G loss: 1.137745]\n",
      "[D loss: 0.705561] [G loss: 1.521919]\n",
      "[D loss: 0.680072] [G loss: 1.461892]\n",
      "[D loss: 0.883180] [G loss: 1.446054]\n",
      "[D loss: 0.979394] [G loss: 1.488329]\n",
      "[D loss: 0.766135] [G loss: 1.338821]\n",
      "[D loss: 0.895024] [G loss: 1.526590]\n",
      "[D loss: 0.886331] [G loss: 1.469578]\n",
      "[D loss: 0.742226] [G loss: 1.421443]\n",
      "[D loss: 0.870058] [G loss: 1.217952]\n",
      "[D loss: 0.811650] [G loss: 1.289318]\n",
      "[D loss: 0.632906] [G loss: 1.752814]\n",
      "[D loss: 0.904761] [G loss: 1.554174]\n",
      "[D loss: 0.501037] [G loss: 1.658962]\n",
      "[D loss: 1.202750] [G loss: 1.814312]\n",
      "[D loss: 0.953412] [G loss: 1.461897]\n",
      "[D loss: 0.732513] [G loss: 1.515310]\n",
      "[D loss: 0.765134] [G loss: 1.597775]\n",
      "[D loss: 0.891085] [G loss: 1.364861]\n",
      "[D loss: 0.700978] [G loss: 1.386648]\n",
      "[D loss: 1.289827] [G loss: 1.322538]\n",
      "[D loss: 0.776525] [G loss: 1.563844]\n",
      "[D loss: 1.079314] [G loss: 1.328505]\n",
      "[D loss: 1.092193] [G loss: 1.389656]\n",
      "[D loss: 0.829756] [G loss: 1.308645]\n",
      "[D loss: 1.032142] [G loss: 1.409558]\n",
      "[D loss: 0.852196] [G loss: 1.435006]\n",
      "[D loss: 0.866765] [G loss: 1.307412]\n",
      "[D loss: 0.687836] [G loss: 1.463258]\n",
      "[D loss: 0.818079] [G loss: 1.516654]\n",
      "[D loss: 0.716423] [G loss: 1.647889]\n",
      "[D loss: 0.961274] [G loss: 1.447791]\n",
      "[D loss: 0.612187] [G loss: 1.498911]\n",
      "[D loss: 0.850824] [G loss: 1.380890]\n",
      "[D loss: 0.717227] [G loss: 1.436644]\n",
      "[D loss: 0.829792] [G loss: 1.189233]\n",
      "[D loss: 0.759795] [G loss: 1.509941]\n",
      "[D loss: 0.843446] [G loss: 1.400779]\n",
      "[D loss: 0.743242] [G loss: 1.569562]\n",
      "[D loss: 0.769556] [G loss: 1.459609]\n",
      "[D loss: 0.983939] [G loss: 1.408983]\n",
      "[D loss: 0.753998] [G loss: 1.309262]\n",
      "[D loss: 0.803732] [G loss: 1.581077]\n",
      "[D loss: 0.837573] [G loss: 1.407292]\n",
      "[D loss: 0.981939] [G loss: 1.501291]\n",
      "[D loss: 1.150388] [G loss: 1.247606]\n",
      "[D loss: 0.850539] [G loss: 1.323176]\n",
      "[D loss: 0.990043] [G loss: 1.322552]\n",
      "[D loss: 0.721759] [G loss: 1.390396]\n",
      "[D loss: 0.863472] [G loss: 1.230672]\n",
      "[D loss: 0.856996] [G loss: 1.293436]\n",
      "[D loss: 0.717890] [G loss: 1.348819]\n",
      "[D loss: 1.176357] [G loss: 1.216073]\n",
      "[D loss: 0.453971] [G loss: 1.403090]\n",
      "[D loss: 0.878458] [G loss: 1.523063]\n",
      "[D loss: 0.740921] [G loss: 1.620941]\n",
      "[D loss: 0.753266] [G loss: 1.508847]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 1.024129] [G loss: 1.238098]\n",
      "[D loss: 0.831053] [G loss: 1.447485]\n",
      "[D loss: 1.043613] [G loss: 1.288557]\n",
      "[D loss: 0.532294] [G loss: 1.517605]\n",
      "[D loss: 0.605537] [G loss: 1.609347]\n",
      "[D loss: 0.773925] [G loss: 1.593652]\n",
      "[D loss: 0.778408] [G loss: 1.509607]\n",
      "[D loss: 0.741179] [G loss: 1.418089]\n",
      "[D loss: 0.939075] [G loss: 1.416459]\n",
      "[D loss: 0.972631] [G loss: 1.364731]\n",
      "[D loss: 0.998695] [G loss: 1.408517]\n",
      "[D loss: 0.931332] [G loss: 1.426384]\n",
      "[D loss: 0.982665] [G loss: 1.256340]\n",
      "[D loss: 0.628626] [G loss: 1.657662]\n",
      "[D loss: 0.770653] [G loss: 1.457560]\n",
      "[D loss: 1.120375] [G loss: 1.121241]\n",
      "[D loss: 0.971935] [G loss: 1.273261]\n",
      "[D loss: 0.887928] [G loss: 1.465629]\n",
      "[D loss: 0.769727] [G loss: 1.568193]\n",
      "[D loss: 0.772656] [G loss: 1.390239]\n",
      "[D loss: 0.859497] [G loss: 1.542788]\n",
      "[D loss: 0.751069] [G loss: 1.436312]\n",
      "[D loss: 0.704303] [G loss: 1.368404]\n",
      "[D loss: 0.895570] [G loss: 1.475493]\n",
      "[D loss: 0.840135] [G loss: 1.446173]\n",
      "[D loss: 0.861225] [G loss: 1.481548]\n",
      "[D loss: 0.749358] [G loss: 1.639014]\n",
      "[D loss: 0.561115] [G loss: 1.617625]\n",
      "[D loss: 0.738744] [G loss: 1.361541]\n",
      "[D loss: 0.792483] [G loss: 1.454120]\n",
      "[D loss: 0.802654] [G loss: 1.537777]\n",
      "[D loss: 0.740225] [G loss: 1.460046]\n",
      "[D loss: 0.862977] [G loss: 1.499705]\n",
      "[D loss: 0.979443] [G loss: 1.583174]\n",
      "[D loss: 0.796986] [G loss: 1.489730]\n",
      "[D loss: 1.000120] [G loss: 1.434489]\n",
      "[D loss: 0.830454] [G loss: 1.650206]\n",
      "[D loss: 0.860295] [G loss: 1.482273]\n",
      "[D loss: 0.946269] [G loss: 1.434269]\n",
      "[D loss: 0.925264] [G loss: 1.441066]\n",
      "[D loss: 0.651701] [G loss: 1.560078]\n",
      "[D loss: 0.776915] [G loss: 1.348721]\n",
      "[D loss: 0.766355] [G loss: 1.388642]\n",
      "[D loss: 0.668280] [G loss: 1.667057]\n",
      "[D loss: 0.725074] [G loss: 1.674072]\n",
      "[D loss: 0.715742] [G loss: 1.512454]\n",
      "[D loss: 0.748899] [G loss: 1.711881]\n",
      "[D loss: 0.557667] [G loss: 1.620332]\n",
      "[D loss: 0.608616] [G loss: 1.726979]\n",
      "[D loss: 0.826868] [G loss: 1.455368]\n",
      "[D loss: 1.009674] [G loss: 1.487828]\n",
      "[D loss: 1.082271] [G loss: 1.332591]\n",
      "[D loss: 1.084205] [G loss: 1.535821]\n",
      "[D loss: 1.033571] [G loss: 1.315940]\n",
      "[D loss: 0.854586] [G loss: 1.309361]\n",
      "[D loss: 1.091686] [G loss: 1.343976]\n",
      "[D loss: 0.648434] [G loss: 1.444414]\n",
      "[D loss: 0.850514] [G loss: 1.297603]\n",
      "[D loss: 1.040481] [G loss: 1.381704]\n",
      "[D loss: 0.965704] [G loss: 1.335730]\n",
      "[D loss: 0.783886] [G loss: 1.589416]\n",
      "[D loss: 0.753610] [G loss: 1.433786]\n",
      "[D loss: 0.815980] [G loss: 1.503409]\n",
      "[D loss: 0.859043] [G loss: 1.506981]\n",
      "[D loss: 0.782509] [G loss: 1.531945]\n",
      "[D loss: 0.672596] [G loss: 1.329978]\n",
      "[D loss: 0.708046] [G loss: 1.467028]\n",
      "[D loss: 0.839497] [G loss: 1.385463]\n",
      "[D loss: 0.751134] [G loss: 1.525860]\n",
      "[D loss: 0.800995] [G loss: 1.374811]\n",
      "[D loss: 0.813188] [G loss: 1.294503]\n",
      "[D loss: 0.614450] [G loss: 1.360307]\n",
      "[D loss: 0.787828] [G loss: 1.563560]\n",
      "[D loss: 0.967858] [G loss: 1.588159]\n",
      "[D loss: 0.827996] [G loss: 1.658374]\n",
      "[D loss: 0.841580] [G loss: 1.428025]\n",
      "[D loss: 0.692513] [G loss: 1.466518]\n",
      "[D loss: 0.719130] [G loss: 1.653895]\n",
      "[D loss: 0.848850] [G loss: 1.234115]\n",
      "[D loss: 0.621115] [G loss: 1.709288]\n",
      "[D loss: 0.680799] [G loss: 1.744702]\n",
      "[D loss: 0.685422] [G loss: 1.833436]\n",
      "[D loss: 0.930570] [G loss: 1.611522]\n",
      "[D loss: 0.753017] [G loss: 1.486581]\n",
      "[D loss: 0.695429] [G loss: 1.781028]\n",
      "[D loss: 0.970408] [G loss: 1.498704]\n",
      "[D loss: 0.776487] [G loss: 1.504986]\n",
      "[D loss: 0.740825] [G loss: 1.573949]\n",
      "[D loss: 0.897699] [G loss: 1.544290]\n",
      "[D loss: 0.798460] [G loss: 1.427704]\n",
      "[D loss: 0.782693] [G loss: 1.240551]\n",
      "[D loss: 0.643806] [G loss: 1.592742]\n",
      "[D loss: 0.643958] [G loss: 1.714128]\n",
      "[D loss: 0.771201] [G loss: 1.389670]\n",
      "[D loss: 0.719891] [G loss: 1.609229]\n",
      "[D loss: 0.903589] [G loss: 1.505090]\n",
      "[D loss: 0.642702] [G loss: 1.326422]\n",
      "[D loss: 0.845465] [G loss: 1.433662]\n",
      "[D loss: 1.068619] [G loss: 1.469617]\n",
      "[D loss: 0.609436] [G loss: 1.508192]\n",
      "[D loss: 0.775025] [G loss: 1.636754]\n",
      "[D loss: 0.772896] [G loss: 1.695397]\n",
      "[D loss: 0.675610] [G loss: 1.502192]\n",
      "[D loss: 0.731303] [G loss: 1.417557]\n",
      "[D loss: 0.627581] [G loss: 1.741004]\n",
      "[D loss: 0.890920] [G loss: 1.508311]\n",
      "[D loss: 1.034966] [G loss: 1.570296]\n",
      "[D loss: 0.858096] [G loss: 1.360218]\n",
      "[D loss: 0.607069] [G loss: 1.627001]\n",
      "[D loss: 0.574013] [G loss: 1.465368]\n",
      "[D loss: 1.066529] [G loss: 1.253882]\n",
      "[D loss: 0.840115] [G loss: 1.566136]\n",
      "[D loss: 0.623429] [G loss: 1.950533]\n",
      "[D loss: 0.972291] [G loss: 1.653052]\n",
      "[D loss: 0.670193] [G loss: 1.419678]\n",
      "[D loss: 0.881636] [G loss: 1.491375]\n",
      "[D loss: 0.814591] [G loss: 1.337601]\n",
      "[D loss: 0.936004] [G loss: 1.307950]\n",
      "[D loss: 0.753874] [G loss: 1.332073]\n",
      "[D loss: 0.870682] [G loss: 1.503054]\n",
      "[D loss: 0.842792] [G loss: 1.498890]\n",
      "[D loss: 0.817567] [G loss: 1.646291]\n",
      "[D loss: 0.733941] [G loss: 1.574288]\n",
      "[D loss: 0.616923] [G loss: 1.583230]\n",
      "[D loss: 0.698515] [G loss: 1.633903]\n",
      "[D loss: 0.890794] [G loss: 1.482936]\n",
      "[D loss: 0.921104] [G loss: 1.614052]\n",
      "[D loss: 0.680597] [G loss: 1.702471]\n",
      "[D loss: 0.765889] [G loss: 1.516187]\n",
      "[D loss: 0.926144] [G loss: 1.488575]\n",
      "[D loss: 1.049247] [G loss: 1.449053]\n",
      "[D loss: 1.182895] [G loss: 1.232529]\n",
      "[D loss: 0.907486] [G loss: 1.589504]\n",
      "[D loss: 1.027831] [G loss: 1.653597]\n",
      "[D loss: 0.735339] [G loss: 1.592562]\n",
      "[D loss: 0.955982] [G loss: 1.464093]\n",
      "[D loss: 0.945116] [G loss: 1.418045]\n",
      "[D loss: 0.828136] [G loss: 1.338510]\n",
      "[D loss: 0.862507] [G loss: 1.344818]\n",
      "[D loss: 0.782234] [G loss: 1.514652]\n",
      "[D loss: 0.964320] [G loss: 1.497032]\n",
      "[D loss: 0.861120] [G loss: 1.270696]\n",
      "[D loss: 0.694889] [G loss: 1.557461]\n",
      "[D loss: 0.794427] [G loss: 1.344858]\n",
      "[D loss: 0.916105] [G loss: 1.367539]\n",
      "[D loss: 0.624120] [G loss: 1.458160]\n",
      "[D loss: 0.922700] [G loss: 1.469852]\n",
      "[D loss: 1.068698] [G loss: 1.453626]\n",
      "[D loss: 0.805872] [G loss: 1.470190]\n",
      "[D loss: 0.975595] [G loss: 1.333258]\n",
      "[D loss: 0.724903] [G loss: 1.532691]\n",
      "[D loss: 0.802990] [G loss: 1.366279]\n",
      "[D loss: 0.843957] [G loss: 1.540407]\n",
      "[D loss: 0.652854] [G loss: 1.693441]\n",
      "[D loss: 0.826474] [G loss: 1.510435]\n",
      "[D loss: 0.683158] [G loss: 1.480946]\n",
      "[D loss: 0.437317] [G loss: 1.715882]\n",
      "[D loss: 0.746010] [G loss: 1.386042]\n",
      "[D loss: 0.816564] [G loss: 1.741030]\n",
      "[D loss: 0.764035] [G loss: 1.578424]\n",
      "[D loss: 0.824894] [G loss: 1.292111]\n",
      "[D loss: 0.673205] [G loss: 1.540450]\n",
      "[D loss: 0.970649] [G loss: 1.539045]\n",
      "[D loss: 0.801771] [G loss: 1.654654]\n",
      "[D loss: 1.104182] [G loss: 1.508698]\n",
      "[D loss: 0.866370] [G loss: 1.814648]\n",
      "[D loss: 0.853370] [G loss: 1.578683]\n",
      "[D loss: 0.836896] [G loss: 1.323259]\n",
      "[D loss: 0.879577] [G loss: 1.403497]\n",
      "[D loss: 0.957320] [G loss: 1.334446]\n",
      "[D loss: 0.979567] [G loss: 1.297898]\n",
      "[D loss: 0.924085] [G loss: 1.448483]\n",
      "[D loss: 0.653397] [G loss: 1.675627]\n",
      "[D loss: 0.861335] [G loss: 1.416777]\n",
      "[D loss: 0.764544] [G loss: 1.381331]\n",
      "[D loss: 0.870141] [G loss: 1.432913]\n",
      "[D loss: 0.689660] [G loss: 1.489672]\n",
      "[D loss: 0.886838] [G loss: 1.175364]\n",
      "[D loss: 0.617855] [G loss: 1.653129]\n",
      "[D loss: 0.918630] [G loss: 1.554902]\n",
      "[D loss: 0.762289] [G loss: 1.521185]\n",
      "[D loss: 0.728170] [G loss: 1.527689]\n",
      "[D loss: 0.919929] [G loss: 1.272594]\n",
      "[D loss: 1.003724] [G loss: 1.376282]\n",
      "[D loss: 0.892568] [G loss: 1.566605]\n",
      "[D loss: 0.797830] [G loss: 1.480018]\n",
      "[D loss: 1.085278] [G loss: 1.659831]\n",
      "[D loss: 0.802779] [G loss: 1.616700]\n",
      "[D loss: 0.862916] [G loss: 1.478881]\n",
      "[D loss: 0.783366] [G loss: 1.351931]\n",
      "[D loss: 0.699608] [G loss: 1.439364]\n",
      "[D loss: 1.111096] [G loss: 1.393065]\n",
      "[D loss: 0.798120] [G loss: 1.516787]\n",
      "[D loss: 0.868957] [G loss: 1.299281]\n",
      "[D loss: 0.751486] [G loss: 1.494333]\n",
      "[D loss: 0.605904] [G loss: 1.616446]\n",
      "[D loss: 0.882277] [G loss: 1.603950]\n",
      "[D loss: 0.914877] [G loss: 1.521755]\n",
      "[D loss: 0.700462] [G loss: 1.553451]\n",
      "[D loss: 0.841470] [G loss: 1.298662]\n",
      "[D loss: 0.902849] [G loss: 1.254146]\n",
      "[D loss: 0.831376] [G loss: 1.547893]\n",
      "[D loss: 0.917920] [G loss: 1.589778]\n",
      "[D loss: 0.776989] [G loss: 1.473212]\n",
      "[D loss: 1.146093] [G loss: 1.456388]\n",
      "[D loss: 1.241097] [G loss: 1.503308]\n",
      "[D loss: 0.764978] [G loss: 1.502345]\n",
      "[D loss: 0.708150] [G loss: 1.551717]\n",
      "[D loss: 0.872182] [G loss: 1.609172]\n",
      "[D loss: 0.694703] [G loss: 1.756709]\n",
      "[D loss: 0.852447] [G loss: 1.317520]\n",
      "[D loss: 0.929311] [G loss: 1.386772]\n",
      "[D loss: 0.988458] [G loss: 1.399245]\n",
      "[D loss: 0.804518] [G loss: 1.466345]\n",
      "[D loss: 1.002052] [G loss: 1.672568]\n",
      "[D loss: 1.180688] [G loss: 1.389539]\n",
      "[D loss: 0.772228] [G loss: 1.285711]\n",
      "[D loss: 0.810760] [G loss: 1.304974]\n",
      "[D loss: 1.104378] [G loss: 1.385040]\n",
      "[D loss: 0.669797] [G loss: 1.306719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.878428] [G loss: 1.576967]\n",
      "[D loss: 0.784220] [G loss: 1.331874]\n",
      "[D loss: 0.912831] [G loss: 1.336422]\n",
      "[D loss: 1.219480] [G loss: 1.379618]\n",
      "[D loss: 0.834059] [G loss: 1.355314]\n",
      "[D loss: 0.553694] [G loss: 1.523209]\n",
      "[D loss: 0.977085] [G loss: 1.303333]\n",
      "[D loss: 0.690626] [G loss: 1.415686]\n",
      "[D loss: 0.812243] [G loss: 1.600636]\n",
      "[D loss: 0.875907] [G loss: 1.395339]\n",
      "[D loss: 1.073915] [G loss: 1.441876]\n",
      "[D loss: 0.691911] [G loss: 1.383140]\n",
      "[D loss: 0.578722] [G loss: 1.454205]\n",
      "[D loss: 0.777999] [G loss: 1.479374]\n",
      "[D loss: 0.861222] [G loss: 1.793040]\n",
      "[D loss: 0.838670] [G loss: 1.707783]\n",
      "[D loss: 0.920665] [G loss: 1.510840]\n",
      "[D loss: 1.086234] [G loss: 1.279427]\n",
      "[D loss: 0.710304] [G loss: 1.735168]\n",
      "[D loss: 0.842006] [G loss: 1.428442]\n",
      "[D loss: 0.701034] [G loss: 1.618493]\n",
      "[D loss: 0.886320] [G loss: 1.581504]\n",
      "[D loss: 0.876710] [G loss: 1.535757]\n",
      "[D loss: 0.847714] [G loss: 1.570644]\n",
      "[D loss: 0.872971] [G loss: 1.525275]\n",
      "[D loss: 0.712812] [G loss: 1.564119]\n",
      "[D loss: 0.832570] [G loss: 1.473940]\n",
      "[D loss: 0.833451] [G loss: 1.304908]\n",
      "[D loss: 0.772142] [G loss: 1.465715]\n",
      "[D loss: 0.820856] [G loss: 1.174618]\n",
      "[D loss: 0.843901] [G loss: 1.420874]\n",
      "[D loss: 0.852691] [G loss: 1.718119]\n",
      "[D loss: 0.760321] [G loss: 1.731203]\n",
      "[D loss: 0.840637] [G loss: 1.401989]\n",
      "[D loss: 0.820834] [G loss: 1.513427]\n",
      "[D loss: 1.129976] [G loss: 1.432729]\n",
      "[D loss: 0.922015] [G loss: 1.631391]\n",
      "[D loss: 0.751467] [G loss: 1.568536]\n",
      "[D loss: 0.904939] [G loss: 1.362622]\n",
      "[D loss: 0.790465] [G loss: 1.462695]\n",
      "[D loss: 0.862027] [G loss: 1.240833]\n",
      "[D loss: 0.842700] [G loss: 1.416026]\n",
      "[D loss: 0.704803] [G loss: 1.542835]\n",
      "[D loss: 0.769948] [G loss: 1.452122]\n",
      "[D loss: 0.798484] [G loss: 1.804626]\n",
      "[D loss: 0.773870] [G loss: 1.303231]\n",
      "[D loss: 1.071750] [G loss: 1.357170]\n",
      "[D loss: 0.842672] [G loss: 1.279669]\n",
      "[D loss: 0.845938] [G loss: 1.340652]\n",
      "[D loss: 0.829565] [G loss: 1.280672]\n",
      "[D loss: 0.882372] [G loss: 1.408489]\n",
      "[D loss: 1.117321] [G loss: 1.251011]\n",
      "[D loss: 0.785702] [G loss: 1.854097]\n",
      "[D loss: 0.671829] [G loss: 1.439938]\n",
      "[D loss: 0.802964] [G loss: 1.529336]\n",
      "[D loss: 0.909064] [G loss: 1.300486]\n",
      "[D loss: 0.782965] [G loss: 1.634777]\n",
      "[D loss: 1.031929] [G loss: 1.453194]\n",
      "[D loss: 0.750008] [G loss: 1.396375]\n",
      "[D loss: 0.982271] [G loss: 1.282063]\n",
      "[D loss: 0.798312] [G loss: 1.333859]\n",
      "[D loss: 0.843441] [G loss: 1.669271]\n",
      "[D loss: 0.968831] [G loss: 1.500938]\n",
      "[D loss: 0.760631] [G loss: 1.422851]\n",
      "[D loss: 0.904618] [G loss: 1.451091]\n",
      "[D loss: 1.036580] [G loss: 1.388732]\n",
      "[D loss: 0.816014] [G loss: 1.438355]\n",
      "[D loss: 0.772673] [G loss: 1.244137]\n",
      "[D loss: 0.830411] [G loss: 1.459632]\n",
      "[D loss: 0.790893] [G loss: 1.484691]\n",
      "[D loss: 0.727369] [G loss: 1.481756]\n",
      "[D loss: 0.930561] [G loss: 1.562668]\n",
      "[D loss: 0.745958] [G loss: 1.326058]\n",
      "[D loss: 0.905793] [G loss: 1.341811]\n",
      "[D loss: 0.650526] [G loss: 1.579217]\n",
      "[D loss: 0.756804] [G loss: 1.475271]\n",
      "[D loss: 0.794490] [G loss: 1.585334]\n",
      "[D loss: 0.970375] [G loss: 1.452555]\n",
      "[D loss: 0.816269] [G loss: 1.353119]\n",
      "[D loss: 0.784680] [G loss: 1.331809]\n",
      "[D loss: 0.885752] [G loss: 1.368111]\n",
      "[D loss: 0.832993] [G loss: 1.364622]\n",
      "[D loss: 0.769551] [G loss: 1.354783]\n",
      "[D loss: 0.684225] [G loss: 1.442801]\n",
      "[D loss: 0.641490] [G loss: 1.475047]\n",
      "[D loss: 0.704444] [G loss: 1.403023]\n",
      "[D loss: 0.885342] [G loss: 1.571560]\n",
      "[D loss: 0.978321] [G loss: 1.390143]\n",
      "[D loss: 0.678184] [G loss: 1.540593]\n",
      "[D loss: 0.866518] [G loss: 1.699573]\n",
      "[D loss: 0.916394] [G loss: 1.236037]\n",
      "[D loss: 0.706861] [G loss: 1.465647]\n",
      "[D loss: 0.787028] [G loss: 1.756400]\n",
      "[D loss: 0.748981] [G loss: 1.634856]\n",
      "[D loss: 0.911028] [G loss: 1.555664]\n",
      "[D loss: 0.947091] [G loss: 1.395470]\n",
      "[D loss: 0.865675] [G loss: 1.304215]\n",
      "[D loss: 0.854613] [G loss: 1.532816]\n",
      "[D loss: 0.905074] [G loss: 1.531766]\n",
      "[D loss: 0.856060] [G loss: 1.484830]\n",
      "[D loss: 0.832814] [G loss: 1.441010]\n",
      "[D loss: 0.731519] [G loss: 1.503406]\n",
      "[D loss: 0.844458] [G loss: 1.499001]\n",
      "[D loss: 0.505843] [G loss: 1.623416]\n",
      "[D loss: 0.808263] [G loss: 1.343561]\n",
      "[D loss: 1.023839] [G loss: 1.428796]\n",
      "[D loss: 0.737690] [G loss: 1.490274]\n",
      "[D loss: 0.721045] [G loss: 1.640256]\n",
      "[D loss: 0.815237] [G loss: 1.710344]\n",
      "[D loss: 1.042818] [G loss: 1.476393]\n",
      "[D loss: 0.791344] [G loss: 1.356040]\n",
      "[D loss: 0.690168] [G loss: 1.379455]\n",
      "[D loss: 0.870043] [G loss: 1.492885]\n",
      "[D loss: 0.745074] [G loss: 1.457952]\n",
      "[D loss: 0.897024] [G loss: 1.333835]\n",
      "[D loss: 0.667526] [G loss: 1.570040]\n",
      "[D loss: 0.795422] [G loss: 1.532548]\n",
      "[D loss: 0.794978] [G loss: 1.656711]\n",
      "[D loss: 0.726088] [G loss: 1.624821]\n",
      "[D loss: 1.074693] [G loss: 1.338537]\n",
      "[D loss: 0.756625] [G loss: 1.434296]\n",
      "[D loss: 0.705380] [G loss: 1.502367]\n",
      "[D loss: 0.527374] [G loss: 1.606662]\n",
      "[D loss: 0.826022] [G loss: 1.452814]\n",
      "[D loss: 0.890108] [G loss: 1.548343]\n",
      "[D loss: 1.104274] [G loss: 1.602411]\n",
      "[D loss: 0.844172] [G loss: 1.601826]\n",
      "[D loss: 0.773402] [G loss: 1.440503]\n",
      "[D loss: 1.010476] [G loss: 1.399332]\n",
      "[D loss: 0.806410] [G loss: 1.475527]\n",
      "[D loss: 0.887826] [G loss: 1.478228]\n",
      "[D loss: 0.600720] [G loss: 1.641332]\n",
      "[D loss: 0.827405] [G loss: 1.505016]\n",
      "[D loss: 0.692647] [G loss: 1.549198]\n",
      "[D loss: 0.785171] [G loss: 1.635248]\n",
      "[D loss: 0.782370] [G loss: 1.395441]\n",
      "[D loss: 0.825963] [G loss: 1.475741]\n",
      "[D loss: 1.197585] [G loss: 1.269287]\n",
      "[D loss: 0.861499] [G loss: 1.677793]\n",
      "[D loss: 0.828149] [G loss: 1.481593]\n",
      "[D loss: 1.085318] [G loss: 1.252789]\n",
      "[D loss: 0.712262] [G loss: 1.434887]\n",
      "[D loss: 0.940622] [G loss: 1.440313]\n",
      "[D loss: 0.877576] [G loss: 1.447679]\n",
      "[D loss: 0.732870] [G loss: 1.262907]\n",
      "[D loss: 0.639473] [G loss: 1.403711]\n",
      "[D loss: 0.682420] [G loss: 1.477091]\n",
      "[D loss: 0.919846] [G loss: 1.503064]\n",
      "[D loss: 0.925362] [G loss: 1.339260]\n",
      "[D loss: 0.728396] [G loss: 1.424868]\n",
      "[D loss: 0.913305] [G loss: 1.410680]\n",
      "[D loss: 0.918018] [G loss: 1.433088]\n",
      "[D loss: 0.875431] [G loss: 1.310993]\n",
      "[D loss: 0.709139] [G loss: 1.588280]\n",
      "[D loss: 0.752195] [G loss: 1.707370]\n",
      "[D loss: 0.842613] [G loss: 1.371800]\n",
      "[D loss: 0.698289] [G loss: 1.529553]\n",
      "[D loss: 0.994733] [G loss: 1.401602]\n",
      "[D loss: 0.810284] [G loss: 1.452049]\n",
      "[D loss: 0.747115] [G loss: 1.735993]\n",
      "[D loss: 0.760543] [G loss: 1.630710]\n",
      "[D loss: 0.934734] [G loss: 1.452109]\n",
      "[D loss: 0.717097] [G loss: 1.534766]\n",
      "[D loss: 0.708889] [G loss: 1.375277]\n",
      "[D loss: 0.941309] [G loss: 1.521675]\n",
      "[D loss: 0.803342] [G loss: 1.515444]\n",
      "[D loss: 0.646415] [G loss: 1.296272]\n",
      "[D loss: 0.692486] [G loss: 1.653980]\n",
      "[D loss: 0.664148] [G loss: 1.506273]\n",
      "[D loss: 0.739170] [G loss: 1.474422]\n",
      "[D loss: 0.762591] [G loss: 1.597242]\n",
      "[D loss: 0.745713] [G loss: 1.523817]\n",
      "[D loss: 0.784465] [G loss: 1.456905]\n",
      "[D loss: 1.023183] [G loss: 1.374909]\n",
      "[D loss: 1.007048] [G loss: 1.314305]\n",
      "[D loss: 1.037283] [G loss: 1.473968]\n",
      "[D loss: 0.779156] [G loss: 1.460613]\n",
      "[D loss: 0.913551] [G loss: 1.764870]\n",
      "[D loss: 0.673814] [G loss: 1.542845]\n",
      "[D loss: 0.933749] [G loss: 1.627466]\n",
      "[D loss: 0.767743] [G loss: 1.397104]\n",
      "[D loss: 0.742079] [G loss: 1.497413]\n",
      "[D loss: 0.880401] [G loss: 1.383164]\n",
      "[D loss: 0.893751] [G loss: 1.425403]\n",
      "[D loss: 0.779920] [G loss: 1.437672]\n",
      "[D loss: 0.871524] [G loss: 1.177878]\n",
      "[D loss: 0.845247] [G loss: 1.253357]\n",
      "[D loss: 0.995076] [G loss: 1.275821]\n",
      "[D loss: 0.809515] [G loss: 1.464180]\n",
      "[D loss: 0.828324] [G loss: 1.345519]\n",
      "[D loss: 0.895589] [G loss: 1.375534]\n",
      "[D loss: 0.699722] [G loss: 1.758970]\n",
      "[D loss: 0.721860] [G loss: 1.692900]\n",
      "[D loss: 0.726695] [G loss: 1.375443]\n",
      "[D loss: 0.633727] [G loss: 1.459746]\n",
      "[D loss: 0.740161] [G loss: 1.488756]\n",
      "[D loss: 0.904870] [G loss: 1.332555]\n",
      "[D loss: 1.009509] [G loss: 1.369943]\n",
      "[D loss: 0.816018] [G loss: 1.463639]\n",
      "[D loss: 0.601901] [G loss: 1.677460]\n",
      "[D loss: 0.901636] [G loss: 1.439370]\n",
      "[D loss: 0.865829] [G loss: 1.437912]\n",
      "[D loss: 0.834671] [G loss: 1.490311]\n",
      "[D loss: 1.059092] [G loss: 1.287295]\n",
      "[D loss: 0.806385] [G loss: 1.526865]\n",
      "[D loss: 0.919315] [G loss: 1.302648]\n",
      "[D loss: 0.798890] [G loss: 1.286970]\n",
      "[D loss: 0.587091] [G loss: 1.760024]\n",
      "[D loss: 0.856362] [G loss: 1.326933]\n",
      "[D loss: 0.918586] [G loss: 1.342056]\n",
      "[D loss: 0.833657] [G loss: 1.519097]\n",
      "[D loss: 0.975237] [G loss: 1.392910]\n",
      "[D loss: 0.789262] [G loss: 1.452988]\n",
      "[D loss: 0.713444] [G loss: 1.527185]\n",
      "[D loss: 0.791769] [G loss: 1.350058]\n",
      "[D loss: 0.865716] [G loss: 1.401999]\n",
      "[D loss: 0.769275] [G loss: 1.580639]\n",
      "[D loss: 1.000008] [G loss: 1.345340]\n",
      "[D loss: 0.644338] [G loss: 1.477943]\n",
      "[D loss: 0.733609] [G loss: 1.694135]\n",
      "[D loss: 1.257067] [G loss: 1.428241]\n",
      "[D loss: 0.866474] [G loss: 1.493791]\n",
      "[D loss: 0.948009] [G loss: 1.457538]\n",
      "[D loss: 0.874807] [G loss: 1.354076]\n",
      "[D loss: 0.837636] [G loss: 1.273123]\n",
      "[D loss: 0.744458] [G loss: 1.394074]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.789370] [G loss: 1.390839]\n",
      "[D loss: 0.609110] [G loss: 1.541454]\n",
      "[D loss: 0.686547] [G loss: 1.608301]\n",
      "[D loss: 0.703983] [G loss: 1.547867]\n",
      "[D loss: 0.749359] [G loss: 1.482283]\n",
      "[D loss: 0.997747] [G loss: 1.583269]\n",
      "[D loss: 0.761181] [G loss: 1.697009]\n",
      "[D loss: 0.897325] [G loss: 1.469571]\n",
      "[D loss: 0.952336] [G loss: 1.384630]\n",
      "[D loss: 0.654649] [G loss: 1.521821]\n",
      "[D loss: 0.629249] [G loss: 1.486415]\n",
      "[D loss: 0.645947] [G loss: 1.575568]\n",
      "[D loss: 0.819744] [G loss: 1.310155]\n",
      "[D loss: 0.754749] [G loss: 1.435809]\n",
      "[D loss: 0.789111] [G loss: 1.483959]\n",
      "[D loss: 0.745380] [G loss: 1.547220]\n",
      "[D loss: 1.031752] [G loss: 1.497995]\n",
      "[D loss: 1.024764] [G loss: 1.522344]\n",
      "[D loss: 0.756025] [G loss: 1.553571]\n",
      "[D loss: 0.899109] [G loss: 1.246512]\n",
      "[D loss: 1.040050] [G loss: 1.416766]\n",
      "[D loss: 0.567034] [G loss: 1.511502]\n",
      "[D loss: 0.711684] [G loss: 1.504287]\n",
      "[D loss: 0.875787] [G loss: 1.490941]\n",
      "[D loss: 0.733903] [G loss: 1.539636]\n",
      "[D loss: 1.134992] [G loss: 1.413067]\n",
      "[D loss: 1.169378] [G loss: 1.395842]\n",
      "[D loss: 0.892207] [G loss: 1.549532]\n",
      "[D loss: 0.833033] [G loss: 1.412177]\n",
      "[D loss: 1.086438] [G loss: 1.348724]\n",
      "[D loss: 0.751932] [G loss: 1.429206]\n",
      "[D loss: 0.840266] [G loss: 1.319193]\n",
      "[D loss: 0.812321] [G loss: 1.543889]\n",
      "[D loss: 0.753583] [G loss: 1.368023]\n",
      "[D loss: 1.056158] [G loss: 1.304410]\n",
      "[D loss: 0.819221] [G loss: 1.432546]\n",
      "[D loss: 1.015883] [G loss: 1.208405]\n",
      "[D loss: 0.801771] [G loss: 1.336260]\n",
      "[D loss: 0.828006] [G loss: 1.508461]\n",
      "[D loss: 1.011415] [G loss: 1.467945]\n",
      "[D loss: 0.765741] [G loss: 1.475493]\n",
      "[D loss: 0.784216] [G loss: 1.451527]\n",
      "[D loss: 0.909253] [G loss: 1.344561]\n",
      "[D loss: 0.863720] [G loss: 1.358931]\n",
      "[D loss: 0.925898] [G loss: 1.299120]\n",
      "[D loss: 1.049626] [G loss: 1.310546]\n",
      "[D loss: 0.836004] [G loss: 1.317457]\n",
      "[D loss: 0.895489] [G loss: 1.480258]\n",
      "[D loss: 0.753687] [G loss: 1.302542]\n",
      "[D loss: 0.858842] [G loss: 1.414071]\n",
      "[D loss: 0.714409] [G loss: 1.556959]\n",
      "[D loss: 0.825276] [G loss: 1.387690]\n",
      "[D loss: 0.813937] [G loss: 1.335022]\n",
      "[D loss: 0.833417] [G loss: 1.352178]\n",
      "[D loss: 0.606970] [G loss: 1.465787]\n",
      "[D loss: 0.733741] [G loss: 1.348991]\n",
      "[D loss: 0.906992] [G loss: 1.508176]\n",
      "[D loss: 0.853248] [G loss: 1.703921]\n",
      "[D loss: 0.773700] [G loss: 1.428493]\n",
      "[D loss: 0.906377] [G loss: 1.444452]\n",
      "[D loss: 0.758701] [G loss: 1.433819]\n",
      "[D loss: 0.994268] [G loss: 1.343152]\n",
      "[D loss: 0.858004] [G loss: 1.527991]\n",
      "[D loss: 0.616137] [G loss: 1.598379]\n",
      "[D loss: 0.684479] [G loss: 1.449417]\n",
      "[D loss: 0.957775] [G loss: 1.489486]\n",
      "[D loss: 0.837697] [G loss: 1.355646]\n",
      "[D loss: 0.779080] [G loss: 1.582432]\n",
      "[D loss: 0.791227] [G loss: 1.328238]\n",
      "[D loss: 0.750585] [G loss: 1.749827]\n",
      "[D loss: 0.616622] [G loss: 1.558629]\n",
      "[D loss: 0.719829] [G loss: 1.550946]\n",
      "[D loss: 1.267965] [G loss: 1.152943]\n",
      "[D loss: 1.111743] [G loss: 1.497656]\n",
      "[D loss: 0.694017] [G loss: 1.518005]\n",
      "[D loss: 0.868627] [G loss: 1.470801]\n",
      "[D loss: 0.926951] [G loss: 1.426807]\n",
      "[D loss: 0.870352] [G loss: 1.467268]\n",
      "[D loss: 0.936328] [G loss: 1.424586]\n",
      "[D loss: 0.813630] [G loss: 1.393008]\n",
      "[D loss: 0.691043] [G loss: 1.552471]\n",
      "[D loss: 0.830255] [G loss: 1.552907]\n",
      "[D loss: 0.849630] [G loss: 1.431508]\n",
      "[D loss: 0.739625] [G loss: 1.465082]\n",
      "[D loss: 0.877453] [G loss: 1.627030]\n",
      "[D loss: 0.937919] [G loss: 1.310583]\n",
      "[D loss: 0.928235] [G loss: 1.327963]\n",
      "[D loss: 0.661899] [G loss: 1.370508]\n",
      "[D loss: 0.787031] [G loss: 1.372212]\n",
      "[D loss: 0.675921] [G loss: 1.720374]\n",
      "[D loss: 0.859631] [G loss: 1.525125]\n",
      "[D loss: 0.915089] [G loss: 1.464680]\n",
      "[D loss: 0.789585] [G loss: 1.496958]\n",
      "[D loss: 0.807034] [G loss: 1.260475]\n",
      "[D loss: 0.738701] [G loss: 1.216672]\n",
      "[D loss: 0.700912] [G loss: 1.472205]\n",
      "[D loss: 0.765432] [G loss: 1.266244]\n",
      "[D loss: 0.832841] [G loss: 1.349931]\n",
      "[D loss: 1.010854] [G loss: 1.607593]\n",
      "[D loss: 0.973122] [G loss: 1.347224]\n",
      "[D loss: 1.056277] [G loss: 1.499091]\n",
      "[D loss: 0.827198] [G loss: 1.364452]\n",
      "[D loss: 0.817071] [G loss: 1.000626]\n",
      "[D loss: 0.851909] [G loss: 1.332074]\n",
      "epoch:9, g_loss:2750.161865234375,d_loss:1561.9366455078125\n",
      "[D loss: 0.932952] [G loss: 1.608088]\n",
      "[D loss: 0.842274] [G loss: 1.411332]\n",
      "[D loss: 1.027325] [G loss: 1.352648]\n",
      "[D loss: 1.118176] [G loss: 1.504461]\n",
      "[D loss: 0.808106] [G loss: 1.377115]\n",
      "[D loss: 0.773512] [G loss: 1.273276]\n",
      "[D loss: 0.729810] [G loss: 1.474025]\n",
      "[D loss: 0.856878] [G loss: 1.460336]\n",
      "[D loss: 0.717434] [G loss: 1.486679]\n",
      "[D loss: 0.881131] [G loss: 1.490979]\n",
      "[D loss: 0.595751] [G loss: 1.377380]\n",
      "[D loss: 0.921960] [G loss: 1.262477]\n",
      "[D loss: 0.755114] [G loss: 1.528030]\n",
      "[D loss: 0.777137] [G loss: 1.500854]\n",
      "[D loss: 0.911984] [G loss: 1.397096]\n",
      "[D loss: 0.652782] [G loss: 1.388447]\n",
      "[D loss: 0.842279] [G loss: 1.402594]\n",
      "[D loss: 0.877122] [G loss: 1.799516]\n",
      "[D loss: 0.889335] [G loss: 1.890226]\n",
      "[D loss: 0.831597] [G loss: 1.257964]\n",
      "[D loss: 1.012148] [G loss: 1.266552]\n",
      "[D loss: 0.845535] [G loss: 1.477639]\n",
      "[D loss: 1.111095] [G loss: 1.370134]\n",
      "[D loss: 0.901289] [G loss: 1.389467]\n",
      "[D loss: 0.769613] [G loss: 1.418473]\n",
      "[D loss: 1.019612] [G loss: 1.386464]\n",
      "[D loss: 1.034364] [G loss: 1.202874]\n",
      "[D loss: 0.930242] [G loss: 1.081476]\n",
      "[D loss: 0.839809] [G loss: 1.338219]\n",
      "[D loss: 1.041943] [G loss: 1.538098]\n",
      "[D loss: 0.908802] [G loss: 1.374795]\n",
      "[D loss: 0.797940] [G loss: 1.412710]\n",
      "[D loss: 0.784304] [G loss: 1.234171]\n",
      "[D loss: 0.822265] [G loss: 1.405336]\n",
      "[D loss: 0.901112] [G loss: 1.552637]\n",
      "[D loss: 0.920969] [G loss: 1.354031]\n",
      "[D loss: 0.730392] [G loss: 1.303157]\n",
      "[D loss: 0.953200] [G loss: 1.217307]\n",
      "[D loss: 0.909847] [G loss: 1.344975]\n",
      "[D loss: 0.886522] [G loss: 1.359481]\n",
      "[D loss: 0.737667] [G loss: 1.593719]\n",
      "[D loss: 0.723563] [G loss: 1.648708]\n",
      "[D loss: 0.887501] [G loss: 1.351648]\n",
      "[D loss: 0.980216] [G loss: 1.239813]\n",
      "[D loss: 0.671935] [G loss: 1.405092]\n",
      "[D loss: 0.773322] [G loss: 1.425339]\n",
      "[D loss: 0.704692] [G loss: 1.485761]\n",
      "[D loss: 0.752903] [G loss: 1.571587]\n",
      "[D loss: 0.947679] [G loss: 1.441641]\n",
      "[D loss: 0.688627] [G loss: 1.422802]\n",
      "[D loss: 0.708589] [G loss: 1.673479]\n",
      "[D loss: 0.673096] [G loss: 1.641490]\n",
      "[D loss: 0.876092] [G loss: 1.403075]\n",
      "[D loss: 0.972226] [G loss: 1.461113]\n",
      "[D loss: 1.028250] [G loss: 1.485424]\n",
      "[D loss: 0.881058] [G loss: 1.426098]\n",
      "[D loss: 0.705836] [G loss: 1.509315]\n",
      "[D loss: 0.771522] [G loss: 1.493070]\n",
      "[D loss: 0.801844] [G loss: 1.531344]\n",
      "[D loss: 0.890019] [G loss: 1.326305]\n",
      "[D loss: 0.744392] [G loss: 1.619743]\n",
      "[D loss: 0.572134] [G loss: 1.676365]\n",
      "[D loss: 0.764959] [G loss: 1.513424]\n",
      "[D loss: 0.663296] [G loss: 1.855133]\n",
      "[D loss: 0.688072] [G loss: 1.647631]\n",
      "[D loss: 0.764945] [G loss: 1.598488]\n",
      "[D loss: 0.649766] [G loss: 1.718387]\n",
      "[D loss: 0.962825] [G loss: 1.400468]\n",
      "[D loss: 1.072861] [G loss: 1.409157]\n",
      "[D loss: 0.694978] [G loss: 1.534210]\n",
      "[D loss: 1.010052] [G loss: 1.402715]\n",
      "[D loss: 0.886852] [G loss: 1.556487]\n",
      "[D loss: 0.947315] [G loss: 1.648798]\n",
      "[D loss: 0.997152] [G loss: 1.457506]\n",
      "[D loss: 0.921387] [G loss: 1.499065]\n",
      "[D loss: 0.834664] [G loss: 1.431986]\n",
      "[D loss: 0.845885] [G loss: 1.469248]\n",
      "[D loss: 0.854823] [G loss: 1.347603]\n",
      "[D loss: 1.067779] [G loss: 1.156272]\n",
      "[D loss: 0.766448] [G loss: 1.367079]\n",
      "[D loss: 0.740711] [G loss: 1.341695]\n",
      "[D loss: 0.828404] [G loss: 1.663774]\n",
      "[D loss: 0.677430] [G loss: 1.534132]\n",
      "[D loss: 0.702555] [G loss: 1.549887]\n",
      "[D loss: 0.822849] [G loss: 1.248865]\n",
      "[D loss: 0.631319] [G loss: 1.460042]\n",
      "[D loss: 0.773189] [G loss: 1.893034]\n",
      "[D loss: 0.787845] [G loss: 1.483156]\n",
      "[D loss: 0.838259] [G loss: 1.802395]\n",
      "[D loss: 0.690941] [G loss: 1.424583]\n",
      "[D loss: 0.864471] [G loss: 1.350333]\n",
      "[D loss: 1.117145] [G loss: 1.277173]\n",
      "[D loss: 0.835930] [G loss: 1.514029]\n",
      "[D loss: 1.206666] [G loss: 1.679463]\n",
      "[D loss: 0.823908] [G loss: 1.427608]\n",
      "[D loss: 0.752327] [G loss: 1.566787]\n",
      "[D loss: 1.117960] [G loss: 1.225387]\n",
      "[D loss: 0.884914] [G loss: 1.244536]\n",
      "[D loss: 0.890500] [G loss: 1.355950]\n",
      "[D loss: 0.729243] [G loss: 1.457198]\n",
      "[D loss: 0.742777] [G loss: 1.585210]\n",
      "[D loss: 0.832635] [G loss: 1.462837]\n",
      "[D loss: 0.720835] [G loss: 1.397612]\n",
      "[D loss: 0.740209] [G loss: 1.575949]\n",
      "[D loss: 1.002214] [G loss: 1.314448]\n",
      "[D loss: 0.880790] [G loss: 1.440070]\n",
      "[D loss: 0.753233] [G loss: 1.431435]\n",
      "[D loss: 0.912035] [G loss: 1.506918]\n",
      "[D loss: 0.704896] [G loss: 1.606236]\n",
      "[D loss: 0.832220] [G loss: 1.544188]\n",
      "[D loss: 0.867475] [G loss: 1.571535]\n",
      "[D loss: 1.040236] [G loss: 1.391602]\n",
      "[D loss: 0.855815] [G loss: 1.499632]\n",
      "[D loss: 0.949753] [G loss: 1.280206]\n",
      "[D loss: 0.695364] [G loss: 1.509636]\n",
      "[D loss: 0.793791] [G loss: 1.487677]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.767410] [G loss: 1.530220]\n",
      "[D loss: 0.968265] [G loss: 1.359809]\n",
      "[D loss: 0.950015] [G loss: 1.358811]\n",
      "[D loss: 1.015911] [G loss: 1.433972]\n",
      "[D loss: 0.737399] [G loss: 1.585327]\n",
      "[D loss: 0.744751] [G loss: 1.529733]\n",
      "[D loss: 1.253461] [G loss: 1.357516]\n",
      "[D loss: 0.943485] [G loss: 1.343344]\n",
      "[D loss: 0.812022] [G loss: 1.437975]\n",
      "[D loss: 0.780409] [G loss: 1.207241]\n",
      "[D loss: 1.038687] [G loss: 1.390479]\n",
      "[D loss: 0.738086] [G loss: 1.358172]\n",
      "[D loss: 0.791582] [G loss: 1.645943]\n",
      "[D loss: 0.846711] [G loss: 1.406720]\n",
      "[D loss: 0.810822] [G loss: 1.302728]\n",
      "[D loss: 0.790181] [G loss: 1.511200]\n",
      "[D loss: 0.590758] [G loss: 1.502523]\n",
      "[D loss: 0.787023] [G loss: 1.342466]\n",
      "[D loss: 1.083424] [G loss: 1.230572]\n",
      "[D loss: 0.973397] [G loss: 1.279320]\n",
      "[D loss: 0.666486] [G loss: 1.481788]\n",
      "[D loss: 0.878797] [G loss: 1.707808]\n",
      "[D loss: 0.632995] [G loss: 1.663624]\n",
      "[D loss: 0.784587] [G loss: 1.303287]\n",
      "[D loss: 0.918890] [G loss: 1.492214]\n",
      "[D loss: 0.926966] [G loss: 1.321390]\n",
      "[D loss: 0.896922] [G loss: 1.507211]\n",
      "[D loss: 0.923366] [G loss: 1.538008]\n",
      "[D loss: 0.797921] [G loss: 1.386247]\n",
      "[D loss: 1.003865] [G loss: 1.433717]\n",
      "[D loss: 0.773213] [G loss: 1.469861]\n",
      "[D loss: 0.785836] [G loss: 1.487970]\n",
      "[D loss: 0.969389] [G loss: 1.518455]\n",
      "[D loss: 0.864499] [G loss: 1.389290]\n",
      "[D loss: 0.877380] [G loss: 1.129982]\n",
      "[D loss: 1.056158] [G loss: 1.154536]\n",
      "[D loss: 0.964264] [G loss: 1.300583]\n",
      "[D loss: 0.754203] [G loss: 1.351753]\n",
      "[D loss: 0.809155] [G loss: 1.469742]\n",
      "[D loss: 0.946796] [G loss: 1.499635]\n",
      "[D loss: 0.621687] [G loss: 1.445708]\n",
      "[D loss: 0.833105] [G loss: 1.340686]\n",
      "[D loss: 0.705364] [G loss: 1.431806]\n",
      "[D loss: 0.588357] [G loss: 1.579585]\n",
      "[D loss: 0.773214] [G loss: 1.626976]\n",
      "[D loss: 0.805221] [G loss: 1.596550]\n",
      "[D loss: 0.770840] [G loss: 1.403656]\n",
      "[D loss: 0.699193] [G loss: 1.489056]\n",
      "[D loss: 0.673567] [G loss: 1.316329]\n",
      "[D loss: 0.864544] [G loss: 1.435980]\n",
      "[D loss: 0.628927] [G loss: 1.622699]\n",
      "[D loss: 0.834041] [G loss: 1.319428]\n",
      "[D loss: 0.636114] [G loss: 1.573099]\n",
      "[D loss: 0.895940] [G loss: 1.467667]\n",
      "[D loss: 0.735095] [G loss: 1.458781]\n",
      "[D loss: 1.004502] [G loss: 1.267056]\n",
      "[D loss: 0.931314] [G loss: 1.452667]\n",
      "[D loss: 0.640916] [G loss: 1.617923]\n",
      "[D loss: 0.800006] [G loss: 1.382367]\n",
      "[D loss: 0.852758] [G loss: 1.707726]\n",
      "[D loss: 1.029962] [G loss: 1.305208]\n",
      "[D loss: 0.859091] [G loss: 1.612433]\n",
      "[D loss: 1.008609] [G loss: 1.876761]\n",
      "[D loss: 1.055204] [G loss: 1.505286]\n",
      "[D loss: 0.654525] [G loss: 1.662416]\n",
      "[D loss: 0.802388] [G loss: 1.714037]\n",
      "[D loss: 0.839003] [G loss: 1.520706]\n",
      "[D loss: 0.781225] [G loss: 1.510552]\n",
      "[D loss: 0.934211] [G loss: 1.503736]\n",
      "[D loss: 0.814376] [G loss: 1.552758]\n",
      "[D loss: 0.830630] [G loss: 1.297771]\n",
      "[D loss: 0.862049] [G loss: 1.484262]\n",
      "[D loss: 0.755563] [G loss: 1.479441]\n",
      "[D loss: 0.990916] [G loss: 1.340145]\n",
      "[D loss: 1.042295] [G loss: 1.335638]\n",
      "[D loss: 0.962179] [G loss: 1.538241]\n",
      "[D loss: 0.755272] [G loss: 1.391649]\n",
      "[D loss: 0.776728] [G loss: 1.372326]\n",
      "[D loss: 0.565099] [G loss: 1.555859]\n",
      "[D loss: 0.751769] [G loss: 1.285060]\n",
      "[D loss: 0.942190] [G loss: 1.547045]\n",
      "[D loss: 0.908705] [G loss: 1.391478]\n",
      "[D loss: 0.898550] [G loss: 1.398791]\n",
      "[D loss: 1.032056] [G loss: 1.412260]\n",
      "[D loss: 0.775355] [G loss: 1.375804]\n",
      "[D loss: 0.672026] [G loss: 1.483753]\n",
      "[D loss: 0.866799] [G loss: 1.423498]\n",
      "[D loss: 0.732707] [G loss: 1.372369]\n",
      "[D loss: 0.846306] [G loss: 1.296158]\n",
      "[D loss: 0.763099] [G loss: 1.346195]\n",
      "[D loss: 0.781728] [G loss: 1.499939]\n",
      "[D loss: 0.787297] [G loss: 1.433950]\n",
      "[D loss: 0.718977] [G loss: 1.431645]\n",
      "[D loss: 0.538071] [G loss: 1.530300]\n",
      "[D loss: 0.879718] [G loss: 1.487147]\n",
      "[D loss: 0.701774] [G loss: 1.498357]\n",
      "[D loss: 0.801998] [G loss: 1.754289]\n",
      "[D loss: 0.937981] [G loss: 1.568337]\n",
      "[D loss: 0.836835] [G loss: 1.536759]\n",
      "[D loss: 0.904020] [G loss: 1.402856]\n",
      "[D loss: 0.733440] [G loss: 1.652752]\n",
      "[D loss: 0.996257] [G loss: 1.495999]\n",
      "[D loss: 0.997760] [G loss: 1.387004]\n",
      "[D loss: 0.823033] [G loss: 1.546860]\n",
      "[D loss: 0.901350] [G loss: 1.453404]\n",
      "[D loss: 0.637854] [G loss: 1.481261]\n",
      "[D loss: 0.944715] [G loss: 1.522479]\n",
      "[D loss: 0.785545] [G loss: 1.370151]\n",
      "[D loss: 0.681859] [G loss: 1.730144]\n",
      "[D loss: 0.952610] [G loss: 1.400094]\n",
      "[D loss: 1.194104] [G loss: 1.366719]\n",
      "[D loss: 0.699426] [G loss: 1.748778]\n",
      "[D loss: 1.038311] [G loss: 1.395502]\n",
      "[D loss: 0.729404] [G loss: 1.449638]\n",
      "[D loss: 0.639760] [G loss: 1.508653]\n",
      "[D loss: 0.851019] [G loss: 1.467704]\n",
      "[D loss: 0.730660] [G loss: 1.279551]\n",
      "[D loss: 1.008356] [G loss: 1.218400]\n",
      "[D loss: 0.839342] [G loss: 1.348042]\n",
      "[D loss: 0.924254] [G loss: 1.272676]\n",
      "[D loss: 0.756538] [G loss: 1.418309]\n",
      "[D loss: 0.850799] [G loss: 1.484582]\n",
      "[D loss: 0.635013] [G loss: 1.509084]\n",
      "[D loss: 0.836286] [G loss: 1.285980]\n",
      "[D loss: 0.686926] [G loss: 1.583031]\n",
      "[D loss: 0.783844] [G loss: 1.557662]\n",
      "[D loss: 0.874285] [G loss: 1.361487]\n",
      "[D loss: 0.811435] [G loss: 1.408526]\n",
      "[D loss: 0.584795] [G loss: 1.731390]\n",
      "[D loss: 0.780562] [G loss: 1.790983]\n",
      "[D loss: 0.822087] [G loss: 1.422154]\n",
      "[D loss: 0.539568] [G loss: 1.455070]\n",
      "[D loss: 0.820104] [G loss: 1.576481]\n",
      "[D loss: 0.916955] [G loss: 1.368873]\n",
      "[D loss: 0.850684] [G loss: 1.399426]\n",
      "[D loss: 0.834669] [G loss: 1.310818]\n",
      "[D loss: 0.697872] [G loss: 1.492030]\n",
      "[D loss: 1.021569] [G loss: 1.664985]\n",
      "[D loss: 0.673502] [G loss: 1.572324]\n",
      "[D loss: 0.606141] [G loss: 1.634637]\n",
      "[D loss: 0.840774] [G loss: 1.436183]\n",
      "[D loss: 0.866340] [G loss: 1.515423]\n",
      "[D loss: 0.755448] [G loss: 1.422483]\n",
      "[D loss: 0.886849] [G loss: 1.488905]\n",
      "[D loss: 0.678559] [G loss: 1.336434]\n",
      "[D loss: 0.939370] [G loss: 1.335364]\n",
      "[D loss: 0.847524] [G loss: 1.470340]\n",
      "[D loss: 0.577785] [G loss: 1.796039]\n",
      "[D loss: 0.752079] [G loss: 1.469194]\n",
      "[D loss: 0.920910] [G loss: 1.441348]\n",
      "[D loss: 0.900701] [G loss: 1.402462]\n",
      "[D loss: 0.733914] [G loss: 1.533340]\n",
      "[D loss: 0.678273] [G loss: 1.653843]\n",
      "[D loss: 0.549546] [G loss: 1.601863]\n",
      "[D loss: 0.812903] [G loss: 1.713366]\n",
      "[D loss: 0.916595] [G loss: 1.432276]\n",
      "[D loss: 1.089318] [G loss: 1.291882]\n",
      "[D loss: 0.799886] [G loss: 1.524590]\n",
      "[D loss: 0.778574] [G loss: 1.285769]\n",
      "[D loss: 0.672030] [G loss: 1.371940]\n",
      "[D loss: 0.843785] [G loss: 1.715674]\n",
      "[D loss: 1.108238] [G loss: 1.459373]\n",
      "[D loss: 0.845626] [G loss: 1.541499]\n",
      "[D loss: 0.874810] [G loss: 1.371407]\n",
      "[D loss: 0.938544] [G loss: 1.444432]\n",
      "[D loss: 0.897155] [G loss: 1.313603]\n",
      "[D loss: 0.818232] [G loss: 1.321899]\n",
      "[D loss: 0.898398] [G loss: 1.316130]\n",
      "[D loss: 0.972356] [G loss: 1.205047]\n",
      "[D loss: 0.817904] [G loss: 1.344547]\n",
      "[D loss: 0.906756] [G loss: 1.339552]\n",
      "[D loss: 0.816124] [G loss: 1.409590]\n",
      "[D loss: 0.786806] [G loss: 1.465152]\n",
      "[D loss: 0.908712] [G loss: 1.254332]\n",
      "[D loss: 1.073065] [G loss: 1.358503]\n",
      "[D loss: 0.868289] [G loss: 1.404884]\n",
      "[D loss: 0.739496] [G loss: 1.435868]\n",
      "[D loss: 1.129429] [G loss: 1.338843]\n",
      "[D loss: 0.723423] [G loss: 1.456455]\n",
      "[D loss: 0.941403] [G loss: 1.444506]\n",
      "[D loss: 0.648711] [G loss: 1.515116]\n",
      "[D loss: 1.185483] [G loss: 1.426913]\n",
      "[D loss: 0.960070] [G loss: 1.268731]\n",
      "[D loss: 0.897052] [G loss: 1.383477]\n",
      "[D loss: 1.025576] [G loss: 1.291806]\n",
      "[D loss: 0.848163] [G loss: 1.357997]\n",
      "[D loss: 0.703045] [G loss: 1.593054]\n",
      "[D loss: 0.879365] [G loss: 1.411844]\n",
      "[D loss: 0.798900] [G loss: 1.384542]\n",
      "[D loss: 0.887686] [G loss: 1.329842]\n",
      "[D loss: 0.808633] [G loss: 1.323132]\n",
      "[D loss: 1.173502] [G loss: 1.214130]\n",
      "[D loss: 0.835596] [G loss: 1.593047]\n",
      "[D loss: 1.111122] [G loss: 1.446620]\n",
      "[D loss: 0.695038] [G loss: 1.306188]\n",
      "[D loss: 0.735846] [G loss: 1.564190]\n",
      "[D loss: 0.833874] [G loss: 1.467334]\n",
      "[D loss: 0.766559] [G loss: 1.362679]\n",
      "[D loss: 0.741347] [G loss: 1.388798]\n",
      "[D loss: 0.887482] [G loss: 1.268935]\n",
      "[D loss: 0.869432] [G loss: 1.483754]\n",
      "[D loss: 0.769843] [G loss: 1.562802]\n",
      "[D loss: 0.961806] [G loss: 1.398036]\n",
      "[D loss: 0.950962] [G loss: 1.425301]\n",
      "[D loss: 0.802250] [G loss: 1.448079]\n",
      "[D loss: 0.725112] [G loss: 1.362002]\n",
      "[D loss: 0.920949] [G loss: 1.406815]\n",
      "[D loss: 0.787008] [G loss: 1.564205]\n",
      "[D loss: 0.664104] [G loss: 1.404997]\n",
      "[D loss: 0.664971] [G loss: 1.669690]\n",
      "[D loss: 0.895801] [G loss: 1.360900]\n",
      "[D loss: 0.816459] [G loss: 1.444853]\n",
      "[D loss: 0.858815] [G loss: 1.468554]\n",
      "[D loss: 0.694639] [G loss: 1.531020]\n",
      "[D loss: 0.807540] [G loss: 1.696125]\n",
      "[D loss: 0.867822] [G loss: 1.534556]\n",
      "[D loss: 0.738029] [G loss: 1.568005]\n",
      "[D loss: 0.950381] [G loss: 1.141800]\n",
      "[D loss: 0.654905] [G loss: 1.655628]\n",
      "[D loss: 0.858844] [G loss: 1.444751]\n",
      "[D loss: 0.803733] [G loss: 1.636186]\n",
      "[D loss: 0.859855] [G loss: 1.636599]\n",
      "[D loss: 0.794831] [G loss: 1.377577]\n",
      "[D loss: 0.950366] [G loss: 1.370608]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.767344] [G loss: 1.333596]\n",
      "[D loss: 1.038765] [G loss: 1.389403]\n",
      "[D loss: 0.760162] [G loss: 1.638017]\n",
      "[D loss: 0.811552] [G loss: 1.665689]\n",
      "[D loss: 0.710651] [G loss: 1.390138]\n",
      "[D loss: 1.109397] [G loss: 1.322110]\n",
      "[D loss: 0.633686] [G loss: 1.393401]\n",
      "[D loss: 0.694066] [G loss: 1.571326]\n",
      "[D loss: 0.848015] [G loss: 1.371055]\n",
      "[D loss: 0.966560] [G loss: 1.365043]\n",
      "[D loss: 0.872239] [G loss: 1.461545]\n",
      "[D loss: 0.644273] [G loss: 1.559329]\n",
      "[D loss: 0.732815] [G loss: 1.513380]\n",
      "[D loss: 0.715345] [G loss: 1.498132]\n",
      "[D loss: 1.044599] [G loss: 1.504167]\n",
      "[D loss: 0.760980] [G loss: 1.537127]\n",
      "[D loss: 0.848243] [G loss: 1.332373]\n",
      "[D loss: 1.005910] [G loss: 1.340201]\n",
      "[D loss: 0.701469] [G loss: 1.445094]\n",
      "[D loss: 0.740274] [G loss: 1.511399]\n",
      "[D loss: 0.841663] [G loss: 1.474846]\n",
      "[D loss: 1.030566] [G loss: 1.291775]\n",
      "[D loss: 0.927361] [G loss: 1.397686]\n",
      "[D loss: 1.007459] [G loss: 1.375654]\n",
      "[D loss: 0.624754] [G loss: 1.543059]\n",
      "[D loss: 0.816744] [G loss: 1.539259]\n",
      "[D loss: 0.788925] [G loss: 1.520221]\n",
      "[D loss: 0.737514] [G loss: 1.561764]\n",
      "[D loss: 0.783770] [G loss: 1.385335]\n",
      "[D loss: 0.878847] [G loss: 1.302517]\n",
      "[D loss: 0.609522] [G loss: 1.641076]\n",
      "[D loss: 0.719481] [G loss: 1.226902]\n",
      "[D loss: 0.732591] [G loss: 1.492758]\n",
      "[D loss: 1.030868] [G loss: 1.564976]\n",
      "[D loss: 1.015707] [G loss: 1.392453]\n",
      "[D loss: 0.854700] [G loss: 1.423995]\n",
      "[D loss: 0.660249] [G loss: 1.550180]\n",
      "[D loss: 0.981959] [G loss: 1.357220]\n",
      "[D loss: 0.937313] [G loss: 1.419647]\n",
      "[D loss: 0.943366] [G loss: 1.602211]\n",
      "[D loss: 0.802669] [G loss: 1.428143]\n",
      "[D loss: 0.870473] [G loss: 1.420786]\n",
      "[D loss: 0.895403] [G loss: 1.456018]\n",
      "[D loss: 0.705455] [G loss: 1.636001]\n",
      "[D loss: 0.846794] [G loss: 1.902602]\n",
      "[D loss: 0.909425] [G loss: 1.539664]\n",
      "[D loss: 0.674273] [G loss: 1.435193]\n",
      "[D loss: 0.738089] [G loss: 1.517764]\n",
      "[D loss: 0.961747] [G loss: 1.297744]\n",
      "[D loss: 0.838770] [G loss: 1.491649]\n",
      "[D loss: 0.687311] [G loss: 1.742715]\n",
      "[D loss: 0.730874] [G loss: 1.191764]\n",
      "[D loss: 0.760465] [G loss: 1.316218]\n",
      "[D loss: 0.845056] [G loss: 1.508647]\n",
      "[D loss: 0.967951] [G loss: 1.516083]\n",
      "[D loss: 0.844604] [G loss: 1.587035]\n",
      "[D loss: 0.684379] [G loss: 1.570901]\n",
      "[D loss: 1.043772] [G loss: 1.325356]\n",
      "[D loss: 0.829845] [G loss: 1.430080]\n",
      "[D loss: 0.857906] [G loss: 1.397588]\n",
      "[D loss: 0.762823] [G loss: 1.467515]\n",
      "[D loss: 0.786712] [G loss: 1.504393]\n",
      "[D loss: 0.874204] [G loss: 1.359426]\n",
      "[D loss: 0.904776] [G loss: 1.481838]\n",
      "[D loss: 0.924003] [G loss: 1.414923]\n",
      "[D loss: 0.871172] [G loss: 1.310909]\n",
      "[D loss: 0.630187] [G loss: 1.501577]\n",
      "[D loss: 0.884939] [G loss: 1.415959]\n",
      "[D loss: 0.849685] [G loss: 1.662591]\n",
      "[D loss: 0.633651] [G loss: 1.791149]\n",
      "[D loss: 1.008649] [G loss: 1.534399]\n",
      "[D loss: 0.768652] [G loss: 1.480727]\n",
      "[D loss: 0.792331] [G loss: 1.393060]\n",
      "[D loss: 0.900557] [G loss: 1.500731]\n",
      "[D loss: 0.817027] [G loss: 1.570715]\n",
      "[D loss: 0.725258] [G loss: 1.659769]\n",
      "[D loss: 0.714354] [G loss: 1.473291]\n",
      "[D loss: 0.694613] [G loss: 1.537590]\n",
      "[D loss: 0.706297] [G loss: 1.411475]\n",
      "[D loss: 0.959024] [G loss: 1.635795]\n",
      "[D loss: 0.804255] [G loss: 1.762325]\n",
      "[D loss: 0.731010] [G loss: 1.509948]\n",
      "[D loss: 0.936041] [G loss: 1.598122]\n",
      "[D loss: 0.789649] [G loss: 1.660574]\n",
      "[D loss: 1.051991] [G loss: 1.438831]\n",
      "[D loss: 0.784986] [G loss: 1.452318]\n",
      "[D loss: 0.818914] [G loss: 1.469648]\n",
      "[D loss: 0.816368] [G loss: 1.532088]\n",
      "[D loss: 0.933710] [G loss: 1.533339]\n",
      "[D loss: 0.735650] [G loss: 1.440552]\n",
      "[D loss: 0.838934] [G loss: 1.494123]\n",
      "[D loss: 0.854122] [G loss: 1.569716]\n",
      "[D loss: 0.828784] [G loss: 1.360661]\n",
      "[D loss: 0.834407] [G loss: 1.294855]\n",
      "[D loss: 1.099822] [G loss: 1.289689]\n",
      "[D loss: 0.702574] [G loss: 1.532323]\n",
      "[D loss: 0.641140] [G loss: 1.579247]\n",
      "[D loss: 0.736189] [G loss: 1.667930]\n",
      "[D loss: 0.851792] [G loss: 1.548177]\n",
      "[D loss: 0.776808] [G loss: 1.433143]\n",
      "[D loss: 0.991160] [G loss: 1.451481]\n",
      "[D loss: 0.713370] [G loss: 1.779912]\n",
      "[D loss: 0.880272] [G loss: 1.231761]\n",
      "[D loss: 0.755759] [G loss: 1.523520]\n",
      "[D loss: 0.750236] [G loss: 1.286794]\n",
      "[D loss: 0.859296] [G loss: 1.578159]\n",
      "[D loss: 0.873100] [G loss: 1.325077]\n",
      "[D loss: 0.885770] [G loss: 1.491336]\n",
      "[D loss: 0.900838] [G loss: 1.517309]\n",
      "[D loss: 0.569746] [G loss: 1.657872]\n",
      "[D loss: 0.661244] [G loss: 1.479213]\n",
      "[D loss: 0.841863] [G loss: 1.576914]\n",
      "[D loss: 0.922357] [G loss: 1.564942]\n",
      "[D loss: 0.803369] [G loss: 1.554118]\n",
      "[D loss: 0.682130] [G loss: 1.766141]\n",
      "[D loss: 0.745295] [G loss: 1.564335]\n",
      "[D loss: 0.774201] [G loss: 1.598582]\n",
      "[D loss: 0.886062] [G loss: 1.501220]\n",
      "[D loss: 0.838217] [G loss: 1.426708]\n",
      "[D loss: 0.834407] [G loss: 1.536321]\n",
      "[D loss: 0.823553] [G loss: 1.226153]\n",
      "[D loss: 0.723479] [G loss: 1.503789]\n",
      "[D loss: 0.722870] [G loss: 1.588187]\n",
      "[D loss: 1.031334] [G loss: 1.268074]\n",
      "[D loss: 1.065209] [G loss: 1.340361]\n",
      "[D loss: 0.836952] [G loss: 1.425403]\n",
      "[D loss: 0.638226] [G loss: 1.466415]\n",
      "[D loss: 0.628443] [G loss: 1.595272]\n",
      "[D loss: 1.000780] [G loss: 1.290868]\n",
      "[D loss: 0.847634] [G loss: 1.411553]\n",
      "[D loss: 1.109326] [G loss: 1.572064]\n",
      "[D loss: 0.833078] [G loss: 1.677431]\n",
      "[D loss: 0.900766] [G loss: 1.491760]\n",
      "[D loss: 0.955619] [G loss: 1.357090]\n",
      "[D loss: 0.536570] [G loss: 1.584689]\n",
      "[D loss: 0.979365] [G loss: 1.584928]\n",
      "[D loss: 0.766228] [G loss: 1.554425]\n",
      "[D loss: 0.683722] [G loss: 1.641024]\n",
      "[D loss: 0.823988] [G loss: 1.406180]\n",
      "[D loss: 0.780537] [G loss: 1.230082]\n",
      "[D loss: 0.755268] [G loss: 1.370656]\n",
      "[D loss: 1.001849] [G loss: 1.408024]\n",
      "[D loss: 0.828634] [G loss: 1.579618]\n",
      "[D loss: 0.899079] [G loss: 1.504937]\n",
      "[D loss: 0.919299] [G loss: 1.474791]\n",
      "[D loss: 0.800212] [G loss: 1.294480]\n",
      "[D loss: 0.675578] [G loss: 1.504177]\n",
      "[D loss: 0.853832] [G loss: 1.494937]\n",
      "[D loss: 0.921963] [G loss: 1.235851]\n",
      "[D loss: 0.752570] [G loss: 1.509314]\n",
      "[D loss: 0.841389] [G loss: 1.291949]\n",
      "[D loss: 0.875332] [G loss: 1.335129]\n",
      "[D loss: 0.864557] [G loss: 1.511222]\n",
      "[D loss: 0.706563] [G loss: 1.341399]\n",
      "[D loss: 0.721900] [G loss: 1.610500]\n",
      "[D loss: 0.750139] [G loss: 1.538126]\n",
      "[D loss: 0.854470] [G loss: 1.679189]\n",
      "[D loss: 0.835216] [G loss: 1.643282]\n",
      "[D loss: 0.776955] [G loss: 1.540219]\n",
      "[D loss: 0.932958] [G loss: 1.617541]\n",
      "[D loss: 0.833157] [G loss: 1.541838]\n",
      "[D loss: 0.773681] [G loss: 1.265637]\n",
      "[D loss: 0.772802] [G loss: 1.449755]\n",
      "[D loss: 0.847222] [G loss: 1.554565]\n",
      "[D loss: 1.182215] [G loss: 1.285440]\n",
      "[D loss: 0.723568] [G loss: 1.621306]\n",
      "[D loss: 0.805026] [G loss: 1.594254]\n",
      "[D loss: 0.806046] [G loss: 1.671812]\n",
      "[D loss: 0.904082] [G loss: 1.560855]\n",
      "[D loss: 0.951141] [G loss: 1.600887]\n",
      "[D loss: 0.942124] [G loss: 1.520260]\n",
      "[D loss: 0.827156] [G loss: 1.576336]\n",
      "[D loss: 1.026435] [G loss: 1.496958]\n",
      "[D loss: 0.818335] [G loss: 1.474694]\n",
      "[D loss: 1.008797] [G loss: 1.435617]\n",
      "[D loss: 0.674937] [G loss: 1.424662]\n",
      "[D loss: 0.824439] [G loss: 1.224785]\n",
      "[D loss: 0.751319] [G loss: 1.360747]\n",
      "[D loss: 0.928067] [G loss: 1.436159]\n",
      "[D loss: 0.759458] [G loss: 1.433789]\n",
      "[D loss: 0.750491] [G loss: 1.386680]\n",
      "[D loss: 0.608314] [G loss: 1.554461]\n",
      "[D loss: 0.901581] [G loss: 1.512452]\n",
      "[D loss: 0.729614] [G loss: 1.605502]\n",
      "[D loss: 0.723264] [G loss: 1.707205]\n",
      "[D loss: 1.237265] [G loss: 1.444503]\n",
      "[D loss: 0.829226] [G loss: 1.506746]\n",
      "[D loss: 0.866070] [G loss: 1.170344]\n",
      "[D loss: 0.774004] [G loss: 1.484348]\n",
      "[D loss: 0.691294] [G loss: 1.465293]\n",
      "[D loss: 0.778144] [G loss: 1.607098]\n",
      "[D loss: 1.034885] [G loss: 1.552296]\n",
      "[D loss: 0.626652] [G loss: 1.429382]\n",
      "[D loss: 0.846569] [G loss: 1.371321]\n",
      "[D loss: 1.078130] [G loss: 1.326638]\n",
      "[D loss: 1.017344] [G loss: 1.405763]\n",
      "[D loss: 0.930898] [G loss: 1.475830]\n",
      "[D loss: 0.988530] [G loss: 1.354798]\n",
      "[D loss: 0.840645] [G loss: 1.532735]\n",
      "[D loss: 0.774638] [G loss: 1.444825]\n",
      "[D loss: 0.812167] [G loss: 1.395125]\n",
      "[D loss: 0.850447] [G loss: 1.508116]\n",
      "[D loss: 0.790700] [G loss: 1.590267]\n",
      "[D loss: 0.711667] [G loss: 1.221224]\n",
      "[D loss: 0.763958] [G loss: 1.351798]\n",
      "[D loss: 0.899671] [G loss: 1.259110]\n",
      "[D loss: 0.791295] [G loss: 1.289424]\n",
      "[D loss: 0.858388] [G loss: 1.632524]\n",
      "[D loss: 0.860437] [G loss: 1.613817]\n",
      "[D loss: 0.966578] [G loss: 1.489165]\n",
      "[D loss: 0.877722] [G loss: 1.597266]\n",
      "[D loss: 1.058486] [G loss: 1.196530]\n",
      "[D loss: 1.094975] [G loss: 1.207146]\n",
      "[D loss: 0.984248] [G loss: 1.176044]\n",
      "[D loss: 0.994436] [G loss: 1.263319]\n",
      "[D loss: 1.014950] [G loss: 1.378623]\n",
      "[D loss: 0.715648] [G loss: 1.492747]\n",
      "[D loss: 0.928787] [G loss: 1.319981]\n",
      "[D loss: 1.033439] [G loss: 1.257882]\n",
      "[D loss: 0.717872] [G loss: 1.271401]\n",
      "[D loss: 0.801496] [G loss: 1.395673]\n",
      "[D loss: 0.827018] [G loss: 1.283303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.815910] [G loss: 1.392430]\n",
      "[D loss: 1.066159] [G loss: 1.305338]\n",
      "[D loss: 0.818843] [G loss: 1.623140]\n",
      "[D loss: 0.682656] [G loss: 1.555844]\n",
      "[D loss: 1.048990] [G loss: 1.234630]\n",
      "[D loss: 0.826581] [G loss: 1.484278]\n",
      "[D loss: 1.022210] [G loss: 1.516916]\n",
      "[D loss: 0.833179] [G loss: 1.530711]\n",
      "[D loss: 0.883403] [G loss: 1.301904]\n",
      "[D loss: 0.959148] [G loss: 1.233775]\n",
      "[D loss: 0.819951] [G loss: 1.575771]\n",
      "[D loss: 0.941987] [G loss: 1.399381]\n",
      "[D loss: 0.602908] [G loss: 1.476359]\n",
      "[D loss: 0.894962] [G loss: 1.587551]\n",
      "[D loss: 0.906422] [G loss: 1.456236]\n",
      "[D loss: 0.745329] [G loss: 1.419492]\n",
      "[D loss: 0.859745] [G loss: 1.452057]\n",
      "[D loss: 0.670936] [G loss: 1.387407]\n",
      "[D loss: 0.995579] [G loss: 1.509362]\n",
      "[D loss: 0.770353] [G loss: 1.367964]\n",
      "[D loss: 0.917692] [G loss: 1.601710]\n",
      "[D loss: 0.901883] [G loss: 1.474857]\n",
      "[D loss: 0.951551] [G loss: 1.513388]\n",
      "[D loss: 0.905003] [G loss: 1.473807]\n",
      "[D loss: 0.985252] [G loss: 1.414004]\n",
      "[D loss: 0.693940] [G loss: 1.635387]\n",
      "[D loss: 1.001230] [G loss: 1.462327]\n",
      "[D loss: 0.768706] [G loss: 1.402216]\n",
      "[D loss: 0.647288] [G loss: 1.578597]\n",
      "[D loss: 0.784203] [G loss: 1.582604]\n",
      "[D loss: 0.810306] [G loss: 1.334435]\n",
      "[D loss: 0.645296] [G loss: 1.463856]\n",
      "[D loss: 0.990964] [G loss: 1.291417]\n",
      "[D loss: 0.720950] [G loss: 1.639254]\n",
      "[D loss: 0.745478] [G loss: 1.482966]\n",
      "[D loss: 0.990026] [G loss: 1.448634]\n",
      "[D loss: 0.814887] [G loss: 1.554677]\n",
      "[D loss: 0.886142] [G loss: 1.277565]\n",
      "[D loss: 0.878650] [G loss: 1.393635]\n",
      "[D loss: 1.070497] [G loss: 1.483133]\n",
      "[D loss: 0.705200] [G loss: 1.734897]\n",
      "[D loss: 0.966050] [G loss: 1.335468]\n",
      "[D loss: 0.841443] [G loss: 1.472920]\n",
      "[D loss: 0.978896] [G loss: 1.488001]\n",
      "[D loss: 0.774213] [G loss: 1.456964]\n",
      "[D loss: 0.640981] [G loss: 1.562073]\n",
      "[D loss: 1.076813] [G loss: 1.216148]\n",
      "[D loss: 0.632916] [G loss: 1.524352]\n",
      "[D loss: 1.056533] [G loss: 1.509312]\n",
      "[D loss: 1.033508] [G loss: 1.495146]\n",
      "[D loss: 0.999670] [G loss: 1.329318]\n",
      "[D loss: 0.915250] [G loss: 1.275436]\n",
      "[D loss: 0.677164] [G loss: 1.348549]\n",
      "[D loss: 0.756533] [G loss: 1.173787]\n",
      "[D loss: 0.936124] [G loss: 1.352708]\n",
      "[D loss: 0.750967] [G loss: 1.344522]\n",
      "[D loss: 0.862132] [G loss: 1.718938]\n",
      "[D loss: 0.794051] [G loss: 1.604439]\n",
      "[D loss: 0.730435] [G loss: 1.382639]\n",
      "[D loss: 0.788366] [G loss: 1.419379]\n",
      "[D loss: 0.940733] [G loss: 1.304581]\n",
      "[D loss: 1.023331] [G loss: 1.215915]\n",
      "[D loss: 0.750751] [G loss: 1.267080]\n",
      "[D loss: 0.678986] [G loss: 1.631690]\n",
      "[D loss: 0.719871] [G loss: 1.591027]\n",
      "[D loss: 0.810680] [G loss: 1.434183]\n",
      "[D loss: 0.731799] [G loss: 1.412823]\n",
      "[D loss: 0.748012] [G loss: 1.591880]\n",
      "[D loss: 1.116662] [G loss: 1.282511]\n",
      "[D loss: 0.760099] [G loss: 1.391908]\n",
      "[D loss: 1.147149] [G loss: 1.322928]\n",
      "[D loss: 0.709814] [G loss: 1.611076]\n",
      "[D loss: 0.975969] [G loss: 1.269595]\n",
      "[D loss: 0.762672] [G loss: 1.604201]\n",
      "[D loss: 0.853415] [G loss: 1.672859]\n",
      "[D loss: 0.886847] [G loss: 1.339456]\n",
      "[D loss: 0.821513] [G loss: 1.402511]\n",
      "[D loss: 0.805721] [G loss: 1.411119]\n",
      "[D loss: 1.110102] [G loss: 1.201808]\n",
      "[D loss: 0.965019] [G loss: 1.541102]\n",
      "[D loss: 0.833649] [G loss: 1.469503]\n",
      "[D loss: 0.894871] [G loss: 1.706272]\n",
      "[D loss: 0.755155] [G loss: 1.492780]\n",
      "[D loss: 0.655064] [G loss: 1.635066]\n",
      "[D loss: 1.018173] [G loss: 1.472774]\n",
      "[D loss: 0.994580] [G loss: 1.450678]\n",
      "[D loss: 0.926848] [G loss: 1.404145]\n",
      "[D loss: 0.936268] [G loss: 1.468309]\n",
      "[D loss: 0.906510] [G loss: 1.198146]\n",
      "[D loss: 0.816940] [G loss: 1.417392]\n",
      "[D loss: 0.975346] [G loss: 1.477961]\n",
      "[D loss: 0.697476] [G loss: 1.587907]\n",
      "[D loss: 0.615881] [G loss: 1.569761]\n",
      "[D loss: 0.840972] [G loss: 1.320494]\n",
      "[D loss: 1.032491] [G loss: 1.308490]\n",
      "[D loss: 0.628358] [G loss: 1.470952]\n",
      "[D loss: 1.058241] [G loss: 1.506472]\n",
      "[D loss: 0.672213] [G loss: 1.403082]\n",
      "[D loss: 0.924328] [G loss: 1.433956]\n",
      "[D loss: 0.790115] [G loss: 1.813212]\n",
      "[D loss: 0.930374] [G loss: 1.406686]\n",
      "[D loss: 0.793778] [G loss: 1.446830]\n",
      "[D loss: 1.018017] [G loss: 1.303643]\n",
      "[D loss: 0.800475] [G loss: 1.443554]\n",
      "[D loss: 0.771533] [G loss: 1.427495]\n",
      "[D loss: 0.947325] [G loss: 1.257944]\n",
      "[D loss: 0.964401] [G loss: 1.328052]\n",
      "[D loss: 1.065980] [G loss: 1.445598]\n",
      "[D loss: 0.665034] [G loss: 1.539467]\n",
      "[D loss: 0.674298] [G loss: 1.430393]\n",
      "[D loss: 0.925974] [G loss: 1.670799]\n",
      "[D loss: 0.885087] [G loss: 1.423651]\n",
      "[D loss: 0.915335] [G loss: 1.497026]\n",
      "[D loss: 0.953436] [G loss: 1.293550]\n",
      "[D loss: 0.957560] [G loss: 1.372467]\n",
      "[D loss: 0.969442] [G loss: 1.539678]\n",
      "[D loss: 0.736657] [G loss: 1.554943]\n",
      "[D loss: 0.932676] [G loss: 1.302727]\n",
      "[D loss: 0.638703] [G loss: 1.432175]\n",
      "[D loss: 0.972041] [G loss: 1.247523]\n",
      "[D loss: 0.734649] [G loss: 1.427576]\n",
      "[D loss: 0.812373] [G loss: 1.433516]\n",
      "[D loss: 0.967262] [G loss: 1.309221]\n",
      "[D loss: 0.863111] [G loss: 1.354460]\n",
      "[D loss: 0.700703] [G loss: 1.403706]\n",
      "[D loss: 0.801287] [G loss: 1.459308]\n",
      "[D loss: 0.861889] [G loss: 1.341041]\n",
      "[D loss: 1.033278] [G loss: 1.428959]\n",
      "[D loss: 0.958759] [G loss: 1.370118]\n",
      "[D loss: 0.839100] [G loss: 1.539464]\n",
      "[D loss: 0.744653] [G loss: 2.001541]\n",
      "[D loss: 0.949865] [G loss: 1.370939]\n",
      "[D loss: 1.056133] [G loss: 1.204328]\n",
      "[D loss: 0.798197] [G loss: 1.545951]\n",
      "[D loss: 0.923108] [G loss: 1.379547]\n",
      "[D loss: 0.565016] [G loss: 1.533721]\n",
      "[D loss: 0.857356] [G loss: 1.266902]\n",
      "[D loss: 0.853942] [G loss: 1.317471]\n",
      "[D loss: 0.819203] [G loss: 1.415693]\n",
      "[D loss: 0.738662] [G loss: 1.452185]\n",
      "[D loss: 0.817414] [G loss: 1.666217]\n",
      "[D loss: 0.647029] [G loss: 1.621510]\n",
      "[D loss: 0.656007] [G loss: 1.522830]\n",
      "[D loss: 0.824386] [G loss: 1.426386]\n",
      "[D loss: 1.079166] [G loss: 1.457580]\n",
      "[D loss: 1.018265] [G loss: 1.354443]\n",
      "[D loss: 0.861191] [G loss: 1.417865]\n",
      "[D loss: 0.866306] [G loss: 1.720412]\n",
      "[D loss: 0.668290] [G loss: 1.765440]\n",
      "[D loss: 0.710363] [G loss: 1.579440]\n",
      "[D loss: 0.831553] [G loss: 1.286116]\n",
      "[D loss: 0.823818] [G loss: 1.311824]\n",
      "[D loss: 0.690065] [G loss: 1.492735]\n",
      "[D loss: 0.781963] [G loss: 1.755603]\n",
      "[D loss: 0.753831] [G loss: 1.619054]\n",
      "[D loss: 0.818731] [G loss: 1.308348]\n",
      "[D loss: 0.736728] [G loss: 1.414754]\n",
      "[D loss: 0.971964] [G loss: 1.225979]\n",
      "[D loss: 0.739684] [G loss: 1.515540]\n",
      "[D loss: 1.061600] [G loss: 1.487580]\n",
      "[D loss: 0.829843] [G loss: 1.595916]\n",
      "[D loss: 0.960960] [G loss: 1.463993]\n",
      "[D loss: 1.050074] [G loss: 1.325406]\n",
      "[D loss: 0.670112] [G loss: 1.467617]\n",
      "[D loss: 0.894860] [G loss: 1.424206]\n",
      "[D loss: 1.016314] [G loss: 1.342961]\n",
      "[D loss: 1.073771] [G loss: 1.496232]\n",
      "[D loss: 0.700464] [G loss: 1.332794]\n",
      "[D loss: 0.790424] [G loss: 1.323324]\n",
      "[D loss: 0.684972] [G loss: 1.521559]\n",
      "[D loss: 0.711488] [G loss: 1.316328]\n",
      "[D loss: 0.952748] [G loss: 1.353956]\n",
      "[D loss: 0.933335] [G loss: 1.314971]\n",
      "[D loss: 0.909876] [G loss: 1.234112]\n",
      "[D loss: 0.873509] [G loss: 1.590838]\n",
      "[D loss: 0.835221] [G loss: 1.412879]\n",
      "[D loss: 0.835172] [G loss: 1.336591]\n",
      "[D loss: 0.868696] [G loss: 1.280380]\n",
      "[D loss: 0.849521] [G loss: 1.626725]\n",
      "[D loss: 0.883689] [G loss: 1.359973]\n",
      "[D loss: 0.710495] [G loss: 1.423945]\n",
      "[D loss: 0.568240] [G loss: 1.348680]\n",
      "[D loss: 0.763100] [G loss: 1.587073]\n",
      "[D loss: 0.737778] [G loss: 1.606304]\n",
      "[D loss: 0.766429] [G loss: 1.507848]\n",
      "[D loss: 0.978392] [G loss: 1.305989]\n",
      "[D loss: 0.914153] [G loss: 1.295790]\n",
      "[D loss: 0.778168] [G loss: 1.611695]\n",
      "[D loss: 0.760263] [G loss: 1.634607]\n",
      "[D loss: 0.904450] [G loss: 1.476995]\n",
      "[D loss: 0.720670] [G loss: 1.453747]\n",
      "[D loss: 0.786766] [G loss: 1.400490]\n",
      "[D loss: 0.851942] [G loss: 1.330500]\n",
      "[D loss: 0.893095] [G loss: 1.495560]\n",
      "[D loss: 0.769109] [G loss: 1.479631]\n",
      "[D loss: 0.793342] [G loss: 1.452403]\n",
      "[D loss: 0.996237] [G loss: 1.296105]\n",
      "[D loss: 0.918620] [G loss: 1.494679]\n",
      "[D loss: 0.889734] [G loss: 1.434888]\n",
      "[D loss: 0.692865] [G loss: 1.385266]\n",
      "[D loss: 0.641171] [G loss: 1.274713]\n",
      "[D loss: 0.679338] [G loss: 1.478313]\n",
      "[D loss: 0.754932] [G loss: 1.433837]\n",
      "[D loss: 0.754218] [G loss: 1.290371]\n",
      "[D loss: 0.698477] [G loss: 1.582510]\n",
      "[D loss: 0.750198] [G loss: 1.313085]\n",
      "[D loss: 0.792519] [G loss: 1.494922]\n",
      "[D loss: 0.795403] [G loss: 1.769932]\n",
      "[D loss: 0.752054] [G loss: 1.726183]\n",
      "[D loss: 0.917271] [G loss: 1.440401]\n",
      "[D loss: 0.979802] [G loss: 1.547389]\n",
      "[D loss: 0.838303] [G loss: 1.608517]\n",
      "[D loss: 0.886480] [G loss: 1.404213]\n",
      "[D loss: 0.774149] [G loss: 1.509590]\n",
      "[D loss: 0.827442] [G loss: 1.459777]\n",
      "[D loss: 0.942288] [G loss: 1.436080]\n",
      "[D loss: 0.850481] [G loss: 1.595862]\n",
      "[D loss: 0.714709] [G loss: 1.470141]\n",
      "[D loss: 0.673839] [G loss: 1.893012]\n",
      "[D loss: 0.858206] [G loss: 1.460720]\n",
      "[D loss: 0.855824] [G loss: 1.469312]\n",
      "[D loss: 0.986464] [G loss: 1.455182]\n",
      "[D loss: 0.498587] [G loss: 1.562586]\n",
      "[D loss: 0.841048] [G loss: 1.579877]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.808787] [G loss: 1.684566]\n",
      "[D loss: 0.809783] [G loss: 1.567262]\n",
      "[D loss: 0.753013] [G loss: 1.441623]\n",
      "[D loss: 0.658590] [G loss: 1.485041]\n",
      "[D loss: 0.775498] [G loss: 1.648969]\n",
      "[D loss: 0.594726] [G loss: 1.694875]\n",
      "[D loss: 1.015736] [G loss: 1.734828]\n",
      "[D loss: 0.750567] [G loss: 1.598398]\n",
      "[D loss: 0.966696] [G loss: 1.466930]\n",
      "[D loss: 0.905368] [G loss: 1.529348]\n",
      "[D loss: 0.940900] [G loss: 1.450689]\n",
      "[D loss: 0.577617] [G loss: 1.632218]\n",
      "[D loss: 0.849482] [G loss: 1.556331]\n",
      "[D loss: 0.969368] [G loss: 1.359695]\n",
      "[D loss: 0.912467] [G loss: 1.366244]\n",
      "[D loss: 0.785516] [G loss: 1.510134]\n",
      "[D loss: 0.674922] [G loss: 1.674316]\n",
      "[D loss: 0.867633] [G loss: 1.322776]\n",
      "[D loss: 0.920346] [G loss: 1.379192]\n",
      "[D loss: 0.921602] [G loss: 1.322483]\n",
      "[D loss: 0.897971] [G loss: 1.447985]\n",
      "[D loss: 0.719953] [G loss: 1.465446]\n",
      "[D loss: 0.763876] [G loss: 1.471265]\n",
      "[D loss: 0.727450] [G loss: 1.816705]\n",
      "[D loss: 0.792964] [G loss: 1.613178]\n",
      "[D loss: 1.059218] [G loss: 1.346295]\n",
      "[D loss: 0.761608] [G loss: 1.519055]\n",
      "[D loss: 0.914361] [G loss: 1.462841]\n",
      "[D loss: 0.841994] [G loss: 1.281066]\n",
      "[D loss: 0.788785] [G loss: 1.429399]\n",
      "[D loss: 0.471515] [G loss: 1.587753]\n",
      "[D loss: 0.886257] [G loss: 1.531360]\n",
      "[D loss: 1.091761] [G loss: 1.290391]\n",
      "[D loss: 0.565577] [G loss: 1.806538]\n",
      "[D loss: 1.099538] [G loss: 1.548281]\n",
      "[D loss: 0.839833] [G loss: 1.366465]\n",
      "[D loss: 0.760551] [G loss: 1.448959]\n",
      "[D loss: 0.997915] [G loss: 1.621055]\n",
      "[D loss: 0.766500] [G loss: 1.527520]\n",
      "[D loss: 1.039224] [G loss: 1.299148]\n",
      "[D loss: 1.021190] [G loss: 1.548886]\n",
      "[D loss: 0.975615] [G loss: 1.531552]\n",
      "[D loss: 0.877223] [G loss: 1.393512]\n",
      "[D loss: 0.797996] [G loss: 1.682421]\n",
      "[D loss: 0.864645] [G loss: 1.374825]\n",
      "[D loss: 1.032111] [G loss: 1.209499]\n",
      "[D loss: 0.798026] [G loss: 1.336428]\n",
      "[D loss: 0.783835] [G loss: 1.598184]\n",
      "[D loss: 0.877763] [G loss: 1.414167]\n",
      "[D loss: 0.647914] [G loss: 1.414697]\n",
      "[D loss: 0.863021] [G loss: 1.573913]\n",
      "[D loss: 0.832992] [G loss: 1.640902]\n",
      "[D loss: 0.727482] [G loss: 1.518817]\n",
      "[D loss: 1.131496] [G loss: 1.319698]\n",
      "[D loss: 1.097004] [G loss: 1.488902]\n",
      "[D loss: 0.772653] [G loss: 1.439215]\n",
      "[D loss: 0.967984] [G loss: 1.378259]\n",
      "[D loss: 1.031507] [G loss: 1.423682]\n",
      "[D loss: 0.806418] [G loss: 1.500536]\n",
      "[D loss: 0.879307] [G loss: 1.389657]\n",
      "[D loss: 0.911871] [G loss: 1.296110]\n",
      "[D loss: 0.878366] [G loss: 1.432045]\n",
      "[D loss: 1.108664] [G loss: 1.310539]\n",
      "[D loss: 0.764345] [G loss: 1.453857]\n",
      "[D loss: 0.689626] [G loss: 1.199786]\n",
      "[D loss: 0.871108] [G loss: 1.577914]\n",
      "[D loss: 0.843881] [G loss: 1.362270]\n",
      "[D loss: 0.669156] [G loss: 1.796738]\n",
      "[D loss: 0.713854] [G loss: 1.627259]\n",
      "[D loss: 0.613781] [G loss: 1.500985]\n",
      "[D loss: 0.856228] [G loss: 1.397909]\n",
      "[D loss: 0.896917] [G loss: 1.245222]\n",
      "[D loss: 0.792154] [G loss: 1.586704]\n",
      "[D loss: 0.944195] [G loss: 1.412765]\n",
      "[D loss: 0.827665] [G loss: 1.677699]\n",
      "[D loss: 1.030655] [G loss: 1.375710]\n",
      "[D loss: 0.744346] [G loss: 1.592587]\n",
      "[D loss: 0.716243] [G loss: 1.653111]\n",
      "[D loss: 0.762951] [G loss: 1.306465]\n",
      "[D loss: 0.836861] [G loss: 1.410897]\n",
      "[D loss: 0.818427] [G loss: 1.374159]\n",
      "[D loss: 0.821742] [G loss: 1.418198]\n",
      "[D loss: 0.988844] [G loss: 1.476788]\n",
      "[D loss: 0.720563] [G loss: 1.591606]\n",
      "[D loss: 0.808676] [G loss: 1.335162]\n",
      "[D loss: 0.738715] [G loss: 1.432669]\n",
      "[D loss: 0.696827] [G loss: 1.407347]\n",
      "[D loss: 0.748034] [G loss: 1.623735]\n",
      "[D loss: 0.970793] [G loss: 1.539376]\n",
      "[D loss: 0.837021] [G loss: 1.527035]\n",
      "[D loss: 0.693549] [G loss: 1.427340]\n",
      "[D loss: 0.891653] [G loss: 1.390248]\n",
      "[D loss: 1.067283] [G loss: 1.622619]\n",
      "[D loss: 0.877441] [G loss: 1.575032]\n",
      "[D loss: 0.656435] [G loss: 1.645170]\n",
      "[D loss: 0.871377] [G loss: 1.472713]\n",
      "[D loss: 0.665528] [G loss: 1.464771]\n",
      "[D loss: 0.756062] [G loss: 1.575743]\n",
      "[D loss: 0.727309] [G loss: 1.481765]\n",
      "[D loss: 0.793786] [G loss: 1.517558]\n",
      "[D loss: 0.898573] [G loss: 1.730060]\n",
      "[D loss: 0.846043] [G loss: 1.434781]\n",
      "[D loss: 0.716409] [G loss: 1.603586]\n",
      "[D loss: 0.961323] [G loss: 1.384736]\n",
      "[D loss: 0.797650] [G loss: 1.378726]\n",
      "[D loss: 1.009768] [G loss: 1.503640]\n",
      "[D loss: 1.045501] [G loss: 1.457277]\n",
      "[D loss: 0.855745] [G loss: 1.444974]\n",
      "[D loss: 0.808199] [G loss: 1.514101]\n",
      "[D loss: 0.754922] [G loss: 1.422903]\n",
      "[D loss: 0.795988] [G loss: 1.387258]\n",
      "[D loss: 0.675735] [G loss: 1.457090]\n",
      "[D loss: 0.817251] [G loss: 1.302011]\n",
      "[D loss: 0.933845] [G loss: 1.545844]\n",
      "[D loss: 0.777198] [G loss: 1.530924]\n",
      "[D loss: 0.649681] [G loss: 1.459002]\n",
      "[D loss: 0.709522] [G loss: 1.397070]\n",
      "[D loss: 0.853520] [G loss: 1.390933]\n",
      "[D loss: 0.736709] [G loss: 1.590866]\n",
      "[D loss: 0.855231] [G loss: 1.558728]\n",
      "[D loss: 0.749961] [G loss: 1.469087]\n",
      "[D loss: 1.071142] [G loss: 1.462639]\n",
      "[D loss: 1.125699] [G loss: 1.263244]\n",
      "[D loss: 0.709758] [G loss: 1.560176]\n",
      "[D loss: 0.970473] [G loss: 1.373948]\n",
      "[D loss: 1.053087] [G loss: 1.447511]\n",
      "[D loss: 1.128332] [G loss: 1.349277]\n",
      "[D loss: 0.752986] [G loss: 1.476818]\n",
      "[D loss: 0.838761] [G loss: 1.575554]\n",
      "[D loss: 1.005178] [G loss: 1.398100]\n",
      "[D loss: 0.702871] [G loss: 1.346189]\n",
      "[D loss: 0.673156] [G loss: 1.433871]\n",
      "[D loss: 0.765790] [G loss: 1.509931]\n",
      "[D loss: 0.842633] [G loss: 1.274991]\n",
      "[D loss: 0.738441] [G loss: 1.189602]\n",
      "[D loss: 1.001808] [G loss: 1.521540]\n",
      "[D loss: 0.733709] [G loss: 1.487717]\n",
      "[D loss: 0.709758] [G loss: 1.618080]\n",
      "[D loss: 0.774710] [G loss: 1.556108]\n",
      "[D loss: 0.881834] [G loss: 1.544805]\n",
      "[D loss: 0.750000] [G loss: 1.453260]\n",
      "[D loss: 0.898673] [G loss: 1.418803]\n",
      "[D loss: 0.935723] [G loss: 1.584309]\n",
      "[D loss: 0.851292] [G loss: 1.509615]\n",
      "[D loss: 0.830410] [G loss: 1.576087]\n",
      "[D loss: 0.701068] [G loss: 1.592618]\n",
      "[D loss: 0.696895] [G loss: 1.529157]\n",
      "[D loss: 0.642431] [G loss: 1.561756]\n",
      "[D loss: 1.143570] [G loss: 1.234370]\n",
      "[D loss: 0.677629] [G loss: 1.527628]\n",
      "[D loss: 0.726706] [G loss: 1.703819]\n",
      "[D loss: 0.847472] [G loss: 1.290665]\n",
      "[D loss: 0.757874] [G loss: 1.356965]\n",
      "[D loss: 0.782711] [G loss: 1.597712]\n",
      "[D loss: 0.692194] [G loss: 1.553079]\n",
      "[D loss: 0.964126] [G loss: 1.171423]\n",
      "[D loss: 0.562634] [G loss: 1.601026]\n",
      "[D loss: 0.878921] [G loss: 1.339895]\n",
      "[D loss: 1.093550] [G loss: 1.472282]\n",
      "[D loss: 0.627612] [G loss: 1.413476]\n",
      "[D loss: 0.917509] [G loss: 1.477726]\n",
      "[D loss: 0.786741] [G loss: 1.416262]\n",
      "[D loss: 0.700623] [G loss: 1.340878]\n",
      "[D loss: 0.809389] [G loss: 1.514248]\n",
      "[D loss: 0.701256] [G loss: 1.619284]\n",
      "[D loss: 1.130575] [G loss: 1.400153]\n",
      "[D loss: 0.747460] [G loss: 1.492221]\n",
      "[D loss: 0.872626] [G loss: 1.251967]\n",
      "[D loss: 0.627852] [G loss: 1.628928]\n",
      "[D loss: 0.825461] [G loss: 1.557935]\n",
      "[D loss: 0.814863] [G loss: 1.508606]\n",
      "[D loss: 1.072760] [G loss: 1.459557]\n",
      "[D loss: 0.714498] [G loss: 1.568534]\n",
      "[D loss: 0.995333] [G loss: 1.371900]\n",
      "[D loss: 0.757931] [G loss: 1.284873]\n",
      "[D loss: 0.979446] [G loss: 1.380336]\n",
      "[D loss: 0.561504] [G loss: 1.571136]\n",
      "[D loss: 0.774804] [G loss: 1.411700]\n",
      "[D loss: 0.836337] [G loss: 1.427831]\n",
      "[D loss: 0.783087] [G loss: 1.493620]\n",
      "[D loss: 0.733278] [G loss: 1.414634]\n",
      "[D loss: 0.962009] [G loss: 1.398390]\n",
      "[D loss: 0.689235] [G loss: 1.652063]\n",
      "[D loss: 0.933899] [G loss: 1.628436]\n",
      "[D loss: 0.773129] [G loss: 1.430347]\n",
      "[D loss: 0.762388] [G loss: 1.419281]\n",
      "[D loss: 0.831486] [G loss: 1.478176]\n",
      "[D loss: 0.810675] [G loss: 1.564574]\n",
      "[D loss: 0.930286] [G loss: 1.355388]\n",
      "[D loss: 0.991116] [G loss: 1.439333]\n",
      "[D loss: 0.910097] [G loss: 1.581600]\n",
      "[D loss: 0.946527] [G loss: 1.311438]\n",
      "[D loss: 0.878646] [G loss: 1.474308]\n",
      "[D loss: 0.648407] [G loss: 1.442114]\n",
      "[D loss: 0.719861] [G loss: 1.620324]\n",
      "[D loss: 0.594033] [G loss: 1.544993]\n",
      "[D loss: 0.893874] [G loss: 1.572788]\n",
      "[D loss: 0.943092] [G loss: 1.376055]\n",
      "[D loss: 0.722061] [G loss: 1.352885]\n",
      "[D loss: 0.828931] [G loss: 1.521739]\n",
      "[D loss: 0.834367] [G loss: 1.358623]\n",
      "[D loss: 0.948915] [G loss: 1.479287]\n",
      "[D loss: 0.859626] [G loss: 1.659566]\n",
      "[D loss: 0.714107] [G loss: 1.620779]\n",
      "[D loss: 0.868005] [G loss: 1.404104]\n",
      "[D loss: 0.879431] [G loss: 1.348361]\n",
      "[D loss: 0.819180] [G loss: 1.483918]\n",
      "[D loss: 0.901900] [G loss: 1.512918]\n",
      "[D loss: 0.658856] [G loss: 1.609074]\n",
      "[D loss: 0.972124] [G loss: 1.258136]\n",
      "[D loss: 0.693102] [G loss: 1.662278]\n",
      "[D loss: 0.862062] [G loss: 1.520385]\n",
      "[D loss: 0.735102] [G loss: 1.538302]\n",
      "[D loss: 0.710128] [G loss: 1.673450]\n",
      "[D loss: 0.830210] [G loss: 1.502365]\n",
      "[D loss: 0.843916] [G loss: 1.385138]\n",
      "[D loss: 0.663419] [G loss: 1.648456]\n",
      "[D loss: 0.978410] [G loss: 1.540101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.678445] [G loss: 1.948153]\n",
      "[D loss: 0.674246] [G loss: 1.377158]\n",
      "[D loss: 0.886173] [G loss: 1.631675]\n",
      "[D loss: 0.917967] [G loss: 1.558346]\n",
      "[D loss: 0.925819] [G loss: 1.647858]\n",
      "[D loss: 1.032038] [G loss: 1.428436]\n",
      "[D loss: 0.653786] [G loss: 1.354205]\n",
      "[D loss: 1.052910] [G loss: 1.179400]\n",
      "[D loss: 0.733498] [G loss: 1.626569]\n",
      "[D loss: 1.000417] [G loss: 1.412408]\n",
      "[D loss: 0.660429] [G loss: 1.490152]\n",
      "[D loss: 0.935788] [G loss: 1.441113]\n",
      "[D loss: 0.969970] [G loss: 1.554390]\n",
      "[D loss: 0.845775] [G loss: 1.454135]\n",
      "[D loss: 0.874100] [G loss: 1.541904]\n",
      "[D loss: 0.726910] [G loss: 1.439938]\n",
      "[D loss: 0.733519] [G loss: 1.287542]\n",
      "[D loss: 0.736154] [G loss: 1.344127]\n",
      "[D loss: 0.626846] [G loss: 1.405432]\n",
      "[D loss: 1.084524] [G loss: 1.438001]\n",
      "[D loss: 0.790671] [G loss: 1.604479]\n",
      "[D loss: 0.880580] [G loss: 1.497264]\n",
      "[D loss: 0.967415] [G loss: 1.363581]\n",
      "[D loss: 0.873986] [G loss: 1.494618]\n",
      "[D loss: 1.035165] [G loss: 1.650724]\n",
      "[D loss: 0.901221] [G loss: 1.330319]\n",
      "[D loss: 0.952196] [G loss: 1.461467]\n",
      "[D loss: 1.011783] [G loss: 1.304515]\n",
      "[D loss: 0.659961] [G loss: 1.481648]\n",
      "[D loss: 0.915515] [G loss: 1.516171]\n",
      "[D loss: 0.934951] [G loss: 1.630417]\n",
      "[D loss: 0.776939] [G loss: 1.357183]\n",
      "[D loss: 0.815824] [G loss: 1.416598]\n",
      "[D loss: 0.847774] [G loss: 1.445912]\n",
      "[D loss: 0.710374] [G loss: 1.560862]\n",
      "[D loss: 0.703345] [G loss: 1.392415]\n",
      "[D loss: 0.861264] [G loss: 1.404182]\n",
      "[D loss: 0.736421] [G loss: 1.444755]\n",
      "[D loss: 0.612450] [G loss: 1.746641]\n",
      "[D loss: 0.857307] [G loss: 1.652110]\n",
      "[D loss: 0.745687] [G loss: 1.677892]\n",
      "[D loss: 0.865297] [G loss: 1.653937]\n",
      "[D loss: 0.695282] [G loss: 1.615186]\n",
      "[D loss: 0.723683] [G loss: 1.503754]\n",
      "[D loss: 1.106541] [G loss: 1.471770]\n",
      "[D loss: 0.941991] [G loss: 1.477105]\n",
      "[D loss: 0.784007] [G loss: 1.340411]\n",
      "[D loss: 0.577911] [G loss: 1.522638]\n",
      "[D loss: 0.840257] [G loss: 1.795722]\n",
      "[D loss: 0.794216] [G loss: 1.513819]\n",
      "[D loss: 0.981737] [G loss: 1.511780]\n",
      "[D loss: 0.690147] [G loss: 1.657263]\n",
      "[D loss: 0.761895] [G loss: 1.448899]\n",
      "[D loss: 0.831817] [G loss: 1.447059]\n",
      "[D loss: 0.800512] [G loss: 1.271686]\n",
      "[D loss: 0.567043] [G loss: 1.651292]\n",
      "[D loss: 0.723541] [G loss: 1.680982]\n",
      "[D loss: 0.980180] [G loss: 1.689073]\n",
      "[D loss: 0.952909] [G loss: 1.422395]\n",
      "[D loss: 0.839736] [G loss: 1.409446]\n",
      "[D loss: 0.854602] [G loss: 1.621545]\n",
      "[D loss: 0.696489] [G loss: 1.304401]\n",
      "[D loss: 0.985190] [G loss: 1.499969]\n",
      "[D loss: 0.851477] [G loss: 1.767746]\n",
      "[D loss: 1.042331] [G loss: 1.731962]\n",
      "[D loss: 0.808622] [G loss: 1.513088]\n",
      "[D loss: 0.836526] [G loss: 1.431385]\n",
      "[D loss: 0.818318] [G loss: 1.532167]\n",
      "[D loss: 0.912657] [G loss: 1.337555]\n",
      "[D loss: 0.898198] [G loss: 1.374465]\n",
      "[D loss: 0.794222] [G loss: 1.397291]\n",
      "[D loss: 0.845325] [G loss: 1.433640]\n",
      "[D loss: 0.659071] [G loss: 1.432431]\n",
      "[D loss: 0.799274] [G loss: 1.500466]\n",
      "[D loss: 0.800876] [G loss: 1.355473]\n",
      "[D loss: 0.911116] [G loss: 1.285920]\n",
      "[D loss: 0.875855] [G loss: 1.395327]\n",
      "[D loss: 0.588119] [G loss: 1.347187]\n",
      "[D loss: 0.770752] [G loss: 1.554427]\n",
      "[D loss: 0.771953] [G loss: 1.122006]\n",
      "[D loss: 0.844331] [G loss: 1.476267]\n",
      "[D loss: 0.689092] [G loss: 1.641277]\n",
      "[D loss: 0.830580] [G loss: 1.373212]\n",
      "[D loss: 0.716388] [G loss: 1.589484]\n",
      "[D loss: 0.669251] [G loss: 1.809999]\n",
      "[D loss: 0.974680] [G loss: 1.510462]\n",
      "[D loss: 0.870237] [G loss: 1.493672]\n",
      "[D loss: 0.912519] [G loss: 1.387723]\n",
      "[D loss: 0.655592] [G loss: 1.467533]\n",
      "[D loss: 0.777757] [G loss: 1.636226]\n",
      "[D loss: 0.895671] [G loss: 1.237307]\n",
      "[D loss: 0.949644] [G loss: 1.692253]\n",
      "[D loss: 0.578627] [G loss: 1.500856]\n",
      "[D loss: 0.838076] [G loss: 1.536152]\n",
      "[D loss: 0.799562] [G loss: 1.386126]\n",
      "[D loss: 0.749255] [G loss: 1.562973]\n",
      "[D loss: 0.919798] [G loss: 1.661318]\n",
      "[D loss: 0.732148] [G loss: 1.648560]\n",
      "[D loss: 0.772768] [G loss: 1.704102]\n",
      "[D loss: 0.618111] [G loss: 1.960347]\n",
      "[D loss: 0.849771] [G loss: 1.338093]\n",
      "[D loss: 0.688215] [G loss: 1.841544]\n",
      "[D loss: 0.943634] [G loss: 1.521256]\n",
      "[D loss: 0.954252] [G loss: 1.459046]\n",
      "[D loss: 0.678050] [G loss: 1.623030]\n",
      "[D loss: 0.750287] [G loss: 1.587665]\n",
      "[D loss: 0.764377] [G loss: 1.449990]\n",
      "[D loss: 0.880929] [G loss: 1.296187]\n",
      "[D loss: 0.740746] [G loss: 1.486531]\n",
      "[D loss: 1.053525] [G loss: 1.452221]\n",
      "[D loss: 0.832566] [G loss: 1.572785]\n",
      "[D loss: 0.887652] [G loss: 1.919988]\n",
      "[D loss: 0.779088] [G loss: 1.422221]\n",
      "[D loss: 0.898512] [G loss: 1.133979]\n",
      "[D loss: 0.870388] [G loss: 1.301106]\n",
      "[D loss: 1.073545] [G loss: 1.615352]\n",
      "[D loss: 0.915098] [G loss: 1.392652]\n",
      "[D loss: 0.843296] [G loss: 1.274230]\n",
      "[D loss: 0.973250] [G loss: 1.422249]\n",
      "[D loss: 0.881186] [G loss: 1.427570]\n",
      "[D loss: 0.764140] [G loss: 1.610962]\n",
      "[D loss: 1.130358] [G loss: 1.167318]\n",
      "[D loss: 0.834574] [G loss: 1.286572]\n",
      "[D loss: 0.651614] [G loss: 1.439437]\n",
      "[D loss: 0.650506] [G loss: 1.548977]\n",
      "[D loss: 1.071109] [G loss: 1.520596]\n",
      "[D loss: 0.887617] [G loss: 1.353240]\n",
      "[D loss: 0.869827] [G loss: 1.502050]\n",
      "[D loss: 0.970545] [G loss: 1.354662]\n",
      "[D loss: 0.773171] [G loss: 1.436991]\n",
      "[D loss: 0.940115] [G loss: 1.252141]\n",
      "[D loss: 0.855342] [G loss: 1.395108]\n",
      "[D loss: 0.791864] [G loss: 1.269476]\n",
      "[D loss: 0.914581] [G loss: 1.436121]\n",
      "[D loss: 0.825741] [G loss: 1.340546]\n",
      "[D loss: 0.883546] [G loss: 1.526671]\n",
      "[D loss: 0.899265] [G loss: 1.481203]\n",
      "[D loss: 0.758858] [G loss: 1.476501]\n",
      "[D loss: 0.909750] [G loss: 1.354542]\n",
      "[D loss: 0.707091] [G loss: 1.295742]\n",
      "[D loss: 0.908706] [G loss: 1.368442]\n",
      "[D loss: 0.809486] [G loss: 1.592085]\n",
      "[D loss: 0.886803] [G loss: 1.770353]\n",
      "[D loss: 0.731322] [G loss: 1.679415]\n",
      "[D loss: 0.732279] [G loss: 1.515786]\n",
      "[D loss: 0.821959] [G loss: 1.746963]\n",
      "[D loss: 0.887314] [G loss: 1.555676]\n",
      "[D loss: 0.989340] [G loss: 1.234421]\n",
      "[D loss: 0.856979] [G loss: 1.503332]\n",
      "[D loss: 0.812235] [G loss: 1.283267]\n",
      "[D loss: 1.035594] [G loss: 1.433863]\n",
      "[D loss: 0.801246] [G loss: 1.353232]\n",
      "[D loss: 0.877779] [G loss: 1.426878]\n",
      "[D loss: 0.911333] [G loss: 1.622133]\n",
      "[D loss: 0.714458] [G loss: 1.562685]\n",
      "[D loss: 0.986867] [G loss: 1.559857]\n",
      "[D loss: 0.827540] [G loss: 1.420809]\n",
      "[D loss: 0.803095] [G loss: 1.344055]\n",
      "[D loss: 1.046851] [G loss: 1.350776]\n",
      "[D loss: 0.743470] [G loss: 1.622058]\n",
      "[D loss: 0.667610] [G loss: 1.619802]\n",
      "[D loss: 0.994023] [G loss: 1.508101]\n",
      "[D loss: 0.921468] [G loss: 1.337825]\n",
      "[D loss: 0.714749] [G loss: 1.364714]\n",
      "[D loss: 0.911902] [G loss: 1.256840]\n",
      "[D loss: 0.881409] [G loss: 1.501140]\n",
      "[D loss: 1.067458] [G loss: 1.414757]\n",
      "[D loss: 1.029965] [G loss: 1.422500]\n",
      "[D loss: 0.822526] [G loss: 1.425408]\n",
      "[D loss: 0.787351] [G loss: 1.572168]\n",
      "[D loss: 0.952098] [G loss: 1.384166]\n",
      "[D loss: 1.087219] [G loss: 1.473157]\n",
      "[D loss: 0.699704] [G loss: 1.481901]\n",
      "[D loss: 0.863650] [G loss: 1.360098]\n",
      "[D loss: 0.776823] [G loss: 1.557371]\n",
      "[D loss: 0.985914] [G loss: 1.273469]\n",
      "[D loss: 0.941135] [G loss: 1.417657]\n",
      "[D loss: 0.738007] [G loss: 1.511099]\n",
      "[D loss: 0.756041] [G loss: 1.660805]\n",
      "[D loss: 0.817823] [G loss: 1.428188]\n",
      "[D loss: 0.899481] [G loss: 1.392143]\n",
      "[D loss: 0.986188] [G loss: 1.408849]\n",
      "[D loss: 0.677906] [G loss: 1.443052]\n",
      "[D loss: 0.837182] [G loss: 1.157796]\n",
      "[D loss: 0.786565] [G loss: 1.585975]\n",
      "[D loss: 0.990884] [G loss: 1.556966]\n",
      "[D loss: 1.055018] [G loss: 1.564641]\n",
      "[D loss: 0.763524] [G loss: 1.495381]\n",
      "[D loss: 0.717659] [G loss: 1.397331]\n",
      "[D loss: 0.953023] [G loss: 1.321454]\n",
      "[D loss: 1.028538] [G loss: 1.238601]\n",
      "[D loss: 0.753650] [G loss: 1.349242]\n",
      "[D loss: 0.772215] [G loss: 1.209224]\n",
      "[D loss: 0.782272] [G loss: 1.451814]\n",
      "[D loss: 0.727245] [G loss: 1.414091]\n",
      "[D loss: 0.801844] [G loss: 1.528211]\n",
      "[D loss: 0.900208] [G loss: 1.731642]\n",
      "[D loss: 0.841630] [G loss: 1.365881]\n",
      "[D loss: 0.829773] [G loss: 1.439344]\n",
      "[D loss: 1.045289] [G loss: 1.433189]\n",
      "[D loss: 1.229453] [G loss: 1.269463]\n",
      "[D loss: 0.889273] [G loss: 1.461488]\n",
      "[D loss: 1.102152] [G loss: 1.283207]\n",
      "[D loss: 0.933814] [G loss: 1.386786]\n",
      "[D loss: 0.938405] [G loss: 1.309071]\n",
      "[D loss: 1.092516] [G loss: 1.375971]\n",
      "[D loss: 0.710347] [G loss: 1.240836]\n",
      "[D loss: 0.954579] [G loss: 1.382112]\n",
      "[D loss: 1.012341] [G loss: 1.155215]\n",
      "[D loss: 0.893434] [G loss: 1.276086]\n",
      "[D loss: 0.628987] [G loss: 1.495545]\n",
      "[D loss: 0.944751] [G loss: 1.434455]\n",
      "[D loss: 0.938484] [G loss: 1.175478]\n",
      "[D loss: 0.841493] [G loss: 1.401365]\n",
      "[D loss: 0.848853] [G loss: 1.181841]\n",
      "[D loss: 0.811171] [G loss: 1.402397]\n",
      "[D loss: 0.704227] [G loss: 1.258012]\n",
      "[D loss: 0.795342] [G loss: 1.439415]\n",
      "[D loss: 0.954038] [G loss: 1.250929]\n",
      "[D loss: 0.759604] [G loss: 1.711290]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.680129] [G loss: 1.357243]\n",
      "[D loss: 0.809047] [G loss: 1.451787]\n",
      "[D loss: 0.944561] [G loss: 1.701678]\n",
      "[D loss: 0.937766] [G loss: 1.156631]\n",
      "[D loss: 0.751608] [G loss: 1.346983]\n",
      "[D loss: 0.716230] [G loss: 1.407508]\n",
      "[D loss: 0.756856] [G loss: 1.437288]\n",
      "[D loss: 0.766525] [G loss: 1.609068]\n",
      "[D loss: 0.835456] [G loss: 1.719065]\n",
      "[D loss: 0.960306] [G loss: 1.546864]\n",
      "[D loss: 0.695800] [G loss: 1.422653]\n",
      "[D loss: 0.910039] [G loss: 1.334286]\n",
      "[D loss: 0.902152] [G loss: 1.248862]\n",
      "[D loss: 0.937027] [G loss: 1.526469]\n",
      "[D loss: 0.984202] [G loss: 1.498174]\n",
      "[D loss: 0.826843] [G loss: 1.546242]\n",
      "[D loss: 0.770249] [G loss: 1.503307]\n",
      "[D loss: 0.685722] [G loss: 1.387394]\n",
      "[D loss: 0.976200] [G loss: 1.438451]\n",
      "[D loss: 0.729208] [G loss: 1.512949]\n",
      "[D loss: 0.947736] [G loss: 1.483140]\n",
      "[D loss: 1.002307] [G loss: 1.364526]\n",
      "[D loss: 0.833624] [G loss: 1.459661]\n",
      "[D loss: 0.874130] [G loss: 1.592766]\n",
      "[D loss: 0.734121] [G loss: 1.432606]\n",
      "[D loss: 1.124569] [G loss: 1.214520]\n",
      "[D loss: 1.072087] [G loss: 1.433584]\n",
      "[D loss: 0.946510] [G loss: 1.485512]\n",
      "[D loss: 0.804711] [G loss: 1.327830]\n",
      "[D loss: 0.786520] [G loss: 1.364342]\n",
      "[D loss: 0.923309] [G loss: 1.403557]\n",
      "[D loss: 0.784709] [G loss: 1.576983]\n",
      "[D loss: 0.626280] [G loss: 1.656909]\n",
      "[D loss: 0.852148] [G loss: 1.564376]\n",
      "[D loss: 0.873906] [G loss: 1.353726]\n",
      "[D loss: 1.132904] [G loss: 1.433251]\n",
      "[D loss: 0.819068] [G loss: 1.347982]\n",
      "[D loss: 0.668453] [G loss: 1.461718]\n",
      "[D loss: 0.931065] [G loss: 1.344240]\n",
      "[D loss: 0.779022] [G loss: 1.478960]\n",
      "[D loss: 0.754777] [G loss: 1.340473]\n",
      "[D loss: 0.874795] [G loss: 1.672829]\n",
      "[D loss: 0.847963] [G loss: 1.496492]\n",
      "[D loss: 0.708451] [G loss: 1.564708]\n",
      "[D loss: 0.733913] [G loss: 1.408585]\n",
      "[D loss: 0.875305] [G loss: 1.296731]\n",
      "[D loss: 0.985271] [G loss: 1.430057]\n",
      "[D loss: 0.774844] [G loss: 1.355644]\n",
      "[D loss: 0.753365] [G loss: 1.512022]\n",
      "[D loss: 0.667067] [G loss: 1.420648]\n",
      "[D loss: 0.679181] [G loss: 1.697542]\n",
      "[D loss: 1.062845] [G loss: 1.346422]\n",
      "[D loss: 0.826974] [G loss: 1.491943]\n",
      "[D loss: 0.869935] [G loss: 1.364460]\n",
      "[D loss: 0.911073] [G loss: 1.468043]\n",
      "[D loss: 1.107577] [G loss: 1.308957]\n",
      "[D loss: 0.891958] [G loss: 1.296945]\n",
      "[D loss: 0.781745] [G loss: 1.467509]\n",
      "[D loss: 0.752422] [G loss: 1.560299]\n",
      "[D loss: 0.837222] [G loss: 1.307516]\n",
      "[D loss: 0.760359] [G loss: 1.454499]\n",
      "[D loss: 1.177567] [G loss: 1.377120]\n",
      "[D loss: 0.812810] [G loss: 1.480455]\n",
      "[D loss: 0.659145] [G loss: 1.451100]\n",
      "[D loss: 0.962645] [G loss: 1.406253]\n",
      "[D loss: 0.963340] [G loss: 1.350390]\n",
      "[D loss: 0.977523] [G loss: 1.386707]\n",
      "[D loss: 0.610346] [G loss: 1.370041]\n",
      "[D loss: 0.797288] [G loss: 1.425083]\n",
      "[D loss: 0.750333] [G loss: 1.664052]\n",
      "[D loss: 0.865209] [G loss: 1.472853]\n",
      "[D loss: 0.933985] [G loss: 1.491945]\n",
      "[D loss: 0.951482] [G loss: 1.329473]\n",
      "[D loss: 0.718682] [G loss: 1.472085]\n",
      "[D loss: 0.781638] [G loss: 1.497467]\n",
      "[D loss: 0.715941] [G loss: 1.520301]\n",
      "[D loss: 0.916386] [G loss: 1.542502]\n",
      "[D loss: 0.928836] [G loss: 1.396692]\n",
      "[D loss: 0.991597] [G loss: 1.463099]\n",
      "[D loss: 0.876792] [G loss: 1.605052]\n",
      "[D loss: 0.667602] [G loss: 1.582135]\n",
      "[D loss: 0.725512] [G loss: 1.604749]\n",
      "[D loss: 0.785641] [G loss: 1.544360]\n",
      "[D loss: 0.899751] [G loss: 1.298259]\n",
      "[D loss: 0.918385] [G loss: 1.321828]\n",
      "[D loss: 0.896633] [G loss: 1.560187]\n",
      "[D loss: 0.680962] [G loss: 1.563119]\n",
      "[D loss: 1.249897] [G loss: 1.552462]\n",
      "[D loss: 0.908855] [G loss: 1.396041]\n",
      "[D loss: 0.984113] [G loss: 1.699461]\n",
      "[D loss: 0.724182] [G loss: 1.367655]\n",
      "[D loss: 1.130365] [G loss: 1.348272]\n",
      "[D loss: 0.814850] [G loss: 1.470093]\n",
      "[D loss: 0.762909] [G loss: 1.387192]\n",
      "[D loss: 0.813834] [G loss: 1.519447]\n",
      "[D loss: 0.704291] [G loss: 1.579743]\n",
      "[D loss: 0.736920] [G loss: 1.314885]\n",
      "[D loss: 0.818700] [G loss: 1.415266]\n",
      "[D loss: 0.903888] [G loss: 1.242279]\n",
      "[D loss: 0.793420] [G loss: 1.476074]\n",
      "[D loss: 0.578045] [G loss: 1.546776]\n",
      "[D loss: 0.853473] [G loss: 1.663201]\n",
      "[D loss: 0.890362] [G loss: 1.435682]\n",
      "[D loss: 0.657812] [G loss: 1.499962]\n",
      "[D loss: 0.967159] [G loss: 1.541149]\n",
      "[D loss: 0.907622] [G loss: 1.253983]\n",
      "[D loss: 0.838553] [G loss: 1.373492]\n",
      "[D loss: 0.817873] [G loss: 1.463676]\n",
      "[D loss: 0.786860] [G loss: 1.388867]\n",
      "[D loss: 0.953238] [G loss: 1.309853]\n",
      "[D loss: 0.589391] [G loss: 1.492967]\n",
      "[D loss: 0.706098] [G loss: 1.363105]\n",
      "[D loss: 0.556522] [G loss: 1.684485]\n",
      "[D loss: 0.796726] [G loss: 1.645650]\n",
      "[D loss: 0.875644] [G loss: 1.608781]\n",
      "[D loss: 1.028158] [G loss: 1.347440]\n",
      "[D loss: 1.020230] [G loss: 1.364390]\n",
      "[D loss: 0.816924] [G loss: 1.620376]\n",
      "[D loss: 0.751549] [G loss: 1.352497]\n",
      "[D loss: 0.831497] [G loss: 1.332291]\n",
      "[D loss: 0.689347] [G loss: 1.391586]\n",
      "[D loss: 0.897550] [G loss: 1.300686]\n",
      "[D loss: 0.774505] [G loss: 1.523800]\n",
      "[D loss: 0.825541] [G loss: 1.295608]\n",
      "[D loss: 0.738485] [G loss: 1.534759]\n",
      "[D loss: 0.819652] [G loss: 1.413561]\n",
      "[D loss: 1.031748] [G loss: 1.440156]\n",
      "[D loss: 0.912037] [G loss: 1.473059]\n",
      "[D loss: 0.729085] [G loss: 1.582547]\n",
      "[D loss: 0.878977] [G loss: 1.336246]\n",
      "[D loss: 0.890061] [G loss: 1.258436]\n",
      "[D loss: 0.799059] [G loss: 1.352874]\n",
      "[D loss: 0.734292] [G loss: 1.632544]\n",
      "[D loss: 0.806263] [G loss: 1.444239]\n",
      "[D loss: 0.952640] [G loss: 1.369765]\n",
      "[D loss: 0.984068] [G loss: 1.373285]\n",
      "[D loss: 0.603803] [G loss: 1.511449]\n",
      "[D loss: 0.879606] [G loss: 1.413230]\n",
      "[D loss: 0.730109] [G loss: 1.357837]\n",
      "[D loss: 1.085186] [G loss: 1.315016]\n",
      "[D loss: 0.931708] [G loss: 1.396984]\n",
      "[D loss: 0.665616] [G loss: 1.503923]\n",
      "[D loss: 0.891692] [G loss: 1.337848]\n",
      "[D loss: 0.993370] [G loss: 1.348037]\n",
      "[D loss: 0.650701] [G loss: 1.702201]\n",
      "[D loss: 0.768866] [G loss: 1.478719]\n",
      "[D loss: 0.592403] [G loss: 1.514857]\n",
      "[D loss: 0.761183] [G loss: 1.534728]\n",
      "[D loss: 0.743893] [G loss: 1.705263]\n",
      "[D loss: 0.993447] [G loss: 1.464632]\n",
      "[D loss: 0.788357] [G loss: 1.374658]\n",
      "[D loss: 0.960053] [G loss: 1.426191]\n",
      "[D loss: 0.727206] [G loss: 1.592376]\n",
      "[D loss: 0.657279] [G loss: 1.531703]\n",
      "[D loss: 0.794653] [G loss: 1.463723]\n",
      "[D loss: 0.572101] [G loss: 1.713614]\n",
      "[D loss: 0.886457] [G loss: 1.619333]\n",
      "[D loss: 0.755830] [G loss: 1.684314]\n",
      "[D loss: 1.066181] [G loss: 1.330123]\n",
      "[D loss: 0.905584] [G loss: 1.370763]\n",
      "[D loss: 0.821831] [G loss: 1.348861]\n",
      "[D loss: 0.750499] [G loss: 1.181111]\n",
      "[D loss: 0.963618] [G loss: 1.434496]\n",
      "[D loss: 0.777100] [G loss: 1.655469]\n",
      "[D loss: 1.001619] [G loss: 1.362960]\n",
      "[D loss: 0.705224] [G loss: 1.439059]\n",
      "[D loss: 0.673574] [G loss: 1.515467]\n",
      "[D loss: 0.952273] [G loss: 1.365607]\n",
      "[D loss: 0.935856] [G loss: 1.292000]\n",
      "[D loss: 0.918160] [G loss: 1.321528]\n",
      "[D loss: 0.828645] [G loss: 1.652139]\n",
      "[D loss: 0.840980] [G loss: 1.629027]\n",
      "[D loss: 0.829715] [G loss: 1.327834]\n",
      "[D loss: 0.991738] [G loss: 1.346388]\n",
      "[D loss: 0.668967] [G loss: 1.449065]\n",
      "[D loss: 1.017323] [G loss: 1.214283]\n",
      "[D loss: 0.827343] [G loss: 1.491836]\n",
      "[D loss: 0.684858] [G loss: 1.500539]\n",
      "[D loss: 0.816377] [G loss: 1.407198]\n",
      "[D loss: 0.986999] [G loss: 1.450686]\n",
      "[D loss: 0.640724] [G loss: 1.400370]\n",
      "[D loss: 0.728837] [G loss: 1.755715]\n",
      "[D loss: 0.862290] [G loss: 1.700079]\n",
      "[D loss: 0.747749] [G loss: 1.660172]\n",
      "[D loss: 0.994672] [G loss: 1.235472]\n",
      "[D loss: 0.773024] [G loss: 1.664969]\n",
      "[D loss: 0.838099] [G loss: 1.528023]\n",
      "[D loss: 0.969244] [G loss: 1.195473]\n",
      "[D loss: 0.644083] [G loss: 1.379524]\n",
      "[D loss: 0.843416] [G loss: 1.344447]\n",
      "[D loss: 0.927746] [G loss: 1.415047]\n",
      "[D loss: 0.806271] [G loss: 1.472769]\n",
      "[D loss: 0.901470] [G loss: 1.428539]\n",
      "[D loss: 0.887209] [G loss: 1.307014]\n",
      "[D loss: 0.785048] [G loss: 1.480958]\n",
      "[D loss: 0.850012] [G loss: 1.451771]\n",
      "[D loss: 0.925981] [G loss: 1.427615]\n",
      "[D loss: 0.790596] [G loss: 1.399064]\n",
      "[D loss: 1.151028] [G loss: 1.315552]\n",
      "[D loss: 0.857291] [G loss: 1.339141]\n",
      "[D loss: 0.801442] [G loss: 1.328717]\n",
      "[D loss: 0.898780] [G loss: 1.377377]\n",
      "[D loss: 0.747883] [G loss: 1.424806]\n",
      "[D loss: 0.828796] [G loss: 1.480243]\n",
      "[D loss: 0.710787] [G loss: 1.520200]\n",
      "[D loss: 0.771285] [G loss: 1.707341]\n",
      "[D loss: 0.854705] [G loss: 1.519916]\n",
      "[D loss: 0.840881] [G loss: 1.403007]\n",
      "[D loss: 0.577255] [G loss: 1.532650]\n",
      "[D loss: 0.937865] [G loss: 1.425491]\n",
      "[D loss: 0.892242] [G loss: 1.456611]\n",
      "[D loss: 0.932183] [G loss: 1.252791]\n",
      "[D loss: 0.855295] [G loss: 1.350743]\n",
      "[D loss: 0.931444] [G loss: 1.630076]\n",
      "[D loss: 0.872681] [G loss: 1.336765]\n",
      "[D loss: 0.948565] [G loss: 1.225932]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.784133] [G loss: 1.221138]\n",
      "[D loss: 0.833900] [G loss: 1.213584]\n",
      "[D loss: 1.056307] [G loss: 1.499860]\n",
      "[D loss: 0.636222] [G loss: 1.268969]\n",
      "[D loss: 0.684911] [G loss: 1.308519]\n",
      "[D loss: 0.944751] [G loss: 1.563491]\n",
      "[D loss: 0.612472] [G loss: 1.420836]\n",
      "[D loss: 0.740506] [G loss: 1.504979]\n",
      "[D loss: 0.836678] [G loss: 1.543801]\n",
      "[D loss: 0.796420] [G loss: 1.493917]\n",
      "[D loss: 0.758442] [G loss: 1.349514]\n",
      "[D loss: 0.779465] [G loss: 1.537163]\n",
      "[D loss: 0.925069] [G loss: 1.529443]\n",
      "[D loss: 0.664784] [G loss: 1.553686]\n",
      "[D loss: 0.846389] [G loss: 1.620438]\n",
      "[D loss: 0.840529] [G loss: 1.778557]\n",
      "[D loss: 0.924860] [G loss: 1.755001]\n",
      "[D loss: 0.887171] [G loss: 1.485153]\n",
      "[D loss: 0.979846] [G loss: 1.470205]\n",
      "[D loss: 0.676802] [G loss: 1.324816]\n",
      "[D loss: 0.802496] [G loss: 1.656078]\n",
      "[D loss: 0.896100] [G loss: 1.351692]\n",
      "[D loss: 0.673140] [G loss: 1.541146]\n",
      "[D loss: 1.047316] [G loss: 1.386407]\n",
      "[D loss: 0.671494] [G loss: 1.454683]\n",
      "[D loss: 0.858974] [G loss: 1.299728]\n",
      "[D loss: 0.946484] [G loss: 1.336925]\n",
      "[D loss: 0.842449] [G loss: 1.403980]\n",
      "[D loss: 0.808336] [G loss: 1.208154]\n",
      "[D loss: 0.819319] [G loss: 1.594763]\n",
      "[D loss: 0.731578] [G loss: 1.402432]\n",
      "[D loss: 0.714802] [G loss: 1.406197]\n",
      "[D loss: 1.043126] [G loss: 1.459931]\n",
      "[D loss: 1.010546] [G loss: 1.262322]\n",
      "[D loss: 1.054427] [G loss: 1.381632]\n",
      "[D loss: 1.061204] [G loss: 1.499703]\n",
      "[D loss: 0.798286] [G loss: 1.423126]\n",
      "[D loss: 0.785000] [G loss: 1.535172]\n",
      "[D loss: 0.918387] [G loss: 1.249460]\n",
      "[D loss: 0.892504] [G loss: 1.326751]\n",
      "[D loss: 0.968568] [G loss: 1.440761]\n",
      "[D loss: 0.874713] [G loss: 1.433979]\n",
      "[D loss: 0.923788] [G loss: 1.272832]\n",
      "[D loss: 0.934969] [G loss: 1.314185]\n",
      "[D loss: 0.980448] [G loss: 1.208994]\n",
      "[D loss: 0.920119] [G loss: 1.376697]\n",
      "[D loss: 1.036936] [G loss: 1.372498]\n",
      "[D loss: 0.908718] [G loss: 1.403405]\n",
      "[D loss: 0.780282] [G loss: 1.439761]\n",
      "[D loss: 0.498864] [G loss: 1.428177]\n",
      "[D loss: 0.930330] [G loss: 1.523800]\n",
      "[D loss: 0.641620] [G loss: 1.451343]\n",
      "[D loss: 0.770992] [G loss: 1.414372]\n",
      "[D loss: 1.060316] [G loss: 1.464016]\n",
      "[D loss: 0.964232] [G loss: 1.268973]\n",
      "[D loss: 0.812302] [G loss: 1.382192]\n",
      "[D loss: 0.656155] [G loss: 1.497565]\n",
      "[D loss: 0.772399] [G loss: 1.679012]\n",
      "[D loss: 0.739867] [G loss: 1.511465]\n",
      "[D loss: 0.758928] [G loss: 1.420378]\n",
      "[D loss: 1.173654] [G loss: 1.429735]\n",
      "[D loss: 1.084192] [G loss: 1.414862]\n",
      "[D loss: 0.818955] [G loss: 1.365415]\n",
      "[D loss: 0.841906] [G loss: 1.371510]\n",
      "[D loss: 0.612000] [G loss: 1.662852]\n",
      "[D loss: 0.977761] [G loss: 1.283272]\n",
      "[D loss: 0.851510] [G loss: 1.465412]\n",
      "[D loss: 0.818518] [G loss: 1.373681]\n",
      "[D loss: 0.809248] [G loss: 1.589080]\n",
      "[D loss: 0.860286] [G loss: 1.543646]\n",
      "[D loss: 0.899840] [G loss: 1.330191]\n",
      "[D loss: 0.854576] [G loss: 1.337016]\n",
      "[D loss: 0.968695] [G loss: 1.397225]\n",
      "[D loss: 0.838436] [G loss: 1.518358]\n",
      "[D loss: 0.670507] [G loss: 1.772405]\n",
      "[D loss: 0.761496] [G loss: 1.403032]\n",
      "[D loss: 0.925266] [G loss: 1.517471]\n",
      "[D loss: 0.745267] [G loss: 1.662987]\n",
      "[D loss: 0.841180] [G loss: 1.439008]\n",
      "[D loss: 0.760763] [G loss: 1.440999]\n",
      "[D loss: 1.071357] [G loss: 1.378802]\n",
      "[D loss: 0.967381] [G loss: 1.468674]\n",
      "[D loss: 0.643671] [G loss: 1.414463]\n",
      "[D loss: 0.964375] [G loss: 1.403287]\n",
      "[D loss: 0.952424] [G loss: 1.452377]\n",
      "[D loss: 0.678266] [G loss: 1.476222]\n",
      "[D loss: 1.099677] [G loss: 1.386147]\n",
      "[D loss: 0.771995] [G loss: 1.394421]\n",
      "[D loss: 0.880266] [G loss: 1.426219]\n",
      "[D loss: 0.794660] [G loss: 1.398094]\n",
      "[D loss: 0.529483] [G loss: 1.518752]\n",
      "[D loss: 0.791121] [G loss: 1.565073]\n",
      "[D loss: 0.819129] [G loss: 1.473331]\n",
      "[D loss: 0.747563] [G loss: 1.482704]\n",
      "[D loss: 0.712117] [G loss: 1.408936]\n",
      "[D loss: 0.853212] [G loss: 1.304217]\n",
      "[D loss: 0.889062] [G loss: 1.510266]\n",
      "[D loss: 0.819578] [G loss: 1.566988]\n",
      "[D loss: 0.572204] [G loss: 1.617169]\n",
      "[D loss: 0.806553] [G loss: 1.485602]\n",
      "[D loss: 1.134224] [G loss: 1.290292]\n",
      "[D loss: 0.615888] [G loss: 1.511296]\n",
      "[D loss: 0.883656] [G loss: 1.327730]\n",
      "[D loss: 0.885850] [G loss: 1.477854]\n",
      "[D loss: 0.920867] [G loss: 1.481624]\n",
      "[D loss: 1.046065] [G loss: 1.365768]\n",
      "[D loss: 0.890285] [G loss: 1.412860]\n",
      "[D loss: 0.907370] [G loss: 1.316973]\n",
      "[D loss: 0.892224] [G loss: 1.400665]\n",
      "[D loss: 0.838463] [G loss: 1.279516]\n",
      "[D loss: 0.997753] [G loss: 1.394700]\n",
      "[D loss: 0.932704] [G loss: 1.669755]\n",
      "[D loss: 0.864225] [G loss: 1.640129]\n",
      "[D loss: 0.717116] [G loss: 1.523602]\n",
      "[D loss: 0.698308] [G loss: 1.318726]\n",
      "[D loss: 0.823692] [G loss: 1.311661]\n",
      "[D loss: 0.870430] [G loss: 1.618338]\n",
      "[D loss: 0.880590] [G loss: 1.386602]\n",
      "[D loss: 0.795860] [G loss: 1.429051]\n",
      "[D loss: 0.697468] [G loss: 1.355814]\n",
      "[D loss: 0.786613] [G loss: 1.389646]\n",
      "[D loss: 0.928150] [G loss: 1.264770]\n",
      "[D loss: 0.891255] [G loss: 1.529084]\n",
      "[D loss: 0.870392] [G loss: 1.498045]\n",
      "[D loss: 0.949849] [G loss: 1.463974]\n",
      "[D loss: 0.770139] [G loss: 1.455478]\n",
      "[D loss: 0.975091] [G loss: 1.388667]\n",
      "[D loss: 0.818172] [G loss: 1.577075]\n",
      "[D loss: 0.651094] [G loss: 1.533119]\n",
      "[D loss: 0.847109] [G loss: 1.448536]\n",
      "[D loss: 0.618863] [G loss: 1.449692]\n",
      "[D loss: 0.676740] [G loss: 1.579453]\n",
      "[D loss: 0.753005] [G loss: 1.539784]\n",
      "[D loss: 0.836082] [G loss: 1.549533]\n",
      "[D loss: 0.716081] [G loss: 1.420183]\n",
      "[D loss: 0.643989] [G loss: 1.459074]\n",
      "[D loss: 0.696442] [G loss: 1.670810]\n",
      "[D loss: 1.161703] [G loss: 1.421141]\n",
      "[D loss: 1.043092] [G loss: 1.361664]\n",
      "[D loss: 0.914497] [G loss: 1.475991]\n",
      "[D loss: 0.600054] [G loss: 1.664273]\n",
      "[D loss: 0.939155] [G loss: 1.818214]\n",
      "[D loss: 0.962631] [G loss: 1.349125]\n",
      "[D loss: 0.931564] [G loss: 1.280040]\n",
      "[D loss: 0.839812] [G loss: 1.348752]\n",
      "[D loss: 0.813169] [G loss: 1.465192]\n",
      "[D loss: 1.006428] [G loss: 1.525533]\n",
      "[D loss: 0.831897] [G loss: 1.374123]\n",
      "[D loss: 0.913861] [G loss: 1.263122]\n",
      "[D loss: 0.772887] [G loss: 1.403804]\n",
      "[D loss: 0.923056] [G loss: 1.345199]\n",
      "[D loss: 0.811614] [G loss: 1.286907]\n",
      "[D loss: 0.828212] [G loss: 1.330895]\n",
      "[D loss: 0.693469] [G loss: 1.590784]\n",
      "[D loss: 1.056456] [G loss: 1.415076]\n",
      "[D loss: 0.832316] [G loss: 1.446710]\n",
      "[D loss: 1.003888] [G loss: 1.472557]\n",
      "[D loss: 0.861863] [G loss: 1.762105]\n",
      "[D loss: 0.951441] [G loss: 1.516705]\n",
      "[D loss: 0.741308] [G loss: 1.464973]\n",
      "[D loss: 0.756309] [G loss: 1.431270]\n",
      "[D loss: 0.701699] [G loss: 1.474587]\n",
      "[D loss: 0.678145] [G loss: 1.658342]\n",
      "[D loss: 0.663400] [G loss: 1.582500]\n",
      "[D loss: 0.861562] [G loss: 1.384054]\n",
      "[D loss: 0.935927] [G loss: 1.181537]\n",
      "[D loss: 0.684368] [G loss: 1.535255]\n",
      "[D loss: 0.851382] [G loss: 1.643479]\n",
      "[D loss: 0.677964] [G loss: 1.717458]\n",
      "[D loss: 0.791155] [G loss: 1.692692]\n",
      "[D loss: 0.788037] [G loss: 1.531189]\n",
      "[D loss: 0.916298] [G loss: 1.436078]\n",
      "[D loss: 0.724961] [G loss: 1.496613]\n",
      "[D loss: 1.085187] [G loss: 1.606288]\n",
      "[D loss: 1.068486] [G loss: 1.499348]\n",
      "[D loss: 0.596434] [G loss: 1.715683]\n",
      "[D loss: 0.817848] [G loss: 1.399587]\n",
      "[D loss: 0.993960] [G loss: 1.305727]\n",
      "[D loss: 0.824907] [G loss: 1.417463]\n",
      "[D loss: 0.976787] [G loss: 1.199003]\n",
      "[D loss: 0.799449] [G loss: 1.383957]\n",
      "[D loss: 0.830733] [G loss: 1.702228]\n",
      "[D loss: 1.011358] [G loss: 1.346218]\n",
      "[D loss: 0.933317] [G loss: 1.537618]\n",
      "[D loss: 0.915990] [G loss: 1.381959]\n",
      "[D loss: 0.943780] [G loss: 1.390608]\n",
      "[D loss: 0.830708] [G loss: 1.248708]\n",
      "[D loss: 0.985969] [G loss: 1.250333]\n",
      "[D loss: 0.936869] [G loss: 1.437389]\n",
      "[D loss: 0.840456] [G loss: 1.341640]\n",
      "[D loss: 0.745355] [G loss: 1.513669]\n",
      "[D loss: 0.780363] [G loss: 1.417676]\n",
      "[D loss: 0.970412] [G loss: 1.341334]\n",
      "[D loss: 0.887363] [G loss: 1.549506]\n",
      "[D loss: 0.820607] [G loss: 1.381765]\n",
      "[D loss: 1.000461] [G loss: 1.182761]\n",
      "[D loss: 0.733466] [G loss: 1.331931]\n",
      "[D loss: 0.854628] [G loss: 1.575449]\n",
      "[D loss: 0.910962] [G loss: 1.618343]\n",
      "[D loss: 0.899151] [G loss: 1.217227]\n",
      "[D loss: 0.761644] [G loss: 1.419829]\n",
      "[D loss: 0.897551] [G loss: 1.348140]\n",
      "[D loss: 0.598405] [G loss: 1.552118]\n",
      "[D loss: 0.764125] [G loss: 1.607145]\n",
      "[D loss: 0.760225] [G loss: 1.268010]\n",
      "[D loss: 0.945583] [G loss: 1.262766]\n",
      "[D loss: 0.731401] [G loss: 1.411410]\n",
      "[D loss: 0.668499] [G loss: 1.380592]\n",
      "[D loss: 0.639997] [G loss: 1.735965]\n",
      "[D loss: 0.778293] [G loss: 1.497322]\n",
      "[D loss: 1.130920] [G loss: 1.363111]\n",
      "[D loss: 0.688152] [G loss: 1.411953]\n",
      "[D loss: 0.806199] [G loss: 1.530060]\n",
      "[D loss: 0.742608] [G loss: 1.671976]\n",
      "[D loss: 0.996309] [G loss: 1.267842]\n",
      "[D loss: 0.844815] [G loss: 1.516658]\n",
      "[D loss: 0.773310] [G loss: 1.518121]\n",
      "[D loss: 0.866070] [G loss: 1.451108]\n",
      "[D loss: 0.882003] [G loss: 1.442673]\n",
      "[D loss: 0.803356] [G loss: 1.412732]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.801444] [G loss: 1.614366]\n",
      "[D loss: 1.152004] [G loss: 1.626637]\n",
      "[D loss: 0.951608] [G loss: 1.456553]\n",
      "[D loss: 0.855534] [G loss: 1.396107]\n",
      "[D loss: 1.031405] [G loss: 1.521763]\n",
      "[D loss: 1.063340] [G loss: 1.506836]\n",
      "[D loss: 0.792749] [G loss: 1.524822]\n",
      "[D loss: 0.758078] [G loss: 1.374568]\n",
      "[D loss: 0.694228] [G loss: 1.489408]\n",
      "[D loss: 0.852083] [G loss: 1.332725]\n",
      "[D loss: 0.756196] [G loss: 1.251894]\n",
      "[D loss: 0.759085] [G loss: 1.430867]\n",
      "[D loss: 0.783880] [G loss: 1.551206]\n",
      "[D loss: 0.910888] [G loss: 1.249573]\n",
      "[D loss: 0.693474] [G loss: 1.381218]\n",
      "[D loss: 0.816674] [G loss: 1.472646]\n",
      "[D loss: 0.867332] [G loss: 1.503138]\n",
      "[D loss: 0.766394] [G loss: 1.380790]\n",
      "[D loss: 0.763084] [G loss: 1.404928]\n",
      "[D loss: 0.895812] [G loss: 1.575720]\n",
      "[D loss: 0.918840] [G loss: 1.455041]\n",
      "[D loss: 0.680238] [G loss: 1.688698]\n",
      "[D loss: 0.790004] [G loss: 1.294613]\n",
      "[D loss: 0.795920] [G loss: 1.506301]\n",
      "[D loss: 0.715559] [G loss: 1.893650]\n",
      "[D loss: 0.868875] [G loss: 1.495164]\n",
      "[D loss: 0.772403] [G loss: 1.506761]\n",
      "[D loss: 0.873590] [G loss: 1.550573]\n",
      "[D loss: 0.788181] [G loss: 1.587573]\n",
      "[D loss: 0.869393] [G loss: 1.299684]\n",
      "[D loss: 1.128448] [G loss: 1.418392]\n",
      "[D loss: 1.153214] [G loss: 1.162234]\n",
      "[D loss: 0.875360] [G loss: 1.326916]\n",
      "[D loss: 0.896733] [G loss: 1.668749]\n",
      "[D loss: 0.824753] [G loss: 1.701008]\n",
      "[D loss: 0.831678] [G loss: 1.361346]\n",
      "[D loss: 0.778210] [G loss: 1.309391]\n",
      "[D loss: 0.800850] [G loss: 1.504871]\n",
      "[D loss: 0.810455] [G loss: 1.325603]\n",
      "[D loss: 1.183749] [G loss: 1.311952]\n",
      "[D loss: 0.748097] [G loss: 1.570058]\n",
      "[D loss: 0.827911] [G loss: 1.439080]\n",
      "[D loss: 0.929161] [G loss: 1.359203]\n",
      "[D loss: 0.741571] [G loss: 1.355113]\n",
      "[D loss: 0.951806] [G loss: 1.423652]\n",
      "[D loss: 0.745791] [G loss: 1.513360]\n",
      "[D loss: 0.747914] [G loss: 1.507818]\n",
      "[D loss: 0.786527] [G loss: 1.483553]\n",
      "[D loss: 0.752067] [G loss: 1.715604]\n",
      "[D loss: 0.808772] [G loss: 1.476717]\n",
      "[D loss: 0.997008] [G loss: 1.361196]\n",
      "[D loss: 0.854686] [G loss: 1.446831]\n",
      "[D loss: 0.710163] [G loss: 1.395241]\n",
      "[D loss: 1.019303] [G loss: 1.279095]\n",
      "[D loss: 0.876057] [G loss: 1.480853]\n",
      "[D loss: 0.841447] [G loss: 1.546963]\n",
      "[D loss: 0.742924] [G loss: 1.515586]\n",
      "[D loss: 0.821916] [G loss: 1.595135]\n",
      "[D loss: 1.092945] [G loss: 1.399591]\n",
      "[D loss: 0.845987] [G loss: 1.372813]\n",
      "[D loss: 0.771677] [G loss: 1.436048]\n",
      "[D loss: 0.700558] [G loss: 1.543594]\n",
      "[D loss: 1.089308] [G loss: 1.255196]\n",
      "[D loss: 0.735723] [G loss: 1.436061]\n",
      "[D loss: 0.774359] [G loss: 1.396499]\n",
      "[D loss: 0.928937] [G loss: 1.404797]\n",
      "[D loss: 0.630475] [G loss: 1.625406]\n",
      "[D loss: 0.849641] [G loss: 1.396371]\n",
      "[D loss: 0.845021] [G loss: 1.470035]\n",
      "[D loss: 0.694280] [G loss: 1.452116]\n",
      "[D loss: 0.867427] [G loss: 1.304756]\n",
      "[D loss: 0.632445] [G loss: 1.664061]\n",
      "[D loss: 0.878515] [G loss: 1.485769]\n",
      "[D loss: 0.535219] [G loss: 1.903549]\n",
      "[D loss: 0.807833] [G loss: 1.520447]\n",
      "[D loss: 1.103122] [G loss: 1.416228]\n",
      "[D loss: 0.678107] [G loss: 1.432225]\n",
      "[D loss: 1.042505] [G loss: 1.406979]\n",
      "[D loss: 0.635082] [G loss: 1.648593]\n",
      "[D loss: 0.883707] [G loss: 1.430475]\n",
      "[D loss: 0.556349] [G loss: 1.299023]\n",
      "[D loss: 0.750012] [G loss: 1.332364]\n",
      "[D loss: 0.807028] [G loss: 1.438385]\n",
      "[D loss: 0.530543] [G loss: 1.431588]\n",
      "[D loss: 0.935875] [G loss: 1.479859]\n",
      "[D loss: 0.771320] [G loss: 1.568591]\n",
      "[D loss: 0.903573] [G loss: 1.297473]\n",
      "[D loss: 0.857704] [G loss: 1.676594]\n",
      "[D loss: 0.625517] [G loss: 1.810165]\n",
      "[D loss: 0.519965] [G loss: 1.607002]\n",
      "[D loss: 0.931448] [G loss: 1.593513]\n",
      "[D loss: 0.962330] [G loss: 1.484986]\n",
      "[D loss: 0.833087] [G loss: 1.509630]\n",
      "[D loss: 0.838786] [G loss: 1.540583]\n",
      "[D loss: 0.944441] [G loss: 1.352010]\n",
      "[D loss: 1.235859] [G loss: 1.201864]\n",
      "[D loss: 0.659391] [G loss: 1.423172]\n",
      "[D loss: 0.931802] [G loss: 1.286966]\n",
      "[D loss: 0.723409] [G loss: 1.357690]\n",
      "[D loss: 0.655351] [G loss: 1.419735]\n",
      "[D loss: 0.701205] [G loss: 1.582167]\n",
      "[D loss: 0.986274] [G loss: 1.408340]\n",
      "[D loss: 0.886847] [G loss: 1.441436]\n",
      "[D loss: 0.774837] [G loss: 1.371877]\n",
      "[D loss: 0.994302] [G loss: 1.253579]\n",
      "[D loss: 0.860488] [G loss: 1.637436]\n",
      "[D loss: 0.782772] [G loss: 1.357968]\n",
      "[D loss: 1.108878] [G loss: 1.260105]\n",
      "[D loss: 0.861451] [G loss: 1.464962]\n",
      "[D loss: 0.788412] [G loss: 1.493465]\n",
      "[D loss: 0.669695] [G loss: 1.402711]\n",
      "[D loss: 0.877126] [G loss: 1.362117]\n",
      "[D loss: 1.159814] [G loss: 1.273726]\n",
      "[D loss: 0.861616] [G loss: 1.661410]\n",
      "[D loss: 0.578215] [G loss: 1.545779]\n",
      "[D loss: 0.740463] [G loss: 1.451392]\n",
      "[D loss: 0.670469] [G loss: 1.434835]\n",
      "[D loss: 0.931175] [G loss: 1.492987]\n",
      "[D loss: 0.923979] [G loss: 1.293107]\n",
      "[D loss: 0.670908] [G loss: 1.590932]\n",
      "[D loss: 1.167250] [G loss: 1.590850]\n",
      "[D loss: 0.833052] [G loss: 1.533438]\n",
      "[D loss: 0.838669] [G loss: 1.474231]\n",
      "[D loss: 1.092155] [G loss: 1.131603]\n",
      "[D loss: 0.794803] [G loss: 1.415196]\n",
      "[D loss: 0.634654] [G loss: 1.443078]\n",
      "[D loss: 0.947695] [G loss: 1.382620]\n",
      "[D loss: 0.957992] [G loss: 1.379370]\n",
      "[D loss: 0.735962] [G loss: 1.629428]\n",
      "[D loss: 0.945173] [G loss: 1.532058]\n",
      "[D loss: 0.748414] [G loss: 1.404663]\n",
      "[D loss: 0.916020] [G loss: 1.481638]\n",
      "[D loss: 0.917921] [G loss: 1.386353]\n",
      "[D loss: 0.884234] [G loss: 1.440149]\n",
      "[D loss: 0.790320] [G loss: 1.396919]\n",
      "[D loss: 0.694318] [G loss: 1.448967]\n",
      "[D loss: 0.911161] [G loss: 1.321213]\n",
      "[D loss: 0.935131] [G loss: 1.358097]\n",
      "[D loss: 0.928174] [G loss: 1.275097]\n",
      "[D loss: 0.936225] [G loss: 1.352117]\n",
      "[D loss: 0.901764] [G loss: 1.371625]\n",
      "[D loss: 0.670406] [G loss: 1.422742]\n",
      "[D loss: 0.764341] [G loss: 1.364454]\n",
      "[D loss: 0.870581] [G loss: 1.475869]\n",
      "[D loss: 0.900675] [G loss: 1.399366]\n",
      "[D loss: 0.826059] [G loss: 1.576586]\n",
      "[D loss: 0.720298] [G loss: 1.479829]\n",
      "[D loss: 0.971245] [G loss: 1.399084]\n",
      "[D loss: 0.879261] [G loss: 1.249796]\n",
      "[D loss: 0.879607] [G loss: 1.362866]\n",
      "[D loss: 0.920009] [G loss: 1.438131]\n",
      "[D loss: 0.750745] [G loss: 1.434798]\n",
      "[D loss: 0.791355] [G loss: 1.507196]\n",
      "[D loss: 0.686237] [G loss: 1.372910]\n",
      "[D loss: 0.772532] [G loss: 1.319762]\n",
      "[D loss: 0.944554] [G loss: 1.266133]\n",
      "[D loss: 0.951408] [G loss: 1.146099]\n",
      "[D loss: 0.963194] [G loss: 1.361382]\n",
      "[D loss: 0.986236] [G loss: 1.660711]\n",
      "[D loss: 0.799558] [G loss: 1.509328]\n",
      "[D loss: 0.905197] [G loss: 1.411597]\n",
      "[D loss: 0.968539] [G loss: 1.326310]\n",
      "[D loss: 0.866652] [G loss: 1.369352]\n",
      "[D loss: 0.759963] [G loss: 1.135970]\n",
      "[D loss: 0.750001] [G loss: 1.309764]\n",
      "[D loss: 0.894942] [G loss: 1.713868]\n",
      "[D loss: 0.787526] [G loss: 1.549089]\n",
      "[D loss: 0.835753] [G loss: 1.566408]\n",
      "[D loss: 0.856975] [G loss: 1.318416]\n",
      "[D loss: 0.979852] [G loss: 1.406454]\n",
      "[D loss: 0.847380] [G loss: 1.505183]\n",
      "[D loss: 0.727222] [G loss: 1.519967]\n",
      "[D loss: 0.811810] [G loss: 1.384051]\n",
      "[D loss: 0.936508] [G loss: 1.451843]\n",
      "[D loss: 0.818614] [G loss: 1.454023]\n",
      "[D loss: 0.749615] [G loss: 1.383812]\n",
      "[D loss: 1.007001] [G loss: 1.351217]\n",
      "[D loss: 0.727960] [G loss: 1.517142]\n",
      "[D loss: 0.681277] [G loss: 1.514336]\n",
      "[D loss: 0.876633] [G loss: 1.738245]\n",
      "[D loss: 0.841846] [G loss: 1.481203]\n",
      "[D loss: 1.051501] [G loss: 1.232946]\n",
      "[D loss: 0.827343] [G loss: 1.374629]\n",
      "[D loss: 0.853211] [G loss: 1.566998]\n",
      "[D loss: 0.801485] [G loss: 1.424628]\n",
      "[D loss: 0.822297] [G loss: 1.385839]\n",
      "[D loss: 0.669188] [G loss: 1.477316]\n",
      "[D loss: 1.055337] [G loss: 1.326332]\n",
      "[D loss: 0.878755] [G loss: 1.372829]\n",
      "[D loss: 0.766258] [G loss: 1.540773]\n",
      "[D loss: 0.577986] [G loss: 1.541094]\n",
      "[D loss: 1.015359] [G loss: 1.361055]\n",
      "[D loss: 1.019450] [G loss: 1.393584]\n",
      "[D loss: 0.747982] [G loss: 1.539107]\n",
      "[D loss: 0.859718] [G loss: 1.423700]\n",
      "[D loss: 0.934173] [G loss: 1.360404]\n",
      "[D loss: 0.808502] [G loss: 1.357358]\n",
      "[D loss: 0.764943] [G loss: 1.448226]\n",
      "[D loss: 0.740632] [G loss: 1.438287]\n",
      "[D loss: 0.922211] [G loss: 1.222526]\n",
      "[D loss: 0.712440] [G loss: 1.279192]\n",
      "[D loss: 0.760282] [G loss: 1.591457]\n",
      "[D loss: 0.827018] [G loss: 1.457032]\n",
      "[D loss: 0.927822] [G loss: 1.410629]\n",
      "[D loss: 0.635751] [G loss: 1.275767]\n",
      "[D loss: 0.973091] [G loss: 1.547139]\n",
      "[D loss: 1.052901] [G loss: 1.199877]\n",
      "[D loss: 1.010160] [G loss: 1.396583]\n",
      "[D loss: 0.787407] [G loss: 1.475436]\n",
      "[D loss: 0.845628] [G loss: 1.607328]\n",
      "[D loss: 0.863101] [G loss: 1.730544]\n",
      "[D loss: 0.632752] [G loss: 1.500551]\n",
      "[D loss: 0.957492] [G loss: 1.463258]\n",
      "[D loss: 0.948508] [G loss: 1.328680]\n",
      "[D loss: 0.611844] [G loss: 1.493607]\n",
      "epoch:10, g_loss:2733.32275390625,d_loss:1573.784912109375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.981831] [G loss: 1.226897]\n",
      "[D loss: 0.839957] [G loss: 1.360359]\n",
      "[D loss: 0.923644] [G loss: 1.348262]\n",
      "[D loss: 0.624365] [G loss: 1.721952]\n",
      "[D loss: 0.727254] [G loss: 1.660154]\n",
      "[D loss: 0.635017] [G loss: 1.465555]\n",
      "[D loss: 0.699882] [G loss: 1.442765]\n",
      "[D loss: 0.697583] [G loss: 1.353566]\n",
      "[D loss: 1.001341] [G loss: 1.451181]\n",
      "[D loss: 0.799547] [G loss: 1.591393]\n",
      "[D loss: 0.834147] [G loss: 1.656772]\n",
      "[D loss: 0.967349] [G loss: 1.504798]\n",
      "[D loss: 0.873273] [G loss: 1.540143]\n",
      "[D loss: 0.856298] [G loss: 1.205064]\n",
      "[D loss: 0.861808] [G loss: 1.314414]\n",
      "[D loss: 0.608980] [G loss: 1.553122]\n",
      "[D loss: 0.907592] [G loss: 1.599320]\n",
      "[D loss: 0.884291] [G loss: 1.614986]\n",
      "[D loss: 0.843714] [G loss: 1.544184]\n",
      "[D loss: 0.995280] [G loss: 1.480797]\n",
      "[D loss: 0.846106] [G loss: 1.472457]\n",
      "[D loss: 0.891116] [G loss: 1.312593]\n",
      "[D loss: 0.712197] [G loss: 1.319814]\n",
      "[D loss: 0.757171] [G loss: 1.487850]\n",
      "[D loss: 0.950489] [G loss: 1.329952]\n",
      "[D loss: 0.749088] [G loss: 1.409453]\n",
      "[D loss: 0.818861] [G loss: 1.645698]\n",
      "[D loss: 0.867100] [G loss: 1.425015]\n",
      "[D loss: 1.053236] [G loss: 1.214646]\n",
      "[D loss: 0.700985] [G loss: 1.383959]\n",
      "[D loss: 0.979432] [G loss: 1.540184]\n",
      "[D loss: 0.705482] [G loss: 1.749570]\n",
      "[D loss: 0.962734] [G loss: 1.278271]\n",
      "[D loss: 0.711654] [G loss: 1.524587]\n",
      "[D loss: 0.910574] [G loss: 1.662564]\n",
      "[D loss: 0.718079] [G loss: 1.704106]\n",
      "[D loss: 0.875550] [G loss: 1.511343]\n",
      "[D loss: 0.797584] [G loss: 1.450656]\n",
      "[D loss: 0.756720] [G loss: 1.474840]\n",
      "[D loss: 0.847780] [G loss: 1.470163]\n",
      "[D loss: 0.792159] [G loss: 1.522685]\n",
      "[D loss: 0.972368] [G loss: 1.482581]\n",
      "[D loss: 0.686572] [G loss: 1.401336]\n",
      "[D loss: 0.744501] [G loss: 1.292221]\n",
      "[D loss: 0.725089] [G loss: 1.373635]\n",
      "[D loss: 0.753411] [G loss: 1.445456]\n",
      "[D loss: 0.896460] [G loss: 1.532218]\n",
      "[D loss: 0.913589] [G loss: 1.549867]\n",
      "[D loss: 0.869550] [G loss: 1.422418]\n",
      "[D loss: 0.676361] [G loss: 1.560695]\n",
      "[D loss: 0.580123] [G loss: 1.398780]\n",
      "[D loss: 0.699914] [G loss: 1.509889]\n",
      "[D loss: 0.785107] [G loss: 1.666831]\n",
      "[D loss: 0.961955] [G loss: 1.361036]\n",
      "[D loss: 0.829263] [G loss: 1.330257]\n",
      "[D loss: 0.721339] [G loss: 1.436426]\n",
      "[D loss: 0.510235] [G loss: 1.924481]\n",
      "[D loss: 0.901146] [G loss: 1.880864]\n",
      "[D loss: 0.807281] [G loss: 1.460407]\n",
      "[D loss: 1.044494] [G loss: 1.632874]\n",
      "[D loss: 0.696499] [G loss: 1.677224]\n",
      "[D loss: 0.991154] [G loss: 1.504933]\n",
      "[D loss: 1.189595] [G loss: 1.297651]\n",
      "[D loss: 0.688795] [G loss: 1.438538]\n",
      "[D loss: 1.017370] [G loss: 1.436076]\n",
      "[D loss: 0.748263] [G loss: 1.414023]\n",
      "[D loss: 1.062072] [G loss: 1.676280]\n",
      "[D loss: 0.718151] [G loss: 1.489472]\n",
      "[D loss: 0.975462] [G loss: 1.496096]\n",
      "[D loss: 0.735102] [G loss: 1.305745]\n",
      "[D loss: 0.731222] [G loss: 1.415965]\n",
      "[D loss: 0.811643] [G loss: 1.275293]\n",
      "[D loss: 0.833481] [G loss: 1.381348]\n",
      "[D loss: 0.925176] [G loss: 1.304932]\n",
      "[D loss: 0.810277] [G loss: 1.536442]\n",
      "[D loss: 0.832563] [G loss: 1.557594]\n",
      "[D loss: 0.703225] [G loss: 1.401869]\n",
      "[D loss: 0.832528] [G loss: 1.506366]\n",
      "[D loss: 0.812297] [G loss: 1.428608]\n",
      "[D loss: 0.863854] [G loss: 1.394565]\n",
      "[D loss: 0.821343] [G loss: 1.382716]\n",
      "[D loss: 1.110179] [G loss: 1.121425]\n",
      "[D loss: 0.848254] [G loss: 1.540199]\n",
      "[D loss: 0.700064] [G loss: 1.905766]\n",
      "[D loss: 0.849402] [G loss: 1.503651]\n",
      "[D loss: 0.906982] [G loss: 1.528901]\n",
      "[D loss: 0.636652] [G loss: 1.617025]\n",
      "[D loss: 0.764641] [G loss: 1.476587]\n",
      "[D loss: 0.815386] [G loss: 1.651490]\n",
      "[D loss: 0.674246] [G loss: 1.592345]\n",
      "[D loss: 0.659846] [G loss: 1.675632]\n",
      "[D loss: 0.700171] [G loss: 1.440788]\n",
      "[D loss: 0.736535] [G loss: 1.406129]\n",
      "[D loss: 0.718992] [G loss: 1.735891]\n",
      "[D loss: 0.777644] [G loss: 1.605994]\n",
      "[D loss: 0.753180] [G loss: 1.549970]\n",
      "[D loss: 0.845719] [G loss: 1.439914]\n",
      "[D loss: 0.937986] [G loss: 1.255891]\n",
      "[D loss: 0.671719] [G loss: 1.722425]\n",
      "[D loss: 0.928756] [G loss: 1.608427]\n",
      "[D loss: 0.748247] [G loss: 1.760028]\n",
      "[D loss: 0.920505] [G loss: 1.324387]\n",
      "[D loss: 0.707084] [G loss: 1.461161]\n",
      "[D loss: 0.716358] [G loss: 1.349507]\n",
      "[D loss: 1.004748] [G loss: 1.270604]\n",
      "[D loss: 0.548397] [G loss: 1.678706]\n",
      "[D loss: 0.777520] [G loss: 1.416970]\n",
      "[D loss: 0.763444] [G loss: 1.404365]\n",
      "[D loss: 0.966711] [G loss: 1.402217]\n",
      "[D loss: 0.831638] [G loss: 1.480967]\n",
      "[D loss: 0.644731] [G loss: 1.609457]\n",
      "[D loss: 0.871356] [G loss: 1.459353]\n",
      "[D loss: 0.628202] [G loss: 1.667681]\n",
      "[D loss: 0.920826] [G loss: 1.649364]\n",
      "[D loss: 0.864640] [G loss: 1.283407]\n",
      "[D loss: 0.844200] [G loss: 1.450759]\n",
      "[D loss: 0.906821] [G loss: 1.764176]\n",
      "[D loss: 0.662539] [G loss: 1.345069]\n",
      "[D loss: 0.729919] [G loss: 1.268803]\n",
      "[D loss: 0.792323] [G loss: 1.447359]\n",
      "[D loss: 0.831746] [G loss: 1.431757]\n",
      "[D loss: 0.959843] [G loss: 1.537831]\n",
      "[D loss: 0.793730] [G loss: 1.698084]\n",
      "[D loss: 0.981365] [G loss: 1.566026]\n",
      "[D loss: 0.822969] [G loss: 1.544059]\n",
      "[D loss: 1.170851] [G loss: 1.375945]\n",
      "[D loss: 0.902191] [G loss: 1.324472]\n",
      "[D loss: 0.934495] [G loss: 1.398046]\n",
      "[D loss: 0.630476] [G loss: 1.594216]\n",
      "[D loss: 0.747512] [G loss: 1.560928]\n",
      "[D loss: 0.630653] [G loss: 1.586163]\n",
      "[D loss: 0.778743] [G loss: 1.713776]\n",
      "[D loss: 0.735886] [G loss: 1.543384]\n",
      "[D loss: 0.746532] [G loss: 1.540163]\n",
      "[D loss: 0.785855] [G loss: 1.376647]\n",
      "[D loss: 1.032795] [G loss: 1.404437]\n",
      "[D loss: 0.786730] [G loss: 1.335578]\n",
      "[D loss: 1.006657] [G loss: 1.449825]\n",
      "[D loss: 0.822897] [G loss: 1.487945]\n",
      "[D loss: 0.736062] [G loss: 1.511155]\n",
      "[D loss: 0.707634] [G loss: 1.775583]\n",
      "[D loss: 0.775857] [G loss: 1.695581]\n",
      "[D loss: 0.862717] [G loss: 1.277489]\n",
      "[D loss: 0.838695] [G loss: 1.676225]\n",
      "[D loss: 0.863034] [G loss: 1.694266]\n",
      "[D loss: 0.958658] [G loss: 1.383272]\n",
      "[D loss: 0.947504] [G loss: 1.572661]\n",
      "[D loss: 0.822350] [G loss: 1.291702]\n",
      "[D loss: 0.803352] [G loss: 1.415830]\n",
      "[D loss: 0.888341] [G loss: 1.447698]\n",
      "[D loss: 0.909078] [G loss: 1.202348]\n",
      "[D loss: 0.582158] [G loss: 1.552827]\n",
      "[D loss: 1.026379] [G loss: 1.513639]\n",
      "[D loss: 0.923713] [G loss: 1.469032]\n",
      "[D loss: 1.095710] [G loss: 1.359669]\n",
      "[D loss: 0.883905] [G loss: 1.369483]\n",
      "[D loss: 0.855335] [G loss: 1.260922]\n",
      "[D loss: 0.685279] [G loss: 1.234110]\n",
      "[D loss: 0.653114] [G loss: 1.550213]\n",
      "[D loss: 0.903286] [G loss: 1.574401]\n",
      "[D loss: 0.720002] [G loss: 1.639739]\n",
      "[D loss: 0.642331] [G loss: 1.698043]\n",
      "[D loss: 0.797245] [G loss: 1.571512]\n",
      "[D loss: 1.307212] [G loss: 1.342600]\n",
      "[D loss: 0.753698] [G loss: 1.594744]\n",
      "[D loss: 0.946277] [G loss: 1.556344]\n",
      "[D loss: 0.974195] [G loss: 1.387615]\n",
      "[D loss: 0.646167] [G loss: 1.600906]\n",
      "[D loss: 0.663055] [G loss: 1.341827]\n",
      "[D loss: 0.890497] [G loss: 1.512664]\n",
      "[D loss: 0.755605] [G loss: 1.458482]\n",
      "[D loss: 1.011752] [G loss: 1.506102]\n",
      "[D loss: 0.824922] [G loss: 1.422344]\n",
      "[D loss: 0.547940] [G loss: 1.437211]\n",
      "[D loss: 0.640402] [G loss: 1.617046]\n",
      "[D loss: 0.783130] [G loss: 1.572423]\n",
      "[D loss: 0.876685] [G loss: 1.505800]\n",
      "[D loss: 0.665998] [G loss: 1.468039]\n",
      "[D loss: 0.936864] [G loss: 1.394561]\n",
      "[D loss: 0.991633] [G loss: 1.621291]\n",
      "[D loss: 0.803645] [G loss: 1.479878]\n",
      "[D loss: 1.139217] [G loss: 1.485696]\n",
      "[D loss: 0.699805] [G loss: 1.546341]\n",
      "[D loss: 0.983773] [G loss: 1.493867]\n",
      "[D loss: 0.805336] [G loss: 1.923410]\n",
      "[D loss: 0.777587] [G loss: 1.935177]\n",
      "[D loss: 0.648924] [G loss: 1.847569]\n",
      "[D loss: 0.585481] [G loss: 1.675391]\n",
      "[D loss: 1.054049] [G loss: 1.258797]\n",
      "[D loss: 0.897190] [G loss: 1.359848]\n",
      "[D loss: 0.758712] [G loss: 1.424249]\n",
      "[D loss: 1.108552] [G loss: 1.433027]\n",
      "[D loss: 0.878889] [G loss: 1.605810]\n",
      "[D loss: 0.904847] [G loss: 1.563088]\n",
      "[D loss: 1.025623] [G loss: 1.203038]\n",
      "[D loss: 0.748429] [G loss: 1.304980]\n",
      "[D loss: 0.682065] [G loss: 1.582673]\n",
      "[D loss: 0.825734] [G loss: 1.683936]\n",
      "[D loss: 0.974232] [G loss: 1.355993]\n",
      "[D loss: 0.915258] [G loss: 1.656584]\n",
      "[D loss: 0.795207] [G loss: 1.312072]\n",
      "[D loss: 0.757947] [G loss: 1.306344]\n",
      "[D loss: 0.913994] [G loss: 1.323714]\n",
      "[D loss: 1.007624] [G loss: 1.378670]\n",
      "[D loss: 0.678253] [G loss: 1.358080]\n",
      "[D loss: 0.594788] [G loss: 1.491367]\n",
      "[D loss: 0.720955] [G loss: 1.772307]\n",
      "[D loss: 0.882174] [G loss: 1.556952]\n",
      "[D loss: 0.916672] [G loss: 1.352350]\n",
      "[D loss: 0.831524] [G loss: 1.703173]\n",
      "[D loss: 0.865151] [G loss: 1.557269]\n",
      "[D loss: 0.869991] [G loss: 1.486780]\n",
      "[D loss: 0.743813] [G loss: 1.384801]\n",
      "[D loss: 0.770754] [G loss: 1.542347]\n",
      "[D loss: 0.935901] [G loss: 1.394632]\n",
      "[D loss: 0.659729] [G loss: 1.643714]\n",
      "[D loss: 0.871315] [G loss: 1.398170]\n",
      "[D loss: 0.997151] [G loss: 1.562640]\n",
      "[D loss: 0.943410] [G loss: 1.694270]\n",
      "[D loss: 0.847579] [G loss: 1.751850]\n",
      "[D loss: 0.685053] [G loss: 1.529736]\n",
      "[D loss: 0.667818] [G loss: 1.521609]\n",
      "[D loss: 0.709226] [G loss: 1.734743]\n",
      "[D loss: 0.767157] [G loss: 1.593354]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.996663] [G loss: 1.357294]\n",
      "[D loss: 0.642013] [G loss: 1.520715]\n",
      "[D loss: 1.006395] [G loss: 1.597129]\n",
      "[D loss: 0.999475] [G loss: 1.410555]\n",
      "[D loss: 0.747213] [G loss: 1.283034]\n",
      "[D loss: 0.821035] [G loss: 1.412884]\n",
      "[D loss: 0.682938] [G loss: 1.695684]\n",
      "[D loss: 0.854693] [G loss: 1.637952]\n",
      "[D loss: 0.768217] [G loss: 1.704027]\n",
      "[D loss: 0.826590] [G loss: 1.653262]\n",
      "[D loss: 0.682624] [G loss: 1.516708]\n",
      "[D loss: 0.932224] [G loss: 1.476845]\n",
      "[D loss: 0.886070] [G loss: 1.309145]\n",
      "[D loss: 0.828483] [G loss: 1.419745]\n",
      "[D loss: 0.760942] [G loss: 1.622262]\n",
      "[D loss: 1.053838] [G loss: 1.583915]\n",
      "[D loss: 0.939469] [G loss: 1.497792]\n",
      "[D loss: 0.585366] [G loss: 1.644410]\n",
      "[D loss: 0.653059] [G loss: 1.659505]\n",
      "[D loss: 1.123559] [G loss: 1.320046]\n",
      "[D loss: 0.853721] [G loss: 1.490538]\n",
      "[D loss: 0.944055] [G loss: 1.402792]\n",
      "[D loss: 0.890691] [G loss: 1.544901]\n",
      "[D loss: 0.684499] [G loss: 1.603165]\n",
      "[D loss: 0.876892] [G loss: 1.410430]\n",
      "[D loss: 0.892928] [G loss: 1.107178]\n",
      "[D loss: 0.728895] [G loss: 1.270597]\n",
      "[D loss: 0.806610] [G loss: 1.375793]\n",
      "[D loss: 0.675479] [G loss: 1.388726]\n",
      "[D loss: 0.904806] [G loss: 1.603239]\n",
      "[D loss: 0.804829] [G loss: 1.456954]\n",
      "[D loss: 0.815542] [G loss: 1.481344]\n",
      "[D loss: 0.767211] [G loss: 1.418027]\n",
      "[D loss: 0.940682] [G loss: 1.414943]\n",
      "[D loss: 1.095614] [G loss: 1.288658]\n",
      "[D loss: 1.000763] [G loss: 1.511523]\n",
      "[D loss: 0.632498] [G loss: 1.382981]\n",
      "[D loss: 0.711690] [G loss: 1.298937]\n",
      "[D loss: 0.585184] [G loss: 1.525054]\n",
      "[D loss: 1.030355] [G loss: 1.397788]\n",
      "[D loss: 0.778744] [G loss: 1.440087]\n",
      "[D loss: 0.784665] [G loss: 1.303918]\n",
      "[D loss: 0.728667] [G loss: 1.512832]\n",
      "[D loss: 0.683245] [G loss: 1.911792]\n",
      "[D loss: 0.845448] [G loss: 1.541422]\n",
      "[D loss: 1.023277] [G loss: 1.435022]\n",
      "[D loss: 0.701314] [G loss: 1.550576]\n",
      "[D loss: 0.925276] [G loss: 1.464947]\n",
      "[D loss: 0.977292] [G loss: 1.704772]\n",
      "[D loss: 0.921432] [G loss: 1.428493]\n",
      "[D loss: 0.962304] [G loss: 1.407337]\n",
      "[D loss: 0.806383] [G loss: 1.332347]\n",
      "[D loss: 0.795872] [G loss: 1.218949]\n",
      "[D loss: 0.933673] [G loss: 1.390506]\n",
      "[D loss: 0.858631] [G loss: 1.587266]\n",
      "[D loss: 0.783599] [G loss: 1.210852]\n",
      "[D loss: 0.695088] [G loss: 1.241653]\n",
      "[D loss: 0.963017] [G loss: 1.256669]\n",
      "[D loss: 0.657743] [G loss: 1.428415]\n",
      "[D loss: 0.913588] [G loss: 1.658552]\n",
      "[D loss: 0.787787] [G loss: 1.378240]\n",
      "[D loss: 0.968099] [G loss: 1.557679]\n",
      "[D loss: 0.864686] [G loss: 1.419663]\n",
      "[D loss: 0.706579] [G loss: 1.391838]\n",
      "[D loss: 0.827274] [G loss: 1.431504]\n",
      "[D loss: 0.838215] [G loss: 1.741603]\n",
      "[D loss: 0.804186] [G loss: 1.474611]\n",
      "[D loss: 0.754614] [G loss: 1.552770]\n",
      "[D loss: 0.978116] [G loss: 1.456733]\n",
      "[D loss: 0.809501] [G loss: 1.265304]\n",
      "[D loss: 0.972072] [G loss: 1.222656]\n",
      "[D loss: 0.814412] [G loss: 1.432668]\n",
      "[D loss: 0.782364] [G loss: 1.461516]\n",
      "[D loss: 0.786154] [G loss: 1.501253]\n",
      "[D loss: 0.793557] [G loss: 1.418398]\n",
      "[D loss: 0.541359] [G loss: 1.532441]\n",
      "[D loss: 0.696411] [G loss: 1.617867]\n",
      "[D loss: 0.687009] [G loss: 1.368337]\n",
      "[D loss: 0.650488] [G loss: 1.682471]\n",
      "[D loss: 0.798812] [G loss: 1.528128]\n",
      "[D loss: 0.984757] [G loss: 1.242853]\n",
      "[D loss: 0.838976] [G loss: 1.631303]\n",
      "[D loss: 0.772737] [G loss: 1.579505]\n",
      "[D loss: 0.952945] [G loss: 1.446826]\n",
      "[D loss: 0.842470] [G loss: 1.654937]\n",
      "[D loss: 0.491526] [G loss: 1.719095]\n",
      "[D loss: 0.751174] [G loss: 1.447288]\n",
      "[D loss: 0.823434] [G loss: 1.450273]\n",
      "[D loss: 1.001350] [G loss: 1.493703]\n",
      "[D loss: 1.002925] [G loss: 1.418767]\n",
      "[D loss: 0.750531] [G loss: 1.603282]\n",
      "[D loss: 0.641616] [G loss: 1.555596]\n",
      "[D loss: 0.759228] [G loss: 1.594358]\n",
      "[D loss: 0.730566] [G loss: 1.595160]\n",
      "[D loss: 0.984667] [G loss: 1.440429]\n",
      "[D loss: 0.887886] [G loss: 1.430156]\n",
      "[D loss: 0.735795] [G loss: 1.519186]\n",
      "[D loss: 0.816958] [G loss: 1.358759]\n",
      "[D loss: 0.790915] [G loss: 1.815617]\n",
      "[D loss: 0.893896] [G loss: 1.552390]\n",
      "[D loss: 0.867508] [G loss: 1.272709]\n",
      "[D loss: 0.720894] [G loss: 1.465533]\n",
      "[D loss: 1.061412] [G loss: 1.393155]\n",
      "[D loss: 0.923432] [G loss: 1.412442]\n",
      "[D loss: 0.711831] [G loss: 1.481662]\n",
      "[D loss: 0.593119] [G loss: 1.370302]\n",
      "[D loss: 1.231117] [G loss: 1.314215]\n",
      "[D loss: 0.711488] [G loss: 1.454622]\n",
      "[D loss: 0.822644] [G loss: 1.380625]\n",
      "[D loss: 0.708876] [G loss: 1.595866]\n",
      "[D loss: 1.205639] [G loss: 1.412081]\n",
      "[D loss: 0.558333] [G loss: 1.601080]\n",
      "[D loss: 0.941163] [G loss: 1.337512]\n",
      "[D loss: 0.805441] [G loss: 1.360221]\n",
      "[D loss: 0.581164] [G loss: 1.535480]\n",
      "[D loss: 0.784690] [G loss: 1.605428]\n",
      "[D loss: 0.813400] [G loss: 1.540703]\n",
      "[D loss: 1.118847] [G loss: 1.513078]\n",
      "[D loss: 0.913030] [G loss: 1.559177]\n",
      "[D loss: 0.606447] [G loss: 1.365873]\n",
      "[D loss: 0.781349] [G loss: 1.432355]\n",
      "[D loss: 0.769819] [G loss: 1.369744]\n",
      "[D loss: 0.763255] [G loss: 1.477352]\n",
      "[D loss: 0.623277] [G loss: 1.714926]\n",
      "[D loss: 0.839990] [G loss: 1.601338]\n",
      "[D loss: 0.557752] [G loss: 1.453725]\n",
      "[D loss: 0.709357] [G loss: 1.771153]\n",
      "[D loss: 0.746265] [G loss: 1.551671]\n",
      "[D loss: 0.609170] [G loss: 1.624246]\n",
      "[D loss: 0.840924] [G loss: 1.362061]\n",
      "[D loss: 0.785295] [G loss: 1.450768]\n",
      "[D loss: 0.715860] [G loss: 1.549297]\n",
      "[D loss: 0.788633] [G loss: 1.765813]\n",
      "[D loss: 0.863400] [G loss: 1.733010]\n",
      "[D loss: 0.957441] [G loss: 1.489054]\n",
      "[D loss: 0.767498] [G loss: 1.384755]\n",
      "[D loss: 0.817203] [G loss: 1.303918]\n",
      "[D loss: 1.128712] [G loss: 1.489917]\n",
      "[D loss: 0.864474] [G loss: 1.597665]\n",
      "[D loss: 0.866377] [G loss: 1.391525]\n",
      "[D loss: 0.914964] [G loss: 1.526971]\n",
      "[D loss: 0.822644] [G loss: 1.583166]\n",
      "[D loss: 0.938426] [G loss: 1.505840]\n",
      "[D loss: 0.660420] [G loss: 1.768365]\n",
      "[D loss: 0.714716] [G loss: 1.545477]\n",
      "[D loss: 0.676317] [G loss: 1.696692]\n",
      "[D loss: 0.766142] [G loss: 1.458519]\n",
      "[D loss: 1.016346] [G loss: 1.380771]\n",
      "[D loss: 0.745755] [G loss: 1.479015]\n",
      "[D loss: 0.756704] [G loss: 1.433536]\n",
      "[D loss: 0.665394] [G loss: 1.521508]\n",
      "[D loss: 0.846635] [G loss: 1.207692]\n",
      "[D loss: 1.022588] [G loss: 1.402325]\n",
      "[D loss: 0.897393] [G loss: 1.163064]\n",
      "[D loss: 0.800434] [G loss: 1.416899]\n",
      "[D loss: 0.886531] [G loss: 1.689116]\n",
      "[D loss: 0.754831] [G loss: 1.608210]\n",
      "[D loss: 0.851099] [G loss: 1.359133]\n",
      "[D loss: 0.746889] [G loss: 1.513913]\n",
      "[D loss: 0.532062] [G loss: 1.565329]\n",
      "[D loss: 0.873540] [G loss: 1.653670]\n",
      "[D loss: 0.700096] [G loss: 1.608375]\n",
      "[D loss: 0.754102] [G loss: 1.567382]\n",
      "[D loss: 0.844451] [G loss: 1.363181]\n",
      "[D loss: 0.638971] [G loss: 1.480507]\n",
      "[D loss: 0.627365] [G loss: 1.689681]\n",
      "[D loss: 0.724397] [G loss: 1.394672]\n",
      "[D loss: 0.864678] [G loss: 1.464906]\n",
      "[D loss: 0.888190] [G loss: 1.526513]\n",
      "[D loss: 0.715538] [G loss: 1.592707]\n",
      "[D loss: 0.613023] [G loss: 1.918465]\n",
      "[D loss: 1.194551] [G loss: 1.624540]\n",
      "[D loss: 0.765870] [G loss: 1.466309]\n",
      "[D loss: 0.921220] [G loss: 1.370894]\n",
      "[D loss: 1.064461] [G loss: 1.417128]\n",
      "[D loss: 0.649791] [G loss: 1.578043]\n",
      "[D loss: 0.824020] [G loss: 1.521158]\n",
      "[D loss: 0.719819] [G loss: 1.373366]\n",
      "[D loss: 0.984192] [G loss: 1.307754]\n",
      "[D loss: 0.923653] [G loss: 1.534173]\n",
      "[D loss: 0.951264] [G loss: 1.306059]\n",
      "[D loss: 0.950040] [G loss: 1.575175]\n",
      "[D loss: 0.724614] [G loss: 1.616261]\n",
      "[D loss: 0.891256] [G loss: 1.474786]\n",
      "[D loss: 0.853081] [G loss: 1.340422]\n",
      "[D loss: 0.528278] [G loss: 1.493584]\n",
      "[D loss: 0.983204] [G loss: 1.473887]\n",
      "[D loss: 0.644625] [G loss: 1.537990]\n",
      "[D loss: 0.727146] [G loss: 1.486189]\n",
      "[D loss: 0.909702] [G loss: 1.559379]\n",
      "[D loss: 0.911750] [G loss: 1.491468]\n",
      "[D loss: 0.684818] [G loss: 1.516860]\n",
      "[D loss: 0.791847] [G loss: 1.485875]\n",
      "[D loss: 0.847627] [G loss: 1.346445]\n",
      "[D loss: 0.804298] [G loss: 1.357785]\n",
      "[D loss: 0.903024] [G loss: 1.462938]\n",
      "[D loss: 0.645030] [G loss: 1.592899]\n",
      "[D loss: 0.836923] [G loss: 1.737346]\n",
      "[D loss: 0.929936] [G loss: 1.893620]\n",
      "[D loss: 0.804111] [G loss: 1.875658]\n",
      "[D loss: 0.700622] [G loss: 1.395436]\n",
      "[D loss: 0.856047] [G loss: 1.451773]\n",
      "[D loss: 0.786917] [G loss: 1.521240]\n",
      "[D loss: 0.620580] [G loss: 1.546915]\n",
      "[D loss: 1.013298] [G loss: 1.678983]\n",
      "[D loss: 0.955231] [G loss: 1.391048]\n",
      "[D loss: 0.782685] [G loss: 1.341254]\n",
      "[D loss: 0.721203] [G loss: 1.406721]\n",
      "[D loss: 0.751112] [G loss: 1.515808]\n",
      "[D loss: 0.781406] [G loss: 1.484584]\n",
      "[D loss: 0.716679] [G loss: 1.613639]\n",
      "[D loss: 0.523769] [G loss: 1.588394]\n",
      "[D loss: 0.985890] [G loss: 1.451388]\n",
      "[D loss: 0.947974] [G loss: 1.514565]\n",
      "[D loss: 0.867257] [G loss: 1.671162]\n",
      "[D loss: 0.932619] [G loss: 1.531263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.718780] [G loss: 1.628860]\n",
      "[D loss: 0.791574] [G loss: 1.446230]\n",
      "[D loss: 0.834205] [G loss: 1.641117]\n",
      "[D loss: 0.873808] [G loss: 1.354959]\n",
      "[D loss: 0.839385] [G loss: 1.404069]\n",
      "[D loss: 0.769053] [G loss: 1.562325]\n",
      "[D loss: 1.002463] [G loss: 1.441824]\n",
      "[D loss: 0.892748] [G loss: 1.461236]\n",
      "[D loss: 0.668905] [G loss: 1.561584]\n",
      "[D loss: 0.445394] [G loss: 1.783580]\n",
      "[D loss: 0.820710] [G loss: 1.774278]\n",
      "[D loss: 0.694633] [G loss: 1.584956]\n",
      "[D loss: 0.796421] [G loss: 1.580956]\n",
      "[D loss: 0.693661] [G loss: 1.617478]\n",
      "[D loss: 0.813000] [G loss: 1.671801]\n",
      "[D loss: 0.795940] [G loss: 1.534118]\n",
      "[D loss: 0.880886] [G loss: 1.477879]\n",
      "[D loss: 0.655956] [G loss: 1.593803]\n",
      "[D loss: 0.892011] [G loss: 1.643592]\n",
      "[D loss: 0.754903] [G loss: 1.565208]\n",
      "[D loss: 0.539671] [G loss: 1.615350]\n",
      "[D loss: 0.613100] [G loss: 1.657631]\n",
      "[D loss: 0.812454] [G loss: 1.592377]\n",
      "[D loss: 0.764164] [G loss: 1.691345]\n",
      "[D loss: 0.690000] [G loss: 1.409776]\n",
      "[D loss: 0.980743] [G loss: 1.446400]\n",
      "[D loss: 0.988900] [G loss: 1.687254]\n",
      "[D loss: 0.900423] [G loss: 1.694409]\n",
      "[D loss: 0.807015] [G loss: 1.472900]\n",
      "[D loss: 0.606476] [G loss: 1.472924]\n",
      "[D loss: 0.941323] [G loss: 1.386759]\n",
      "[D loss: 0.845326] [G loss: 1.445200]\n",
      "[D loss: 0.732044] [G loss: 1.616196]\n",
      "[D loss: 0.932239] [G loss: 1.519819]\n",
      "[D loss: 0.661621] [G loss: 1.372527]\n",
      "[D loss: 0.960503] [G loss: 1.324515]\n",
      "[D loss: 0.527048] [G loss: 1.688430]\n",
      "[D loss: 0.702512] [G loss: 1.558475]\n",
      "[D loss: 0.876864] [G loss: 1.535416]\n",
      "[D loss: 0.965928] [G loss: 1.533917]\n",
      "[D loss: 0.771650] [G loss: 1.446974]\n",
      "[D loss: 1.053496] [G loss: 1.380934]\n",
      "[D loss: 1.073789] [G loss: 1.383796]\n",
      "[D loss: 0.851163] [G loss: 1.263850]\n",
      "[D loss: 0.805164] [G loss: 1.405905]\n",
      "[D loss: 0.678668] [G loss: 1.318585]\n",
      "[D loss: 0.787111] [G loss: 1.348616]\n",
      "[D loss: 0.665116] [G loss: 1.555807]\n",
      "[D loss: 0.754612] [G loss: 1.578408]\n",
      "[D loss: 0.837214] [G loss: 1.561629]\n",
      "[D loss: 0.835501] [G loss: 1.372473]\n",
      "[D loss: 1.094724] [G loss: 1.327843]\n",
      "[D loss: 0.795094] [G loss: 1.535927]\n",
      "[D loss: 0.751942] [G loss: 1.439700]\n",
      "[D loss: 1.081229] [G loss: 1.457972]\n",
      "[D loss: 1.168442] [G loss: 1.489087]\n",
      "[D loss: 0.630229] [G loss: 1.503054]\n",
      "[D loss: 0.662154] [G loss: 1.389459]\n",
      "[D loss: 1.072804] [G loss: 1.416531]\n",
      "[D loss: 0.817150] [G loss: 1.437647]\n",
      "[D loss: 0.862760] [G loss: 1.375366]\n",
      "[D loss: 0.729010] [G loss: 1.585589]\n",
      "[D loss: 0.690878] [G loss: 1.439719]\n",
      "[D loss: 0.785654] [G loss: 1.382644]\n",
      "[D loss: 0.868044] [G loss: 1.469949]\n",
      "[D loss: 0.866167] [G loss: 1.395794]\n",
      "[D loss: 0.794392] [G loss: 1.349267]\n",
      "[D loss: 0.910780] [G loss: 1.450467]\n",
      "[D loss: 0.491196] [G loss: 1.445189]\n",
      "[D loss: 0.834023] [G loss: 1.460975]\n",
      "[D loss: 1.037882] [G loss: 1.486223]\n",
      "[D loss: 0.875611] [G loss: 1.423425]\n",
      "[D loss: 0.742410] [G loss: 1.432548]\n",
      "[D loss: 0.728424] [G loss: 1.342195]\n",
      "[D loss: 0.743607] [G loss: 1.401089]\n",
      "[D loss: 1.075447] [G loss: 1.347082]\n",
      "[D loss: 0.996340] [G loss: 1.508759]\n",
      "[D loss: 0.783195] [G loss: 1.483769]\n",
      "[D loss: 0.839233] [G loss: 1.102841]\n",
      "[D loss: 0.805544] [G loss: 1.348698]\n",
      "[D loss: 0.701805] [G loss: 1.414115]\n",
      "[D loss: 0.808179] [G loss: 1.683294]\n",
      "[D loss: 0.847765] [G loss: 1.594089]\n",
      "[D loss: 0.802557] [G loss: 1.417737]\n",
      "[D loss: 0.820221] [G loss: 1.567282]\n",
      "[D loss: 1.006745] [G loss: 1.461477]\n",
      "[D loss: 0.823387] [G loss: 1.419688]\n",
      "[D loss: 0.920513] [G loss: 1.502169]\n",
      "[D loss: 0.659930] [G loss: 1.283396]\n",
      "[D loss: 0.854907] [G loss: 1.591018]\n",
      "[D loss: 0.694249] [G loss: 1.483225]\n",
      "[D loss: 0.916740] [G loss: 1.347322]\n",
      "[D loss: 0.922056] [G loss: 1.300966]\n",
      "[D loss: 0.703500] [G loss: 1.584349]\n",
      "[D loss: 0.600339] [G loss: 1.538764]\n",
      "[D loss: 0.770118] [G loss: 1.648773]\n",
      "[D loss: 0.676692] [G loss: 1.500540]\n",
      "[D loss: 0.981832] [G loss: 1.586719]\n",
      "[D loss: 0.938791] [G loss: 1.353495]\n",
      "[D loss: 0.813877] [G loss: 1.646783]\n",
      "[D loss: 0.892201] [G loss: 1.622248]\n",
      "[D loss: 1.033070] [G loss: 1.445957]\n",
      "[D loss: 0.772068] [G loss: 1.716871]\n",
      "[D loss: 0.838057] [G loss: 1.686279]\n",
      "[D loss: 0.762825] [G loss: 1.601967]\n",
      "[D loss: 0.826122] [G loss: 1.278008]\n",
      "[D loss: 0.704796] [G loss: 1.401211]\n",
      "[D loss: 0.616185] [G loss: 1.595490]\n",
      "[D loss: 0.729254] [G loss: 1.451466]\n",
      "[D loss: 1.081360] [G loss: 1.313038]\n",
      "[D loss: 0.530038] [G loss: 1.753811]\n",
      "[D loss: 0.797740] [G loss: 1.754070]\n",
      "[D loss: 0.689292] [G loss: 1.578507]\n",
      "[D loss: 0.677209] [G loss: 1.585513]\n",
      "[D loss: 0.673055] [G loss: 1.571875]\n",
      "[D loss: 0.963587] [G loss: 1.424585]\n",
      "[D loss: 0.966184] [G loss: 1.501186]\n",
      "[D loss: 0.830647] [G loss: 1.376723]\n",
      "[D loss: 0.684215] [G loss: 1.816203]\n",
      "[D loss: 0.962361] [G loss: 1.464868]\n",
      "[D loss: 0.789591] [G loss: 1.531459]\n",
      "[D loss: 1.103725] [G loss: 1.387426]\n",
      "[D loss: 0.765846] [G loss: 1.501063]\n",
      "[D loss: 0.537540] [G loss: 1.558792]\n",
      "[D loss: 0.793123] [G loss: 1.640773]\n",
      "[D loss: 0.958618] [G loss: 1.410092]\n",
      "[D loss: 0.751863] [G loss: 1.629877]\n",
      "[D loss: 0.919689] [G loss: 1.553364]\n",
      "[D loss: 1.017506] [G loss: 1.624263]\n",
      "[D loss: 0.973041] [G loss: 1.210629]\n",
      "[D loss: 0.705592] [G loss: 1.381889]\n",
      "[D loss: 0.877177] [G loss: 1.496144]\n",
      "[D loss: 0.792219] [G loss: 1.361755]\n",
      "[D loss: 0.894985] [G loss: 1.516176]\n",
      "[D loss: 1.151731] [G loss: 1.449862]\n",
      "[D loss: 0.911444] [G loss: 1.465935]\n",
      "[D loss: 0.932228] [G loss: 1.528163]\n",
      "[D loss: 0.688641] [G loss: 1.539814]\n",
      "[D loss: 0.943719] [G loss: 1.433648]\n",
      "[D loss: 0.669609] [G loss: 1.343028]\n",
      "[D loss: 0.648811] [G loss: 1.539697]\n",
      "[D loss: 0.902471] [G loss: 1.556819]\n",
      "[D loss: 0.834987] [G loss: 1.412822]\n",
      "[D loss: 0.619300] [G loss: 1.476621]\n",
      "[D loss: 0.819281] [G loss: 1.614130]\n",
      "[D loss: 0.862733] [G loss: 1.418429]\n",
      "[D loss: 1.028394] [G loss: 1.604988]\n",
      "[D loss: 0.776212] [G loss: 1.459176]\n",
      "[D loss: 0.686988] [G loss: 1.490738]\n",
      "[D loss: 1.022939] [G loss: 1.504729]\n",
      "[D loss: 0.801257] [G loss: 1.357695]\n",
      "[D loss: 0.879150] [G loss: 1.273003]\n",
      "[D loss: 0.791629] [G loss: 1.346113]\n",
      "[D loss: 0.890264] [G loss: 1.586301]\n",
      "[D loss: 0.546557] [G loss: 1.607589]\n",
      "[D loss: 0.843251] [G loss: 1.417122]\n",
      "[D loss: 0.727223] [G loss: 1.441905]\n",
      "[D loss: 0.742257] [G loss: 1.484689]\n",
      "[D loss: 0.784866] [G loss: 1.508252]\n",
      "[D loss: 0.922517] [G loss: 1.552446]\n",
      "[D loss: 0.654256] [G loss: 1.429774]\n",
      "[D loss: 0.761287] [G loss: 1.555236]\n",
      "[D loss: 0.830878] [G loss: 1.224196]\n",
      "[D loss: 0.850711] [G loss: 1.394906]\n",
      "[D loss: 0.892621] [G loss: 1.519354]\n",
      "[D loss: 0.831853] [G loss: 1.394468]\n",
      "[D loss: 0.624282] [G loss: 1.389671]\n",
      "[D loss: 1.091813] [G loss: 1.372367]\n",
      "[D loss: 0.955929] [G loss: 1.406529]\n",
      "[D loss: 0.816431] [G loss: 1.570736]\n",
      "[D loss: 0.688555] [G loss: 1.392288]\n",
      "[D loss: 0.844214] [G loss: 1.499704]\n",
      "[D loss: 0.792707] [G loss: 1.551605]\n",
      "[D loss: 0.659836] [G loss: 1.431405]\n",
      "[D loss: 0.571023] [G loss: 1.631389]\n",
      "[D loss: 0.860764] [G loss: 1.710145]\n",
      "[D loss: 0.926710] [G loss: 1.556140]\n",
      "[D loss: 0.971760] [G loss: 1.434213]\n",
      "[D loss: 0.797255] [G loss: 1.570240]\n",
      "[D loss: 0.653430] [G loss: 1.536595]\n",
      "[D loss: 0.859613] [G loss: 1.688165]\n",
      "[D loss: 0.813850] [G loss: 1.611089]\n",
      "[D loss: 0.712697] [G loss: 1.527361]\n",
      "[D loss: 0.869811] [G loss: 1.388791]\n",
      "[D loss: 1.017712] [G loss: 1.300017]\n",
      "[D loss: 0.731795] [G loss: 1.364986]\n",
      "[D loss: 0.952728] [G loss: 1.344643]\n",
      "[D loss: 0.715968] [G loss: 1.627496]\n",
      "[D loss: 0.913865] [G loss: 1.241225]\n",
      "[D loss: 0.788697] [G loss: 1.384683]\n",
      "[D loss: 0.593698] [G loss: 1.591777]\n",
      "[D loss: 0.559410] [G loss: 1.790714]\n",
      "[D loss: 0.942671] [G loss: 1.475587]\n",
      "[D loss: 0.647160] [G loss: 1.517517]\n",
      "[D loss: 0.968676] [G loss: 1.397855]\n",
      "[D loss: 0.728870] [G loss: 1.549807]\n",
      "[D loss: 0.889725] [G loss: 1.482378]\n",
      "[D loss: 0.776291] [G loss: 1.571463]\n",
      "[D loss: 0.799323] [G loss: 1.529197]\n",
      "[D loss: 0.813232] [G loss: 1.559156]\n",
      "[D loss: 0.756046] [G loss: 1.610755]\n",
      "[D loss: 0.621146] [G loss: 1.451789]\n",
      "[D loss: 0.914805] [G loss: 1.396711]\n",
      "[D loss: 0.844217] [G loss: 1.626455]\n",
      "[D loss: 0.705816] [G loss: 1.451666]\n",
      "[D loss: 0.600845] [G loss: 1.670716]\n",
      "[D loss: 0.633860] [G loss: 1.763958]\n",
      "[D loss: 0.874148] [G loss: 1.587273]\n",
      "[D loss: 0.788883] [G loss: 1.535631]\n",
      "[D loss: 0.797390] [G loss: 1.738234]\n",
      "[D loss: 0.926436] [G loss: 1.540240]\n",
      "[D loss: 0.797497] [G loss: 1.356948]\n",
      "[D loss: 0.785572] [G loss: 1.549055]\n",
      "[D loss: 1.084269] [G loss: 1.482431]\n",
      "[D loss: 0.793586] [G loss: 1.353458]\n",
      "[D loss: 0.836653] [G loss: 1.216220]\n",
      "[D loss: 0.862267] [G loss: 1.463187]\n",
      "[D loss: 0.767174] [G loss: 1.385521]\n",
      "[D loss: 0.659368] [G loss: 1.459076]\n",
      "[D loss: 0.639527] [G loss: 1.713406]\n",
      "[D loss: 0.984810] [G loss: 1.518537]\n",
      "[D loss: 0.828761] [G loss: 1.581125]\n",
      "[D loss: 0.896652] [G loss: 1.377732]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.923843] [G loss: 1.257011]\n",
      "[D loss: 0.865195] [G loss: 1.402621]\n",
      "[D loss: 0.794985] [G loss: 1.464847]\n",
      "[D loss: 0.971933] [G loss: 1.569577]\n",
      "[D loss: 0.864235] [G loss: 1.447310]\n",
      "[D loss: 0.846867] [G loss: 1.299164]\n",
      "[D loss: 0.841312] [G loss: 1.509395]\n",
      "[D loss: 0.620733] [G loss: 1.689648]\n",
      "[D loss: 0.838704] [G loss: 1.387285]\n",
      "[D loss: 0.736952] [G loss: 1.523832]\n",
      "[D loss: 0.692675] [G loss: 1.529435]\n",
      "[D loss: 0.750575] [G loss: 1.603404]\n",
      "[D loss: 0.691543] [G loss: 1.676499]\n",
      "[D loss: 0.932518] [G loss: 1.495057]\n",
      "[D loss: 1.082330] [G loss: 1.278334]\n",
      "[D loss: 0.813941] [G loss: 1.703013]\n",
      "[D loss: 0.617573] [G loss: 1.556196]\n",
      "[D loss: 0.896277] [G loss: 1.483779]\n",
      "[D loss: 0.397690] [G loss: 1.640013]\n",
      "[D loss: 0.697890] [G loss: 1.503929]\n",
      "[D loss: 0.759096] [G loss: 1.482606]\n",
      "[D loss: 1.111068] [G loss: 1.609715]\n",
      "[D loss: 0.676970] [G loss: 1.505346]\n",
      "[D loss: 0.862927] [G loss: 1.693721]\n",
      "[D loss: 0.871212] [G loss: 1.553632]\n",
      "[D loss: 1.166314] [G loss: 1.370487]\n",
      "[D loss: 0.716974] [G loss: 1.696726]\n",
      "[D loss: 0.876844] [G loss: 1.358141]\n",
      "[D loss: 0.759228] [G loss: 1.364386]\n",
      "[D loss: 0.975852] [G loss: 1.443215]\n",
      "[D loss: 0.763397] [G loss: 1.567968]\n",
      "[D loss: 0.479526] [G loss: 1.547862]\n",
      "[D loss: 0.710945] [G loss: 1.391160]\n",
      "[D loss: 0.686077] [G loss: 1.614443]\n",
      "[D loss: 0.774794] [G loss: 1.687745]\n",
      "[D loss: 0.774834] [G loss: 1.615743]\n",
      "[D loss: 0.803304] [G loss: 1.466275]\n",
      "[D loss: 0.766480] [G loss: 1.290821]\n",
      "[D loss: 0.644895] [G loss: 1.435939]\n",
      "[D loss: 0.769654] [G loss: 1.607701]\n",
      "[D loss: 0.962870] [G loss: 1.348034]\n",
      "[D loss: 0.773839] [G loss: 1.490753]\n",
      "[D loss: 0.910821] [G loss: 1.433385]\n",
      "[D loss: 0.885457] [G loss: 1.488878]\n",
      "[D loss: 0.656885] [G loss: 1.628549]\n",
      "[D loss: 0.697811] [G loss: 1.467948]\n",
      "[D loss: 0.914789] [G loss: 1.614744]\n",
      "[D loss: 0.938197] [G loss: 1.542323]\n",
      "[D loss: 0.705610] [G loss: 1.392174]\n",
      "[D loss: 0.661846] [G loss: 1.551885]\n",
      "[D loss: 0.936481] [G loss: 1.826924]\n",
      "[D loss: 0.763632] [G loss: 1.713153]\n",
      "[D loss: 0.844143] [G loss: 1.326346]\n",
      "[D loss: 0.655407] [G loss: 1.581585]\n",
      "[D loss: 0.583666] [G loss: 1.575104]\n",
      "[D loss: 1.121908] [G loss: 1.325617]\n",
      "[D loss: 0.783295] [G loss: 1.456457]\n",
      "[D loss: 0.917164] [G loss: 1.404537]\n",
      "[D loss: 0.912399] [G loss: 1.435720]\n",
      "[D loss: 0.801676] [G loss: 1.457425]\n",
      "[D loss: 0.852169] [G loss: 1.456931]\n",
      "[D loss: 0.676784] [G loss: 1.908420]\n",
      "[D loss: 0.647915] [G loss: 1.571016]\n",
      "[D loss: 0.845852] [G loss: 1.474919]\n",
      "[D loss: 0.907288] [G loss: 1.328330]\n",
      "[D loss: 0.734465] [G loss: 1.602849]\n",
      "[D loss: 0.919393] [G loss: 1.517937]\n",
      "[D loss: 0.661878] [G loss: 1.490036]\n",
      "[D loss: 0.943139] [G loss: 1.357448]\n",
      "[D loss: 1.045859] [G loss: 1.597518]\n",
      "[D loss: 0.598431] [G loss: 1.364805]\n",
      "[D loss: 0.691904] [G loss: 1.380374]\n",
      "[D loss: 1.078046] [G loss: 1.358568]\n",
      "[D loss: 1.006906] [G loss: 1.283937]\n",
      "[D loss: 0.930084] [G loss: 1.286260]\n",
      "[D loss: 0.761741] [G loss: 1.304558]\n",
      "[D loss: 0.768541] [G loss: 1.352122]\n",
      "[D loss: 0.719881] [G loss: 1.497881]\n",
      "[D loss: 0.749695] [G loss: 1.554493]\n",
      "[D loss: 0.814249] [G loss: 1.596366]\n",
      "[D loss: 0.795596] [G loss: 1.493899]\n",
      "[D loss: 0.790820] [G loss: 1.409432]\n",
      "[D loss: 0.919384] [G loss: 1.560813]\n",
      "[D loss: 0.795300] [G loss: 1.416357]\n",
      "[D loss: 1.054927] [G loss: 1.346119]\n",
      "[D loss: 0.748499] [G loss: 1.199089]\n",
      "[D loss: 0.696314] [G loss: 1.423954]\n",
      "[D loss: 0.778279] [G loss: 1.586341]\n",
      "[D loss: 0.842768] [G loss: 1.448023]\n",
      "[D loss: 0.690248] [G loss: 1.480531]\n",
      "[D loss: 0.889538] [G loss: 1.583300]\n",
      "[D loss: 0.941139] [G loss: 1.281014]\n",
      "[D loss: 1.146034] [G loss: 1.413812]\n",
      "[D loss: 0.711868] [G loss: 1.404787]\n",
      "[D loss: 0.638631] [G loss: 1.401531]\n",
      "[D loss: 0.846359] [G loss: 1.383418]\n",
      "[D loss: 0.928468] [G loss: 1.248722]\n",
      "[D loss: 0.702969] [G loss: 1.630106]\n",
      "[D loss: 0.908992] [G loss: 1.591788]\n",
      "[D loss: 0.743782] [G loss: 1.672218]\n",
      "[D loss: 0.902854] [G loss: 1.286374]\n",
      "[D loss: 1.086272] [G loss: 1.355600]\n",
      "[D loss: 0.660229] [G loss: 1.218659]\n",
      "[D loss: 0.752658] [G loss: 1.429290]\n",
      "[D loss: 0.792291] [G loss: 1.397306]\n",
      "[D loss: 0.858686] [G loss: 1.438567]\n",
      "[D loss: 0.875969] [G loss: 1.559855]\n",
      "[D loss: 0.659911] [G loss: 1.296859]\n",
      "[D loss: 0.684258] [G loss: 1.628831]\n",
      "[D loss: 0.801796] [G loss: 1.484997]\n",
      "[D loss: 0.800053] [G loss: 1.466526]\n",
      "[D loss: 0.903431] [G loss: 1.618759]\n",
      "[D loss: 0.868513] [G loss: 1.347462]\n",
      "[D loss: 1.048485] [G loss: 1.424178]\n",
      "[D loss: 0.787380] [G loss: 1.428223]\n",
      "[D loss: 0.924761] [G loss: 1.379696]\n",
      "[D loss: 1.014770] [G loss: 1.486768]\n",
      "[D loss: 0.785565] [G loss: 1.465412]\n",
      "[D loss: 0.771700] [G loss: 1.426309]\n",
      "[D loss: 0.876256] [G loss: 1.252417]\n",
      "[D loss: 0.799991] [G loss: 1.487721]\n",
      "[D loss: 0.818727] [G loss: 1.462422]\n",
      "[D loss: 0.824750] [G loss: 1.388068]\n",
      "[D loss: 0.728162] [G loss: 1.524341]\n",
      "[D loss: 0.777948] [G loss: 1.212474]\n",
      "[D loss: 0.590239] [G loss: 1.441987]\n",
      "[D loss: 0.591999] [G loss: 1.536525]\n",
      "[D loss: 0.988851] [G loss: 1.508346]\n",
      "[D loss: 1.080722] [G loss: 1.555190]\n",
      "[D loss: 0.988967] [G loss: 1.446777]\n",
      "[D loss: 0.728842] [G loss: 1.570697]\n",
      "[D loss: 0.694253] [G loss: 1.389919]\n",
      "[D loss: 0.843424] [G loss: 1.343595]\n",
      "[D loss: 0.779058] [G loss: 1.579680]\n",
      "[D loss: 0.780113] [G loss: 1.471605]\n",
      "[D loss: 0.743504] [G loss: 1.336315]\n",
      "[D loss: 0.834093] [G loss: 1.382221]\n",
      "[D loss: 0.802594] [G loss: 1.517977]\n",
      "[D loss: 0.730598] [G loss: 1.605516]\n",
      "[D loss: 0.995501] [G loss: 1.519931]\n",
      "[D loss: 0.807429] [G loss: 1.620518]\n",
      "[D loss: 0.964276] [G loss: 1.616914]\n",
      "[D loss: 0.871353] [G loss: 1.292010]\n",
      "[D loss: 1.282121] [G loss: 1.226392]\n",
      "[D loss: 0.891992] [G loss: 1.668396]\n",
      "[D loss: 0.768832] [G loss: 1.387362]\n",
      "[D loss: 0.759304] [G loss: 1.452085]\n",
      "[D loss: 0.792411] [G loss: 1.609636]\n",
      "[D loss: 0.969518] [G loss: 1.380901]\n",
      "[D loss: 1.012750] [G loss: 1.160712]\n",
      "[D loss: 0.813499] [G loss: 1.351817]\n",
      "[D loss: 0.643038] [G loss: 1.356132]\n",
      "[D loss: 0.908181] [G loss: 1.418397]\n",
      "[D loss: 0.799592] [G loss: 1.683704]\n",
      "[D loss: 0.808435] [G loss: 1.595685]\n",
      "[D loss: 0.606233] [G loss: 1.545456]\n",
      "[D loss: 0.976053] [G loss: 1.337839]\n",
      "[D loss: 0.833832] [G loss: 1.309506]\n",
      "[D loss: 0.850689] [G loss: 1.442280]\n",
      "[D loss: 0.808064] [G loss: 1.355409]\n",
      "[D loss: 0.798980] [G loss: 1.615309]\n",
      "[D loss: 0.823031] [G loss: 1.643399]\n",
      "[D loss: 0.757362] [G loss: 1.720613]\n",
      "[D loss: 0.795047] [G loss: 1.414310]\n",
      "[D loss: 0.738712] [G loss: 1.415632]\n",
      "[D loss: 0.700596] [G loss: 1.473420]\n",
      "[D loss: 0.904821] [G loss: 1.541659]\n",
      "[D loss: 0.708812] [G loss: 1.470240]\n",
      "[D loss: 0.766246] [G loss: 1.366104]\n",
      "[D loss: 0.960750] [G loss: 1.429311]\n",
      "[D loss: 0.945485] [G loss: 1.446858]\n",
      "[D loss: 0.969688] [G loss: 1.445347]\n",
      "[D loss: 0.657973] [G loss: 1.688298]\n",
      "[D loss: 0.953744] [G loss: 1.418312]\n",
      "[D loss: 1.016011] [G loss: 1.488670]\n",
      "[D loss: 0.869867] [G loss: 1.508101]\n",
      "[D loss: 0.936874] [G loss: 1.520312]\n",
      "[D loss: 0.975022] [G loss: 1.382618]\n",
      "[D loss: 0.932376] [G loss: 1.373391]\n",
      "[D loss: 0.831899] [G loss: 1.313680]\n",
      "[D loss: 0.737736] [G loss: 1.484018]\n",
      "[D loss: 0.649823] [G loss: 1.504575]\n",
      "[D loss: 0.839334] [G loss: 1.402637]\n",
      "[D loss: 0.688426] [G loss: 1.419169]\n",
      "[D loss: 0.730510] [G loss: 1.518399]\n",
      "[D loss: 0.855856] [G loss: 1.574727]\n",
      "[D loss: 0.939555] [G loss: 1.362316]\n",
      "[D loss: 0.906639] [G loss: 1.428701]\n",
      "[D loss: 0.806320] [G loss: 1.367057]\n",
      "[D loss: 0.786688] [G loss: 1.441317]\n",
      "[D loss: 0.767968] [G loss: 1.434260]\n",
      "[D loss: 0.794650] [G loss: 1.449279]\n",
      "[D loss: 0.861056] [G loss: 1.398890]\n",
      "[D loss: 1.012676] [G loss: 1.374875]\n",
      "[D loss: 1.015502] [G loss: 1.087973]\n",
      "[D loss: 0.822164] [G loss: 1.455955]\n",
      "[D loss: 1.012213] [G loss: 1.227862]\n",
      "[D loss: 0.731149] [G loss: 1.565116]\n",
      "[D loss: 0.679943] [G loss: 1.691620]\n",
      "[D loss: 0.730430] [G loss: 1.291738]\n",
      "[D loss: 0.937272] [G loss: 1.356539]\n",
      "[D loss: 0.842474] [G loss: 1.251711]\n",
      "[D loss: 1.074570] [G loss: 1.213930]\n",
      "[D loss: 0.879842] [G loss: 1.236851]\n",
      "[D loss: 0.966611] [G loss: 1.398178]\n",
      "[D loss: 0.772360] [G loss: 1.514101]\n",
      "[D loss: 0.642667] [G loss: 1.639255]\n",
      "[D loss: 0.563398] [G loss: 1.639365]\n",
      "[D loss: 0.982542] [G loss: 1.379794]\n",
      "[D loss: 0.758979] [G loss: 1.273647]\n",
      "[D loss: 0.953343] [G loss: 1.528265]\n",
      "[D loss: 0.857456] [G loss: 1.537565]\n",
      "[D loss: 0.852141] [G loss: 1.498003]\n",
      "[D loss: 0.918764] [G loss: 1.395286]\n",
      "[D loss: 0.944120] [G loss: 1.512696]\n",
      "[D loss: 0.872259] [G loss: 1.141569]\n",
      "[D loss: 0.923714] [G loss: 1.258207]\n",
      "[D loss: 0.782511] [G loss: 1.569043]\n",
      "[D loss: 0.764811] [G loss: 1.683881]\n",
      "[D loss: 0.860201] [G loss: 1.531718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.884152] [G loss: 1.658465]\n",
      "[D loss: 0.954731] [G loss: 1.357395]\n",
      "[D loss: 0.967694] [G loss: 1.374939]\n",
      "[D loss: 0.778597] [G loss: 1.545168]\n",
      "[D loss: 0.683486] [G loss: 1.507587]\n",
      "[D loss: 0.865239] [G loss: 1.337071]\n",
      "[D loss: 0.738371] [G loss: 1.531011]\n",
      "[D loss: 0.854788] [G loss: 1.396643]\n",
      "[D loss: 0.855463] [G loss: 1.527087]\n",
      "[D loss: 0.705101] [G loss: 1.453393]\n",
      "[D loss: 0.832533] [G loss: 1.357057]\n",
      "[D loss: 0.750145] [G loss: 1.391788]\n",
      "[D loss: 0.945494] [G loss: 1.414420]\n",
      "[D loss: 0.637559] [G loss: 1.694193]\n",
      "[D loss: 0.885507] [G loss: 1.450634]\n",
      "[D loss: 0.774223] [G loss: 1.661991]\n",
      "[D loss: 0.799484] [G loss: 1.652639]\n",
      "[D loss: 0.861953] [G loss: 1.625053]\n",
      "[D loss: 0.802482] [G loss: 1.527688]\n",
      "[D loss: 0.959125] [G loss: 1.462168]\n",
      "[D loss: 0.635575] [G loss: 1.361333]\n",
      "[D loss: 0.651655] [G loss: 1.370362]\n",
      "[D loss: 0.542872] [G loss: 1.531555]\n",
      "[D loss: 0.679056] [G loss: 1.732260]\n",
      "[D loss: 0.890842] [G loss: 1.534878]\n",
      "[D loss: 0.902986] [G loss: 1.722367]\n",
      "[D loss: 0.822275] [G loss: 1.550605]\n",
      "[D loss: 0.767615] [G loss: 1.578777]\n",
      "[D loss: 0.709283] [G loss: 1.323792]\n",
      "[D loss: 0.697941] [G loss: 1.413790]\n",
      "[D loss: 1.004883] [G loss: 1.480306]\n",
      "[D loss: 1.032367] [G loss: 1.355063]\n",
      "[D loss: 0.838540] [G loss: 1.488129]\n",
      "[D loss: 0.800585] [G loss: 1.509377]\n",
      "[D loss: 0.766899] [G loss: 1.521706]\n",
      "[D loss: 0.880034] [G loss: 1.296445]\n",
      "[D loss: 0.700070] [G loss: 1.477195]\n",
      "[D loss: 0.962543] [G loss: 1.297480]\n",
      "[D loss: 0.787023] [G loss: 1.381443]\n",
      "[D loss: 0.809172] [G loss: 1.299649]\n",
      "[D loss: 0.899496] [G loss: 1.332852]\n",
      "[D loss: 0.762368] [G loss: 1.555359]\n",
      "[D loss: 0.751258] [G loss: 1.536908]\n",
      "[D loss: 0.849630] [G loss: 1.450929]\n",
      "[D loss: 1.059761] [G loss: 1.250953]\n",
      "[D loss: 0.946419] [G loss: 1.583348]\n",
      "[D loss: 0.665260] [G loss: 1.502667]\n",
      "[D loss: 0.947164] [G loss: 1.391834]\n",
      "[D loss: 0.710096] [G loss: 1.540798]\n",
      "[D loss: 1.017452] [G loss: 1.488834]\n",
      "[D loss: 0.727284] [G loss: 1.667234]\n",
      "[D loss: 1.114125] [G loss: 1.471982]\n",
      "[D loss: 0.777876] [G loss: 1.397459]\n",
      "[D loss: 0.764655] [G loss: 1.445319]\n",
      "[D loss: 1.077208] [G loss: 1.317099]\n",
      "[D loss: 0.749043] [G loss: 1.410873]\n",
      "[D loss: 0.866336] [G loss: 1.294256]\n",
      "[D loss: 0.645549] [G loss: 1.347584]\n",
      "[D loss: 0.881534] [G loss: 1.348756]\n",
      "[D loss: 0.701988] [G loss: 1.604225]\n",
      "[D loss: 0.762018] [G loss: 1.395722]\n",
      "[D loss: 0.937573] [G loss: 1.199087]\n",
      "[D loss: 0.810171] [G loss: 1.543662]\n",
      "[D loss: 0.806968] [G loss: 1.411318]\n",
      "[D loss: 0.589814] [G loss: 1.392757]\n",
      "[D loss: 0.762822] [G loss: 1.543483]\n",
      "[D loss: 0.837569] [G loss: 1.778691]\n",
      "[D loss: 1.160281] [G loss: 1.546034]\n",
      "[D loss: 0.713501] [G loss: 1.465131]\n",
      "[D loss: 0.842506] [G loss: 1.415685]\n",
      "[D loss: 0.625917] [G loss: 1.408796]\n",
      "[D loss: 0.857307] [G loss: 1.624178]\n",
      "[D loss: 1.057296] [G loss: 1.395509]\n",
      "[D loss: 0.893073] [G loss: 1.478557]\n",
      "[D loss: 0.784905] [G loss: 1.426863]\n",
      "[D loss: 0.762973] [G loss: 1.251877]\n",
      "[D loss: 0.812244] [G loss: 1.471319]\n",
      "[D loss: 0.716024] [G loss: 1.491625]\n",
      "[D loss: 0.764971] [G loss: 1.624040]\n",
      "[D loss: 0.780303] [G loss: 1.438714]\n",
      "[D loss: 0.751772] [G loss: 1.449668]\n",
      "[D loss: 0.818202] [G loss: 1.526234]\n",
      "[D loss: 0.873567] [G loss: 1.293926]\n",
      "[D loss: 0.581718] [G loss: 1.700322]\n",
      "[D loss: 0.787039] [G loss: 1.468361]\n",
      "[D loss: 0.984140] [G loss: 1.671393]\n",
      "[D loss: 0.664886] [G loss: 1.593549]\n",
      "[D loss: 0.838531] [G loss: 1.424815]\n",
      "[D loss: 0.827521] [G loss: 1.494586]\n",
      "[D loss: 0.691711] [G loss: 1.547473]\n",
      "[D loss: 0.862607] [G loss: 1.386124]\n",
      "[D loss: 0.691234] [G loss: 1.578407]\n",
      "[D loss: 0.658303] [G loss: 1.449849]\n",
      "[D loss: 0.976688] [G loss: 1.346726]\n",
      "[D loss: 0.670482] [G loss: 1.578176]\n",
      "[D loss: 0.713358] [G loss: 1.801203]\n",
      "[D loss: 0.827985] [G loss: 1.590345]\n",
      "[D loss: 0.641476] [G loss: 1.507446]\n",
      "[D loss: 0.858544] [G loss: 1.392535]\n",
      "[D loss: 0.722380] [G loss: 1.817130]\n",
      "[D loss: 0.872686] [G loss: 1.556227]\n",
      "[D loss: 0.824033] [G loss: 1.337442]\n",
      "[D loss: 0.802619] [G loss: 1.471059]\n",
      "[D loss: 0.883774] [G loss: 1.521773]\n",
      "[D loss: 0.810891] [G loss: 1.300378]\n",
      "[D loss: 1.021048] [G loss: 1.412379]\n",
      "[D loss: 0.870027] [G loss: 1.571064]\n",
      "[D loss: 0.815198] [G loss: 1.552948]\n",
      "[D loss: 0.879812] [G loss: 1.444102]\n",
      "[D loss: 0.701191] [G loss: 1.592156]\n",
      "[D loss: 0.836759] [G loss: 1.404743]\n",
      "[D loss: 0.659893] [G loss: 1.471652]\n",
      "[D loss: 0.893349] [G loss: 1.511870]\n",
      "[D loss: 0.717380] [G loss: 1.707831]\n",
      "[D loss: 0.774237] [G loss: 1.575691]\n",
      "[D loss: 0.758497] [G loss: 1.755866]\n",
      "[D loss: 0.882401] [G loss: 1.532460]\n",
      "[D loss: 0.754558] [G loss: 1.517015]\n",
      "[D loss: 0.653829] [G loss: 1.523681]\n",
      "[D loss: 0.680761] [G loss: 1.276458]\n",
      "[D loss: 0.920911] [G loss: 1.031936]\n",
      "[D loss: 0.682525] [G loss: 1.639688]\n",
      "[D loss: 1.008086] [G loss: 1.661849]\n",
      "[D loss: 0.703410] [G loss: 2.035641]\n",
      "[D loss: 0.981022] [G loss: 1.309763]\n",
      "[D loss: 0.677913] [G loss: 1.742828]\n",
      "[D loss: 0.929812] [G loss: 1.543636]\n",
      "[D loss: 0.803400] [G loss: 1.614225]\n",
      "[D loss: 0.909225] [G loss: 1.668906]\n",
      "[D loss: 1.146857] [G loss: 1.319392]\n",
      "[D loss: 0.912757] [G loss: 1.339691]\n",
      "[D loss: 0.862927] [G loss: 1.543411]\n",
      "[D loss: 0.998595] [G loss: 1.310305]\n",
      "[D loss: 0.794897] [G loss: 1.449081]\n",
      "[D loss: 0.809646] [G loss: 1.594055]\n",
      "[D loss: 0.969571] [G loss: 1.523034]\n",
      "[D loss: 0.737079] [G loss: 1.387292]\n",
      "[D loss: 0.826986] [G loss: 1.370819]\n",
      "[D loss: 0.838145] [G loss: 1.284591]\n",
      "[D loss: 0.824495] [G loss: 1.677946]\n",
      "[D loss: 0.864952] [G loss: 1.337349]\n",
      "[D loss: 0.700168] [G loss: 1.647524]\n",
      "[D loss: 0.860092] [G loss: 1.457312]\n",
      "[D loss: 0.944717] [G loss: 1.356932]\n",
      "[D loss: 0.694830] [G loss: 1.418010]\n",
      "[D loss: 0.957384] [G loss: 1.328511]\n",
      "[D loss: 0.585209] [G loss: 1.346109]\n",
      "[D loss: 0.774497] [G loss: 1.334394]\n",
      "[D loss: 0.855817] [G loss: 1.359824]\n",
      "[D loss: 0.684546] [G loss: 1.415456]\n",
      "[D loss: 1.061044] [G loss: 1.284464]\n",
      "[D loss: 0.789716] [G loss: 1.211825]\n",
      "[D loss: 0.854454] [G loss: 1.624681]\n",
      "[D loss: 1.034889] [G loss: 1.651840]\n",
      "[D loss: 0.847768] [G loss: 1.507138]\n",
      "[D loss: 0.968577] [G loss: 1.171396]\n",
      "[D loss: 0.735207] [G loss: 1.550900]\n",
      "[D loss: 0.853956] [G loss: 1.527230]\n",
      "[D loss: 0.847330] [G loss: 1.249323]\n",
      "[D loss: 0.623965] [G loss: 1.613815]\n",
      "[D loss: 0.871832] [G loss: 1.670975]\n",
      "[D loss: 0.798062] [G loss: 1.420974]\n",
      "[D loss: 0.871679] [G loss: 1.378825]\n",
      "[D loss: 0.774235] [G loss: 1.458255]\n",
      "[D loss: 0.721844] [G loss: 1.407532]\n",
      "[D loss: 0.845751] [G loss: 1.524949]\n",
      "[D loss: 0.892168] [G loss: 1.421759]\n",
      "[D loss: 0.592408] [G loss: 1.427086]\n",
      "[D loss: 0.684274] [G loss: 1.564354]\n",
      "[D loss: 0.918386] [G loss: 1.619282]\n",
      "[D loss: 0.936652] [G loss: 1.423148]\n",
      "[D loss: 0.791908] [G loss: 1.559463]\n",
      "[D loss: 0.997753] [G loss: 1.503911]\n",
      "[D loss: 0.732456] [G loss: 1.297933]\n",
      "[D loss: 0.720032] [G loss: 1.582238]\n",
      "[D loss: 1.035728] [G loss: 1.298328]\n",
      "[D loss: 0.662567] [G loss: 1.597160]\n",
      "[D loss: 0.953055] [G loss: 1.313203]\n",
      "[D loss: 0.827252] [G loss: 1.326325]\n",
      "[D loss: 0.824986] [G loss: 1.453819]\n",
      "[D loss: 0.870366] [G loss: 1.454904]\n",
      "[D loss: 1.034330] [G loss: 1.354543]\n",
      "[D loss: 0.954829] [G loss: 1.209990]\n",
      "[D loss: 0.905869] [G loss: 1.474521]\n",
      "[D loss: 0.798402] [G loss: 1.375231]\n",
      "[D loss: 0.975242] [G loss: 1.513929]\n",
      "[D loss: 0.836887] [G loss: 1.464039]\n",
      "[D loss: 1.218969] [G loss: 1.283056]\n",
      "[D loss: 0.762461] [G loss: 1.487871]\n",
      "[D loss: 0.807380] [G loss: 1.295049]\n",
      "[D loss: 0.669441] [G loss: 1.295271]\n",
      "[D loss: 0.887260] [G loss: 1.543531]\n",
      "[D loss: 0.900991] [G loss: 1.634297]\n",
      "[D loss: 0.973206] [G loss: 1.442753]\n",
      "[D loss: 0.874464] [G loss: 1.212341]\n",
      "[D loss: 0.831852] [G loss: 1.489343]\n",
      "[D loss: 0.909687] [G loss: 1.533964]\n",
      "[D loss: 0.737502] [G loss: 1.410759]\n",
      "[D loss: 1.041311] [G loss: 1.326976]\n",
      "[D loss: 0.675147] [G loss: 1.437595]\n",
      "[D loss: 0.907514] [G loss: 1.393281]\n",
      "[D loss: 0.701239] [G loss: 1.338460]\n",
      "[D loss: 0.874328] [G loss: 1.419118]\n",
      "[D loss: 0.963428] [G loss: 1.294833]\n",
      "[D loss: 0.825172] [G loss: 1.488236]\n",
      "[D loss: 0.867172] [G loss: 1.449473]\n",
      "[D loss: 0.937516] [G loss: 1.391767]\n",
      "[D loss: 0.884574] [G loss: 1.507792]\n",
      "[D loss: 0.804911] [G loss: 1.531247]\n",
      "[D loss: 0.849649] [G loss: 1.312271]\n",
      "[D loss: 0.863576] [G loss: 1.431447]\n",
      "[D loss: 0.947485] [G loss: 1.640849]\n",
      "[D loss: 0.738674] [G loss: 1.217681]\n",
      "[D loss: 0.810234] [G loss: 1.345024]\n",
      "[D loss: 1.140125] [G loss: 1.436639]\n",
      "[D loss: 0.868015] [G loss: 1.395951]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.958149] [G loss: 1.396393]\n",
      "[D loss: 0.922716] [G loss: 1.363995]\n",
      "[D loss: 0.879078] [G loss: 1.420297]\n",
      "[D loss: 0.950533] [G loss: 1.374527]\n",
      "[D loss: 0.722431] [G loss: 1.254767]\n",
      "[D loss: 0.797518] [G loss: 1.675002]\n",
      "[D loss: 0.866360] [G loss: 1.510532]\n",
      "[D loss: 0.871156] [G loss: 1.345864]\n",
      "[D loss: 1.066355] [G loss: 1.335049]\n",
      "[D loss: 0.751406] [G loss: 1.684836]\n",
      "[D loss: 0.781750] [G loss: 1.610460]\n",
      "[D loss: 1.017483] [G loss: 1.319063]\n",
      "[D loss: 0.916023] [G loss: 1.280053]\n",
      "[D loss: 0.852975] [G loss: 1.334347]\n",
      "[D loss: 0.676098] [G loss: 1.522849]\n",
      "[D loss: 0.748384] [G loss: 1.505249]\n",
      "[D loss: 0.762128] [G loss: 1.515027]\n",
      "[D loss: 0.864387] [G loss: 1.383241]\n",
      "[D loss: 0.870086] [G loss: 1.557043]\n",
      "[D loss: 0.745059] [G loss: 1.587494]\n",
      "[D loss: 0.788630] [G loss: 1.402736]\n",
      "[D loss: 0.734470] [G loss: 1.689849]\n",
      "[D loss: 0.923068] [G loss: 1.494680]\n",
      "[D loss: 1.239254] [G loss: 1.253805]\n",
      "[D loss: 0.673102] [G loss: 1.618466]\n",
      "[D loss: 0.647609] [G loss: 1.660636]\n",
      "[D loss: 0.927550] [G loss: 1.338546]\n",
      "[D loss: 0.727638] [G loss: 1.554829]\n",
      "[D loss: 0.886880] [G loss: 1.343585]\n",
      "[D loss: 0.487493] [G loss: 1.684850]\n",
      "[D loss: 0.657596] [G loss: 1.580773]\n",
      "[D loss: 0.807653] [G loss: 1.464949]\n",
      "[D loss: 0.947148] [G loss: 1.530170]\n",
      "[D loss: 0.841859] [G loss: 1.591939]\n",
      "[D loss: 0.760757] [G loss: 1.603072]\n",
      "[D loss: 0.728475] [G loss: 1.378232]\n",
      "[D loss: 1.062892] [G loss: 1.412532]\n",
      "[D loss: 0.578876] [G loss: 1.589587]\n",
      "[D loss: 0.968011] [G loss: 1.400220]\n",
      "[D loss: 0.866483] [G loss: 1.621382]\n",
      "[D loss: 0.625708] [G loss: 1.789981]\n",
      "[D loss: 0.787182] [G loss: 1.554555]\n",
      "[D loss: 0.601506] [G loss: 1.544340]\n",
      "[D loss: 0.953596] [G loss: 1.446507]\n",
      "[D loss: 0.950973] [G loss: 1.645463]\n",
      "[D loss: 0.866597] [G loss: 1.521520]\n",
      "[D loss: 0.819914] [G loss: 1.487247]\n",
      "[D loss: 0.463758] [G loss: 1.675656]\n",
      "[D loss: 1.156947] [G loss: 1.357574]\n",
      "[D loss: 0.773857] [G loss: 1.394665]\n",
      "[D loss: 0.906836] [G loss: 1.375444]\n",
      "[D loss: 0.807517] [G loss: 1.665316]\n",
      "[D loss: 0.861194] [G loss: 1.546513]\n",
      "[D loss: 0.569481] [G loss: 1.762065]\n",
      "[D loss: 0.955924] [G loss: 1.514017]\n",
      "[D loss: 0.984503] [G loss: 1.340527]\n",
      "[D loss: 0.775726] [G loss: 1.625549]\n",
      "[D loss: 0.959653] [G loss: 1.445904]\n",
      "[D loss: 0.801094] [G loss: 1.343978]\n",
      "[D loss: 0.578975] [G loss: 1.543174]\n",
      "[D loss: 0.677631] [G loss: 1.408345]\n",
      "[D loss: 0.826693] [G loss: 1.495968]\n",
      "[D loss: 0.813139] [G loss: 1.417819]\n",
      "[D loss: 0.894031] [G loss: 1.618506]\n",
      "[D loss: 0.577050] [G loss: 1.494033]\n",
      "[D loss: 0.803774] [G loss: 1.336846]\n",
      "[D loss: 0.904716] [G loss: 1.571861]\n",
      "[D loss: 0.824768] [G loss: 1.686191]\n",
      "[D loss: 0.779668] [G loss: 1.789112]\n",
      "[D loss: 0.734295] [G loss: 1.508842]\n",
      "[D loss: 1.021386] [G loss: 1.290223]\n",
      "[D loss: 0.920654] [G loss: 1.306239]\n",
      "[D loss: 0.901666] [G loss: 1.609264]\n",
      "[D loss: 0.757492] [G loss: 1.534156]\n",
      "[D loss: 0.887439] [G loss: 1.482117]\n",
      "[D loss: 1.095111] [G loss: 1.482081]\n",
      "[D loss: 0.884963] [G loss: 1.351278]\n",
      "[D loss: 0.670254] [G loss: 1.492850]\n",
      "[D loss: 1.040612] [G loss: 1.228602]\n",
      "[D loss: 0.960611] [G loss: 1.410502]\n",
      "[D loss: 1.087240] [G loss: 1.231001]\n",
      "[D loss: 0.868820] [G loss: 1.464249]\n",
      "[D loss: 0.954861] [G loss: 1.418432]\n",
      "[D loss: 0.622125] [G loss: 1.615229]\n",
      "[D loss: 0.976245] [G loss: 1.214833]\n",
      "[D loss: 0.710392] [G loss: 1.491624]\n",
      "[D loss: 0.875116] [G loss: 1.513331]\n",
      "[D loss: 0.678805] [G loss: 1.486983]\n",
      "[D loss: 0.709545] [G loss: 1.430418]\n",
      "[D loss: 0.764762] [G loss: 1.462049]\n",
      "[D loss: 0.815834] [G loss: 1.396616]\n",
      "[D loss: 0.713477] [G loss: 1.338391]\n",
      "[D loss: 0.742900] [G loss: 1.484074]\n",
      "[D loss: 0.694207] [G loss: 1.421654]\n",
      "[D loss: 0.866863] [G loss: 1.404002]\n",
      "[D loss: 0.558896] [G loss: 1.784287]\n",
      "[D loss: 0.632994] [G loss: 1.597856]\n",
      "[D loss: 0.816986] [G loss: 1.428810]\n",
      "[D loss: 0.810390] [G loss: 1.533639]\n",
      "[D loss: 1.055556] [G loss: 1.386320]\n",
      "[D loss: 0.844847] [G loss: 1.414361]\n",
      "[D loss: 0.634208] [G loss: 1.724706]\n",
      "[D loss: 0.663759] [G loss: 1.572145]\n",
      "[D loss: 0.987574] [G loss: 1.497334]\n",
      "[D loss: 0.763441] [G loss: 1.391901]\n",
      "[D loss: 0.808520] [G loss: 1.623751]\n",
      "[D loss: 0.889052] [G loss: 1.310831]\n",
      "[D loss: 0.863415] [G loss: 1.750803]\n",
      "[D loss: 0.971570] [G loss: 1.290231]\n",
      "[D loss: 1.130213] [G loss: 1.291146]\n",
      "[D loss: 0.981096] [G loss: 1.493763]\n",
      "[D loss: 0.517336] [G loss: 1.555568]\n",
      "[D loss: 0.762440] [G loss: 1.698648]\n",
      "[D loss: 0.930127] [G loss: 1.552292]\n",
      "[D loss: 0.851123] [G loss: 1.239650]\n",
      "[D loss: 0.731667] [G loss: 1.335765]\n",
      "[D loss: 0.750472] [G loss: 1.425862]\n",
      "[D loss: 0.800503] [G loss: 1.379160]\n",
      "[D loss: 1.156454] [G loss: 1.173723]\n",
      "[D loss: 0.770738] [G loss: 1.309150]\n",
      "[D loss: 0.741631] [G loss: 1.558285]\n",
      "[D loss: 0.870026] [G loss: 1.544773]\n",
      "[D loss: 0.893483] [G loss: 1.715603]\n",
      "[D loss: 0.862369] [G loss: 1.400666]\n",
      "[D loss: 0.876325] [G loss: 1.400275]\n",
      "[D loss: 0.715233] [G loss: 1.359918]\n",
      "[D loss: 0.892379] [G loss: 1.251477]\n",
      "[D loss: 1.032784] [G loss: 1.384878]\n",
      "[D loss: 0.770038] [G loss: 1.459982]\n",
      "[D loss: 1.034821] [G loss: 1.369915]\n",
      "[D loss: 1.004276] [G loss: 1.561228]\n",
      "[D loss: 0.725762] [G loss: 1.417030]\n",
      "[D loss: 0.849171] [G loss: 1.403072]\n",
      "[D loss: 0.875842] [G loss: 1.454993]\n",
      "[D loss: 0.763062] [G loss: 1.282286]\n",
      "[D loss: 0.643034] [G loss: 1.451783]\n",
      "[D loss: 0.770166] [G loss: 1.240705]\n",
      "[D loss: 0.985330] [G loss: 1.382515]\n",
      "[D loss: 0.736377] [G loss: 1.453432]\n",
      "[D loss: 0.943947] [G loss: 1.509184]\n",
      "[D loss: 1.145850] [G loss: 1.600034]\n",
      "[D loss: 0.794763] [G loss: 1.538438]\n",
      "[D loss: 0.644478] [G loss: 1.498541]\n",
      "[D loss: 0.733272] [G loss: 1.379510]\n",
      "[D loss: 0.991798] [G loss: 1.158620]\n",
      "[D loss: 0.934177] [G loss: 1.398244]\n",
      "[D loss: 0.694658] [G loss: 1.620342]\n",
      "[D loss: 0.735664] [G loss: 1.413965]\n",
      "[D loss: 0.945287] [G loss: 1.329462]\n",
      "[D loss: 0.971103] [G loss: 1.358789]\n",
      "[D loss: 0.843315] [G loss: 1.383041]\n",
      "[D loss: 0.614771] [G loss: 1.481463]\n",
      "[D loss: 0.881434] [G loss: 1.589495]\n",
      "[D loss: 0.727309] [G loss: 1.454920]\n",
      "[D loss: 0.538915] [G loss: 1.407290]\n",
      "[D loss: 0.912641] [G loss: 1.442305]\n",
      "[D loss: 1.072181] [G loss: 1.467465]\n",
      "[D loss: 0.988871] [G loss: 1.396342]\n",
      "[D loss: 0.736312] [G loss: 1.468732]\n",
      "[D loss: 0.928672] [G loss: 1.390067]\n",
      "[D loss: 0.817434] [G loss: 1.356927]\n",
      "[D loss: 0.836059] [G loss: 1.251325]\n",
      "[D loss: 0.714372] [G loss: 1.447514]\n",
      "[D loss: 0.872546] [G loss: 1.545876]\n",
      "[D loss: 0.967192] [G loss: 1.490610]\n",
      "[D loss: 0.793533] [G loss: 1.553149]\n",
      "[D loss: 0.719436] [G loss: 1.641391]\n",
      "[D loss: 0.626218] [G loss: 1.576976]\n",
      "[D loss: 0.627443] [G loss: 1.524421]\n",
      "[D loss: 0.995290] [G loss: 1.387716]\n",
      "[D loss: 0.879509] [G loss: 1.410155]\n",
      "[D loss: 0.861772] [G loss: 1.428651]\n",
      "[D loss: 0.840852] [G loss: 1.384310]\n",
      "[D loss: 1.076735] [G loss: 1.484734]\n",
      "[D loss: 0.895605] [G loss: 1.482060]\n",
      "[D loss: 0.945242] [G loss: 1.491241]\n",
      "[D loss: 0.775514] [G loss: 1.556751]\n",
      "[D loss: 0.992258] [G loss: 1.259871]\n",
      "[D loss: 0.657112] [G loss: 1.426116]\n",
      "[D loss: 0.911905] [G loss: 1.368996]\n",
      "[D loss: 0.913932] [G loss: 1.680203]\n",
      "[D loss: 0.819186] [G loss: 1.583531]\n",
      "[D loss: 0.767837] [G loss: 1.392371]\n",
      "[D loss: 0.944534] [G loss: 1.329088]\n",
      "[D loss: 0.779408] [G loss: 1.346550]\n",
      "[D loss: 0.635719] [G loss: 1.636258]\n",
      "[D loss: 0.734544] [G loss: 1.511433]\n",
      "[D loss: 0.693022] [G loss: 1.523185]\n",
      "[D loss: 0.886284] [G loss: 1.508488]\n",
      "[D loss: 0.717390] [G loss: 1.600153]\n",
      "[D loss: 0.960008] [G loss: 1.147105]\n",
      "[D loss: 0.738150] [G loss: 1.696864]\n",
      "[D loss: 1.079111] [G loss: 1.623635]\n",
      "[D loss: 0.904359] [G loss: 1.393933]\n",
      "[D loss: 0.690500] [G loss: 1.383373]\n",
      "[D loss: 0.794666] [G loss: 1.357186]\n",
      "[D loss: 0.906031] [G loss: 1.651415]\n",
      "[D loss: 1.017403] [G loss: 1.280264]\n",
      "[D loss: 0.924314] [G loss: 1.517343]\n",
      "[D loss: 0.770079] [G loss: 1.513935]\n",
      "[D loss: 0.758464] [G loss: 1.275890]\n",
      "[D loss: 0.867957] [G loss: 1.192668]\n",
      "[D loss: 0.757656] [G loss: 1.439672]\n",
      "[D loss: 0.852409] [G loss: 1.579946]\n",
      "[D loss: 0.690850] [G loss: 1.607740]\n",
      "[D loss: 0.939356] [G loss: 1.530098]\n",
      "[D loss: 0.867182] [G loss: 1.380762]\n",
      "[D loss: 0.877441] [G loss: 1.448257]\n",
      "[D loss: 0.687520] [G loss: 1.380222]\n",
      "[D loss: 0.736967] [G loss: 1.600318]\n",
      "[D loss: 0.608631] [G loss: 1.636555]\n",
      "[D loss: 0.760732] [G loss: 1.452925]\n",
      "[D loss: 0.714200] [G loss: 1.697691]\n",
      "[D loss: 1.010486] [G loss: 1.623969]\n",
      "[D loss: 0.753212] [G loss: 1.716227]\n",
      "[D loss: 0.761516] [G loss: 1.386241]\n",
      "[D loss: 0.746059] [G loss: 1.491747]\n",
      "[D loss: 0.871551] [G loss: 1.308735]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.714916] [G loss: 1.441742]\n",
      "[D loss: 1.011629] [G loss: 1.484537]\n",
      "[D loss: 0.615514] [G loss: 1.470902]\n",
      "[D loss: 0.987391] [G loss: 1.451185]\n",
      "[D loss: 0.654448] [G loss: 1.691311]\n",
      "[D loss: 0.747303] [G loss: 1.480000]\n",
      "[D loss: 0.775701] [G loss: 1.588103]\n",
      "[D loss: 0.870195] [G loss: 1.573062]\n",
      "[D loss: 0.686877] [G loss: 1.622089]\n",
      "[D loss: 1.010562] [G loss: 1.352667]\n",
      "[D loss: 0.574307] [G loss: 1.521256]\n",
      "[D loss: 0.967449] [G loss: 1.420262]\n",
      "[D loss: 0.905238] [G loss: 1.364442]\n",
      "[D loss: 0.559623] [G loss: 1.566564]\n",
      "[D loss: 0.862095] [G loss: 1.444875]\n",
      "[D loss: 0.822283] [G loss: 1.578189]\n",
      "[D loss: 0.741371] [G loss: 1.513495]\n",
      "[D loss: 0.800721] [G loss: 1.411338]\n",
      "[D loss: 0.902871] [G loss: 1.577163]\n",
      "[D loss: 0.869085] [G loss: 1.627491]\n",
      "[D loss: 0.688617] [G loss: 1.633577]\n",
      "[D loss: 0.860239] [G loss: 1.478965]\n",
      "[D loss: 0.905522] [G loss: 1.106826]\n",
      "[D loss: 0.877332] [G loss: 1.553640]\n",
      "[D loss: 1.115826] [G loss: 1.316725]\n",
      "[D loss: 0.786423] [G loss: 1.488843]\n",
      "[D loss: 0.863233] [G loss: 1.268580]\n",
      "[D loss: 0.805267] [G loss: 1.478045]\n",
      "[D loss: 0.817348] [G loss: 1.348737]\n",
      "[D loss: 0.751984] [G loss: 1.529339]\n",
      "[D loss: 0.973396] [G loss: 1.340639]\n",
      "[D loss: 0.781178] [G loss: 1.471486]\n",
      "[D loss: 1.092282] [G loss: 1.464435]\n",
      "[D loss: 0.771842] [G loss: 1.697587]\n",
      "[D loss: 0.857012] [G loss: 1.294256]\n",
      "[D loss: 0.801075] [G loss: 1.399665]\n",
      "[D loss: 0.736425] [G loss: 1.403096]\n",
      "[D loss: 0.875762] [G loss: 1.331614]\n",
      "[D loss: 0.666087] [G loss: 1.613789]\n",
      "[D loss: 0.803523] [G loss: 1.525897]\n",
      "[D loss: 1.036013] [G loss: 1.600751]\n",
      "[D loss: 0.810684] [G loss: 1.809792]\n",
      "[D loss: 0.747887] [G loss: 1.466928]\n",
      "[D loss: 0.864325] [G loss: 1.267431]\n",
      "[D loss: 0.826531] [G loss: 1.395172]\n",
      "[D loss: 0.812237] [G loss: 1.508874]\n",
      "[D loss: 0.837174] [G loss: 1.232849]\n",
      "[D loss: 0.840423] [G loss: 1.539484]\n",
      "[D loss: 0.853835] [G loss: 1.300932]\n",
      "[D loss: 0.760373] [G loss: 1.648798]\n",
      "[D loss: 0.839771] [G loss: 1.504848]\n",
      "[D loss: 0.573446] [G loss: 1.622654]\n",
      "[D loss: 0.987397] [G loss: 1.454754]\n",
      "[D loss: 0.913945] [G loss: 1.584522]\n",
      "[D loss: 0.868250] [G loss: 1.386430]\n",
      "[D loss: 0.986595] [G loss: 1.408407]\n",
      "[D loss: 1.016050] [G loss: 1.392182]\n",
      "[D loss: 0.787268] [G loss: 1.523393]\n",
      "[D loss: 0.772506] [G loss: 1.589639]\n",
      "[D loss: 0.643474] [G loss: 1.466720]\n",
      "[D loss: 0.860635] [G loss: 1.303597]\n",
      "[D loss: 0.950040] [G loss: 1.523183]\n",
      "[D loss: 0.614901] [G loss: 1.410199]\n",
      "[D loss: 1.102583] [G loss: 1.614556]\n",
      "[D loss: 0.665032] [G loss: 1.409033]\n",
      "[D loss: 0.876417] [G loss: 1.372252]\n",
      "[D loss: 0.940133] [G loss: 1.403808]\n",
      "[D loss: 1.012053] [G loss: 1.515285]\n",
      "[D loss: 0.704427] [G loss: 1.629618]\n",
      "[D loss: 0.792384] [G loss: 1.370801]\n",
      "[D loss: 0.944876] [G loss: 1.622027]\n",
      "[D loss: 0.950016] [G loss: 1.209722]\n",
      "[D loss: 0.965040] [G loss: 1.314559]\n",
      "[D loss: 0.914227] [G loss: 1.465830]\n",
      "[D loss: 0.978338] [G loss: 1.232077]\n",
      "[D loss: 1.008139] [G loss: 1.445089]\n",
      "[D loss: 0.693783] [G loss: 1.428528]\n",
      "[D loss: 0.796412] [G loss: 1.367555]\n",
      "[D loss: 0.561949] [G loss: 1.721988]\n",
      "[D loss: 0.765005] [G loss: 1.291614]\n",
      "[D loss: 0.878401] [G loss: 1.553376]\n",
      "[D loss: 0.673019] [G loss: 1.640913]\n",
      "[D loss: 0.614673] [G loss: 1.701774]\n",
      "[D loss: 0.646288] [G loss: 1.505568]\n",
      "[D loss: 0.781324] [G loss: 1.482764]\n",
      "[D loss: 1.006706] [G loss: 1.431422]\n",
      "[D loss: 0.834761] [G loss: 1.495053]\n",
      "[D loss: 0.907383] [G loss: 1.733339]\n",
      "[D loss: 0.917366] [G loss: 1.309325]\n",
      "[D loss: 0.922285] [G loss: 1.439419]\n",
      "[D loss: 0.837718] [G loss: 1.324762]\n",
      "[D loss: 0.774662] [G loss: 1.253330]\n",
      "[D loss: 0.702133] [G loss: 1.675826]\n",
      "[D loss: 0.725904] [G loss: 1.590463]\n",
      "[D loss: 0.771842] [G loss: 1.597003]\n",
      "[D loss: 0.807565] [G loss: 1.462153]\n",
      "[D loss: 0.724385] [G loss: 1.436053]\n",
      "[D loss: 1.017531] [G loss: 1.302109]\n",
      "[D loss: 0.676366] [G loss: 1.690093]\n",
      "[D loss: 0.863876] [G loss: 1.542254]\n",
      "[D loss: 0.833687] [G loss: 1.515625]\n",
      "[D loss: 0.795399] [G loss: 1.335885]\n",
      "[D loss: 0.614563] [G loss: 1.567129]\n",
      "[D loss: 0.921756] [G loss: 1.459335]\n",
      "[D loss: 0.829547] [G loss: 1.759628]\n",
      "[D loss: 0.813273] [G loss: 1.573817]\n",
      "[D loss: 0.942692] [G loss: 1.240370]\n",
      "[D loss: 1.165568] [G loss: 1.364636]\n",
      "[D loss: 0.698544] [G loss: 1.581160]\n",
      "[D loss: 0.844665] [G loss: 1.385620]\n",
      "[D loss: 0.708016] [G loss: 1.629604]\n",
      "[D loss: 0.822384] [G loss: 1.506093]\n",
      "[D loss: 0.794446] [G loss: 1.760388]\n",
      "[D loss: 0.820408] [G loss: 1.463529]\n",
      "[D loss: 0.902502] [G loss: 1.329049]\n",
      "[D loss: 1.083750] [G loss: 1.387240]\n",
      "[D loss: 0.956350] [G loss: 1.220026]\n",
      "[D loss: 0.968036] [G loss: 1.355374]\n",
      "[D loss: 0.862156] [G loss: 1.437589]\n",
      "[D loss: 1.289439] [G loss: 1.274670]\n",
      "[D loss: 0.986208] [G loss: 1.305140]\n",
      "[D loss: 0.796712] [G loss: 1.362419]\n",
      "[D loss: 0.755779] [G loss: 1.282713]\n",
      "[D loss: 0.854681] [G loss: 1.345952]\n",
      "[D loss: 1.060460] [G loss: 1.374786]\n",
      "[D loss: 0.648656] [G loss: 1.704325]\n",
      "[D loss: 0.903090] [G loss: 1.367772]\n",
      "[D loss: 0.962033] [G loss: 1.322801]\n",
      "[D loss: 0.781546] [G loss: 1.263474]\n",
      "[D loss: 0.978058] [G loss: 1.303416]\n",
      "[D loss: 1.031407] [G loss: 1.098774]\n",
      "[D loss: 0.784144] [G loss: 1.438279]\n",
      "[D loss: 0.909959] [G loss: 1.246119]\n",
      "[D loss: 0.902178] [G loss: 1.415813]\n",
      "[D loss: 0.849558] [G loss: 1.470643]\n",
      "[D loss: 0.877536] [G loss: 1.277387]\n",
      "[D loss: 0.887773] [G loss: 1.403287]\n",
      "[D loss: 0.842405] [G loss: 1.374859]\n",
      "[D loss: 0.768999] [G loss: 1.318242]\n",
      "[D loss: 0.869915] [G loss: 1.335565]\n",
      "[D loss: 0.781250] [G loss: 1.399791]\n",
      "[D loss: 0.773841] [G loss: 1.305588]\n",
      "[D loss: 0.682662] [G loss: 1.335883]\n",
      "[D loss: 0.666456] [G loss: 1.241949]\n",
      "[D loss: 0.735923] [G loss: 1.703578]\n",
      "[D loss: 1.005860] [G loss: 1.512689]\n",
      "[D loss: 0.733412] [G loss: 1.448530]\n",
      "[D loss: 0.914086] [G loss: 1.544042]\n",
      "[D loss: 0.857264] [G loss: 1.531147]\n",
      "[D loss: 1.032607] [G loss: 1.537776]\n",
      "[D loss: 0.771030] [G loss: 1.533972]\n",
      "[D loss: 1.030967] [G loss: 1.433080]\n",
      "[D loss: 0.932905] [G loss: 1.759533]\n",
      "[D loss: 0.811183] [G loss: 1.476045]\n",
      "[D loss: 0.889189] [G loss: 1.321849]\n",
      "[D loss: 0.684913] [G loss: 1.414641]\n",
      "[D loss: 0.843437] [G loss: 1.155221]\n",
      "[D loss: 0.727348] [G loss: 1.466457]\n",
      "[D loss: 0.984798] [G loss: 1.550496]\n",
      "[D loss: 0.814524] [G loss: 1.626285]\n",
      "[D loss: 1.048072] [G loss: 1.521138]\n",
      "[D loss: 0.909126] [G loss: 1.237889]\n",
      "[D loss: 0.880779] [G loss: 1.404172]\n",
      "[D loss: 0.905702] [G loss: 1.368543]\n",
      "[D loss: 1.028939] [G loss: 1.383500]\n",
      "[D loss: 0.866376] [G loss: 1.457256]\n",
      "[D loss: 0.874577] [G loss: 1.338899]\n",
      "[D loss: 0.691761] [G loss: 1.525440]\n",
      "[D loss: 0.806254] [G loss: 1.722533]\n",
      "[D loss: 0.925554] [G loss: 1.404573]\n",
      "[D loss: 1.052922] [G loss: 1.613367]\n",
      "[D loss: 0.717905] [G loss: 1.458983]\n",
      "[D loss: 1.117683] [G loss: 1.277920]\n",
      "[D loss: 0.838830] [G loss: 1.410623]\n",
      "[D loss: 0.879156] [G loss: 1.291224]\n",
      "[D loss: 0.918295] [G loss: 1.293635]\n",
      "[D loss: 0.793436] [G loss: 1.421808]\n",
      "[D loss: 0.952001] [G loss: 1.336475]\n",
      "[D loss: 0.856180] [G loss: 1.372923]\n",
      "[D loss: 0.763943] [G loss: 1.449075]\n",
      "[D loss: 0.740238] [G loss: 1.424775]\n",
      "[D loss: 0.982573] [G loss: 1.633835]\n",
      "[D loss: 0.642310] [G loss: 1.408483]\n",
      "[D loss: 0.987404] [G loss: 1.412496]\n",
      "[D loss: 0.933555] [G loss: 1.541844]\n",
      "[D loss: 0.659594] [G loss: 1.438558]\n",
      "[D loss: 0.677760] [G loss: 1.534962]\n",
      "[D loss: 0.594414] [G loss: 1.562955]\n",
      "[D loss: 0.841259] [G loss: 1.409347]\n",
      "[D loss: 0.732229] [G loss: 1.613452]\n",
      "[D loss: 0.847593] [G loss: 1.545593]\n",
      "[D loss: 0.653377] [G loss: 1.636959]\n",
      "[D loss: 0.989185] [G loss: 1.447574]\n",
      "[D loss: 1.213810] [G loss: 1.463746]\n",
      "[D loss: 0.680125] [G loss: 1.469197]\n",
      "[D loss: 0.773570] [G loss: 1.398615]\n",
      "[D loss: 0.825289] [G loss: 1.439100]\n",
      "[D loss: 0.675975] [G loss: 1.470903]\n",
      "[D loss: 0.936472] [G loss: 1.353658]\n",
      "[D loss: 0.695041] [G loss: 1.539085]\n",
      "[D loss: 0.843219] [G loss: 1.545160]\n",
      "[D loss: 0.849294] [G loss: 1.332933]\n",
      "[D loss: 0.814255] [G loss: 1.620742]\n",
      "[D loss: 0.955669] [G loss: 1.347706]\n",
      "[D loss: 0.796063] [G loss: 1.332617]\n",
      "[D loss: 0.822990] [G loss: 1.348953]\n",
      "[D loss: 1.104544] [G loss: 1.187850]\n",
      "[D loss: 1.098437] [G loss: 1.379347]\n",
      "[D loss: 0.748867] [G loss: 1.596668]\n",
      "[D loss: 0.985265] [G loss: 1.417020]\n",
      "[D loss: 0.870066] [G loss: 1.789658]\n",
      "[D loss: 1.088827] [G loss: 1.338908]\n",
      "[D loss: 0.815228] [G loss: 1.316493]\n",
      "[D loss: 0.668854] [G loss: 1.336769]\n",
      "[D loss: 0.881019] [G loss: 1.247923]\n",
      "[D loss: 0.829208] [G loss: 1.250140]\n",
      "[D loss: 0.642875] [G loss: 1.367590]\n",
      "[D loss: 0.915660] [G loss: 1.311078]\n",
      "[D loss: 0.704731] [G loss: 1.387653]\n",
      "[D loss: 0.535007] [G loss: 1.426327]\n",
      "[D loss: 0.634673] [G loss: 1.439083]\n",
      "[D loss: 0.675843] [G loss: 1.324664]\n",
      "[D loss: 0.731870] [G loss: 1.530666]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.761095] [G loss: 1.532577]\n",
      "[D loss: 1.138255] [G loss: 1.471791]\n",
      "[D loss: 1.008209] [G loss: 1.558010]\n",
      "[D loss: 1.086671] [G loss: 1.313008]\n",
      "[D loss: 0.792307] [G loss: 1.487455]\n",
      "[D loss: 0.715759] [G loss: 1.504110]\n",
      "[D loss: 0.899570] [G loss: 1.408837]\n",
      "[D loss: 0.887478] [G loss: 1.310352]\n",
      "[D loss: 0.650883] [G loss: 1.399582]\n",
      "[D loss: 0.843813] [G loss: 1.312889]\n",
      "[D loss: 0.751545] [G loss: 1.273354]\n",
      "[D loss: 0.559627] [G loss: 1.542874]\n",
      "[D loss: 1.060550] [G loss: 1.624321]\n",
      "[D loss: 0.922653] [G loss: 1.577412]\n",
      "[D loss: 0.785348] [G loss: 1.544021]\n",
      "[D loss: 0.851855] [G loss: 1.391765]\n",
      "[D loss: 0.899581] [G loss: 1.292501]\n",
      "[D loss: 0.909759] [G loss: 1.312611]\n",
      "[D loss: 0.773651] [G loss: 1.489036]\n",
      "[D loss: 0.758735] [G loss: 1.480036]\n",
      "[D loss: 0.846650] [G loss: 1.370566]\n",
      "[D loss: 0.704696] [G loss: 1.409241]\n",
      "[D loss: 0.858741] [G loss: 1.497867]\n",
      "[D loss: 0.834315] [G loss: 1.784707]\n",
      "[D loss: 0.717641] [G loss: 1.568289]\n",
      "[D loss: 0.783907] [G loss: 1.309034]\n",
      "[D loss: 0.696527] [G loss: 1.493654]\n",
      "[D loss: 0.846789] [G loss: 1.315712]\n",
      "[D loss: 0.906374] [G loss: 1.502665]\n",
      "[D loss: 0.772393] [G loss: 1.454950]\n",
      "[D loss: 0.744614] [G loss: 1.524789]\n",
      "[D loss: 1.063862] [G loss: 1.286250]\n",
      "[D loss: 0.765051] [G loss: 1.392680]\n",
      "[D loss: 0.816618] [G loss: 1.597642]\n",
      "[D loss: 0.689553] [G loss: 1.288489]\n",
      "[D loss: 0.531777] [G loss: 1.543506]\n",
      "[D loss: 0.683854] [G loss: 1.523647]\n",
      "[D loss: 0.753014] [G loss: 1.623273]\n",
      "[D loss: 0.883404] [G loss: 1.438196]\n",
      "[D loss: 1.289336] [G loss: 1.401829]\n",
      "[D loss: 0.662036] [G loss: 1.419019]\n",
      "[D loss: 0.803719] [G loss: 1.534914]\n",
      "[D loss: 0.916892] [G loss: 1.411028]\n",
      "[D loss: 0.847195] [G loss: 1.450960]\n",
      "[D loss: 0.694951] [G loss: 1.782924]\n",
      "[D loss: 0.675957] [G loss: 1.690497]\n",
      "[D loss: 0.982034] [G loss: 1.781855]\n",
      "[D loss: 0.993088] [G loss: 1.413547]\n",
      "[D loss: 0.780650] [G loss: 1.609068]\n",
      "[D loss: 0.860073] [G loss: 1.302070]\n",
      "[D loss: 1.103468] [G loss: 1.409608]\n",
      "[D loss: 0.875130] [G loss: 1.558169]\n",
      "[D loss: 0.856457] [G loss: 1.593505]\n",
      "[D loss: 0.791172] [G loss: 1.289635]\n",
      "[D loss: 0.904998] [G loss: 1.383538]\n",
      "[D loss: 0.734260] [G loss: 1.602453]\n",
      "[D loss: 0.972413] [G loss: 1.623410]\n",
      "[D loss: 0.679846] [G loss: 1.513764]\n",
      "[D loss: 0.851172] [G loss: 1.685102]\n",
      "[D loss: 0.849560] [G loss: 1.395598]\n",
      "[D loss: 1.033659] [G loss: 1.366997]\n",
      "[D loss: 0.844572] [G loss: 1.475048]\n",
      "[D loss: 0.818524] [G loss: 1.494347]\n",
      "[D loss: 0.957775] [G loss: 1.555713]\n",
      "[D loss: 0.925464] [G loss: 1.359350]\n",
      "[D loss: 0.815299] [G loss: 1.261846]\n",
      "[D loss: 0.659758] [G loss: 1.411500]\n",
      "[D loss: 0.877496] [G loss: 1.513650]\n",
      "[D loss: 0.809044] [G loss: 1.666218]\n",
      "[D loss: 0.859289] [G loss: 1.431411]\n",
      "[D loss: 0.824527] [G loss: 1.410901]\n",
      "[D loss: 0.947360] [G loss: 1.661190]\n",
      "[D loss: 0.794247] [G loss: 1.508416]\n",
      "[D loss: 0.832142] [G loss: 1.613388]\n",
      "[D loss: 0.867546] [G loss: 1.385601]\n",
      "[D loss: 0.872852] [G loss: 1.507828]\n",
      "[D loss: 0.821087] [G loss: 1.536103]\n",
      "[D loss: 0.778743] [G loss: 1.606561]\n",
      "[D loss: 0.918897] [G loss: 1.589332]\n",
      "[D loss: 0.902762] [G loss: 1.403157]\n",
      "[D loss: 0.864337] [G loss: 1.209595]\n",
      "[D loss: 0.972437] [G loss: 1.571706]\n",
      "[D loss: 0.843826] [G loss: 1.323136]\n",
      "[D loss: 0.799541] [G loss: 1.545055]\n",
      "[D loss: 0.981699] [G loss: 1.409514]\n",
      "[D loss: 0.798545] [G loss: 1.625708]\n",
      "[D loss: 0.931595] [G loss: 1.279471]\n",
      "[D loss: 0.755828] [G loss: 1.662422]\n",
      "[D loss: 0.605757] [G loss: 1.545675]\n",
      "[D loss: 0.902014] [G loss: 1.351188]\n",
      "[D loss: 0.958993] [G loss: 1.272558]\n",
      "[D loss: 1.059357] [G loss: 1.294809]\n",
      "[D loss: 0.759561] [G loss: 1.406772]\n",
      "[D loss: 0.939855] [G loss: 1.235360]\n",
      "[D loss: 0.648135] [G loss: 1.337089]\n",
      "[D loss: 0.775382] [G loss: 1.405514]\n",
      "[D loss: 0.923470] [G loss: 1.394496]\n",
      "[D loss: 0.985701] [G loss: 1.319384]\n",
      "[D loss: 0.801568] [G loss: 1.611803]\n",
      "[D loss: 0.807111] [G loss: 1.271361]\n",
      "[D loss: 0.828370] [G loss: 1.273416]\n",
      "[D loss: 0.994805] [G loss: 1.186157]\n",
      "[D loss: 0.846923] [G loss: 1.340073]\n",
      "[D loss: 0.889109] [G loss: 1.527477]\n",
      "[D loss: 0.786990] [G loss: 1.698846]\n",
      "[D loss: 0.893010] [G loss: 1.342436]\n",
      "[D loss: 0.967290] [G loss: 1.256548]\n",
      "[D loss: 0.916510] [G loss: 1.391375]\n",
      "[D loss: 0.607569] [G loss: 1.393162]\n",
      "[D loss: 0.913921] [G loss: 1.275854]\n",
      "[D loss: 0.872756] [G loss: 1.551715]\n",
      "[D loss: 0.914047] [G loss: 1.382756]\n",
      "[D loss: 0.740382] [G loss: 1.591067]\n",
      "[D loss: 0.776811] [G loss: 1.485317]\n",
      "[D loss: 0.693121] [G loss: 1.429302]\n",
      "[D loss: 0.732799] [G loss: 1.385576]\n",
      "[D loss: 1.005105] [G loss: 1.178202]\n",
      "[D loss: 0.648776] [G loss: 1.476358]\n",
      "[D loss: 0.695633] [G loss: 1.576783]\n",
      "[D loss: 0.995136] [G loss: 1.434826]\n",
      "[D loss: 0.786385] [G loss: 1.605172]\n",
      "[D loss: 0.866664] [G loss: 1.562164]\n",
      "[D loss: 0.891677] [G loss: 1.342000]\n",
      "[D loss: 0.735204] [G loss: 1.270188]\n",
      "[D loss: 0.720210] [G loss: 1.336819]\n",
      "[D loss: 0.923411] [G loss: 1.301079]\n",
      "[D loss: 0.870448] [G loss: 1.639737]\n",
      "[D loss: 0.791739] [G loss: 1.441829]\n",
      "[D loss: 0.805241] [G loss: 1.672138]\n",
      "[D loss: 0.957717] [G loss: 1.438917]\n",
      "[D loss: 0.762656] [G loss: 1.417583]\n",
      "[D loss: 0.907044] [G loss: 1.400126]\n",
      "[D loss: 0.904046] [G loss: 1.429062]\n",
      "[D loss: 0.666264] [G loss: 1.451900]\n",
      "[D loss: 0.894011] [G loss: 1.272959]\n",
      "[D loss: 0.763647] [G loss: 1.494519]\n",
      "[D loss: 0.892245] [G loss: 1.380249]\n",
      "[D loss: 0.956303] [G loss: 1.334524]\n",
      "[D loss: 0.977521] [G loss: 1.362402]\n",
      "[D loss: 0.985972] [G loss: 1.443988]\n",
      "[D loss: 0.606174] [G loss: 1.547157]\n",
      "[D loss: 0.660408] [G loss: 1.412125]\n",
      "[D loss: 0.831625] [G loss: 1.184443]\n",
      "[D loss: 0.767074] [G loss: 1.507384]\n",
      "[D loss: 0.773029] [G loss: 1.468112]\n",
      "[D loss: 0.852502] [G loss: 1.232836]\n",
      "[D loss: 0.865330] [G loss: 1.420012]\n",
      "[D loss: 0.873391] [G loss: 1.380296]\n",
      "[D loss: 0.860242] [G loss: 1.450434]\n",
      "[D loss: 0.941858] [G loss: 1.445094]\n",
      "[D loss: 0.839162] [G loss: 1.289543]\n",
      "[D loss: 0.978298] [G loss: 1.333463]\n",
      "[D loss: 0.777482] [G loss: 1.339000]\n",
      "[D loss: 0.801114] [G loss: 1.433918]\n",
      "[D loss: 0.920762] [G loss: 1.366121]\n",
      "[D loss: 0.945918] [G loss: 1.422241]\n",
      "[D loss: 0.755141] [G loss: 1.410709]\n",
      "[D loss: 0.873186] [G loss: 1.412405]\n",
      "[D loss: 0.865989] [G loss: 1.482682]\n",
      "[D loss: 0.808776] [G loss: 1.289371]\n",
      "[D loss: 0.820143] [G loss: 1.300511]\n",
      "[D loss: 1.004605] [G loss: 1.340042]\n",
      "[D loss: 0.737341] [G loss: 1.493963]\n",
      "[D loss: 0.854928] [G loss: 1.604593]\n",
      "[D loss: 0.988507] [G loss: 1.424121]\n",
      "[D loss: 0.711540] [G loss: 1.751352]\n",
      "[D loss: 0.809035] [G loss: 1.484968]\n",
      "[D loss: 0.672821] [G loss: 1.448343]\n",
      "[D loss: 0.951910] [G loss: 1.261308]\n",
      "[D loss: 0.767505] [G loss: 1.404446]\n",
      "[D loss: 0.696026] [G loss: 1.450667]\n",
      "[D loss: 0.999877] [G loss: 1.246772]\n",
      "[D loss: 0.898571] [G loss: 1.361423]\n",
      "[D loss: 0.855120] [G loss: 1.306039]\n",
      "[D loss: 1.004375] [G loss: 1.192822]\n",
      "[D loss: 0.976009] [G loss: 1.196321]\n",
      "[D loss: 0.873847] [G loss: 1.353049]\n",
      "[D loss: 0.882732] [G loss: 1.457900]\n",
      "[D loss: 0.638130] [G loss: 1.549083]\n",
      "[D loss: 0.706173] [G loss: 1.530399]\n",
      "[D loss: 0.938688] [G loss: 1.484621]\n",
      "[D loss: 0.839585] [G loss: 1.271920]\n",
      "[D loss: 0.950423] [G loss: 1.376220]\n",
      "[D loss: 0.751647] [G loss: 1.574701]\n",
      "[D loss: 0.778599] [G loss: 1.476830]\n",
      "[D loss: 0.943919] [G loss: 1.400721]\n",
      "[D loss: 0.756473] [G loss: 1.384071]\n",
      "[D loss: 0.822630] [G loss: 1.296519]\n",
      "[D loss: 0.640013] [G loss: 1.455568]\n",
      "[D loss: 0.953121] [G loss: 1.422556]\n",
      "[D loss: 0.910818] [G loss: 1.277578]\n",
      "[D loss: 0.835246] [G loss: 1.472458]\n",
      "[D loss: 0.758052] [G loss: 1.405337]\n",
      "[D loss: 0.917968] [G loss: 1.497375]\n",
      "[D loss: 0.845323] [G loss: 1.439819]\n",
      "[D loss: 1.000384] [G loss: 1.604428]\n",
      "[D loss: 0.737250] [G loss: 1.451457]\n",
      "[D loss: 0.724120] [G loss: 1.407552]\n",
      "[D loss: 0.743721] [G loss: 1.437170]\n",
      "[D loss: 0.912302] [G loss: 1.402059]\n",
      "[D loss: 0.859824] [G loss: 1.431499]\n",
      "[D loss: 0.887448] [G loss: 1.480277]\n",
      "[D loss: 0.852471] [G loss: 1.467943]\n",
      "[D loss: 0.692544] [G loss: 1.521729]\n",
      "[D loss: 0.871466] [G loss: 1.483898]\n",
      "[D loss: 0.907124] [G loss: 1.555527]\n",
      "[D loss: 1.012110] [G loss: 1.550538]\n",
      "[D loss: 0.860245] [G loss: 1.407416]\n",
      "[D loss: 1.054118] [G loss: 1.442052]\n",
      "[D loss: 0.846123] [G loss: 1.517616]\n",
      "[D loss: 0.726952] [G loss: 1.358476]\n",
      "[D loss: 1.034029] [G loss: 1.490648]\n",
      "[D loss: 0.992115] [G loss: 1.402286]\n",
      "[D loss: 0.928219] [G loss: 1.388101]\n",
      "[D loss: 0.833290] [G loss: 1.187481]\n",
      "[D loss: 0.787933] [G loss: 1.519142]\n",
      "[D loss: 0.756233] [G loss: 1.407036]\n",
      "[D loss: 0.817608] [G loss: 1.469439]\n",
      "[D loss: 1.006572] [G loss: 1.179737]\n",
      "[D loss: 0.716400] [G loss: 1.371471]\n",
      "[D loss: 0.902266] [G loss: 1.579228]\n",
      "[D loss: 0.915114] [G loss: 1.562534]\n",
      "[D loss: 1.028404] [G loss: 1.442129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.979731] [G loss: 1.269914]\n",
      "[D loss: 0.854773] [G loss: 1.268115]\n",
      "[D loss: 0.797462] [G loss: 1.319945]\n",
      "[D loss: 0.827777] [G loss: 1.269464]\n",
      "[D loss: 0.839339] [G loss: 1.395129]\n",
      "[D loss: 0.754059] [G loss: 1.362253]\n",
      "[D loss: 0.747772] [G loss: 1.561463]\n",
      "[D loss: 0.802215] [G loss: 1.426378]\n",
      "[D loss: 0.897279] [G loss: 1.456946]\n",
      "[D loss: 0.796567] [G loss: 1.233630]\n",
      "[D loss: 0.813241] [G loss: 1.370576]\n",
      "[D loss: 0.798308] [G loss: 1.277002]\n",
      "[D loss: 0.616012] [G loss: 1.518901]\n",
      "[D loss: 0.854336] [G loss: 1.529901]\n",
      "[D loss: 0.833525] [G loss: 1.470192]\n",
      "[D loss: 0.990228] [G loss: 1.392621]\n",
      "[D loss: 0.863668] [G loss: 1.399546]\n",
      "[D loss: 0.731825] [G loss: 1.492632]\n",
      "[D loss: 0.972539] [G loss: 1.519117]\n",
      "[D loss: 0.816623] [G loss: 1.538703]\n",
      "[D loss: 0.881531] [G loss: 1.387584]\n",
      "[D loss: 0.712885] [G loss: 1.453767]\n",
      "[D loss: 0.852810] [G loss: 1.630060]\n",
      "[D loss: 0.890062] [G loss: 1.373448]\n",
      "[D loss: 1.037751] [G loss: 1.344511]\n",
      "[D loss: 0.743778] [G loss: 1.525210]\n",
      "[D loss: 0.814208] [G loss: 1.547529]\n",
      "[D loss: 0.940266] [G loss: 1.366829]\n",
      "[D loss: 0.960212] [G loss: 1.491104]\n",
      "[D loss: 0.899465] [G loss: 1.646334]\n",
      "[D loss: 0.891948] [G loss: 1.347884]\n",
      "[D loss: 0.880125] [G loss: 1.456101]\n",
      "[D loss: 0.915953] [G loss: 1.490449]\n",
      "[D loss: 0.777076] [G loss: 1.350946]\n",
      "[D loss: 0.858542] [G loss: 1.183860]\n",
      "[D loss: 0.840495] [G loss: 1.285209]\n",
      "[D loss: 0.732540] [G loss: 1.577078]\n",
      "[D loss: 0.891164] [G loss: 1.432505]\n",
      "[D loss: 0.944090] [G loss: 1.546878]\n",
      "[D loss: 0.852388] [G loss: 1.501795]\n",
      "[D loss: 0.858624] [G loss: 1.605620]\n",
      "[D loss: 0.680543] [G loss: 1.316729]\n",
      "[D loss: 1.027798] [G loss: 1.304892]\n",
      "[D loss: 0.661324] [G loss: 1.686950]\n",
      "[D loss: 0.950083] [G loss: 1.425831]\n",
      "[D loss: 0.814262] [G loss: 1.345987]\n",
      "[D loss: 0.748294] [G loss: 1.541581]\n",
      "[D loss: 0.766752] [G loss: 1.464746]\n",
      "[D loss: 0.968136] [G loss: 1.244229]\n",
      "[D loss: 0.886661] [G loss: 1.171480]\n",
      "[D loss: 0.788097] [G loss: 1.543625]\n",
      "[D loss: 1.003761] [G loss: 1.694561]\n",
      "[D loss: 0.749809] [G loss: 1.542704]\n",
      "[D loss: 0.924079] [G loss: 1.382688]\n",
      "[D loss: 0.927504] [G loss: 1.363264]\n",
      "[D loss: 1.094341] [G loss: 1.405603]\n",
      "[D loss: 0.823842] [G loss: 1.473615]\n",
      "[D loss: 0.683217] [G loss: 1.407667]\n",
      "[D loss: 0.609213] [G loss: 1.538755]\n",
      "[D loss: 1.030642] [G loss: 1.319496]\n",
      "[D loss: 0.892654] [G loss: 1.393522]\n",
      "[D loss: 1.036896] [G loss: 1.379549]\n",
      "[D loss: 1.017694] [G loss: 1.315293]\n",
      "[D loss: 0.787926] [G loss: 1.304023]\n",
      "[D loss: 0.755503] [G loss: 1.364869]\n",
      "[D loss: 0.932541] [G loss: 1.344254]\n",
      "[D loss: 0.873788] [G loss: 1.506764]\n",
      "[D loss: 0.764402] [G loss: 1.590622]\n",
      "[D loss: 0.886206] [G loss: 1.298014]\n",
      "[D loss: 1.000833] [G loss: 1.188200]\n",
      "[D loss: 0.769463] [G loss: 1.524589]\n",
      "[D loss: 0.872428] [G loss: 1.428456]\n",
      "[D loss: 0.832516] [G loss: 1.244425]\n",
      "[D loss: 0.512651] [G loss: 1.437730]\n",
      "[D loss: 0.729287] [G loss: 1.531631]\n",
      "[D loss: 0.916568] [G loss: 1.423704]\n",
      "[D loss: 0.983375] [G loss: 1.159753]\n",
      "[D loss: 0.947374] [G loss: 1.353192]\n",
      "[D loss: 0.864862] [G loss: 1.557436]\n",
      "[D loss: 1.012071] [G loss: 1.439775]\n",
      "[D loss: 0.734151] [G loss: 1.363740]\n",
      "[D loss: 0.714119] [G loss: 1.523214]\n",
      "[D loss: 1.006424] [G loss: 1.309936]\n",
      "[D loss: 0.629925] [G loss: 1.546690]\n",
      "[D loss: 1.112703] [G loss: 1.164324]\n",
      "[D loss: 0.890464] [G loss: 1.535931]\n",
      "[D loss: 0.907991] [G loss: 1.410948]\n",
      "[D loss: 0.741035] [G loss: 1.423228]\n",
      "[D loss: 0.974822] [G loss: 1.504313]\n",
      "[D loss: 1.029491] [G loss: 1.421447]\n",
      "[D loss: 0.719940] [G loss: 1.431751]\n",
      "[D loss: 1.067308] [G loss: 1.329839]\n",
      "[D loss: 0.825088] [G loss: 1.368577]\n",
      "[D loss: 0.999519] [G loss: 1.354260]\n",
      "[D loss: 0.845624] [G loss: 1.440766]\n",
      "[D loss: 0.716277] [G loss: 1.601181]\n",
      "[D loss: 0.814294] [G loss: 1.340226]\n",
      "[D loss: 1.067390] [G loss: 1.476359]\n",
      "[D loss: 0.786175] [G loss: 1.220491]\n",
      "[D loss: 0.879676] [G loss: 1.386966]\n",
      "[D loss: 0.933821] [G loss: 1.285622]\n",
      "[D loss: 0.879772] [G loss: 1.566245]\n",
      "[D loss: 0.732597] [G loss: 1.614870]\n",
      "[D loss: 0.821505] [G loss: 1.362180]\n",
      "[D loss: 0.823842] [G loss: 1.382025]\n",
      "[D loss: 0.724219] [G loss: 1.428152]\n",
      "[D loss: 0.946666] [G loss: 1.509378]\n",
      "[D loss: 0.770396] [G loss: 1.475350]\n",
      "[D loss: 1.060719] [G loss: 1.405250]\n",
      "[D loss: 0.830823] [G loss: 1.261798]\n",
      "[D loss: 0.813875] [G loss: 1.315441]\n",
      "[D loss: 1.124808] [G loss: 1.321725]\n",
      "epoch:11, g_loss:2752.06884765625,d_loss:1556.654296875\n",
      "[D loss: 0.842008] [G loss: 1.503453]\n",
      "[D loss: 0.979717] [G loss: 1.302757]\n",
      "[D loss: 1.078732] [G loss: 1.392681]\n",
      "[D loss: 0.824242] [G loss: 1.592384]\n",
      "[D loss: 0.652129] [G loss: 1.500354]\n",
      "[D loss: 1.188657] [G loss: 1.366755]\n",
      "[D loss: 0.755810] [G loss: 1.447749]\n",
      "[D loss: 0.773017] [G loss: 1.737847]\n",
      "[D loss: 0.905075] [G loss: 1.458909]\n",
      "[D loss: 0.649534] [G loss: 1.316715]\n",
      "[D loss: 0.680425] [G loss: 1.431775]\n",
      "[D loss: 0.989200] [G loss: 1.365310]\n",
      "[D loss: 0.819644] [G loss: 1.369537]\n",
      "[D loss: 0.756448] [G loss: 1.472019]\n",
      "[D loss: 0.870883] [G loss: 1.417502]\n",
      "[D loss: 0.713396] [G loss: 1.404028]\n",
      "[D loss: 0.775686] [G loss: 1.447491]\n",
      "[D loss: 0.881434] [G loss: 1.303741]\n",
      "[D loss: 0.738723] [G loss: 1.653125]\n",
      "[D loss: 0.814589] [G loss: 1.411920]\n",
      "[D loss: 0.756862] [G loss: 1.449286]\n",
      "[D loss: 0.772091] [G loss: 1.598663]\n",
      "[D loss: 1.220085] [G loss: 1.351876]\n",
      "[D loss: 0.914667] [G loss: 1.402932]\n",
      "[D loss: 0.791548] [G loss: 1.368836]\n",
      "[D loss: 0.727103] [G loss: 1.528868]\n",
      "[D loss: 0.993114] [G loss: 1.258371]\n",
      "[D loss: 0.998047] [G loss: 1.416718]\n",
      "[D loss: 0.791579] [G loss: 1.319205]\n",
      "[D loss: 0.775039] [G loss: 1.519917]\n",
      "[D loss: 0.998690] [G loss: 1.439210]\n",
      "[D loss: 0.782608] [G loss: 1.558934]\n",
      "[D loss: 0.668569] [G loss: 1.434204]\n",
      "[D loss: 0.855357] [G loss: 1.493189]\n",
      "[D loss: 0.922164] [G loss: 1.274534]\n",
      "[D loss: 0.769944] [G loss: 1.472406]\n",
      "[D loss: 0.713237] [G loss: 1.524001]\n",
      "[D loss: 0.987009] [G loss: 1.540620]\n",
      "[D loss: 0.819582] [G loss: 1.570373]\n",
      "[D loss: 0.956808] [G loss: 1.422720]\n",
      "[D loss: 0.865662] [G loss: 1.249556]\n",
      "[D loss: 0.754324] [G loss: 1.111256]\n",
      "[D loss: 0.876182] [G loss: 1.479068]\n",
      "[D loss: 0.666513] [G loss: 1.299538]\n",
      "[D loss: 0.626261] [G loss: 1.372123]\n",
      "[D loss: 0.923508] [G loss: 1.450296]\n",
      "[D loss: 0.824300] [G loss: 1.533456]\n",
      "[D loss: 0.907822] [G loss: 1.386249]\n",
      "[D loss: 0.851169] [G loss: 1.292651]\n",
      "[D loss: 1.059998] [G loss: 1.475051]\n",
      "[D loss: 0.935585] [G loss: 1.402521]\n",
      "[D loss: 0.707491] [G loss: 1.403758]\n",
      "[D loss: 0.707306] [G loss: 1.279388]\n",
      "[D loss: 0.722637] [G loss: 1.340094]\n",
      "[D loss: 1.017675] [G loss: 1.235271]\n",
      "[D loss: 0.878064] [G loss: 1.267539]\n",
      "[D loss: 0.835604] [G loss: 1.358734]\n",
      "[D loss: 1.112855] [G loss: 1.390426]\n",
      "[D loss: 0.883447] [G loss: 1.519308]\n",
      "[D loss: 0.878211] [G loss: 1.243641]\n",
      "[D loss: 0.811286] [G loss: 1.408083]\n",
      "[D loss: 0.747162] [G loss: 1.504236]\n",
      "[D loss: 0.895197] [G loss: 1.416699]\n",
      "[D loss: 0.870358] [G loss: 1.258007]\n",
      "[D loss: 0.896241] [G loss: 1.446302]\n",
      "[D loss: 0.875013] [G loss: 1.339956]\n",
      "[D loss: 0.783448] [G loss: 1.501203]\n",
      "[D loss: 0.825286] [G loss: 1.395547]\n",
      "[D loss: 0.807781] [G loss: 1.463093]\n",
      "[D loss: 0.959172] [G loss: 1.434329]\n",
      "[D loss: 0.942221] [G loss: 1.271567]\n",
      "[D loss: 0.858282] [G loss: 1.612623]\n",
      "[D loss: 1.016972] [G loss: 1.628676]\n",
      "[D loss: 0.715773] [G loss: 1.684751]\n",
      "[D loss: 0.762855] [G loss: 1.299196]\n",
      "[D loss: 1.039091] [G loss: 1.182644]\n",
      "[D loss: 0.805500] [G loss: 1.359249]\n",
      "[D loss: 0.851764] [G loss: 1.352312]\n",
      "[D loss: 0.885192] [G loss: 1.623580]\n",
      "[D loss: 0.834366] [G loss: 1.294510]\n",
      "[D loss: 0.738738] [G loss: 1.441458]\n",
      "[D loss: 0.749791] [G loss: 1.256881]\n",
      "[D loss: 0.801375] [G loss: 1.336478]\n",
      "[D loss: 0.921351] [G loss: 1.281072]\n",
      "[D loss: 1.008415] [G loss: 1.216930]\n",
      "[D loss: 0.883050] [G loss: 1.417943]\n",
      "[D loss: 0.849729] [G loss: 1.362043]\n",
      "[D loss: 0.776935] [G loss: 1.516032]\n",
      "[D loss: 1.020616] [G loss: 1.398011]\n",
      "[D loss: 0.776375] [G loss: 1.459687]\n",
      "[D loss: 0.792338] [G loss: 1.463088]\n",
      "[D loss: 0.818575] [G loss: 1.480790]\n",
      "[D loss: 0.727679] [G loss: 1.548707]\n",
      "[D loss: 0.949618] [G loss: 1.441143]\n",
      "[D loss: 0.860575] [G loss: 1.589907]\n",
      "[D loss: 0.959545] [G loss: 1.489087]\n",
      "[D loss: 0.927634] [G loss: 1.444944]\n",
      "[D loss: 0.804646] [G loss: 1.382396]\n",
      "[D loss: 0.852997] [G loss: 1.298824]\n",
      "[D loss: 0.885952] [G loss: 1.353262]\n",
      "[D loss: 0.809371] [G loss: 1.501342]\n",
      "[D loss: 1.161501] [G loss: 1.141653]\n",
      "[D loss: 0.966250] [G loss: 1.338353]\n",
      "[D loss: 0.741300] [G loss: 1.420399]\n",
      "[D loss: 0.724160] [G loss: 1.418147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.893803] [G loss: 1.466153]\n",
      "[D loss: 0.684847] [G loss: 1.473288]\n",
      "[D loss: 0.775597] [G loss: 1.431442]\n",
      "[D loss: 0.950651] [G loss: 1.588613]\n",
      "[D loss: 0.764068] [G loss: 1.484597]\n",
      "[D loss: 1.077221] [G loss: 1.441999]\n",
      "[D loss: 0.834646] [G loss: 1.422802]\n",
      "[D loss: 0.924137] [G loss: 1.462344]\n",
      "[D loss: 0.768105] [G loss: 1.441607]\n",
      "[D loss: 0.855961] [G loss: 1.467356]\n",
      "[D loss: 0.694721] [G loss: 1.391877]\n",
      "[D loss: 0.781596] [G loss: 1.624314]\n",
      "[D loss: 0.752951] [G loss: 1.567967]\n",
      "[D loss: 0.870263] [G loss: 1.627252]\n",
      "[D loss: 0.963014] [G loss: 1.394224]\n",
      "[D loss: 0.827063] [G loss: 1.363231]\n",
      "[D loss: 0.898154] [G loss: 1.195063]\n",
      "[D loss: 0.869899] [G loss: 1.306981]\n",
      "[D loss: 0.523544] [G loss: 1.449413]\n",
      "[D loss: 0.723997] [G loss: 1.477240]\n",
      "[D loss: 0.873709] [G loss: 1.499355]\n",
      "[D loss: 0.789166] [G loss: 1.335965]\n",
      "[D loss: 0.676229] [G loss: 1.538627]\n",
      "[D loss: 0.999883] [G loss: 1.311138]\n",
      "[D loss: 0.818489] [G loss: 1.461078]\n",
      "[D loss: 0.751645] [G loss: 1.583156]\n",
      "[D loss: 0.971373] [G loss: 1.380966]\n",
      "[D loss: 0.642688] [G loss: 1.459177]\n",
      "[D loss: 0.793232] [G loss: 1.583507]\n",
      "[D loss: 0.991890] [G loss: 1.370441]\n",
      "[D loss: 0.779621] [G loss: 1.478021]\n",
      "[D loss: 0.793989] [G loss: 1.433441]\n",
      "[D loss: 0.972926] [G loss: 1.321613]\n",
      "[D loss: 0.993192] [G loss: 1.352983]\n",
      "[D loss: 0.764794] [G loss: 1.487299]\n",
      "[D loss: 0.755648] [G loss: 1.411264]\n",
      "[D loss: 0.674484] [G loss: 1.339104]\n",
      "[D loss: 0.851219] [G loss: 1.460916]\n",
      "[D loss: 0.915600] [G loss: 1.361160]\n",
      "[D loss: 0.745095] [G loss: 1.399034]\n",
      "[D loss: 0.842805] [G loss: 1.516894]\n",
      "[D loss: 0.842150] [G loss: 1.252212]\n",
      "[D loss: 0.731868] [G loss: 1.385524]\n",
      "[D loss: 0.736124] [G loss: 1.429252]\n",
      "[D loss: 1.048594] [G loss: 1.501669]\n",
      "[D loss: 0.788889] [G loss: 1.259535]\n",
      "[D loss: 0.852472] [G loss: 1.464325]\n",
      "[D loss: 0.820597] [G loss: 1.611086]\n",
      "[D loss: 1.267717] [G loss: 1.116609]\n",
      "[D loss: 0.802350] [G loss: 1.389886]\n",
      "[D loss: 1.052159] [G loss: 1.129365]\n",
      "[D loss: 1.106532] [G loss: 1.298285]\n",
      "[D loss: 0.716147] [G loss: 1.361621]\n",
      "[D loss: 0.900553] [G loss: 1.486041]\n",
      "[D loss: 0.841877] [G loss: 1.469373]\n",
      "[D loss: 0.800758] [G loss: 1.234209]\n",
      "[D loss: 0.673867] [G loss: 1.368707]\n",
      "[D loss: 0.893479] [G loss: 1.454217]\n",
      "[D loss: 0.835168] [G loss: 1.249031]\n",
      "[D loss: 0.718833] [G loss: 1.319913]\n",
      "[D loss: 0.882324] [G loss: 1.396415]\n",
      "[D loss: 0.793826] [G loss: 1.376468]\n",
      "[D loss: 0.819391] [G loss: 1.631621]\n",
      "[D loss: 0.795931] [G loss: 1.680923]\n",
      "[D loss: 0.860675] [G loss: 1.441966]\n",
      "[D loss: 0.674251] [G loss: 1.466527]\n",
      "[D loss: 1.092117] [G loss: 1.267247]\n",
      "[D loss: 0.792671] [G loss: 1.466653]\n",
      "[D loss: 0.590643] [G loss: 1.677998]\n",
      "[D loss: 0.888389] [G loss: 1.395306]\n",
      "[D loss: 0.769260] [G loss: 1.631715]\n",
      "[D loss: 0.808029] [G loss: 1.380232]\n",
      "[D loss: 0.777659] [G loss: 1.413207]\n",
      "[D loss: 0.895491] [G loss: 1.435951]\n",
      "[D loss: 0.901906] [G loss: 1.496093]\n",
      "[D loss: 0.821360] [G loss: 1.518771]\n",
      "[D loss: 0.901949] [G loss: 1.279816]\n",
      "[D loss: 0.841337] [G loss: 1.307225]\n",
      "[D loss: 0.802314] [G loss: 1.488192]\n",
      "[D loss: 0.955247] [G loss: 1.339477]\n",
      "[D loss: 0.825994] [G loss: 1.619494]\n",
      "[D loss: 0.766087] [G loss: 1.765236]\n",
      "[D loss: 0.810373] [G loss: 1.600193]\n",
      "[D loss: 0.993708] [G loss: 1.515841]\n",
      "[D loss: 0.645415] [G loss: 1.430384]\n",
      "[D loss: 0.777970] [G loss: 1.551555]\n",
      "[D loss: 0.841252] [G loss: 1.322983]\n",
      "[D loss: 0.697984] [G loss: 1.417216]\n",
      "[D loss: 0.956544] [G loss: 1.318622]\n",
      "[D loss: 0.951642] [G loss: 1.667736]\n",
      "[D loss: 0.821589] [G loss: 1.340300]\n",
      "[D loss: 0.713441] [G loss: 1.322858]\n",
      "[D loss: 0.970163] [G loss: 1.388836]\n",
      "[D loss: 0.776691] [G loss: 1.459685]\n",
      "[D loss: 0.803803] [G loss: 1.639876]\n",
      "[D loss: 0.753475] [G loss: 1.700206]\n",
      "[D loss: 0.763300] [G loss: 1.593508]\n",
      "[D loss: 0.725314] [G loss: 1.557374]\n",
      "[D loss: 0.722766] [G loss: 1.489319]\n",
      "[D loss: 0.838619] [G loss: 1.661844]\n",
      "[D loss: 0.969683] [G loss: 1.576007]\n",
      "[D loss: 1.109388] [G loss: 1.437566]\n",
      "[D loss: 1.052914] [G loss: 1.460689]\n",
      "[D loss: 0.797429] [G loss: 1.528541]\n",
      "[D loss: 0.820794] [G loss: 1.395458]\n",
      "[D loss: 1.032095] [G loss: 1.669769]\n",
      "[D loss: 0.927430] [G loss: 1.441947]\n",
      "[D loss: 0.826680] [G loss: 1.568399]\n",
      "[D loss: 0.846165] [G loss: 1.552368]\n",
      "[D loss: 0.858146] [G loss: 1.760980]\n",
      "[D loss: 0.719342] [G loss: 1.470633]\n",
      "[D loss: 0.786808] [G loss: 1.419756]\n",
      "[D loss: 0.858668] [G loss: 1.587999]\n",
      "[D loss: 0.919680] [G loss: 1.312764]\n",
      "[D loss: 0.857481] [G loss: 1.372026]\n",
      "[D loss: 0.947527] [G loss: 1.445138]\n",
      "[D loss: 0.768736] [G loss: 1.674751]\n",
      "[D loss: 0.807050] [G loss: 1.318400]\n",
      "[D loss: 0.883048] [G loss: 1.285526]\n",
      "[D loss: 0.811869] [G loss: 1.381469]\n",
      "[D loss: 0.791155] [G loss: 1.279297]\n",
      "[D loss: 0.983174] [G loss: 1.341402]\n",
      "[D loss: 0.912280] [G loss: 1.227863]\n",
      "[D loss: 0.918434] [G loss: 1.715859]\n",
      "[D loss: 0.905536] [G loss: 1.323796]\n",
      "[D loss: 0.907252] [G loss: 1.314891]\n",
      "[D loss: 0.864512] [G loss: 1.284245]\n",
      "[D loss: 0.728264] [G loss: 1.287465]\n",
      "[D loss: 0.596554] [G loss: 1.553433]\n",
      "[D loss: 0.890303] [G loss: 1.341234]\n",
      "[D loss: 0.782320] [G loss: 1.398337]\n",
      "[D loss: 0.792022] [G loss: 1.485928]\n",
      "[D loss: 0.933005] [G loss: 1.522134]\n",
      "[D loss: 1.122698] [G loss: 1.291559]\n",
      "[D loss: 0.960357] [G loss: 1.420004]\n",
      "[D loss: 0.947501] [G loss: 1.421284]\n",
      "[D loss: 0.959011] [G loss: 1.575221]\n",
      "[D loss: 0.975363] [G loss: 1.408428]\n",
      "[D loss: 0.864554] [G loss: 1.248415]\n",
      "[D loss: 0.776228] [G loss: 1.239482]\n",
      "[D loss: 0.850681] [G loss: 1.387127]\n",
      "[D loss: 0.719462] [G loss: 1.439062]\n",
      "[D loss: 0.898651] [G loss: 1.492762]\n",
      "[D loss: 0.689242] [G loss: 1.406211]\n",
      "[D loss: 0.973956] [G loss: 1.345914]\n",
      "[D loss: 0.777051] [G loss: 1.387860]\n",
      "[D loss: 0.842821] [G loss: 1.362902]\n",
      "[D loss: 0.869324] [G loss: 1.494400]\n",
      "[D loss: 1.034597] [G loss: 1.224576]\n",
      "[D loss: 0.808657] [G loss: 1.341635]\n",
      "[D loss: 0.842501] [G loss: 1.431031]\n",
      "[D loss: 0.884473] [G loss: 1.422287]\n",
      "[D loss: 0.811777] [G loss: 1.323863]\n",
      "[D loss: 0.947285] [G loss: 1.146194]\n",
      "[D loss: 0.864200] [G loss: 1.454800]\n",
      "[D loss: 1.073094] [G loss: 1.408745]\n",
      "[D loss: 0.861893] [G loss: 1.559589]\n",
      "[D loss: 0.849394] [G loss: 1.283825]\n",
      "[D loss: 0.617346] [G loss: 1.480303]\n",
      "[D loss: 0.700031] [G loss: 1.463354]\n",
      "[D loss: 0.737825] [G loss: 1.503217]\n",
      "[D loss: 0.865919] [G loss: 1.540127]\n",
      "[D loss: 0.960393] [G loss: 1.356959]\n",
      "[D loss: 0.897914] [G loss: 1.525395]\n",
      "[D loss: 0.836588] [G loss: 1.547892]\n",
      "[D loss: 0.812633] [G loss: 1.442383]\n",
      "[D loss: 0.937942] [G loss: 1.284306]\n",
      "[D loss: 0.978022] [G loss: 1.375221]\n",
      "[D loss: 0.822201] [G loss: 1.428487]\n",
      "[D loss: 1.078575] [G loss: 1.426012]\n",
      "[D loss: 0.834147] [G loss: 1.453130]\n",
      "[D loss: 0.864083] [G loss: 1.517981]\n",
      "[D loss: 0.558943] [G loss: 1.580533]\n",
      "[D loss: 0.591726] [G loss: 1.571955]\n",
      "[D loss: 0.669220] [G loss: 1.531024]\n",
      "[D loss: 0.962501] [G loss: 1.435542]\n",
      "[D loss: 0.677593] [G loss: 1.930709]\n",
      "[D loss: 1.022244] [G loss: 1.483309]\n",
      "[D loss: 0.769016] [G loss: 1.468104]\n",
      "[D loss: 0.775222] [G loss: 1.523955]\n",
      "[D loss: 0.729754] [G loss: 1.478036]\n",
      "[D loss: 0.750714] [G loss: 1.543015]\n",
      "[D loss: 0.792774] [G loss: 1.732215]\n",
      "[D loss: 0.978775] [G loss: 1.657267]\n",
      "[D loss: 0.973049] [G loss: 1.497278]\n",
      "[D loss: 0.823020] [G loss: 1.488193]\n",
      "[D loss: 0.990121] [G loss: 1.355860]\n",
      "[D loss: 0.788476] [G loss: 1.433311]\n",
      "[D loss: 0.989881] [G loss: 1.606149]\n",
      "[D loss: 0.628519] [G loss: 1.407907]\n",
      "[D loss: 0.797788] [G loss: 1.636332]\n",
      "[D loss: 0.706198] [G loss: 1.456265]\n",
      "[D loss: 0.905601] [G loss: 1.552036]\n",
      "[D loss: 0.821911] [G loss: 1.363207]\n",
      "[D loss: 0.859038] [G loss: 1.575463]\n",
      "[D loss: 0.923211] [G loss: 1.339608]\n",
      "[D loss: 0.771190] [G loss: 1.426186]\n",
      "[D loss: 0.958276] [G loss: 1.594765]\n",
      "[D loss: 0.640232] [G loss: 1.437756]\n",
      "[D loss: 0.919764] [G loss: 1.493526]\n",
      "[D loss: 0.913586] [G loss: 1.500306]\n",
      "[D loss: 0.881977] [G loss: 1.308612]\n",
      "[D loss: 0.958905] [G loss: 1.371086]\n",
      "[D loss: 0.818602] [G loss: 1.416068]\n",
      "[D loss: 0.874990] [G loss: 1.391419]\n",
      "[D loss: 0.822566] [G loss: 1.314351]\n",
      "[D loss: 0.741397] [G loss: 1.464854]\n",
      "[D loss: 0.632267] [G loss: 1.441900]\n",
      "[D loss: 0.869907] [G loss: 1.740996]\n",
      "[D loss: 0.708985] [G loss: 1.486990]\n",
      "[D loss: 0.980340] [G loss: 1.462649]\n",
      "[D loss: 0.683449] [G loss: 1.341579]\n",
      "[D loss: 1.038693] [G loss: 1.172388]\n",
      "[D loss: 0.825703] [G loss: 1.588728]\n",
      "[D loss: 0.889334] [G loss: 1.459343]\n",
      "[D loss: 0.754052] [G loss: 1.723215]\n",
      "[D loss: 0.918470] [G loss: 1.306553]\n",
      "[D loss: 0.742184] [G loss: 1.439835]\n",
      "[D loss: 0.769547] [G loss: 1.434549]\n",
      "[D loss: 0.807014] [G loss: 1.251792]\n",
      "[D loss: 0.881994] [G loss: 1.523752]\n",
      "[D loss: 0.927562] [G loss: 1.607791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.680097] [G loss: 1.400005]\n",
      "[D loss: 0.833944] [G loss: 1.207590]\n",
      "[D loss: 0.929379] [G loss: 1.341141]\n",
      "[D loss: 0.646881] [G loss: 1.429649]\n",
      "[D loss: 0.989214] [G loss: 1.348738]\n",
      "[D loss: 0.842412] [G loss: 1.627858]\n",
      "[D loss: 0.939586] [G loss: 1.299396]\n",
      "[D loss: 0.906132] [G loss: 1.585936]\n",
      "[D loss: 0.599562] [G loss: 1.476482]\n",
      "[D loss: 0.763435] [G loss: 1.455136]\n",
      "[D loss: 0.770427] [G loss: 1.492159]\n",
      "[D loss: 1.055581] [G loss: 1.630779]\n",
      "[D loss: 0.628872] [G loss: 1.567779]\n",
      "[D loss: 0.545659] [G loss: 1.630903]\n",
      "[D loss: 0.761818] [G loss: 1.348528]\n",
      "[D loss: 0.793275] [G loss: 1.497603]\n",
      "[D loss: 0.995499] [G loss: 1.320524]\n",
      "[D loss: 1.000834] [G loss: 1.400948]\n",
      "[D loss: 0.791432] [G loss: 1.451195]\n",
      "[D loss: 0.764772] [G loss: 1.372529]\n",
      "[D loss: 0.957102] [G loss: 1.509257]\n",
      "[D loss: 0.966959] [G loss: 1.426602]\n",
      "[D loss: 0.652582] [G loss: 1.667705]\n",
      "[D loss: 1.056607] [G loss: 1.451042]\n",
      "[D loss: 0.879128] [G loss: 1.459179]\n",
      "[D loss: 0.915234] [G loss: 1.506544]\n",
      "[D loss: 1.030446] [G loss: 1.527498]\n",
      "[D loss: 0.723509] [G loss: 1.448132]\n",
      "[D loss: 0.760108] [G loss: 1.660299]\n",
      "[D loss: 1.072903] [G loss: 1.218354]\n",
      "[D loss: 0.916221] [G loss: 1.343399]\n",
      "[D loss: 0.913822] [G loss: 1.437467]\n",
      "[D loss: 0.814442] [G loss: 1.465524]\n",
      "[D loss: 0.828380] [G loss: 1.492216]\n",
      "[D loss: 0.743047] [G loss: 1.504422]\n",
      "[D loss: 1.029819] [G loss: 1.154088]\n",
      "[D loss: 0.781672] [G loss: 1.335238]\n",
      "[D loss: 0.701750] [G loss: 1.372571]\n",
      "[D loss: 0.773208] [G loss: 1.344909]\n",
      "[D loss: 1.004670] [G loss: 1.470704]\n",
      "[D loss: 0.761540] [G loss: 1.468742]\n",
      "[D loss: 0.885503] [G loss: 1.430428]\n",
      "[D loss: 0.926189] [G loss: 1.368200]\n",
      "[D loss: 0.782066] [G loss: 1.418380]\n",
      "[D loss: 0.933252] [G loss: 1.336956]\n",
      "[D loss: 0.700819] [G loss: 1.475220]\n",
      "[D loss: 0.905715] [G loss: 1.461352]\n",
      "[D loss: 0.683739] [G loss: 1.482737]\n",
      "[D loss: 1.030486] [G loss: 1.358851]\n",
      "[D loss: 0.855573] [G loss: 1.602341]\n",
      "[D loss: 0.857091] [G loss: 1.339417]\n",
      "[D loss: 0.783096] [G loss: 1.468257]\n",
      "[D loss: 0.917762] [G loss: 1.264349]\n",
      "[D loss: 0.981829] [G loss: 1.309679]\n",
      "[D loss: 0.755814] [G loss: 1.447860]\n",
      "[D loss: 0.690409] [G loss: 1.555966]\n",
      "[D loss: 0.787658] [G loss: 1.467420]\n",
      "[D loss: 0.988501] [G loss: 1.541378]\n",
      "[D loss: 0.763110] [G loss: 1.369482]\n",
      "[D loss: 0.935576] [G loss: 1.298712]\n",
      "[D loss: 0.766839] [G loss: 1.433369]\n",
      "[D loss: 0.597727] [G loss: 1.649483]\n",
      "[D loss: 0.817453] [G loss: 1.369243]\n",
      "[D loss: 0.874213] [G loss: 1.260195]\n",
      "[D loss: 0.572207] [G loss: 1.459182]\n",
      "[D loss: 0.765232] [G loss: 1.611636]\n",
      "[D loss: 0.794866] [G loss: 1.530201]\n",
      "[D loss: 0.870557] [G loss: 1.843723]\n",
      "[D loss: 0.921126] [G loss: 1.254252]\n",
      "[D loss: 1.205226] [G loss: 1.542396]\n",
      "[D loss: 0.890860] [G loss: 1.530611]\n",
      "[D loss: 0.980925] [G loss: 1.280405]\n",
      "[D loss: 0.757324] [G loss: 1.682178]\n",
      "[D loss: 0.879369] [G loss: 1.471199]\n",
      "[D loss: 0.887880] [G loss: 1.439534]\n",
      "[D loss: 0.722512] [G loss: 1.342225]\n",
      "[D loss: 0.764706] [G loss: 1.487042]\n",
      "[D loss: 0.984330] [G loss: 1.321342]\n",
      "[D loss: 0.915796] [G loss: 1.377216]\n",
      "[D loss: 0.942777] [G loss: 1.392079]\n",
      "[D loss: 0.913655] [G loss: 1.347228]\n",
      "[D loss: 0.593213] [G loss: 1.658072]\n",
      "[D loss: 0.600228] [G loss: 1.766927]\n",
      "[D loss: 0.893439] [G loss: 1.474192]\n",
      "[D loss: 0.742325] [G loss: 1.490175]\n",
      "[D loss: 0.518169] [G loss: 1.472150]\n",
      "[D loss: 0.843885] [G loss: 1.549192]\n",
      "[D loss: 0.903995] [G loss: 1.309661]\n",
      "[D loss: 0.772549] [G loss: 1.594549]\n",
      "[D loss: 1.050622] [G loss: 1.562842]\n",
      "[D loss: 0.688777] [G loss: 1.428245]\n",
      "[D loss: 0.841970] [G loss: 1.417552]\n",
      "[D loss: 0.837796] [G loss: 1.466322]\n",
      "[D loss: 0.715591] [G loss: 1.658548]\n",
      "[D loss: 0.881048] [G loss: 1.309307]\n",
      "[D loss: 0.804533] [G loss: 1.477039]\n",
      "[D loss: 0.733954] [G loss: 1.463180]\n",
      "[D loss: 0.996953] [G loss: 1.612845]\n",
      "[D loss: 0.980105] [G loss: 1.294332]\n",
      "[D loss: 0.744559] [G loss: 1.321682]\n",
      "[D loss: 0.970462] [G loss: 1.410475]\n",
      "[D loss: 1.028348] [G loss: 1.430224]\n",
      "[D loss: 0.626185] [G loss: 1.679391]\n",
      "[D loss: 0.994570] [G loss: 1.526952]\n",
      "[D loss: 0.761837] [G loss: 1.534133]\n",
      "[D loss: 0.728634] [G loss: 1.451822]\n",
      "[D loss: 0.816658] [G loss: 1.452497]\n",
      "[D loss: 0.829935] [G loss: 1.198764]\n",
      "[D loss: 0.884618] [G loss: 1.643964]\n",
      "[D loss: 1.054690] [G loss: 1.145336]\n",
      "[D loss: 0.763245] [G loss: 1.274504]\n",
      "[D loss: 0.983951] [G loss: 1.559010]\n",
      "[D loss: 0.869688] [G loss: 1.375065]\n",
      "[D loss: 0.672532] [G loss: 1.392234]\n",
      "[D loss: 0.757148] [G loss: 1.307776]\n",
      "[D loss: 0.895553] [G loss: 1.505394]\n",
      "[D loss: 0.968594] [G loss: 1.328854]\n",
      "[D loss: 0.803355] [G loss: 1.453737]\n",
      "[D loss: 1.023555] [G loss: 1.284205]\n",
      "[D loss: 0.749431] [G loss: 1.308465]\n",
      "[D loss: 0.836374] [G loss: 1.563101]\n",
      "[D loss: 0.734006] [G loss: 1.397204]\n",
      "[D loss: 0.933318] [G loss: 1.380379]\n",
      "[D loss: 0.720512] [G loss: 1.491701]\n",
      "[D loss: 0.826767] [G loss: 1.514034]\n",
      "[D loss: 0.815289] [G loss: 1.468042]\n",
      "[D loss: 0.765420] [G loss: 1.513728]\n",
      "[D loss: 0.831756] [G loss: 1.366028]\n",
      "[D loss: 0.784947] [G loss: 1.316643]\n",
      "[D loss: 0.825623] [G loss: 1.364568]\n",
      "[D loss: 0.704863] [G loss: 1.537648]\n",
      "[D loss: 0.613781] [G loss: 1.482941]\n",
      "[D loss: 0.815779] [G loss: 1.411078]\n",
      "[D loss: 0.828354] [G loss: 1.713969]\n",
      "[D loss: 0.830865] [G loss: 1.388663]\n",
      "[D loss: 0.770135] [G loss: 1.524264]\n",
      "[D loss: 0.860411] [G loss: 1.581163]\n",
      "[D loss: 0.827883] [G loss: 1.611967]\n",
      "[D loss: 0.718171] [G loss: 1.701644]\n",
      "[D loss: 0.791907] [G loss: 1.827793]\n",
      "[D loss: 0.884651] [G loss: 1.477727]\n",
      "[D loss: 0.779223] [G loss: 1.430826]\n",
      "[D loss: 0.658433] [G loss: 1.732247]\n",
      "[D loss: 0.832413] [G loss: 1.761416]\n",
      "[D loss: 0.797112] [G loss: 1.282705]\n",
      "[D loss: 0.741883] [G loss: 1.614560]\n",
      "[D loss: 0.699367] [G loss: 1.797823]\n",
      "[D loss: 0.845967] [G loss: 1.638786]\n",
      "[D loss: 0.662154] [G loss: 1.447567]\n",
      "[D loss: 0.785374] [G loss: 1.349728]\n",
      "[D loss: 0.882640] [G loss: 1.443629]\n",
      "[D loss: 0.809930] [G loss: 1.550902]\n",
      "[D loss: 0.998178] [G loss: 1.570987]\n",
      "[D loss: 0.881017] [G loss: 1.529773]\n",
      "[D loss: 0.805267] [G loss: 1.736212]\n",
      "[D loss: 0.892740] [G loss: 1.683776]\n",
      "[D loss: 0.953856] [G loss: 1.670220]\n",
      "[D loss: 0.783471] [G loss: 1.240320]\n",
      "[D loss: 0.854797] [G loss: 1.344933]\n",
      "[D loss: 1.108615] [G loss: 1.287716]\n",
      "[D loss: 0.781767] [G loss: 1.376055]\n",
      "[D loss: 0.833975] [G loss: 1.468260]\n",
      "[D loss: 0.995473] [G loss: 1.419454]\n",
      "[D loss: 0.822317] [G loss: 1.362027]\n",
      "[D loss: 0.926167] [G loss: 1.198820]\n",
      "[D loss: 0.962352] [G loss: 1.565023]\n",
      "[D loss: 1.153008] [G loss: 1.346726]\n",
      "[D loss: 0.672301] [G loss: 1.332355]\n",
      "[D loss: 0.642231] [G loss: 1.429773]\n",
      "[D loss: 0.829166] [G loss: 1.330187]\n",
      "[D loss: 0.644720] [G loss: 1.517913]\n",
      "[D loss: 0.781450] [G loss: 1.446416]\n",
      "[D loss: 0.687974] [G loss: 1.476107]\n",
      "[D loss: 0.863909] [G loss: 1.642030]\n",
      "[D loss: 0.634185] [G loss: 1.370932]\n",
      "[D loss: 0.750307] [G loss: 1.262170]\n",
      "[D loss: 0.733748] [G loss: 1.419638]\n",
      "[D loss: 0.754277] [G loss: 1.495907]\n",
      "[D loss: 0.962056] [G loss: 1.474541]\n",
      "[D loss: 0.727925] [G loss: 1.828848]\n",
      "[D loss: 0.754460] [G loss: 1.435860]\n",
      "[D loss: 0.807193] [G loss: 1.855175]\n",
      "[D loss: 0.719738] [G loss: 1.756736]\n",
      "[D loss: 0.895790] [G loss: 1.562384]\n",
      "[D loss: 0.636188] [G loss: 1.631718]\n",
      "[D loss: 0.802078] [G loss: 1.480491]\n",
      "[D loss: 0.922330] [G loss: 1.535800]\n",
      "[D loss: 0.684213] [G loss: 1.412094]\n",
      "[D loss: 0.823247] [G loss: 1.524958]\n",
      "[D loss: 0.826768] [G loss: 1.418406]\n",
      "[D loss: 0.745642] [G loss: 1.612785]\n",
      "[D loss: 0.920404] [G loss: 1.437979]\n",
      "[D loss: 0.790349] [G loss: 1.536227]\n",
      "[D loss: 0.852905] [G loss: 1.494379]\n",
      "[D loss: 0.759660] [G loss: 1.488854]\n",
      "[D loss: 0.880701] [G loss: 1.364292]\n",
      "[D loss: 1.030323] [G loss: 1.382013]\n",
      "[D loss: 0.797995] [G loss: 1.520625]\n",
      "[D loss: 0.828130] [G loss: 1.483059]\n",
      "[D loss: 0.593341] [G loss: 1.405276]\n",
      "[D loss: 0.747339] [G loss: 1.655131]\n",
      "[D loss: 0.899897] [G loss: 1.555809]\n",
      "[D loss: 0.870838] [G loss: 1.472690]\n",
      "[D loss: 0.647909] [G loss: 1.772260]\n",
      "[D loss: 0.825166] [G loss: 1.520332]\n",
      "[D loss: 0.904929] [G loss: 1.448795]\n",
      "[D loss: 0.843131] [G loss: 1.540990]\n",
      "[D loss: 0.759257] [G loss: 1.721555]\n",
      "[D loss: 0.717561] [G loss: 1.443048]\n",
      "[D loss: 0.705328] [G loss: 1.464687]\n",
      "[D loss: 0.825597] [G loss: 1.682443]\n",
      "[D loss: 0.669011] [G loss: 1.550870]\n",
      "[D loss: 0.797361] [G loss: 1.339929]\n",
      "[D loss: 0.745580] [G loss: 1.538560]\n",
      "[D loss: 0.863111] [G loss: 1.533270]\n",
      "[D loss: 0.862990] [G loss: 1.411575]\n",
      "[D loss: 0.679464] [G loss: 1.493303]\n",
      "[D loss: 0.873010] [G loss: 1.475836]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.805033] [G loss: 1.371380]\n",
      "[D loss: 0.957114] [G loss: 1.414615]\n",
      "[D loss: 0.797907] [G loss: 1.344889]\n",
      "[D loss: 0.815443] [G loss: 1.609869]\n",
      "[D loss: 0.770464] [G loss: 1.575925]\n",
      "[D loss: 0.815632] [G loss: 1.512975]\n",
      "[D loss: 0.798389] [G loss: 1.347043]\n",
      "[D loss: 0.937004] [G loss: 1.548007]\n",
      "[D loss: 1.113441] [G loss: 1.329520]\n",
      "[D loss: 0.832857] [G loss: 1.204321]\n",
      "[D loss: 0.816945] [G loss: 1.436250]\n",
      "[D loss: 0.770971] [G loss: 1.631433]\n",
      "[D loss: 0.943421] [G loss: 1.501555]\n",
      "[D loss: 0.672627] [G loss: 1.741725]\n",
      "[D loss: 0.714839] [G loss: 1.524323]\n",
      "[D loss: 1.030258] [G loss: 1.500471]\n",
      "[D loss: 0.926627] [G loss: 1.285178]\n",
      "[D loss: 0.616759] [G loss: 1.494729]\n",
      "[D loss: 0.677076] [G loss: 1.503881]\n",
      "[D loss: 0.631284] [G loss: 1.474008]\n",
      "[D loss: 0.789363] [G loss: 1.561537]\n",
      "[D loss: 0.739186] [G loss: 1.657308]\n",
      "[D loss: 0.773615] [G loss: 1.606111]\n",
      "[D loss: 0.777473] [G loss: 1.606559]\n",
      "[D loss: 0.800282] [G loss: 1.522202]\n",
      "[D loss: 0.720808] [G loss: 1.507776]\n",
      "[D loss: 0.582066] [G loss: 1.541215]\n",
      "[D loss: 0.671563] [G loss: 1.612120]\n",
      "[D loss: 0.933638] [G loss: 1.409723]\n",
      "[D loss: 0.835714] [G loss: 1.547669]\n",
      "[D loss: 0.936725] [G loss: 1.654255]\n",
      "[D loss: 0.765729] [G loss: 1.772450]\n",
      "[D loss: 0.723896] [G loss: 1.409569]\n",
      "[D loss: 0.799402] [G loss: 1.240059]\n",
      "[D loss: 0.948886] [G loss: 1.673975]\n",
      "[D loss: 0.676074] [G loss: 1.593381]\n",
      "[D loss: 0.785969] [G loss: 1.578163]\n",
      "[D loss: 0.833015] [G loss: 1.401865]\n",
      "[D loss: 0.731353] [G loss: 1.436901]\n",
      "[D loss: 1.024466] [G loss: 1.783123]\n",
      "[D loss: 0.821263] [G loss: 1.402337]\n",
      "[D loss: 0.839494] [G loss: 1.388972]\n",
      "[D loss: 1.013267] [G loss: 1.382748]\n",
      "[D loss: 1.008150] [G loss: 1.348653]\n",
      "[D loss: 0.777621] [G loss: 1.445198]\n",
      "[D loss: 0.800851] [G loss: 1.425963]\n",
      "[D loss: 0.906073] [G loss: 1.463577]\n",
      "[D loss: 1.178638] [G loss: 1.466754]\n",
      "[D loss: 0.764905] [G loss: 1.494843]\n",
      "[D loss: 0.801849] [G loss: 1.906302]\n",
      "[D loss: 0.927201] [G loss: 1.503341]\n",
      "[D loss: 0.783807] [G loss: 1.501545]\n",
      "[D loss: 0.873081] [G loss: 1.340781]\n",
      "[D loss: 0.803624] [G loss: 1.503727]\n",
      "[D loss: 0.926706] [G loss: 1.409937]\n",
      "[D loss: 0.972955] [G loss: 1.243507]\n",
      "[D loss: 0.861722] [G loss: 1.272403]\n",
      "[D loss: 0.879702] [G loss: 1.489654]\n",
      "[D loss: 0.864870] [G loss: 1.654218]\n",
      "[D loss: 1.282595] [G loss: 1.267718]\n",
      "[D loss: 0.852943] [G loss: 1.563766]\n",
      "[D loss: 0.628935] [G loss: 1.434419]\n",
      "[D loss: 0.749982] [G loss: 1.367230]\n",
      "[D loss: 0.649232] [G loss: 1.388789]\n",
      "[D loss: 0.598864] [G loss: 1.494167]\n",
      "[D loss: 0.578912] [G loss: 1.282557]\n",
      "[D loss: 0.654757] [G loss: 1.299161]\n",
      "[D loss: 0.701779] [G loss: 1.391097]\n",
      "[D loss: 1.024264] [G loss: 1.512136]\n",
      "[D loss: 0.745715] [G loss: 1.453759]\n",
      "[D loss: 0.699355] [G loss: 1.590091]\n",
      "[D loss: 0.797967] [G loss: 1.512072]\n",
      "[D loss: 0.610388] [G loss: 1.528061]\n",
      "[D loss: 0.544946] [G loss: 1.604091]\n",
      "[D loss: 0.879999] [G loss: 1.518182]\n",
      "[D loss: 0.671829] [G loss: 1.615221]\n",
      "[D loss: 0.665025] [G loss: 1.844218]\n",
      "[D loss: 0.821933] [G loss: 1.641906]\n",
      "[D loss: 0.832027] [G loss: 1.441306]\n",
      "[D loss: 0.929205] [G loss: 1.475695]\n",
      "[D loss: 0.931911] [G loss: 1.516907]\n",
      "[D loss: 0.854928] [G loss: 1.463016]\n",
      "[D loss: 0.919918] [G loss: 1.534156]\n",
      "[D loss: 1.199819] [G loss: 1.496041]\n",
      "[D loss: 0.758037] [G loss: 1.319922]\n",
      "[D loss: 0.886560] [G loss: 1.708224]\n",
      "[D loss: 0.692630] [G loss: 1.469814]\n",
      "[D loss: 0.957866] [G loss: 1.472148]\n",
      "[D loss: 0.849475] [G loss: 1.388380]\n",
      "[D loss: 0.871918] [G loss: 1.368038]\n",
      "[D loss: 0.798413] [G loss: 1.421734]\n",
      "[D loss: 0.804234] [G loss: 1.308199]\n",
      "[D loss: 0.666758] [G loss: 1.537332]\n",
      "[D loss: 0.667885] [G loss: 1.465415]\n",
      "[D loss: 0.730327] [G loss: 1.481411]\n",
      "[D loss: 0.786067] [G loss: 1.433830]\n",
      "[D loss: 0.739509] [G loss: 1.399083]\n",
      "[D loss: 0.619774] [G loss: 1.531997]\n",
      "[D loss: 0.948706] [G loss: 1.290482]\n",
      "[D loss: 0.597356] [G loss: 1.516980]\n",
      "[D loss: 0.662904] [G loss: 1.455602]\n",
      "[D loss: 0.727714] [G loss: 1.591016]\n",
      "[D loss: 1.006715] [G loss: 1.451419]\n",
      "[D loss: 0.821432] [G loss: 1.658493]\n",
      "[D loss: 0.780713] [G loss: 1.358931]\n",
      "[D loss: 0.501022] [G loss: 1.605633]\n",
      "[D loss: 0.972090] [G loss: 1.537082]\n",
      "[D loss: 0.984251] [G loss: 1.697122]\n",
      "[D loss: 0.683273] [G loss: 1.530735]\n",
      "[D loss: 1.231051] [G loss: 1.598137]\n",
      "[D loss: 0.847390] [G loss: 1.563426]\n",
      "[D loss: 0.932657] [G loss: 1.388712]\n",
      "[D loss: 0.791100] [G loss: 1.593262]\n",
      "[D loss: 0.699363] [G loss: 1.453115]\n",
      "[D loss: 0.791896] [G loss: 1.458379]\n",
      "[D loss: 0.910639] [G loss: 1.518922]\n",
      "[D loss: 0.735777] [G loss: 1.451796]\n",
      "[D loss: 0.819372] [G loss: 1.465230]\n",
      "[D loss: 0.868142] [G loss: 1.366454]\n",
      "[D loss: 0.836696] [G loss: 1.437952]\n",
      "[D loss: 0.839507] [G loss: 1.459485]\n",
      "[D loss: 0.900061] [G loss: 1.251195]\n",
      "[D loss: 0.817569] [G loss: 1.535873]\n",
      "[D loss: 0.990527] [G loss: 1.205830]\n",
      "[D loss: 1.053327] [G loss: 1.400797]\n",
      "[D loss: 1.037434] [G loss: 1.521293]\n",
      "[D loss: 0.767127] [G loss: 1.419605]\n",
      "[D loss: 0.723309] [G loss: 1.289549]\n",
      "[D loss: 0.856457] [G loss: 1.451706]\n",
      "[D loss: 1.109328] [G loss: 1.401844]\n",
      "[D loss: 0.872513] [G loss: 1.433342]\n",
      "[D loss: 0.786947] [G loss: 1.509727]\n",
      "[D loss: 0.835070] [G loss: 1.403261]\n",
      "[D loss: 0.739322] [G loss: 1.395689]\n",
      "[D loss: 0.873636] [G loss: 1.423190]\n",
      "[D loss: 0.736262] [G loss: 1.287333]\n",
      "[D loss: 0.857382] [G loss: 1.313222]\n",
      "[D loss: 0.813620] [G loss: 1.540576]\n",
      "[D loss: 0.648591] [G loss: 1.537030]\n",
      "[D loss: 0.899660] [G loss: 1.555672]\n",
      "[D loss: 0.918662] [G loss: 1.599977]\n",
      "[D loss: 0.751926] [G loss: 1.438091]\n",
      "[D loss: 0.875206] [G loss: 1.687304]\n",
      "[D loss: 0.899989] [G loss: 1.589138]\n",
      "[D loss: 0.771796] [G loss: 1.358740]\n",
      "[D loss: 0.880381] [G loss: 1.376821]\n",
      "[D loss: 0.965584] [G loss: 1.365429]\n",
      "[D loss: 0.969486] [G loss: 1.447936]\n",
      "[D loss: 0.872571] [G loss: 1.446741]\n",
      "[D loss: 0.791963] [G loss: 1.727299]\n",
      "[D loss: 0.610650] [G loss: 1.539678]\n",
      "[D loss: 0.639455] [G loss: 1.556266]\n",
      "[D loss: 0.764487] [G loss: 1.520862]\n",
      "[D loss: 0.639216] [G loss: 1.227837]\n",
      "[D loss: 0.780371] [G loss: 1.290551]\n",
      "[D loss: 0.785648] [G loss: 1.587644]\n",
      "[D loss: 0.778170] [G loss: 1.605079]\n",
      "[D loss: 0.867472] [G loss: 1.442979]\n",
      "[D loss: 0.562311] [G loss: 1.607148]\n",
      "[D loss: 0.944545] [G loss: 1.575915]\n",
      "[D loss: 0.768229] [G loss: 1.643989]\n",
      "[D loss: 0.968843] [G loss: 1.220084]\n",
      "[D loss: 0.791690] [G loss: 1.418115]\n",
      "[D loss: 0.945463] [G loss: 1.449571]\n",
      "[D loss: 0.840855] [G loss: 1.469198]\n",
      "[D loss: 0.679349] [G loss: 1.466486]\n",
      "[D loss: 0.844168] [G loss: 1.221030]\n",
      "[D loss: 0.823249] [G loss: 1.539392]\n",
      "[D loss: 0.723825] [G loss: 1.638345]\n",
      "[D loss: 0.950571] [G loss: 1.584624]\n",
      "[D loss: 0.962151] [G loss: 1.592604]\n",
      "[D loss: 0.940345] [G loss: 1.327775]\n",
      "[D loss: 0.775982] [G loss: 1.718740]\n",
      "[D loss: 0.699799] [G loss: 1.558429]\n",
      "[D loss: 0.905472] [G loss: 1.317374]\n",
      "[D loss: 0.771862] [G loss: 1.689099]\n",
      "[D loss: 0.973491] [G loss: 1.545928]\n",
      "[D loss: 0.986679] [G loss: 1.395949]\n",
      "[D loss: 0.910265] [G loss: 1.478224]\n",
      "[D loss: 0.850402] [G loss: 1.291844]\n",
      "[D loss: 0.762799] [G loss: 1.314162]\n",
      "[D loss: 0.657824] [G loss: 1.472207]\n",
      "[D loss: 0.753947] [G loss: 1.387224]\n",
      "[D loss: 0.947196] [G loss: 1.547568]\n",
      "[D loss: 0.771831] [G loss: 1.542549]\n",
      "[D loss: 0.748716] [G loss: 1.193314]\n",
      "[D loss: 0.620391] [G loss: 1.489468]\n",
      "[D loss: 0.797105] [G loss: 1.338334]\n",
      "[D loss: 0.921958] [G loss: 1.448665]\n",
      "[D loss: 0.939435] [G loss: 1.359499]\n",
      "[D loss: 0.976794] [G loss: 1.447772]\n",
      "[D loss: 0.905055] [G loss: 1.498121]\n",
      "[D loss: 0.753422] [G loss: 1.671553]\n",
      "[D loss: 0.634684] [G loss: 1.519443]\n",
      "[D loss: 1.004209] [G loss: 1.432175]\n",
      "[D loss: 0.720166] [G loss: 1.422575]\n",
      "[D loss: 0.989218] [G loss: 1.467241]\n",
      "[D loss: 0.652825] [G loss: 1.592301]\n",
      "[D loss: 0.775469] [G loss: 1.438938]\n",
      "[D loss: 0.870505] [G loss: 1.506313]\n",
      "[D loss: 0.778717] [G loss: 1.446561]\n",
      "[D loss: 0.961062] [G loss: 1.339274]\n",
      "[D loss: 0.943493] [G loss: 1.125839]\n",
      "[D loss: 0.861667] [G loss: 1.522060]\n",
      "[D loss: 0.812078] [G loss: 1.467097]\n",
      "[D loss: 0.888479] [G loss: 1.397846]\n",
      "[D loss: 0.898303] [G loss: 1.376886]\n",
      "[D loss: 0.672827] [G loss: 1.248811]\n",
      "[D loss: 0.837020] [G loss: 1.311214]\n",
      "[D loss: 0.889004] [G loss: 1.577526]\n",
      "[D loss: 0.711729] [G loss: 1.450846]\n",
      "[D loss: 0.816995] [G loss: 1.547388]\n",
      "[D loss: 0.628709] [G loss: 1.407013]\n",
      "[D loss: 0.782905] [G loss: 1.538531]\n",
      "[D loss: 0.563835] [G loss: 1.819308]\n",
      "[D loss: 0.735098] [G loss: 1.480681]\n",
      "[D loss: 0.696113] [G loss: 1.445200]\n",
      "[D loss: 0.857179] [G loss: 1.615443]\n",
      "[D loss: 0.898146] [G loss: 1.684999]\n",
      "[D loss: 0.749681] [G loss: 1.643415]\n",
      "[D loss: 0.742140] [G loss: 1.594513]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 1.125425] [G loss: 1.366749]\n",
      "[D loss: 0.816050] [G loss: 1.526625]\n",
      "[D loss: 0.881797] [G loss: 1.323145]\n",
      "[D loss: 0.839159] [G loss: 1.762032]\n",
      "[D loss: 0.793221] [G loss: 1.503302]\n",
      "[D loss: 0.935489] [G loss: 1.331959]\n",
      "[D loss: 0.737502] [G loss: 1.409328]\n",
      "[D loss: 0.907142] [G loss: 1.438306]\n",
      "[D loss: 0.904307] [G loss: 1.400067]\n",
      "[D loss: 0.544773] [G loss: 1.613912]\n",
      "[D loss: 0.880689] [G loss: 1.492507]\n",
      "[D loss: 0.852183] [G loss: 1.486110]\n",
      "[D loss: 0.850365] [G loss: 1.326234]\n",
      "[D loss: 0.955558] [G loss: 1.314895]\n",
      "[D loss: 0.714378] [G loss: 1.629370]\n",
      "[D loss: 0.761917] [G loss: 1.433690]\n",
      "[D loss: 0.870645] [G loss: 1.435077]\n",
      "[D loss: 0.771434] [G loss: 1.503267]\n",
      "[D loss: 0.893614] [G loss: 1.575136]\n",
      "[D loss: 1.091887] [G loss: 1.393265]\n",
      "[D loss: 0.753280] [G loss: 1.346603]\n",
      "[D loss: 0.855360] [G loss: 1.436636]\n",
      "[D loss: 0.757020] [G loss: 1.682691]\n",
      "[D loss: 0.708089] [G loss: 1.530558]\n",
      "[D loss: 0.964116] [G loss: 1.438961]\n",
      "[D loss: 1.398533] [G loss: 1.365609]\n",
      "[D loss: 1.055389] [G loss: 1.490747]\n",
      "[D loss: 0.699138] [G loss: 1.372646]\n",
      "[D loss: 0.758684] [G loss: 1.420792]\n",
      "[D loss: 0.888925] [G loss: 1.467217]\n",
      "[D loss: 0.822228] [G loss: 1.471961]\n",
      "[D loss: 0.797271] [G loss: 1.408243]\n",
      "[D loss: 0.699667] [G loss: 1.319773]\n",
      "[D loss: 0.632173] [G loss: 1.485704]\n",
      "[D loss: 0.751553] [G loss: 1.493598]\n",
      "[D loss: 0.925430] [G loss: 1.397693]\n",
      "[D loss: 0.543541] [G loss: 1.720859]\n",
      "[D loss: 0.773523] [G loss: 1.332658]\n",
      "[D loss: 0.866266] [G loss: 1.451009]\n",
      "[D loss: 0.877708] [G loss: 1.547169]\n",
      "[D loss: 0.543966] [G loss: 1.665237]\n",
      "[D loss: 0.915608] [G loss: 1.429038]\n",
      "[D loss: 0.792833] [G loss: 1.559431]\n",
      "[D loss: 0.704847] [G loss: 1.519566]\n",
      "[D loss: 0.796253] [G loss: 1.444127]\n",
      "[D loss: 0.971062] [G loss: 1.487553]\n",
      "[D loss: 0.934973] [G loss: 1.380832]\n",
      "[D loss: 0.649954] [G loss: 1.445526]\n",
      "[D loss: 0.664409] [G loss: 1.240051]\n",
      "[D loss: 0.711615] [G loss: 1.675002]\n",
      "[D loss: 0.863094] [G loss: 1.496628]\n",
      "[D loss: 0.917007] [G loss: 1.437805]\n",
      "[D loss: 0.736667] [G loss: 1.948280]\n",
      "[D loss: 0.863190] [G loss: 1.425515]\n",
      "[D loss: 0.737236] [G loss: 1.553209]\n",
      "[D loss: 0.749371] [G loss: 1.499644]\n",
      "[D loss: 0.750287] [G loss: 1.514894]\n",
      "[D loss: 0.873418] [G loss: 1.598717]\n",
      "[D loss: 0.860217] [G loss: 1.473166]\n",
      "[D loss: 0.721418] [G loss: 1.632982]\n",
      "[D loss: 0.932255] [G loss: 1.452039]\n",
      "[D loss: 0.768808] [G loss: 1.435800]\n",
      "[D loss: 0.911721] [G loss: 1.463458]\n",
      "[D loss: 0.777143] [G loss: 1.661112]\n",
      "[D loss: 0.741505] [G loss: 1.409288]\n",
      "[D loss: 0.975205] [G loss: 1.490691]\n",
      "[D loss: 0.967468] [G loss: 1.363668]\n",
      "[D loss: 0.931974] [G loss: 1.452860]\n",
      "[D loss: 0.974452] [G loss: 1.418283]\n",
      "[D loss: 0.620626] [G loss: 1.493031]\n",
      "[D loss: 0.698238] [G loss: 1.462790]\n",
      "[D loss: 0.637411] [G loss: 1.561380]\n",
      "[D loss: 0.780239] [G loss: 1.436621]\n",
      "[D loss: 0.842520] [G loss: 1.537916]\n",
      "[D loss: 0.816311] [G loss: 1.524247]\n",
      "[D loss: 0.883101] [G loss: 1.566002]\n",
      "[D loss: 0.777382] [G loss: 1.544588]\n",
      "[D loss: 0.996419] [G loss: 1.306615]\n",
      "[D loss: 1.011044] [G loss: 1.318199]\n",
      "[D loss: 0.852928] [G loss: 1.264404]\n",
      "[D loss: 0.871254] [G loss: 1.646172]\n",
      "[D loss: 1.013242] [G loss: 1.485745]\n",
      "[D loss: 0.858726] [G loss: 1.665256]\n",
      "[D loss: 0.614658] [G loss: 1.479694]\n",
      "[D loss: 0.652369] [G loss: 1.635231]\n",
      "[D loss: 1.022740] [G loss: 1.435444]\n",
      "[D loss: 0.744336] [G loss: 1.429293]\n",
      "[D loss: 0.755918] [G loss: 1.483185]\n",
      "[D loss: 1.057366] [G loss: 1.240560]\n",
      "[D loss: 0.906363] [G loss: 1.402980]\n",
      "[D loss: 0.704439] [G loss: 1.411849]\n",
      "[D loss: 0.928578] [G loss: 1.213770]\n",
      "[D loss: 0.677189] [G loss: 1.436138]\n",
      "[D loss: 0.818534] [G loss: 1.338622]\n",
      "[D loss: 0.789790] [G loss: 1.149627]\n",
      "[D loss: 1.122364] [G loss: 1.565130]\n",
      "[D loss: 0.769454] [G loss: 1.389642]\n",
      "[D loss: 0.888228] [G loss: 1.356792]\n",
      "[D loss: 0.792242] [G loss: 1.465690]\n",
      "[D loss: 0.854133] [G loss: 1.441477]\n",
      "[D loss: 0.687439] [G loss: 1.325566]\n",
      "[D loss: 1.024375] [G loss: 1.345236]\n",
      "[D loss: 0.744064] [G loss: 1.305588]\n",
      "[D loss: 0.756321] [G loss: 1.627149]\n",
      "[D loss: 0.959764] [G loss: 1.550341]\n",
      "[D loss: 0.843064] [G loss: 1.494106]\n",
      "[D loss: 0.827200] [G loss: 1.443743]\n",
      "[D loss: 0.821515] [G loss: 1.528194]\n",
      "[D loss: 0.899336] [G loss: 1.346432]\n",
      "[D loss: 0.770622] [G loss: 1.348875]\n",
      "[D loss: 1.082467] [G loss: 1.264285]\n",
      "[D loss: 0.774173] [G loss: 1.491552]\n",
      "[D loss: 0.712632] [G loss: 1.338024]\n",
      "[D loss: 0.809849] [G loss: 1.697496]\n",
      "[D loss: 0.936289] [G loss: 1.515117]\n",
      "[D loss: 0.737734] [G loss: 1.596558]\n",
      "[D loss: 0.698004] [G loss: 1.746441]\n",
      "[D loss: 0.877413] [G loss: 1.541260]\n",
      "[D loss: 0.730066] [G loss: 1.560223]\n",
      "[D loss: 0.941725] [G loss: 1.583137]\n",
      "[D loss: 0.892646] [G loss: 1.273442]\n",
      "[D loss: 0.806622] [G loss: 1.507251]\n",
      "[D loss: 0.903415] [G loss: 1.423985]\n",
      "[D loss: 0.836921] [G loss: 1.564983]\n",
      "[D loss: 0.831207] [G loss: 1.597244]\n",
      "[D loss: 0.822198] [G loss: 1.527126]\n",
      "[D loss: 0.712172] [G loss: 1.559976]\n",
      "[D loss: 0.774203] [G loss: 1.361796]\n",
      "[D loss: 0.871587] [G loss: 1.546430]\n",
      "[D loss: 0.809569] [G loss: 1.454871]\n",
      "[D loss: 0.721586] [G loss: 1.292431]\n",
      "[D loss: 0.809436] [G loss: 1.308936]\n",
      "[D loss: 0.712792] [G loss: 1.238966]\n",
      "[D loss: 0.873615] [G loss: 1.379094]\n",
      "[D loss: 0.787611] [G loss: 1.392783]\n",
      "[D loss: 0.956777] [G loss: 1.619959]\n",
      "[D loss: 0.762640] [G loss: 1.550362]\n",
      "[D loss: 0.733233] [G loss: 1.486128]\n",
      "[D loss: 0.958125] [G loss: 1.468971]\n",
      "[D loss: 0.689908] [G loss: 1.574892]\n",
      "[D loss: 0.823120] [G loss: 1.425678]\n",
      "[D loss: 0.876818] [G loss: 1.254157]\n",
      "[D loss: 0.729149] [G loss: 1.330739]\n",
      "[D loss: 0.803052] [G loss: 1.608007]\n",
      "[D loss: 0.623320] [G loss: 1.500807]\n",
      "[D loss: 1.152700] [G loss: 1.497602]\n",
      "[D loss: 0.841906] [G loss: 1.427969]\n",
      "[D loss: 0.799571] [G loss: 1.479870]\n",
      "[D loss: 0.864632] [G loss: 1.518312]\n",
      "[D loss: 0.735042] [G loss: 1.347454]\n",
      "[D loss: 0.746183] [G loss: 1.312979]\n",
      "[D loss: 0.584271] [G loss: 1.625648]\n",
      "[D loss: 1.040395] [G loss: 1.527541]\n",
      "[D loss: 1.002629] [G loss: 1.576895]\n",
      "[D loss: 0.866349] [G loss: 1.422127]\n",
      "[D loss: 0.761929] [G loss: 1.667571]\n",
      "[D loss: 0.882062] [G loss: 1.379083]\n",
      "[D loss: 0.854708] [G loss: 1.460140]\n",
      "[D loss: 0.823461] [G loss: 1.406411]\n",
      "[D loss: 0.641143] [G loss: 1.608060]\n",
      "[D loss: 0.938548] [G loss: 1.453909]\n",
      "[D loss: 0.753169] [G loss: 1.628300]\n",
      "[D loss: 0.970125] [G loss: 1.280823]\n",
      "[D loss: 1.221044] [G loss: 1.329115]\n",
      "[D loss: 0.939519] [G loss: 1.581006]\n",
      "[D loss: 1.038195] [G loss: 1.418492]\n",
      "[D loss: 0.909803] [G loss: 1.541143]\n",
      "[D loss: 0.697171] [G loss: 1.398200]\n",
      "[D loss: 0.624861] [G loss: 1.476008]\n",
      "[D loss: 0.862161] [G loss: 1.314532]\n",
      "[D loss: 0.865231] [G loss: 1.486715]\n",
      "[D loss: 0.639522] [G loss: 1.721787]\n",
      "[D loss: 0.869026] [G loss: 1.943141]\n",
      "[D loss: 0.780947] [G loss: 1.511805]\n",
      "[D loss: 0.695409] [G loss: 1.348024]\n",
      "[D loss: 0.910374] [G loss: 1.305933]\n",
      "[D loss: 0.621978] [G loss: 1.622071]\n",
      "[D loss: 1.023158] [G loss: 1.417350]\n",
      "[D loss: 0.887410] [G loss: 1.482242]\n",
      "[D loss: 0.681696] [G loss: 1.482335]\n",
      "[D loss: 0.744889] [G loss: 1.378044]\n",
      "[D loss: 0.691804] [G loss: 1.603572]\n",
      "[D loss: 0.750219] [G loss: 1.488245]\n",
      "[D loss: 0.890949] [G loss: 1.455836]\n",
      "[D loss: 0.876473] [G loss: 1.401382]\n",
      "[D loss: 0.742127] [G loss: 1.548273]\n",
      "[D loss: 0.859866] [G loss: 1.598056]\n",
      "[D loss: 0.846492] [G loss: 1.434575]\n",
      "[D loss: 0.702576] [G loss: 1.467845]\n",
      "[D loss: 0.798585] [G loss: 1.315701]\n",
      "[D loss: 0.877051] [G loss: 1.410139]\n",
      "[D loss: 0.876145] [G loss: 1.369932]\n",
      "[D loss: 0.848394] [G loss: 1.427691]\n",
      "[D loss: 1.035229] [G loss: 1.368706]\n",
      "[D loss: 1.000267] [G loss: 1.457684]\n",
      "[D loss: 0.785859] [G loss: 1.336168]\n",
      "[D loss: 0.967693] [G loss: 1.274120]\n",
      "[D loss: 0.820557] [G loss: 1.448384]\n",
      "[D loss: 0.913610] [G loss: 1.311422]\n",
      "[D loss: 1.096447] [G loss: 1.198190]\n",
      "[D loss: 0.956008] [G loss: 1.252283]\n",
      "[D loss: 0.919405] [G loss: 1.434440]\n",
      "[D loss: 0.738982] [G loss: 1.377082]\n",
      "[D loss: 0.952694] [G loss: 1.317471]\n",
      "[D loss: 0.798826] [G loss: 1.512491]\n",
      "[D loss: 0.816067] [G loss: 1.500908]\n",
      "[D loss: 0.790053] [G loss: 1.316592]\n",
      "[D loss: 0.787444] [G loss: 1.537920]\n",
      "[D loss: 0.780924] [G loss: 1.406084]\n",
      "[D loss: 0.643470] [G loss: 1.322184]\n",
      "[D loss: 0.781219] [G loss: 1.527651]\n",
      "[D loss: 1.024310] [G loss: 1.445474]\n",
      "[D loss: 0.881379] [G loss: 1.411957]\n",
      "[D loss: 0.713237] [G loss: 1.678946]\n",
      "[D loss: 0.773791] [G loss: 1.410248]\n",
      "[D loss: 0.813280] [G loss: 1.313675]\n",
      "[D loss: 0.749199] [G loss: 1.257876]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.713865] [G loss: 1.538070]\n",
      "[D loss: 0.722267] [G loss: 1.629925]\n",
      "[D loss: 1.042103] [G loss: 1.638192]\n",
      "[D loss: 1.152496] [G loss: 1.346453]\n",
      "[D loss: 0.891193] [G loss: 1.290383]\n",
      "[D loss: 0.931291] [G loss: 1.365233]\n",
      "[D loss: 1.021881] [G loss: 1.292104]\n",
      "[D loss: 0.824248] [G loss: 1.739220]\n",
      "[D loss: 0.804498] [G loss: 1.503161]\n",
      "[D loss: 0.932587] [G loss: 1.671592]\n",
      "[D loss: 0.765732] [G loss: 1.418341]\n",
      "[D loss: 0.906642] [G loss: 1.242273]\n",
      "[D loss: 0.801823] [G loss: 1.421474]\n",
      "[D loss: 0.782016] [G loss: 1.331739]\n",
      "[D loss: 0.819030] [G loss: 1.397241]\n",
      "[D loss: 0.786169] [G loss: 1.229528]\n",
      "[D loss: 0.933434] [G loss: 1.441746]\n",
      "[D loss: 0.941225] [G loss: 1.497475]\n",
      "[D loss: 0.897396] [G loss: 1.498625]\n",
      "[D loss: 0.588125] [G loss: 1.582373]\n",
      "[D loss: 1.040355] [G loss: 1.591122]\n",
      "[D loss: 0.802716] [G loss: 1.586400]\n",
      "[D loss: 0.884897] [G loss: 1.361485]\n",
      "[D loss: 0.859186] [G loss: 1.403590]\n",
      "[D loss: 0.823875] [G loss: 1.289016]\n",
      "[D loss: 0.908752] [G loss: 1.496850]\n",
      "[D loss: 0.817691] [G loss: 1.640291]\n",
      "[D loss: 0.679873] [G loss: 1.568497]\n",
      "[D loss: 0.970807] [G loss: 1.407303]\n",
      "[D loss: 0.865211] [G loss: 1.579027]\n",
      "[D loss: 0.894057] [G loss: 1.640325]\n",
      "[D loss: 0.688780] [G loss: 1.534027]\n",
      "[D loss: 1.167935] [G loss: 1.217933]\n",
      "[D loss: 0.822220] [G loss: 1.399842]\n",
      "[D loss: 0.644987] [G loss: 1.351926]\n",
      "[D loss: 0.751307] [G loss: 1.446776]\n",
      "[D loss: 1.036202] [G loss: 1.322740]\n",
      "[D loss: 0.915407] [G loss: 1.666631]\n",
      "[D loss: 1.023444] [G loss: 1.533959]\n",
      "[D loss: 0.794727] [G loss: 1.449598]\n",
      "[D loss: 0.773997] [G loss: 1.534503]\n",
      "[D loss: 0.954629] [G loss: 1.186124]\n",
      "[D loss: 0.867787] [G loss: 1.442187]\n",
      "[D loss: 0.845499] [G loss: 1.523480]\n",
      "[D loss: 0.662452] [G loss: 1.667992]\n",
      "[D loss: 0.915489] [G loss: 1.655077]\n",
      "[D loss: 0.743415] [G loss: 1.638771]\n",
      "[D loss: 1.044784] [G loss: 1.580881]\n",
      "[D loss: 0.904443] [G loss: 1.338672]\n",
      "[D loss: 0.865526] [G loss: 1.413874]\n",
      "[D loss: 0.642849] [G loss: 1.370246]\n",
      "[D loss: 0.866230] [G loss: 1.538201]\n",
      "[D loss: 0.812281] [G loss: 1.377210]\n",
      "[D loss: 0.904925] [G loss: 1.399506]\n",
      "[D loss: 0.926588] [G loss: 1.424750]\n",
      "[D loss: 0.838311] [G loss: 1.434595]\n",
      "[D loss: 0.725289] [G loss: 1.383051]\n",
      "[D loss: 0.829196] [G loss: 1.341601]\n",
      "[D loss: 1.051819] [G loss: 1.123020]\n",
      "[D loss: 0.904651] [G loss: 1.293771]\n",
      "[D loss: 0.870860] [G loss: 1.467406]\n",
      "[D loss: 1.030329] [G loss: 1.415543]\n",
      "[D loss: 0.673030] [G loss: 1.520308]\n",
      "[D loss: 1.083540] [G loss: 1.341024]\n",
      "[D loss: 0.818653] [G loss: 1.562677]\n",
      "[D loss: 0.594540] [G loss: 1.428650]\n",
      "[D loss: 0.986179] [G loss: 1.289318]\n",
      "[D loss: 0.736400] [G loss: 1.501694]\n",
      "[D loss: 0.986534] [G loss: 1.293686]\n",
      "[D loss: 0.675292] [G loss: 1.386467]\n",
      "[D loss: 0.722993] [G loss: 1.407919]\n",
      "[D loss: 0.703349] [G loss: 1.689730]\n",
      "[D loss: 0.784456] [G loss: 1.600437]\n",
      "[D loss: 1.016658] [G loss: 1.676192]\n",
      "[D loss: 1.146628] [G loss: 1.586540]\n",
      "[D loss: 0.874619] [G loss: 1.463965]\n",
      "[D loss: 0.947947] [G loss: 1.292528]\n",
      "[D loss: 0.867765] [G loss: 1.305665]\n",
      "[D loss: 0.863087] [G loss: 1.507310]\n",
      "[D loss: 0.801285] [G loss: 1.398676]\n",
      "[D loss: 0.771544] [G loss: 1.553519]\n",
      "[D loss: 0.754677] [G loss: 1.407136]\n",
      "[D loss: 0.757124] [G loss: 1.385378]\n",
      "[D loss: 0.896767] [G loss: 1.495062]\n",
      "[D loss: 0.748393] [G loss: 1.358527]\n",
      "[D loss: 0.884044] [G loss: 1.457808]\n",
      "[D loss: 0.777352] [G loss: 1.358449]\n",
      "[D loss: 0.692803] [G loss: 1.709678]\n",
      "[D loss: 0.943935] [G loss: 1.520331]\n",
      "[D loss: 0.820365] [G loss: 1.434861]\n",
      "[D loss: 0.819916] [G loss: 1.399924]\n",
      "[D loss: 0.685111] [G loss: 1.522288]\n",
      "[D loss: 0.834648] [G loss: 1.420990]\n",
      "[D loss: 0.543427] [G loss: 1.468281]\n",
      "[D loss: 0.826031] [G loss: 1.240547]\n",
      "[D loss: 1.093966] [G loss: 1.455730]\n",
      "[D loss: 1.051725] [G loss: 1.393238]\n",
      "[D loss: 0.864370] [G loss: 1.319250]\n",
      "[D loss: 0.794777] [G loss: 1.545428]\n",
      "[D loss: 1.124739] [G loss: 1.329973]\n",
      "[D loss: 0.611808] [G loss: 1.460240]\n",
      "[D loss: 0.992341] [G loss: 1.423904]\n",
      "[D loss: 0.953118] [G loss: 1.252077]\n",
      "[D loss: 1.043775] [G loss: 1.392352]\n",
      "[D loss: 0.818985] [G loss: 1.188440]\n",
      "[D loss: 0.794256] [G loss: 1.341452]\n",
      "[D loss: 0.952233] [G loss: 1.520290]\n",
      "[D loss: 0.816626] [G loss: 1.467809]\n",
      "[D loss: 0.577335] [G loss: 1.496828]\n",
      "[D loss: 0.781495] [G loss: 1.262573]\n",
      "[D loss: 0.745884] [G loss: 1.413159]\n",
      "[D loss: 0.865713] [G loss: 1.371508]\n",
      "[D loss: 0.788128] [G loss: 1.558636]\n",
      "[D loss: 0.803964] [G loss: 1.383184]\n",
      "[D loss: 0.798176] [G loss: 1.502468]\n",
      "[D loss: 0.708735] [G loss: 1.384823]\n",
      "[D loss: 0.859744] [G loss: 1.397575]\n",
      "[D loss: 0.691143] [G loss: 1.478659]\n",
      "[D loss: 0.780941] [G loss: 1.515386]\n",
      "[D loss: 0.888930] [G loss: 1.557525]\n",
      "[D loss: 0.986405] [G loss: 1.535192]\n",
      "[D loss: 0.788329] [G loss: 1.544989]\n",
      "[D loss: 0.801739] [G loss: 1.403636]\n",
      "[D loss: 0.802349] [G loss: 1.316933]\n",
      "[D loss: 0.945026] [G loss: 1.283724]\n",
      "[D loss: 0.663986] [G loss: 1.352999]\n",
      "[D loss: 0.861201] [G loss: 1.491589]\n",
      "[D loss: 0.634790] [G loss: 1.633395]\n",
      "[D loss: 0.635514] [G loss: 1.651052]\n",
      "[D loss: 1.078706] [G loss: 1.458567]\n",
      "[D loss: 0.937802] [G loss: 1.500041]\n",
      "[D loss: 0.717776] [G loss: 1.528935]\n",
      "[D loss: 0.774774] [G loss: 1.310286]\n",
      "[D loss: 0.865160] [G loss: 1.466057]\n",
      "[D loss: 0.876717] [G loss: 1.391461]\n",
      "[D loss: 0.663660] [G loss: 1.434697]\n",
      "[D loss: 0.865345] [G loss: 1.439214]\n",
      "[D loss: 0.767230] [G loss: 1.492573]\n",
      "[D loss: 1.068385] [G loss: 1.538686]\n",
      "[D loss: 0.852420] [G loss: 1.180401]\n",
      "[D loss: 0.786436] [G loss: 1.515954]\n",
      "[D loss: 0.803287] [G loss: 1.690266]\n",
      "[D loss: 0.711008] [G loss: 1.510899]\n",
      "[D loss: 1.014852] [G loss: 1.466675]\n",
      "[D loss: 0.913997] [G loss: 1.421001]\n",
      "[D loss: 0.892557] [G loss: 1.204065]\n",
      "[D loss: 0.941704] [G loss: 1.313774]\n",
      "[D loss: 0.932487] [G loss: 1.490101]\n",
      "[D loss: 0.663266] [G loss: 1.531470]\n",
      "[D loss: 0.856058] [G loss: 1.607586]\n",
      "[D loss: 0.969184] [G loss: 1.445875]\n",
      "[D loss: 0.807051] [G loss: 1.388366]\n",
      "[D loss: 0.773770] [G loss: 1.155591]\n",
      "[D loss: 0.984080] [G loss: 1.541734]\n",
      "[D loss: 0.954005] [G loss: 1.325510]\n",
      "[D loss: 0.638942] [G loss: 1.690815]\n",
      "[D loss: 0.896831] [G loss: 1.486601]\n",
      "[D loss: 0.793828] [G loss: 1.293813]\n",
      "[D loss: 0.813110] [G loss: 1.367033]\n",
      "[D loss: 0.663338] [G loss: 1.601963]\n",
      "[D loss: 0.728146] [G loss: 1.377678]\n",
      "[D loss: 0.798274] [G loss: 1.530016]\n",
      "[D loss: 0.822866] [G loss: 1.539598]\n",
      "[D loss: 0.828742] [G loss: 1.447082]\n",
      "[D loss: 0.781625] [G loss: 1.471220]\n",
      "[D loss: 0.987378] [G loss: 1.407930]\n",
      "[D loss: 0.891725] [G loss: 1.628575]\n",
      "[D loss: 0.895248] [G loss: 1.509294]\n",
      "[D loss: 0.897359] [G loss: 1.234556]\n",
      "[D loss: 0.917503] [G loss: 1.370074]\n",
      "[D loss: 0.667358] [G loss: 1.694488]\n",
      "[D loss: 0.702344] [G loss: 1.536283]\n",
      "[D loss: 0.737744] [G loss: 1.372551]\n",
      "[D loss: 0.604328] [G loss: 1.492042]\n",
      "[D loss: 0.871640] [G loss: 1.530360]\n",
      "[D loss: 0.556363] [G loss: 1.582254]\n",
      "[D loss: 0.925955] [G loss: 1.621426]\n",
      "[D loss: 0.605816] [G loss: 1.503219]\n",
      "[D loss: 0.858305] [G loss: 1.350868]\n",
      "[D loss: 1.074172] [G loss: 1.524352]\n",
      "[D loss: 0.888607] [G loss: 1.402965]\n",
      "[D loss: 0.856788] [G loss: 1.489699]\n",
      "[D loss: 0.750110] [G loss: 1.533887]\n",
      "[D loss: 0.901430] [G loss: 1.535116]\n",
      "[D loss: 0.895888] [G loss: 1.484695]\n",
      "[D loss: 0.796941] [G loss: 1.338868]\n",
      "[D loss: 0.839656] [G loss: 1.554612]\n",
      "[D loss: 0.835776] [G loss: 1.412874]\n",
      "[D loss: 0.944831] [G loss: 1.354449]\n",
      "[D loss: 0.735762] [G loss: 1.508460]\n",
      "[D loss: 0.871664] [G loss: 1.606822]\n",
      "[D loss: 0.948586] [G loss: 1.473905]\n",
      "[D loss: 0.928322] [G loss: 1.561756]\n",
      "[D loss: 0.673375] [G loss: 1.449127]\n",
      "[D loss: 0.974201] [G loss: 1.352628]\n",
      "[D loss: 0.968431] [G loss: 1.281080]\n",
      "[D loss: 0.731847] [G loss: 1.302572]\n",
      "[D loss: 0.684311] [G loss: 1.640028]\n",
      "[D loss: 0.881766] [G loss: 1.571627]\n",
      "[D loss: 1.020597] [G loss: 1.368386]\n",
      "[D loss: 0.952047] [G loss: 1.504369]\n",
      "[D loss: 0.740458] [G loss: 1.584237]\n",
      "[D loss: 0.636370] [G loss: 1.545380]\n",
      "[D loss: 0.750533] [G loss: 1.526885]\n",
      "[D loss: 0.813288] [G loss: 1.725838]\n",
      "[D loss: 1.024212] [G loss: 1.348257]\n",
      "[D loss: 0.971895] [G loss: 1.378873]\n",
      "[D loss: 0.691782] [G loss: 1.335722]\n",
      "[D loss: 0.976879] [G loss: 1.469186]\n",
      "[D loss: 0.958147] [G loss: 1.339031]\n",
      "[D loss: 0.745028] [G loss: 1.668308]\n",
      "[D loss: 0.892349] [G loss: 1.495130]\n",
      "[D loss: 0.900965] [G loss: 1.398170]\n",
      "[D loss: 0.897456] [G loss: 1.416386]\n",
      "[D loss: 0.846708] [G loss: 1.455899]\n",
      "[D loss: 0.641537] [G loss: 1.470001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 1.049816] [G loss: 1.423880]\n",
      "[D loss: 0.651897] [G loss: 1.521024]\n",
      "[D loss: 1.037428] [G loss: 1.316905]\n",
      "[D loss: 0.762311] [G loss: 1.472427]\n",
      "[D loss: 0.735158] [G loss: 1.387841]\n",
      "[D loss: 0.770253] [G loss: 1.390936]\n",
      "[D loss: 0.965747] [G loss: 1.582261]\n",
      "[D loss: 0.880069] [G loss: 1.427077]\n",
      "[D loss: 1.010610] [G loss: 1.403439]\n",
      "[D loss: 1.152887] [G loss: 1.330486]\n",
      "[D loss: 0.809188] [G loss: 1.642466]\n",
      "[D loss: 0.950053] [G loss: 1.387326]\n",
      "[D loss: 0.840916] [G loss: 1.461277]\n",
      "[D loss: 0.873373] [G loss: 1.301216]\n",
      "[D loss: 0.813880] [G loss: 1.174872]\n",
      "[D loss: 0.729726] [G loss: 1.373688]\n",
      "[D loss: 0.709585] [G loss: 1.455776]\n",
      "[D loss: 0.860217] [G loss: 1.643251]\n",
      "[D loss: 0.639553] [G loss: 1.731324]\n",
      "[D loss: 0.706600] [G loss: 1.394637]\n",
      "[D loss: 0.829957] [G loss: 1.490813]\n",
      "[D loss: 0.718853] [G loss: 1.424426]\n",
      "[D loss: 0.785541] [G loss: 1.335860]\n",
      "[D loss: 0.910788] [G loss: 1.353909]\n",
      "[D loss: 0.770146] [G loss: 1.509588]\n",
      "[D loss: 0.923462] [G loss: 1.589584]\n",
      "[D loss: 1.100836] [G loss: 1.468344]\n",
      "[D loss: 0.798447] [G loss: 1.496527]\n",
      "[D loss: 0.814793] [G loss: 1.231027]\n",
      "[D loss: 0.722923] [G loss: 1.370691]\n",
      "[D loss: 0.984748] [G loss: 1.391951]\n",
      "[D loss: 0.852148] [G loss: 1.284213]\n",
      "[D loss: 0.949242] [G loss: 1.284160]\n",
      "[D loss: 0.820468] [G loss: 1.267383]\n",
      "[D loss: 0.770380] [G loss: 1.486399]\n",
      "[D loss: 0.753890] [G loss: 1.469668]\n",
      "[D loss: 0.728738] [G loss: 1.705459]\n",
      "[D loss: 0.873568] [G loss: 1.384365]\n",
      "[D loss: 0.780138] [G loss: 1.542193]\n",
      "[D loss: 1.050548] [G loss: 1.508280]\n",
      "[D loss: 0.880225] [G loss: 1.290104]\n",
      "[D loss: 1.004398] [G loss: 1.308133]\n",
      "[D loss: 0.769153] [G loss: 1.540538]\n",
      "[D loss: 0.832350] [G loss: 1.417362]\n",
      "[D loss: 0.720664] [G loss: 1.459799]\n",
      "[D loss: 0.598224] [G loss: 1.436323]\n",
      "[D loss: 0.686561] [G loss: 1.555701]\n",
      "[D loss: 1.068064] [G loss: 1.400948]\n",
      "[D loss: 0.594449] [G loss: 1.423379]\n",
      "[D loss: 0.771382] [G loss: 1.433312]\n",
      "[D loss: 0.991848] [G loss: 1.474142]\n",
      "[D loss: 0.674150] [G loss: 1.612233]\n",
      "[D loss: 0.849127] [G loss: 1.363823]\n",
      "[D loss: 0.742782] [G loss: 1.529014]\n",
      "[D loss: 0.786363] [G loss: 1.341851]\n",
      "[D loss: 0.666755] [G loss: 1.483948]\n",
      "[D loss: 1.136573] [G loss: 1.331162]\n",
      "[D loss: 1.068578] [G loss: 1.457109]\n",
      "[D loss: 0.815230] [G loss: 1.437733]\n",
      "[D loss: 0.612540] [G loss: 1.720220]\n",
      "[D loss: 0.819344] [G loss: 1.377157]\n",
      "[D loss: 0.733390] [G loss: 1.533598]\n",
      "[D loss: 0.659321] [G loss: 1.556711]\n",
      "[D loss: 0.850882] [G loss: 1.514509]\n",
      "[D loss: 0.819552] [G loss: 1.541209]\n",
      "[D loss: 1.119758] [G loss: 1.290851]\n",
      "[D loss: 0.806371] [G loss: 1.565378]\n",
      "[D loss: 0.884514] [G loss: 1.362797]\n",
      "[D loss: 0.655026] [G loss: 1.531713]\n",
      "[D loss: 0.600497] [G loss: 1.536182]\n",
      "[D loss: 0.917760] [G loss: 1.292740]\n",
      "[D loss: 0.873305] [G loss: 1.141736]\n",
      "[D loss: 0.918119] [G loss: 1.400902]\n",
      "[D loss: 0.937835] [G loss: 1.595738]\n",
      "[D loss: 0.627421] [G loss: 1.506174]\n",
      "[D loss: 0.727025] [G loss: 1.269971]\n",
      "[D loss: 0.741902] [G loss: 1.710474]\n",
      "[D loss: 0.717365] [G loss: 1.560678]\n",
      "[D loss: 0.834234] [G loss: 1.719394]\n",
      "[D loss: 1.012268] [G loss: 1.530762]\n",
      "[D loss: 1.043141] [G loss: 1.574618]\n",
      "[D loss: 0.990048] [G loss: 1.328314]\n",
      "[D loss: 0.713774] [G loss: 1.308397]\n",
      "[D loss: 0.820559] [G loss: 1.365652]\n",
      "[D loss: 0.973228] [G loss: 1.177752]\n",
      "[D loss: 0.801400] [G loss: 1.704466]\n",
      "[D loss: 0.812284] [G loss: 1.471903]\n",
      "[D loss: 0.863317] [G loss: 1.630236]\n",
      "[D loss: 0.864463] [G loss: 1.387983]\n",
      "[D loss: 0.796809] [G loss: 1.288928]\n",
      "[D loss: 0.697984] [G loss: 1.464546]\n",
      "[D loss: 0.913181] [G loss: 1.472710]\n",
      "[D loss: 0.891216] [G loss: 1.331584]\n",
      "[D loss: 0.775880] [G loss: 1.477831]\n",
      "[D loss: 0.790633] [G loss: 1.326087]\n",
      "[D loss: 0.835143] [G loss: 1.547981]\n",
      "[D loss: 1.028973] [G loss: 1.723085]\n",
      "[D loss: 0.848077] [G loss: 1.713219]\n",
      "[D loss: 0.882981] [G loss: 1.668717]\n",
      "[D loss: 0.667384] [G loss: 1.543188]\n",
      "[D loss: 0.904819] [G loss: 1.300079]\n",
      "[D loss: 0.894894] [G loss: 1.370704]\n",
      "[D loss: 1.022624] [G loss: 1.110177]\n",
      "[D loss: 0.894670] [G loss: 1.301405]\n",
      "[D loss: 0.752603] [G loss: 1.747049]\n",
      "[D loss: 0.728994] [G loss: 1.349892]\n",
      "[D loss: 0.510364] [G loss: 1.537656]\n",
      "[D loss: 0.905005] [G loss: 1.364750]\n",
      "[D loss: 0.727629] [G loss: 1.394756]\n",
      "[D loss: 0.627194] [G loss: 1.774065]\n",
      "[D loss: 1.151692] [G loss: 1.595847]\n",
      "[D loss: 0.612370] [G loss: 1.562793]\n",
      "[D loss: 0.723388] [G loss: 1.499177]\n",
      "[D loss: 0.774193] [G loss: 1.602760]\n",
      "[D loss: 0.874980] [G loss: 1.436080]\n",
      "[D loss: 0.734867] [G loss: 1.521292]\n",
      "[D loss: 0.750842] [G loss: 1.484205]\n",
      "[D loss: 1.071570] [G loss: 1.680980]\n",
      "[D loss: 0.698254] [G loss: 1.473134]\n",
      "[D loss: 0.814850] [G loss: 1.455440]\n",
      "[D loss: 1.165491] [G loss: 1.182560]\n",
      "[D loss: 0.647974] [G loss: 1.440808]\n",
      "[D loss: 0.873070] [G loss: 1.562527]\n",
      "[D loss: 0.722698] [G loss: 1.514165]\n",
      "[D loss: 0.883442] [G loss: 1.305612]\n",
      "[D loss: 0.869070] [G loss: 1.521026]\n",
      "[D loss: 1.044253] [G loss: 1.380832]\n",
      "[D loss: 0.822398] [G loss: 1.285932]\n",
      "[D loss: 0.701235] [G loss: 1.248319]\n",
      "[D loss: 1.104682] [G loss: 1.379541]\n",
      "[D loss: 0.679762] [G loss: 1.658527]\n",
      "[D loss: 0.922202] [G loss: 1.413201]\n",
      "[D loss: 0.909517] [G loss: 1.341953]\n",
      "[D loss: 0.951396] [G loss: 1.586292]\n",
      "[D loss: 0.746994] [G loss: 1.436579]\n",
      "[D loss: 0.871475] [G loss: 1.534469]\n",
      "[D loss: 0.908304] [G loss: 1.402456]\n",
      "[D loss: 0.792931] [G loss: 1.546996]\n",
      "[D loss: 1.112037] [G loss: 1.334573]\n",
      "[D loss: 0.610272] [G loss: 1.539594]\n",
      "[D loss: 0.908275] [G loss: 1.352129]\n",
      "[D loss: 0.783547] [G loss: 1.517636]\n",
      "[D loss: 0.876821] [G loss: 1.358676]\n",
      "[D loss: 0.796213] [G loss: 1.374947]\n",
      "[D loss: 0.823326] [G loss: 1.549225]\n",
      "[D loss: 0.811038] [G loss: 1.334663]\n",
      "[D loss: 0.814561] [G loss: 1.304647]\n",
      "[D loss: 0.609219] [G loss: 1.241656]\n",
      "[D loss: 0.825787] [G loss: 1.361198]\n",
      "[D loss: 0.872987] [G loss: 1.462819]\n",
      "[D loss: 0.803785] [G loss: 1.510785]\n",
      "[D loss: 0.517830] [G loss: 1.566736]\n",
      "[D loss: 0.891963] [G loss: 1.572378]\n",
      "[D loss: 0.809346] [G loss: 1.524257]\n",
      "[D loss: 0.736506] [G loss: 1.435075]\n",
      "[D loss: 0.824486] [G loss: 1.485334]\n",
      "[D loss: 0.811781] [G loss: 1.323894]\n",
      "[D loss: 0.852667] [G loss: 1.523977]\n",
      "[D loss: 0.750452] [G loss: 1.720098]\n",
      "[D loss: 1.010895] [G loss: 1.334421]\n",
      "[D loss: 0.647210] [G loss: 1.501227]\n",
      "[D loss: 0.951198] [G loss: 1.218752]\n",
      "[D loss: 0.932001] [G loss: 1.251426]\n",
      "[D loss: 0.880041] [G loss: 1.450167]\n",
      "[D loss: 0.927260] [G loss: 1.348274]\n",
      "[D loss: 1.053723] [G loss: 1.469838]\n",
      "[D loss: 0.718835] [G loss: 1.368142]\n",
      "[D loss: 0.907973] [G loss: 1.452908]\n",
      "[D loss: 0.838651] [G loss: 1.532807]\n",
      "[D loss: 1.005363] [G loss: 1.277366]\n",
      "[D loss: 1.112185] [G loss: 1.616678]\n",
      "[D loss: 0.964383] [G loss: 1.343583]\n",
      "[D loss: 0.913467] [G loss: 1.157123]\n",
      "[D loss: 0.849578] [G loss: 1.208003]\n",
      "[D loss: 0.752333] [G loss: 1.266221]\n",
      "[D loss: 0.675402] [G loss: 1.408658]\n",
      "[D loss: 0.817393] [G loss: 1.504247]\n",
      "[D loss: 0.876877] [G loss: 1.397761]\n",
      "[D loss: 0.794600] [G loss: 1.392722]\n",
      "[D loss: 0.829399] [G loss: 1.529428]\n",
      "[D loss: 0.774219] [G loss: 1.465183]\n",
      "[D loss: 0.789747] [G loss: 1.309039]\n",
      "[D loss: 0.756839] [G loss: 1.525484]\n",
      "[D loss: 0.973335] [G loss: 1.351976]\n",
      "[D loss: 0.838698] [G loss: 1.358896]\n",
      "[D loss: 0.798443] [G loss: 1.414980]\n",
      "[D loss: 0.688873] [G loss: 1.494078]\n",
      "[D loss: 0.901583] [G loss: 1.311086]\n",
      "[D loss: 0.971219] [G loss: 1.513574]\n",
      "[D loss: 0.791161] [G loss: 1.508518]\n",
      "[D loss: 0.820701] [G loss: 1.268634]\n",
      "[D loss: 0.873440] [G loss: 1.201290]\n",
      "[D loss: 0.790481] [G loss: 1.477584]\n",
      "[D loss: 0.829630] [G loss: 1.419935]\n",
      "[D loss: 0.785946] [G loss: 1.421632]\n",
      "[D loss: 1.004503] [G loss: 1.467693]\n",
      "[D loss: 0.892545] [G loss: 1.200542]\n",
      "[D loss: 0.769424] [G loss: 1.433693]\n",
      "[D loss: 0.913610] [G loss: 1.334833]\n",
      "[D loss: 0.736602] [G loss: 1.413500]\n",
      "[D loss: 0.838062] [G loss: 1.392630]\n",
      "[D loss: 0.697091] [G loss: 1.494530]\n",
      "[D loss: 0.908017] [G loss: 1.482633]\n",
      "[D loss: 0.872086] [G loss: 1.138558]\n",
      "[D loss: 0.757610] [G loss: 1.502190]\n",
      "[D loss: 0.960893] [G loss: 1.361891]\n",
      "[D loss: 0.753269] [G loss: 1.485528]\n",
      "[D loss: 0.843953] [G loss: 1.667573]\n",
      "[D loss: 0.849286] [G loss: 1.734214]\n",
      "[D loss: 0.702962] [G loss: 1.621951]\n",
      "[D loss: 0.861972] [G loss: 1.302586]\n",
      "[D loss: 0.744491] [G loss: 1.369181]\n",
      "[D loss: 0.829724] [G loss: 1.582019]\n",
      "[D loss: 0.797920] [G loss: 1.680117]\n",
      "[D loss: 0.855601] [G loss: 1.601092]\n",
      "[D loss: 0.740870] [G loss: 1.609032]\n",
      "[D loss: 0.745412] [G loss: 1.601552]\n",
      "[D loss: 0.755203] [G loss: 1.821229]\n",
      "[D loss: 0.699306] [G loss: 1.531428]\n",
      "[D loss: 0.681201] [G loss: 1.331606]\n",
      "[D loss: 0.996141] [G loss: 1.378879]\n",
      "[D loss: 0.914347] [G loss: 1.347729]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.853122] [G loss: 1.475211]\n",
      "[D loss: 0.714395] [G loss: 1.617812]\n",
      "[D loss: 0.750063] [G loss: 1.559333]\n",
      "[D loss: 0.637987] [G loss: 1.653023]\n",
      "[D loss: 1.142706] [G loss: 1.333072]\n",
      "[D loss: 0.859205] [G loss: 1.357103]\n",
      "[D loss: 0.650235] [G loss: 1.575799]\n",
      "[D loss: 0.793229] [G loss: 1.584492]\n",
      "[D loss: 0.719247] [G loss: 1.433575]\n",
      "[D loss: 0.764914] [G loss: 1.577115]\n",
      "[D loss: 0.865324] [G loss: 1.538894]\n",
      "[D loss: 0.713797] [G loss: 1.748323]\n",
      "[D loss: 0.916132] [G loss: 1.316542]\n",
      "[D loss: 0.936352] [G loss: 1.284653]\n",
      "[D loss: 0.789613] [G loss: 1.436990]\n",
      "[D loss: 0.815746] [G loss: 1.507752]\n",
      "[D loss: 0.959799] [G loss: 1.614590]\n",
      "[D loss: 1.012293] [G loss: 1.499171]\n",
      "[D loss: 0.828772] [G loss: 1.580821]\n",
      "[D loss: 0.613177] [G loss: 1.589942]\n",
      "[D loss: 0.962904] [G loss: 1.692037]\n",
      "[D loss: 0.825148] [G loss: 1.547659]\n",
      "[D loss: 0.866789] [G loss: 1.458098]\n",
      "[D loss: 0.917163] [G loss: 1.507721]\n",
      "[D loss: 0.687241] [G loss: 1.291562]\n",
      "[D loss: 0.834799] [G loss: 1.391485]\n",
      "[D loss: 0.713594] [G loss: 1.499518]\n",
      "[D loss: 0.894020] [G loss: 1.560690]\n",
      "[D loss: 0.644771] [G loss: 1.624992]\n",
      "[D loss: 0.861528] [G loss: 1.319549]\n",
      "[D loss: 0.634084] [G loss: 1.603352]\n",
      "[D loss: 1.015272] [G loss: 1.483755]\n",
      "[D loss: 0.715097] [G loss: 1.607656]\n",
      "[D loss: 0.886531] [G loss: 1.708198]\n",
      "[D loss: 0.870954] [G loss: 1.497818]\n",
      "[D loss: 0.876239] [G loss: 1.188254]\n",
      "[D loss: 0.923993] [G loss: 1.486250]\n",
      "[D loss: 0.870343] [G loss: 1.489158]\n",
      "[D loss: 0.704285] [G loss: 1.478318]\n",
      "[D loss: 0.691811] [G loss: 1.346090]\n",
      "[D loss: 0.678954] [G loss: 1.471689]\n",
      "[D loss: 0.871605] [G loss: 1.378677]\n",
      "[D loss: 0.710423] [G loss: 1.554018]\n",
      "[D loss: 0.809645] [G loss: 1.468400]\n",
      "[D loss: 0.888099] [G loss: 1.531230]\n",
      "[D loss: 0.796860] [G loss: 1.788629]\n",
      "[D loss: 0.933650] [G loss: 1.396966]\n",
      "[D loss: 0.888507] [G loss: 1.714756]\n",
      "[D loss: 0.937244] [G loss: 1.580835]\n",
      "[D loss: 0.674083] [G loss: 1.587520]\n",
      "[D loss: 0.889950] [G loss: 1.473875]\n",
      "[D loss: 0.789809] [G loss: 1.554139]\n",
      "[D loss: 0.814084] [G loss: 1.554652]\n",
      "[D loss: 0.996555] [G loss: 1.467111]\n",
      "[D loss: 0.873950] [G loss: 1.686957]\n",
      "[D loss: 0.986188] [G loss: 1.597104]\n",
      "[D loss: 0.814126] [G loss: 1.536202]\n",
      "[D loss: 0.917821] [G loss: 1.263780]\n",
      "[D loss: 1.258483] [G loss: 1.134010]\n",
      "[D loss: 1.177427] [G loss: 1.467325]\n",
      "[D loss: 0.817533] [G loss: 1.244876]\n",
      "[D loss: 0.850704] [G loss: 1.328555]\n",
      "[D loss: 0.945541] [G loss: 1.480346]\n",
      "[D loss: 1.062936] [G loss: 1.301261]\n",
      "[D loss: 0.843899] [G loss: 1.330675]\n",
      "[D loss: 0.898460] [G loss: 1.441447]\n",
      "[D loss: 0.932225] [G loss: 1.412746]\n",
      "[D loss: 0.936266] [G loss: 1.223909]\n",
      "[D loss: 0.790411] [G loss: 1.336933]\n",
      "[D loss: 0.920769] [G loss: 1.297794]\n",
      "[D loss: 0.941778] [G loss: 1.281577]\n",
      "[D loss: 0.671372] [G loss: 1.266513]\n",
      "[D loss: 0.938397] [G loss: 1.437565]\n",
      "[D loss: 0.531426] [G loss: 1.492875]\n",
      "[D loss: 0.686107] [G loss: 1.616286]\n",
      "[D loss: 0.781335] [G loss: 1.698015]\n",
      "[D loss: 0.764631] [G loss: 1.567693]\n",
      "[D loss: 0.795454] [G loss: 1.431092]\n",
      "[D loss: 0.926593] [G loss: 1.325862]\n",
      "[D loss: 0.719705] [G loss: 1.528853]\n",
      "[D loss: 1.150612] [G loss: 1.382724]\n",
      "[D loss: 1.127333] [G loss: 1.279050]\n",
      "[D loss: 0.825362] [G loss: 1.360748]\n",
      "[D loss: 0.812616] [G loss: 1.277038]\n",
      "[D loss: 0.982875] [G loss: 1.430149]\n",
      "[D loss: 0.728258] [G loss: 1.344648]\n",
      "[D loss: 0.723196] [G loss: 1.506016]\n",
      "[D loss: 0.923107] [G loss: 1.476627]\n",
      "[D loss: 1.015484] [G loss: 1.304283]\n",
      "[D loss: 0.673173] [G loss: 1.546324]\n",
      "[D loss: 0.827363] [G loss: 1.127018]\n",
      "[D loss: 0.737622] [G loss: 1.336864]\n",
      "[D loss: 1.101922] [G loss: 1.406418]\n",
      "[D loss: 0.765987] [G loss: 1.696267]\n",
      "[D loss: 0.769761] [G loss: 1.483393]\n",
      "[D loss: 0.883379] [G loss: 1.400033]\n",
      "[D loss: 1.021704] [G loss: 1.377661]\n",
      "[D loss: 0.944059] [G loss: 1.286358]\n",
      "[D loss: 0.758332] [G loss: 1.516724]\n",
      "[D loss: 0.629390] [G loss: 1.622058]\n",
      "[D loss: 0.712334] [G loss: 1.361886]\n",
      "[D loss: 0.962816] [G loss: 1.514114]\n",
      "[D loss: 0.918047] [G loss: 1.333822]\n",
      "[D loss: 0.844158] [G loss: 1.409580]\n",
      "[D loss: 0.925011] [G loss: 1.484785]\n",
      "[D loss: 0.902342] [G loss: 1.290391]\n",
      "[D loss: 0.969385] [G loss: 1.369081]\n",
      "[D loss: 0.824885] [G loss: 1.309333]\n",
      "[D loss: 1.005922] [G loss: 1.379847]\n",
      "[D loss: 0.646858] [G loss: 1.349845]\n",
      "[D loss: 0.955987] [G loss: 1.504736]\n",
      "[D loss: 0.844667] [G loss: 1.305868]\n",
      "[D loss: 0.786037] [G loss: 1.430587]\n",
      "[D loss: 0.815674] [G loss: 1.491226]\n",
      "[D loss: 0.955227] [G loss: 1.411568]\n",
      "[D loss: 0.707775] [G loss: 1.545414]\n",
      "[D loss: 1.054463] [G loss: 1.261075]\n",
      "[D loss: 0.955641] [G loss: 1.399499]\n",
      "[D loss: 0.896068] [G loss: 1.450856]\n",
      "[D loss: 0.966568] [G loss: 1.220947]\n",
      "[D loss: 0.862481] [G loss: 1.330112]\n",
      "[D loss: 1.021818] [G loss: 1.452783]\n",
      "[D loss: 0.953612] [G loss: 1.426077]\n",
      "[D loss: 0.955600] [G loss: 1.223803]\n",
      "[D loss: 0.621771] [G loss: 1.411566]\n",
      "[D loss: 0.886424] [G loss: 1.234310]\n",
      "[D loss: 0.712094] [G loss: 1.283881]\n",
      "[D loss: 0.829328] [G loss: 1.336604]\n",
      "[D loss: 0.812676] [G loss: 1.363191]\n",
      "[D loss: 0.805196] [G loss: 1.568353]\n",
      "[D loss: 0.879971] [G loss: 1.365227]\n",
      "[D loss: 0.740414] [G loss: 1.273009]\n",
      "[D loss: 0.705119] [G loss: 1.468068]\n",
      "[D loss: 0.768118] [G loss: 1.501099]\n",
      "[D loss: 0.843266] [G loss: 1.439167]\n",
      "[D loss: 0.968253] [G loss: 1.428666]\n",
      "[D loss: 1.020105] [G loss: 1.391057]\n",
      "[D loss: 0.731010] [G loss: 1.375512]\n",
      "[D loss: 0.709414] [G loss: 1.392422]\n",
      "[D loss: 0.886251] [G loss: 1.411943]\n",
      "[D loss: 0.876425] [G loss: 1.427826]\n",
      "[D loss: 0.703139] [G loss: 1.446375]\n",
      "[D loss: 0.739277] [G loss: 1.352499]\n",
      "[D loss: 0.720839] [G loss: 1.442669]\n",
      "[D loss: 0.636574] [G loss: 1.748949]\n",
      "[D loss: 0.729066] [G loss: 1.745758]\n",
      "[D loss: 0.799241] [G loss: 1.393131]\n",
      "[D loss: 0.844465] [G loss: 1.576908]\n",
      "[D loss: 0.693579] [G loss: 1.645780]\n",
      "[D loss: 0.928600] [G loss: 1.419472]\n",
      "[D loss: 0.864185] [G loss: 1.443549]\n",
      "[D loss: 0.738312] [G loss: 1.746086]\n",
      "[D loss: 0.762356] [G loss: 1.349079]\n",
      "[D loss: 0.994789] [G loss: 1.212279]\n",
      "[D loss: 0.962092] [G loss: 1.378800]\n",
      "[D loss: 0.798079] [G loss: 1.492924]\n",
      "[D loss: 0.802342] [G loss: 1.335727]\n",
      "[D loss: 1.154403] [G loss: 1.272466]\n",
      "[D loss: 0.764293] [G loss: 1.634448]\n",
      "[D loss: 0.908173] [G loss: 1.442874]\n",
      "[D loss: 0.728126] [G loss: 1.903739]\n",
      "[D loss: 0.754702] [G loss: 1.661816]\n",
      "[D loss: 0.898640] [G loss: 1.417658]\n",
      "[D loss: 0.842503] [G loss: 1.451187]\n",
      "[D loss: 0.857257] [G loss: 1.371137]\n",
      "[D loss: 0.802763] [G loss: 1.336670]\n",
      "[D loss: 0.720011] [G loss: 1.380325]\n",
      "[D loss: 0.981618] [G loss: 1.503608]\n",
      "[D loss: 0.853582] [G loss: 1.532270]\n",
      "[D loss: 0.746593] [G loss: 1.628120]\n",
      "[D loss: 0.857192] [G loss: 1.462073]\n",
      "[D loss: 0.656485] [G loss: 1.524628]\n",
      "[D loss: 0.912993] [G loss: 1.234761]\n",
      "[D loss: 0.872399] [G loss: 1.432885]\n",
      "[D loss: 0.756770] [G loss: 1.417118]\n",
      "[D loss: 0.803241] [G loss: 1.379486]\n",
      "[D loss: 0.925852] [G loss: 1.410640]\n",
      "[D loss: 1.129084] [G loss: 1.160491]\n",
      "[D loss: 0.777494] [G loss: 1.198839]\n",
      "[D loss: 0.734317] [G loss: 1.513563]\n",
      "[D loss: 0.892939] [G loss: 1.398371]\n",
      "[D loss: 0.930026] [G loss: 1.475726]\n",
      "[D loss: 0.791714] [G loss: 1.304005]\n",
      "[D loss: 1.003836] [G loss: 1.315889]\n",
      "[D loss: 0.653104] [G loss: 1.412232]\n",
      "[D loss: 1.035501] [G loss: 1.449374]\n",
      "[D loss: 0.689789] [G loss: 1.509373]\n",
      "[D loss: 0.990526] [G loss: 1.536981]\n",
      "[D loss: 0.770230] [G loss: 1.452636]\n",
      "[D loss: 1.022731] [G loss: 1.273923]\n",
      "[D loss: 0.791086] [G loss: 1.461325]\n",
      "[D loss: 0.781045] [G loss: 1.377526]\n",
      "[D loss: 0.860202] [G loss: 1.225540]\n",
      "[D loss: 0.794364] [G loss: 1.331271]\n",
      "[D loss: 0.735241] [G loss: 1.451753]\n",
      "[D loss: 0.885762] [G loss: 1.368320]\n",
      "[D loss: 0.747510] [G loss: 1.421029]\n",
      "[D loss: 0.886767] [G loss: 1.459047]\n",
      "[D loss: 0.653942] [G loss: 1.495276]\n",
      "[D loss: 1.105172] [G loss: 1.319014]\n",
      "[D loss: 0.843085] [G loss: 1.638577]\n",
      "[D loss: 0.915769] [G loss: 1.381574]\n",
      "[D loss: 0.708163] [G loss: 1.552225]\n",
      "[D loss: 0.805654] [G loss: 1.338621]\n",
      "[D loss: 0.945033] [G loss: 1.598942]\n",
      "[D loss: 0.802369] [G loss: 1.491596]\n",
      "[D loss: 1.108346] [G loss: 1.398266]\n",
      "[D loss: 0.838252] [G loss: 1.565618]\n",
      "[D loss: 0.840807] [G loss: 1.533799]\n",
      "[D loss: 1.065442] [G loss: 1.555198]\n",
      "[D loss: 0.805430] [G loss: 1.659673]\n",
      "[D loss: 0.776061] [G loss: 1.594015]\n",
      "[D loss: 0.822863] [G loss: 1.295669]\n",
      "[D loss: 0.685962] [G loss: 1.346571]\n",
      "[D loss: 0.819230] [G loss: 1.400301]\n",
      "[D loss: 0.776137] [G loss: 1.543640]\n",
      "[D loss: 0.845550] [G loss: 1.554135]\n",
      "[D loss: 0.847360] [G loss: 1.601161]\n",
      "[D loss: 0.840481] [G loss: 1.477708]\n",
      "[D loss: 0.788455] [G loss: 1.603615]\n",
      "[D loss: 0.801919] [G loss: 1.488058]\n",
      "[D loss: 0.802071] [G loss: 1.441168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.507947] [G loss: 1.616127]\n",
      "[D loss: 0.758174] [G loss: 1.600902]\n",
      "[D loss: 0.608541] [G loss: 1.516753]\n",
      "[D loss: 0.637847] [G loss: 1.605344]\n",
      "[D loss: 0.927924] [G loss: 1.258155]\n",
      "[D loss: 0.790330] [G loss: 1.483307]\n",
      "[D loss: 1.113371] [G loss: 1.491671]\n",
      "[D loss: 0.983346] [G loss: 1.429485]\n",
      "[D loss: 0.862276] [G loss: 1.663940]\n",
      "[D loss: 0.970946] [G loss: 1.563659]\n",
      "[D loss: 0.840897] [G loss: 1.425022]\n",
      "[D loss: 0.828738] [G loss: 1.419255]\n",
      "[D loss: 0.809517] [G loss: 1.283734]\n",
      "[D loss: 0.853199] [G loss: 1.211698]\n",
      "[D loss: 1.123323] [G loss: 1.447731]\n",
      "[D loss: 1.127428] [G loss: 1.308531]\n",
      "[D loss: 0.822870] [G loss: 1.423071]\n",
      "[D loss: 0.533054] [G loss: 1.554183]\n",
      "[D loss: 0.724088] [G loss: 1.589956]\n",
      "[D loss: 0.902600] [G loss: 1.266201]\n",
      "[D loss: 0.679450] [G loss: 1.537240]\n",
      "[D loss: 0.769596] [G loss: 1.592236]\n",
      "[D loss: 0.969062] [G loss: 1.422696]\n",
      "[D loss: 0.779385] [G loss: 1.875433]\n",
      "[D loss: 0.777076] [G loss: 1.618356]\n",
      "[D loss: 0.753553] [G loss: 1.534046]\n",
      "[D loss: 1.017706] [G loss: 1.300555]\n",
      "[D loss: 0.945098] [G loss: 1.372639]\n",
      "[D loss: 0.739956] [G loss: 1.426681]\n",
      "[D loss: 0.952493] [G loss: 1.426102]\n",
      "[D loss: 0.959887] [G loss: 1.495375]\n",
      "[D loss: 0.642497] [G loss: 1.485847]\n",
      "[D loss: 0.806381] [G loss: 1.514477]\n",
      "[D loss: 1.004599] [G loss: 1.448487]\n",
      "[D loss: 0.757220] [G loss: 1.551520]\n",
      "[D loss: 0.911861] [G loss: 1.361287]\n",
      "[D loss: 0.809049] [G loss: 1.553916]\n",
      "[D loss: 0.760699] [G loss: 1.362968]\n",
      "[D loss: 0.816365] [G loss: 1.544302]\n",
      "[D loss: 0.845567] [G loss: 1.346939]\n",
      "[D loss: 0.906726] [G loss: 1.445712]\n",
      "[D loss: 0.806491] [G loss: 1.534123]\n",
      "[D loss: 0.794497] [G loss: 1.518733]\n",
      "[D loss: 0.561597] [G loss: 1.608408]\n",
      "[D loss: 0.846223] [G loss: 1.241121]\n",
      "[D loss: 0.976884] [G loss: 1.595517]\n",
      "[D loss: 0.943876] [G loss: 1.517161]\n",
      "[D loss: 0.820020] [G loss: 1.735272]\n",
      "[D loss: 1.179586] [G loss: 1.590123]\n",
      "[D loss: 0.721889] [G loss: 1.571126]\n",
      "[D loss: 0.862800] [G loss: 1.314299]\n",
      "[D loss: 0.751764] [G loss: 1.414545]\n",
      "[D loss: 0.785684] [G loss: 1.587067]\n",
      "[D loss: 0.610189] [G loss: 1.508505]\n",
      "[D loss: 0.916267] [G loss: 1.425146]\n",
      "[D loss: 0.730392] [G loss: 1.303564]\n",
      "[D loss: 0.733442] [G loss: 1.434424]\n",
      "[D loss: 0.964437] [G loss: 1.449180]\n",
      "[D loss: 1.081225] [G loss: 1.323696]\n",
      "[D loss: 0.894093] [G loss: 1.526407]\n",
      "[D loss: 0.877187] [G loss: 1.407850]\n",
      "[D loss: 1.005284] [G loss: 1.401747]\n",
      "[D loss: 0.909882] [G loss: 1.461532]\n",
      "[D loss: 0.773950] [G loss: 1.262723]\n",
      "[D loss: 1.134235] [G loss: 1.188489]\n",
      "[D loss: 0.964829] [G loss: 1.382701]\n",
      "[D loss: 0.714641] [G loss: 1.484454]\n",
      "[D loss: 0.611227] [G loss: 1.507762]\n",
      "[D loss: 0.933457] [G loss: 1.434436]\n",
      "[D loss: 0.905566] [G loss: 1.326927]\n",
      "[D loss: 0.954851] [G loss: 1.434013]\n",
      "[D loss: 0.865790] [G loss: 1.326112]\n",
      "[D loss: 0.664158] [G loss: 1.485111]\n",
      "[D loss: 0.804093] [G loss: 1.447838]\n",
      "[D loss: 0.687401] [G loss: 1.611558]\n",
      "[D loss: 1.243739] [G loss: 1.350865]\n",
      "[D loss: 0.952219] [G loss: 1.446527]\n",
      "[D loss: 0.634639] [G loss: 1.423115]\n",
      "[D loss: 0.855772] [G loss: 1.174966]\n",
      "[D loss: 0.667603] [G loss: 1.484370]\n",
      "[D loss: 1.092158] [G loss: 1.451946]\n",
      "[D loss: 0.758380] [G loss: 1.500825]\n",
      "[D loss: 0.672438] [G loss: 1.616647]\n",
      "[D loss: 0.892193] [G loss: 1.521146]\n",
      "[D loss: 0.802231] [G loss: 1.468611]\n",
      "[D loss: 0.706601] [G loss: 1.598309]\n",
      "[D loss: 0.687931] [G loss: 1.337889]\n",
      "[D loss: 0.734934] [G loss: 1.494329]\n",
      "[D loss: 0.774516] [G loss: 1.427327]\n",
      "[D loss: 1.077509] [G loss: 1.336321]\n",
      "[D loss: 0.935097] [G loss: 1.713151]\n",
      "[D loss: 0.742272] [G loss: 1.602262]\n",
      "[D loss: 0.929542] [G loss: 1.543745]\n",
      "[D loss: 0.830213] [G loss: 1.446704]\n",
      "[D loss: 0.745472] [G loss: 1.425376]\n",
      "[D loss: 0.673057] [G loss: 1.456125]\n",
      "[D loss: 0.728494] [G loss: 1.416915]\n",
      "[D loss: 0.894478] [G loss: 1.566316]\n",
      "[D loss: 0.930398] [G loss: 1.394480]\n",
      "[D loss: 0.960800] [G loss: 1.458359]\n",
      "[D loss: 1.192837] [G loss: 1.259079]\n",
      "[D loss: 0.827159] [G loss: 1.316213]\n",
      "[D loss: 0.794958] [G loss: 1.643903]\n",
      "[D loss: 1.003677] [G loss: 1.356294]\n",
      "[D loss: 1.034043] [G loss: 1.556076]\n",
      "[D loss: 0.683257] [G loss: 1.719680]\n",
      "[D loss: 0.595634] [G loss: 1.596124]\n",
      "[D loss: 0.803888] [G loss: 1.479450]\n",
      "[D loss: 0.820318] [G loss: 1.258837]\n",
      "[D loss: 0.979424] [G loss: 1.287509]\n",
      "[D loss: 0.968062] [G loss: 1.569085]\n",
      "[D loss: 0.726378] [G loss: 1.290964]\n",
      "[D loss: 0.824974] [G loss: 1.334700]\n",
      "[D loss: 0.703045] [G loss: 1.293468]\n",
      "[D loss: 0.942632] [G loss: 1.313306]\n",
      "[D loss: 0.590683] [G loss: 1.482930]\n",
      "[D loss: 0.584458] [G loss: 1.275178]\n",
      "[D loss: 0.802371] [G loss: 1.512776]\n",
      "[D loss: 1.005248] [G loss: 1.566232]\n",
      "[D loss: 0.901617] [G loss: 1.354060]\n",
      "[D loss: 0.728850] [G loss: 1.353454]\n",
      "[D loss: 0.884841] [G loss: 1.312845]\n",
      "[D loss: 0.801742] [G loss: 1.417035]\n",
      "[D loss: 0.935843] [G loss: 1.284151]\n",
      "[D loss: 0.751656] [G loss: 1.584895]\n",
      "[D loss: 0.625424] [G loss: 1.437510]\n",
      "[D loss: 1.023223] [G loss: 1.595757]\n",
      "[D loss: 0.891008] [G loss: 1.637851]\n",
      "[D loss: 0.973846] [G loss: 1.527959]\n",
      "[D loss: 0.737092] [G loss: 1.509170]\n",
      "[D loss: 0.779901] [G loss: 1.235825]\n",
      "[D loss: 0.760641] [G loss: 1.485398]\n",
      "[D loss: 0.652968] [G loss: 1.441283]\n",
      "[D loss: 0.919698] [G loss: 1.373981]\n",
      "[D loss: 0.946550] [G loss: 1.342235]\n",
      "[D loss: 0.978072] [G loss: 1.388664]\n",
      "[D loss: 0.716989] [G loss: 1.451295]\n",
      "[D loss: 0.856764] [G loss: 1.509673]\n",
      "[D loss: 0.703074] [G loss: 1.509427]\n",
      "[D loss: 1.131131] [G loss: 1.484792]\n",
      "[D loss: 0.700104] [G loss: 1.410486]\n",
      "[D loss: 0.797356] [G loss: 1.454429]\n",
      "[D loss: 1.029880] [G loss: 1.406887]\n",
      "[D loss: 0.829330] [G loss: 1.540502]\n",
      "[D loss: 0.786327] [G loss: 1.436627]\n",
      "[D loss: 0.932551] [G loss: 1.333283]\n",
      "[D loss: 1.117860] [G loss: 1.371740]\n",
      "[D loss: 0.779955] [G loss: 1.440256]\n",
      "[D loss: 0.780703] [G loss: 1.703891]\n",
      "[D loss: 0.769976] [G loss: 1.862725]\n",
      "[D loss: 0.834479] [G loss: 1.465354]\n",
      "[D loss: 0.758676] [G loss: 1.571467]\n",
      "[D loss: 0.849788] [G loss: 1.568602]\n",
      "[D loss: 0.909078] [G loss: 1.559247]\n",
      "[D loss: 1.174668] [G loss: 1.253377]\n",
      "[D loss: 0.681952] [G loss: 1.448466]\n",
      "[D loss: 0.893764] [G loss: 1.293802]\n",
      "[D loss: 1.085856] [G loss: 1.324772]\n",
      "[D loss: 0.864142] [G loss: 1.359765]\n",
      "[D loss: 0.877581] [G loss: 1.461541]\n",
      "[D loss: 0.829131] [G loss: 1.489300]\n",
      "[D loss: 0.821229] [G loss: 1.489184]\n",
      "[D loss: 1.063318] [G loss: 1.333676]\n",
      "[D loss: 0.546962] [G loss: 1.504126]\n",
      "[D loss: 0.975578] [G loss: 1.417382]\n",
      "[D loss: 0.932697] [G loss: 1.462942]\n",
      "[D loss: 0.658226] [G loss: 1.368912]\n",
      "[D loss: 0.946339] [G loss: 1.384502]\n",
      "[D loss: 0.866374] [G loss: 1.306640]\n",
      "[D loss: 0.838510] [G loss: 1.417895]\n",
      "[D loss: 0.882917] [G loss: 1.264893]\n",
      "[D loss: 0.753008] [G loss: 1.250462]\n",
      "[D loss: 0.788832] [G loss: 1.420284]\n",
      "[D loss: 0.715526] [G loss: 1.264843]\n",
      "[D loss: 1.001820] [G loss: 1.626244]\n",
      "[D loss: 1.010104] [G loss: 1.384522]\n",
      "[D loss: 0.893639] [G loss: 1.506124]\n",
      "[D loss: 0.677150] [G loss: 1.463793]\n",
      "[D loss: 0.808069] [G loss: 1.268903]\n",
      "[D loss: 0.679919] [G loss: 1.321519]\n",
      "[D loss: 0.560884] [G loss: 1.450317]\n",
      "[D loss: 0.782657] [G loss: 1.395199]\n",
      "[D loss: 0.877610] [G loss: 1.473285]\n",
      "[D loss: 0.859907] [G loss: 1.607706]\n",
      "[D loss: 0.769151] [G loss: 1.445682]\n",
      "[D loss: 0.971627] [G loss: 1.492216]\n",
      "[D loss: 0.885591] [G loss: 1.233824]\n",
      "[D loss: 0.810205] [G loss: 1.474891]\n",
      "[D loss: 0.693859] [G loss: 1.382207]\n",
      "[D loss: 0.833686] [G loss: 1.342528]\n",
      "[D loss: 0.941463] [G loss: 1.415916]\n",
      "[D loss: 0.726923] [G loss: 1.570802]\n",
      "[D loss: 0.927870] [G loss: 1.522592]\n",
      "[D loss: 0.826725] [G loss: 1.398073]\n",
      "[D loss: 0.892916] [G loss: 1.372365]\n",
      "[D loss: 0.892133] [G loss: 1.355752]\n",
      "[D loss: 0.883724] [G loss: 1.457998]\n",
      "[D loss: 0.859265] [G loss: 1.459753]\n",
      "[D loss: 0.921762] [G loss: 1.410103]\n",
      "[D loss: 1.025955] [G loss: 1.393924]\n",
      "[D loss: 0.757703] [G loss: 1.489392]\n",
      "[D loss: 0.835145] [G loss: 1.464707]\n",
      "[D loss: 0.937545] [G loss: 1.294888]\n",
      "[D loss: 0.812650] [G loss: 1.386827]\n",
      "[D loss: 0.825962] [G loss: 1.343816]\n",
      "[D loss: 0.964828] [G loss: 1.195611]\n",
      "[D loss: 0.694052] [G loss: 1.389769]\n",
      "[D loss: 0.691299] [G loss: 1.414992]\n",
      "[D loss: 0.761747] [G loss: 1.620675]\n",
      "[D loss: 0.719191] [G loss: 1.446960]\n",
      "[D loss: 0.917734] [G loss: 1.316018]\n",
      "[D loss: 0.794987] [G loss: 1.338825]\n",
      "[D loss: 0.756511] [G loss: 1.361559]\n",
      "[D loss: 0.771837] [G loss: 1.591799]\n",
      "[D loss: 0.825275] [G loss: 1.316592]\n",
      "[D loss: 0.779420] [G loss: 1.712050]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.673000] [G loss: 1.637372]\n",
      "[D loss: 0.834404] [G loss: 1.421970]\n",
      "[D loss: 0.704855] [G loss: 1.741146]\n",
      "[D loss: 0.796926] [G loss: 1.306985]\n",
      "[D loss: 0.925697] [G loss: 1.682537]\n",
      "[D loss: 0.826338] [G loss: 1.729944]\n",
      "[D loss: 0.774777] [G loss: 1.674697]\n",
      "[D loss: 0.814210] [G loss: 1.748162]\n",
      "[D loss: 0.941753] [G loss: 1.456826]\n",
      "[D loss: 0.825293] [G loss: 1.616975]\n",
      "[D loss: 0.894871] [G loss: 1.478554]\n",
      "[D loss: 0.816196] [G loss: 1.213857]\n",
      "[D loss: 0.716316] [G loss: 1.494735]\n",
      "[D loss: 0.685062] [G loss: 1.492399]\n",
      "[D loss: 0.889545] [G loss: 1.455387]\n",
      "[D loss: 0.868945] [G loss: 1.336623]\n",
      "epoch:12, g_loss:2727.732421875,d_loss:1567.2000732421875\n",
      "[D loss: 0.978086] [G loss: 1.307847]\n",
      "[D loss: 0.953631] [G loss: 1.409937]\n",
      "[D loss: 0.729051] [G loss: 1.337006]\n",
      "[D loss: 0.837810] [G loss: 1.486316]\n",
      "[D loss: 0.928841] [G loss: 1.622958]\n",
      "[D loss: 0.792378] [G loss: 1.529559]\n",
      "[D loss: 0.744847] [G loss: 1.304133]\n",
      "[D loss: 0.673877] [G loss: 1.540450]\n",
      "[D loss: 0.611743] [G loss: 1.595659]\n",
      "[D loss: 0.643964] [G loss: 1.374817]\n",
      "[D loss: 0.754363] [G loss: 1.439548]\n",
      "[D loss: 0.833642] [G loss: 1.711702]\n",
      "[D loss: 0.880115] [G loss: 1.466434]\n",
      "[D loss: 0.708332] [G loss: 1.477418]\n",
      "[D loss: 0.881768] [G loss: 1.443105]\n",
      "[D loss: 0.531134] [G loss: 1.854468]\n",
      "[D loss: 0.736826] [G loss: 1.645076]\n",
      "[D loss: 0.713876] [G loss: 1.384657]\n",
      "[D loss: 0.885087] [G loss: 1.286672]\n",
      "[D loss: 0.995316] [G loss: 1.608529]\n",
      "[D loss: 0.746046] [G loss: 1.639525]\n",
      "[D loss: 0.641040] [G loss: 1.497010]\n",
      "[D loss: 0.822997] [G loss: 1.626172]\n",
      "[D loss: 0.925614] [G loss: 1.519136]\n",
      "[D loss: 0.871242] [G loss: 1.395649]\n",
      "[D loss: 0.851063] [G loss: 1.531940]\n",
      "[D loss: 0.735801] [G loss: 1.556128]\n",
      "[D loss: 0.843114] [G loss: 1.438381]\n",
      "[D loss: 1.030587] [G loss: 1.453378]\n",
      "[D loss: 0.813872] [G loss: 1.380818]\n",
      "[D loss: 0.750454] [G loss: 1.382261]\n",
      "[D loss: 0.767363] [G loss: 1.491195]\n",
      "[D loss: 0.949249] [G loss: 1.457011]\n",
      "[D loss: 0.826905] [G loss: 1.553638]\n",
      "[D loss: 0.884652] [G loss: 1.446817]\n",
      "[D loss: 0.659868] [G loss: 1.461633]\n",
      "[D loss: 0.806544] [G loss: 1.359628]\n",
      "[D loss: 0.765633] [G loss: 1.459574]\n",
      "[D loss: 1.015156] [G loss: 1.510646]\n",
      "[D loss: 0.817312] [G loss: 1.695476]\n",
      "[D loss: 0.873877] [G loss: 1.623448]\n",
      "[D loss: 0.914158] [G loss: 1.608721]\n",
      "[D loss: 0.969215] [G loss: 1.368811]\n",
      "[D loss: 0.870380] [G loss: 1.388737]\n",
      "[D loss: 0.717061] [G loss: 1.623457]\n",
      "[D loss: 0.868315] [G loss: 1.401930]\n",
      "[D loss: 0.880370] [G loss: 1.449267]\n",
      "[D loss: 0.864396] [G loss: 1.416571]\n",
      "[D loss: 0.935627] [G loss: 1.510479]\n",
      "[D loss: 1.002514] [G loss: 1.558071]\n",
      "[D loss: 0.924753] [G loss: 1.321450]\n",
      "[D loss: 0.909787] [G loss: 1.318712]\n",
      "[D loss: 0.875936] [G loss: 1.489665]\n",
      "[D loss: 0.746381] [G loss: 1.254918]\n",
      "[D loss: 0.538198] [G loss: 1.442330]\n",
      "[D loss: 0.699561] [G loss: 1.548377]\n",
      "[D loss: 0.851399] [G loss: 1.601351]\n",
      "[D loss: 0.899238] [G loss: 1.567986]\n",
      "[D loss: 0.710057] [G loss: 1.252214]\n",
      "[D loss: 0.606926] [G loss: 1.444430]\n",
      "[D loss: 0.862370] [G loss: 1.548658]\n",
      "[D loss: 0.779473] [G loss: 1.407139]\n",
      "[D loss: 0.757072] [G loss: 1.978865]\n",
      "[D loss: 0.808705] [G loss: 1.628964]\n",
      "[D loss: 1.005738] [G loss: 1.351964]\n",
      "[D loss: 0.653942] [G loss: 1.543475]\n",
      "[D loss: 0.985589] [G loss: 1.368753]\n",
      "[D loss: 0.710763] [G loss: 1.428982]\n",
      "[D loss: 0.633693] [G loss: 1.503232]\n",
      "[D loss: 0.649284] [G loss: 1.459350]\n",
      "[D loss: 1.231664] [G loss: 1.436798]\n",
      "[D loss: 0.773249] [G loss: 1.353814]\n",
      "[D loss: 1.038305] [G loss: 1.437853]\n",
      "[D loss: 1.029332] [G loss: 1.621053]\n",
      "[D loss: 1.083054] [G loss: 1.583477]\n",
      "[D loss: 0.581242] [G loss: 1.551054]\n",
      "[D loss: 0.687874] [G loss: 1.527574]\n",
      "[D loss: 0.881833] [G loss: 1.411644]\n",
      "[D loss: 0.789947] [G loss: 1.561167]\n",
      "[D loss: 0.938557] [G loss: 1.780852]\n",
      "[D loss: 0.961967] [G loss: 1.573601]\n",
      "[D loss: 0.776035] [G loss: 1.396149]\n",
      "[D loss: 1.003656] [G loss: 1.136074]\n",
      "[D loss: 1.110199] [G loss: 1.264423]\n",
      "[D loss: 0.679531] [G loss: 1.276278]\n",
      "[D loss: 0.679362] [G loss: 1.548633]\n",
      "[D loss: 0.851189] [G loss: 1.580822]\n",
      "[D loss: 0.839625] [G loss: 1.543315]\n",
      "[D loss: 0.790166] [G loss: 1.595675]\n",
      "[D loss: 0.792148] [G loss: 1.693474]\n",
      "[D loss: 0.809604] [G loss: 1.692206]\n",
      "[D loss: 0.866835] [G loss: 1.413049]\n",
      "[D loss: 0.907719] [G loss: 1.304126]\n",
      "[D loss: 0.839275] [G loss: 1.552077]\n",
      "[D loss: 0.763263] [G loss: 1.456103]\n",
      "[D loss: 0.699171] [G loss: 1.436892]\n",
      "[D loss: 0.872101] [G loss: 1.426860]\n",
      "[D loss: 0.764384] [G loss: 1.371321]\n",
      "[D loss: 0.793269] [G loss: 1.466875]\n",
      "[D loss: 0.734821] [G loss: 1.490516]\n",
      "[D loss: 0.694965] [G loss: 1.640279]\n",
      "[D loss: 0.951289] [G loss: 1.475290]\n",
      "[D loss: 0.875201] [G loss: 1.505482]\n",
      "[D loss: 0.720430] [G loss: 1.528916]\n",
      "[D loss: 0.671196] [G loss: 1.770884]\n",
      "[D loss: 1.051259] [G loss: 1.520247]\n",
      "[D loss: 0.855146] [G loss: 1.422280]\n",
      "[D loss: 1.026710] [G loss: 1.422883]\n",
      "[D loss: 0.701439] [G loss: 1.474508]\n",
      "[D loss: 0.865237] [G loss: 1.417239]\n",
      "[D loss: 0.708637] [G loss: 1.443665]\n",
      "[D loss: 0.971154] [G loss: 1.318703]\n",
      "[D loss: 0.608511] [G loss: 1.535006]\n",
      "[D loss: 0.785966] [G loss: 1.631920]\n",
      "[D loss: 1.054322] [G loss: 1.261566]\n",
      "[D loss: 1.018962] [G loss: 1.483699]\n",
      "[D loss: 0.962501] [G loss: 1.217296]\n",
      "[D loss: 0.863751] [G loss: 1.251237]\n",
      "[D loss: 1.042897] [G loss: 1.332580]\n",
      "[D loss: 0.656333] [G loss: 1.346640]\n",
      "[D loss: 0.889335] [G loss: 1.239055]\n",
      "[D loss: 0.947628] [G loss: 1.189003]\n",
      "[D loss: 0.772412] [G loss: 1.480059]\n",
      "[D loss: 0.629593] [G loss: 1.492652]\n",
      "[D loss: 0.790180] [G loss: 1.500673]\n",
      "[D loss: 0.855601] [G loss: 1.642437]\n",
      "[D loss: 0.758213] [G loss: 1.398508]\n",
      "[D loss: 0.928985] [G loss: 1.377033]\n",
      "[D loss: 0.628626] [G loss: 1.800711]\n",
      "[D loss: 0.939216] [G loss: 1.274259]\n",
      "[D loss: 0.813213] [G loss: 1.292915]\n",
      "[D loss: 0.724487] [G loss: 1.523955]\n",
      "[D loss: 0.729380] [G loss: 1.523543]\n",
      "[D loss: 0.828036] [G loss: 1.381948]\n",
      "[D loss: 0.795298] [G loss: 1.409959]\n",
      "[D loss: 0.748651] [G loss: 1.766641]\n",
      "[D loss: 0.859280] [G loss: 1.545795]\n",
      "[D loss: 1.079351] [G loss: 1.668400]\n",
      "[D loss: 0.814798] [G loss: 1.439689]\n",
      "[D loss: 0.868909] [G loss: 1.434488]\n",
      "[D loss: 0.716889] [G loss: 1.716187]\n",
      "[D loss: 0.845681] [G loss: 1.489448]\n",
      "[D loss: 0.751668] [G loss: 1.407237]\n",
      "[D loss: 0.758632] [G loss: 1.287640]\n",
      "[D loss: 0.820630] [G loss: 1.528270]\n",
      "[D loss: 0.775456] [G loss: 1.540560]\n",
      "[D loss: 0.968572] [G loss: 1.453125]\n",
      "[D loss: 1.055192] [G loss: 1.548735]\n",
      "[D loss: 0.839719] [G loss: 1.468922]\n",
      "[D loss: 0.793722] [G loss: 1.504721]\n",
      "[D loss: 0.714652] [G loss: 1.438903]\n",
      "[D loss: 0.703755] [G loss: 1.497254]\n",
      "[D loss: 0.779229] [G loss: 1.310570]\n",
      "[D loss: 0.765561] [G loss: 1.339242]\n",
      "[D loss: 0.821525] [G loss: 1.400566]\n",
      "[D loss: 0.875977] [G loss: 1.405777]\n",
      "[D loss: 0.491009] [G loss: 1.772553]\n",
      "[D loss: 0.770146] [G loss: 1.685726]\n",
      "[D loss: 0.813945] [G loss: 1.561715]\n",
      "[D loss: 0.902267] [G loss: 1.667250]\n",
      "[D loss: 0.637202] [G loss: 1.702707]\n",
      "[D loss: 0.888313] [G loss: 1.354944]\n",
      "[D loss: 0.655385] [G loss: 1.531922]\n",
      "[D loss: 0.764157] [G loss: 1.640388]\n",
      "[D loss: 1.082860] [G loss: 1.443255]\n",
      "[D loss: 0.644394] [G loss: 1.737326]\n",
      "[D loss: 0.831845] [G loss: 1.770969]\n",
      "[D loss: 0.731906] [G loss: 1.819701]\n",
      "[D loss: 0.959846] [G loss: 1.413879]\n",
      "[D loss: 0.751963] [G loss: 1.411747]\n",
      "[D loss: 0.774659] [G loss: 1.586149]\n",
      "[D loss: 0.806697] [G loss: 1.563572]\n",
      "[D loss: 0.571800] [G loss: 1.534255]\n",
      "[D loss: 0.884815] [G loss: 1.452359]\n",
      "[D loss: 0.859378] [G loss: 1.283611]\n",
      "[D loss: 0.704547] [G loss: 1.490850]\n",
      "[D loss: 0.858303] [G loss: 1.307705]\n",
      "[D loss: 0.881109] [G loss: 1.413175]\n",
      "[D loss: 0.825143] [G loss: 1.278520]\n",
      "[D loss: 0.632010] [G loss: 1.408945]\n",
      "[D loss: 0.960595] [G loss: 1.352736]\n",
      "[D loss: 0.702963] [G loss: 1.623257]\n",
      "[D loss: 0.814848] [G loss: 1.414934]\n",
      "[D loss: 0.934098] [G loss: 1.760058]\n",
      "[D loss: 0.877012] [G loss: 1.425476]\n",
      "[D loss: 0.935966] [G loss: 1.396274]\n",
      "[D loss: 0.912972] [G loss: 1.362328]\n",
      "[D loss: 0.748376] [G loss: 1.640270]\n",
      "[D loss: 1.027491] [G loss: 1.545611]\n",
      "[D loss: 0.937868] [G loss: 1.683816]\n",
      "[D loss: 0.881576] [G loss: 1.423740]\n",
      "[D loss: 0.889324] [G loss: 1.369686]\n",
      "[D loss: 0.777994] [G loss: 1.448835]\n",
      "[D loss: 0.728796] [G loss: 1.716943]\n",
      "[D loss: 0.898148] [G loss: 1.661426]\n",
      "[D loss: 0.964217] [G loss: 1.576871]\n",
      "[D loss: 0.913883] [G loss: 1.329288]\n",
      "[D loss: 0.689259] [G loss: 1.473760]\n",
      "[D loss: 0.877146] [G loss: 1.451997]\n",
      "[D loss: 0.786578] [G loss: 1.158978]\n",
      "[D loss: 0.869293] [G loss: 1.392879]\n",
      "[D loss: 0.865984] [G loss: 1.379255]\n",
      "[D loss: 0.858867] [G loss: 1.408241]\n",
      "[D loss: 1.102040] [G loss: 1.220487]\n",
      "[D loss: 0.786368] [G loss: 1.448444]\n",
      "[D loss: 0.779787] [G loss: 1.376082]\n",
      "[D loss: 0.922755] [G loss: 1.705314]\n",
      "[D loss: 1.072135] [G loss: 1.300083]\n",
      "[D loss: 0.659688] [G loss: 1.552319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.807150] [G loss: 1.401947]\n",
      "[D loss: 0.857482] [G loss: 1.374067]\n",
      "[D loss: 1.039709] [G loss: 1.393094]\n",
      "[D loss: 0.914609] [G loss: 1.409816]\n",
      "[D loss: 0.933282] [G loss: 1.236141]\n",
      "[D loss: 0.792604] [G loss: 1.339561]\n",
      "[D loss: 0.817014] [G loss: 1.578308]\n",
      "[D loss: 0.927753] [G loss: 1.203023]\n",
      "[D loss: 1.093062] [G loss: 1.328320]\n",
      "[D loss: 0.825491] [G loss: 1.323798]\n",
      "[D loss: 0.889244] [G loss: 1.395393]\n",
      "[D loss: 1.057088] [G loss: 1.489409]\n",
      "[D loss: 0.732865] [G loss: 1.389317]\n",
      "[D loss: 0.660226] [G loss: 1.494259]\n",
      "[D loss: 0.967697] [G loss: 1.390196]\n",
      "[D loss: 1.100741] [G loss: 1.405083]\n",
      "[D loss: 0.788132] [G loss: 1.452174]\n",
      "[D loss: 0.969421] [G loss: 1.147211]\n",
      "[D loss: 0.611258] [G loss: 1.442141]\n",
      "[D loss: 0.825223] [G loss: 1.587785]\n",
      "[D loss: 0.838588] [G loss: 1.674373]\n",
      "[D loss: 0.776963] [G loss: 1.246171]\n",
      "[D loss: 0.924512] [G loss: 1.321724]\n",
      "[D loss: 0.934391] [G loss: 1.198986]\n",
      "[D loss: 0.780015] [G loss: 1.356210]\n",
      "[D loss: 0.777674] [G loss: 1.318853]\n",
      "[D loss: 0.949401] [G loss: 1.563466]\n",
      "[D loss: 0.952672] [G loss: 1.624691]\n",
      "[D loss: 0.849056] [G loss: 1.379392]\n",
      "[D loss: 0.875568] [G loss: 1.352972]\n",
      "[D loss: 0.933091] [G loss: 1.368342]\n",
      "[D loss: 0.810258] [G loss: 1.511426]\n",
      "[D loss: 0.784712] [G loss: 1.376918]\n",
      "[D loss: 0.907697] [G loss: 1.406424]\n",
      "[D loss: 0.861509] [G loss: 1.393385]\n",
      "[D loss: 0.787792] [G loss: 1.447294]\n",
      "[D loss: 0.888636] [G loss: 1.367300]\n",
      "[D loss: 0.846060] [G loss: 1.470776]\n",
      "[D loss: 0.738440] [G loss: 1.408073]\n",
      "[D loss: 0.850509] [G loss: 1.496066]\n",
      "[D loss: 0.727064] [G loss: 1.362242]\n",
      "[D loss: 0.626258] [G loss: 1.316364]\n",
      "[D loss: 0.811183] [G loss: 1.362731]\n",
      "[D loss: 0.847052] [G loss: 1.504303]\n",
      "[D loss: 0.880139] [G loss: 1.532283]\n",
      "[D loss: 0.760499] [G loss: 1.425219]\n",
      "[D loss: 0.844892] [G loss: 1.515436]\n",
      "[D loss: 0.686805] [G loss: 1.417109]\n",
      "[D loss: 0.736955] [G loss: 1.564335]\n",
      "[D loss: 1.025943] [G loss: 1.552219]\n",
      "[D loss: 0.699789] [G loss: 1.500160]\n",
      "[D loss: 0.892794] [G loss: 1.569299]\n",
      "[D loss: 0.699777] [G loss: 1.287782]\n",
      "[D loss: 0.686796] [G loss: 1.728185]\n",
      "[D loss: 0.825321] [G loss: 1.502146]\n",
      "[D loss: 0.903796] [G loss: 1.398355]\n",
      "[D loss: 0.988382] [G loss: 1.285637]\n",
      "[D loss: 0.762661] [G loss: 1.343484]\n",
      "[D loss: 0.716231] [G loss: 1.626947]\n",
      "[D loss: 0.736186] [G loss: 1.568309]\n",
      "[D loss: 0.624407] [G loss: 1.496690]\n",
      "[D loss: 0.995025] [G loss: 1.502059]\n",
      "[D loss: 0.786573] [G loss: 1.498379]\n",
      "[D loss: 0.635855] [G loss: 1.557013]\n",
      "[D loss: 0.834948] [G loss: 1.453623]\n",
      "[D loss: 0.618455] [G loss: 1.508144]\n",
      "[D loss: 0.967737] [G loss: 1.507179]\n",
      "[D loss: 0.706429] [G loss: 1.578346]\n",
      "[D loss: 0.822527] [G loss: 1.293350]\n",
      "[D loss: 0.741992] [G loss: 1.508186]\n",
      "[D loss: 1.004865] [G loss: 1.395213]\n",
      "[D loss: 0.768225] [G loss: 1.340033]\n",
      "[D loss: 1.011898] [G loss: 1.359798]\n",
      "[D loss: 1.133058] [G loss: 1.286389]\n",
      "[D loss: 0.699270] [G loss: 1.667279]\n",
      "[D loss: 0.682864] [G loss: 1.480430]\n",
      "[D loss: 0.725328] [G loss: 1.660599]\n",
      "[D loss: 0.739945] [G loss: 1.470454]\n",
      "[D loss: 0.596685] [G loss: 1.697172]\n",
      "[D loss: 0.785871] [G loss: 1.630292]\n",
      "[D loss: 0.774241] [G loss: 1.618953]\n",
      "[D loss: 1.031487] [G loss: 1.695228]\n",
      "[D loss: 1.020481] [G loss: 1.588409]\n",
      "[D loss: 0.967558] [G loss: 1.605284]\n",
      "[D loss: 0.745537] [G loss: 1.505891]\n",
      "[D loss: 0.922213] [G loss: 1.569252]\n",
      "[D loss: 0.923448] [G loss: 1.424676]\n",
      "[D loss: 0.799639] [G loss: 1.345705]\n",
      "[D loss: 0.800425] [G loss: 1.425777]\n",
      "[D loss: 0.824664] [G loss: 1.467612]\n",
      "[D loss: 0.860259] [G loss: 1.409410]\n",
      "[D loss: 0.862683] [G loss: 1.741755]\n",
      "[D loss: 1.015433] [G loss: 1.517454]\n",
      "[D loss: 0.837044] [G loss: 1.200506]\n",
      "[D loss: 0.764359] [G loss: 1.530645]\n",
      "[D loss: 0.952982] [G loss: 1.279191]\n",
      "[D loss: 0.880650] [G loss: 1.593010]\n",
      "[D loss: 0.757609] [G loss: 1.425288]\n",
      "[D loss: 1.105779] [G loss: 1.275956]\n",
      "[D loss: 0.712954] [G loss: 1.428302]\n",
      "[D loss: 0.713991] [G loss: 1.421775]\n",
      "[D loss: 0.795965] [G loss: 1.643809]\n",
      "[D loss: 0.910586] [G loss: 1.697791]\n",
      "[D loss: 0.749630] [G loss: 1.661339]\n",
      "[D loss: 0.731793] [G loss: 1.544791]\n",
      "[D loss: 0.945612] [G loss: 1.247902]\n",
      "[D loss: 0.863327] [G loss: 1.547806]\n",
      "[D loss: 0.495035] [G loss: 1.607089]\n",
      "[D loss: 0.878517] [G loss: 1.752096]\n",
      "[D loss: 0.855334] [G loss: 1.543590]\n",
      "[D loss: 0.520037] [G loss: 1.554941]\n",
      "[D loss: 1.066159] [G loss: 1.390505]\n",
      "[D loss: 0.925123] [G loss: 1.528514]\n",
      "[D loss: 0.797714] [G loss: 1.764068]\n",
      "[D loss: 0.734284] [G loss: 1.472959]\n",
      "[D loss: 0.769187] [G loss: 1.576234]\n",
      "[D loss: 0.809844] [G loss: 1.648633]\n",
      "[D loss: 0.963868] [G loss: 1.438033]\n",
      "[D loss: 0.726340] [G loss: 1.551283]\n",
      "[D loss: 0.912284] [G loss: 1.450679]\n",
      "[D loss: 0.935327] [G loss: 1.570106]\n",
      "[D loss: 0.668407] [G loss: 1.795764]\n",
      "[D loss: 0.756800] [G loss: 1.748108]\n",
      "[D loss: 0.831693] [G loss: 1.449847]\n",
      "[D loss: 0.777833] [G loss: 1.531814]\n",
      "[D loss: 1.121599] [G loss: 1.415827]\n",
      "[D loss: 0.940290] [G loss: 1.612475]\n",
      "[D loss: 0.721854] [G loss: 1.708375]\n",
      "[D loss: 1.036983] [G loss: 1.712405]\n",
      "[D loss: 0.867749] [G loss: 1.293075]\n",
      "[D loss: 0.940768] [G loss: 1.423431]\n",
      "[D loss: 0.781907] [G loss: 1.309072]\n",
      "[D loss: 0.690806] [G loss: 1.344303]\n",
      "[D loss: 0.847399] [G loss: 1.347685]\n",
      "[D loss: 0.731233] [G loss: 1.634004]\n",
      "[D loss: 0.699879] [G loss: 1.563070]\n",
      "[D loss: 0.697490] [G loss: 1.636593]\n",
      "[D loss: 0.670204] [G loss: 1.437639]\n",
      "[D loss: 0.951988] [G loss: 1.576988]\n",
      "[D loss: 0.755244] [G loss: 1.551795]\n",
      "[D loss: 0.676940] [G loss: 1.448179]\n",
      "[D loss: 0.879294] [G loss: 1.435545]\n",
      "[D loss: 0.900339] [G loss: 1.229554]\n",
      "[D loss: 0.854956] [G loss: 1.383998]\n",
      "[D loss: 0.577197] [G loss: 1.591891]\n",
      "[D loss: 0.857161] [G loss: 1.345286]\n",
      "[D loss: 0.970550] [G loss: 1.550856]\n",
      "[D loss: 1.133473] [G loss: 2.005929]\n",
      "[D loss: 1.004534] [G loss: 1.384217]\n",
      "[D loss: 0.931983] [G loss: 1.173413]\n",
      "[D loss: 1.003325] [G loss: 1.319898]\n",
      "[D loss: 0.675286] [G loss: 1.574564]\n",
      "[D loss: 0.834572] [G loss: 1.800372]\n",
      "[D loss: 0.795650] [G loss: 1.445393]\n",
      "[D loss: 0.686321] [G loss: 1.510982]\n",
      "[D loss: 0.733255] [G loss: 1.489585]\n",
      "[D loss: 0.895364] [G loss: 1.395598]\n",
      "[D loss: 0.703877] [G loss: 1.342686]\n",
      "[D loss: 0.918414] [G loss: 1.307093]\n",
      "[D loss: 0.660516] [G loss: 1.540774]\n",
      "[D loss: 0.545336] [G loss: 1.883154]\n",
      "[D loss: 0.835084] [G loss: 1.690641]\n",
      "[D loss: 0.712646] [G loss: 1.568375]\n",
      "[D loss: 0.784357] [G loss: 1.456052]\n",
      "[D loss: 0.677238] [G loss: 1.455328]\n",
      "[D loss: 0.725354] [G loss: 1.627207]\n",
      "[D loss: 0.856002] [G loss: 1.413203]\n",
      "[D loss: 0.785244] [G loss: 1.452626]\n",
      "[D loss: 0.660166] [G loss: 1.662661]\n",
      "[D loss: 0.850133] [G loss: 1.549556]\n",
      "[D loss: 0.819227] [G loss: 1.384386]\n",
      "[D loss: 0.717480] [G loss: 1.546096]\n",
      "[D loss: 1.008003] [G loss: 1.466158]\n",
      "[D loss: 0.886267] [G loss: 1.357431]\n",
      "[D loss: 0.591612] [G loss: 1.670536]\n",
      "[D loss: 0.619040] [G loss: 1.672493]\n",
      "[D loss: 0.917211] [G loss: 1.721287]\n",
      "[D loss: 0.848297] [G loss: 1.309700]\n",
      "[D loss: 0.878375] [G loss: 1.185998]\n",
      "[D loss: 0.707434] [G loss: 1.715220]\n",
      "[D loss: 0.796295] [G loss: 1.643206]\n",
      "[D loss: 0.696718] [G loss: 1.606786]\n",
      "[D loss: 0.697452] [G loss: 1.504535]\n",
      "[D loss: 0.615662] [G loss: 1.512591]\n",
      "[D loss: 0.774964] [G loss: 1.658474]\n",
      "[D loss: 0.646987] [G loss: 1.617585]\n",
      "[D loss: 0.458160] [G loss: 1.901701]\n",
      "[D loss: 0.703538] [G loss: 1.648744]\n",
      "[D loss: 0.931423] [G loss: 1.504958]\n",
      "[D loss: 0.849417] [G loss: 1.307779]\n",
      "[D loss: 0.846594] [G loss: 1.728544]\n",
      "[D loss: 0.715169] [G loss: 1.600730]\n",
      "[D loss: 0.699783] [G loss: 1.451403]\n",
      "[D loss: 0.785722] [G loss: 1.239397]\n",
      "[D loss: 0.826775] [G loss: 1.284426]\n",
      "[D loss: 0.789522] [G loss: 1.677625]\n",
      "[D loss: 0.850411] [G loss: 1.838287]\n",
      "[D loss: 0.794438] [G loss: 1.635235]\n",
      "[D loss: 0.955123] [G loss: 1.503061]\n",
      "[D loss: 0.807913] [G loss: 1.361030]\n",
      "[D loss: 0.894157] [G loss: 1.344316]\n",
      "[D loss: 0.865798] [G loss: 1.582740]\n",
      "[D loss: 0.662798] [G loss: 1.598479]\n",
      "[D loss: 0.974833] [G loss: 1.290156]\n",
      "[D loss: 0.847744] [G loss: 1.507001]\n",
      "[D loss: 0.798854] [G loss: 1.393177]\n",
      "[D loss: 0.669756] [G loss: 1.571506]\n",
      "[D loss: 0.915950] [G loss: 1.470392]\n",
      "[D loss: 0.862658] [G loss: 1.642858]\n",
      "[D loss: 0.872864] [G loss: 1.612834]\n",
      "[D loss: 0.846341] [G loss: 1.587910]\n",
      "[D loss: 0.751405] [G loss: 1.397767]\n",
      "[D loss: 0.708716] [G loss: 1.466179]\n",
      "[D loss: 0.679672] [G loss: 1.448706]\n",
      "[D loss: 0.956763] [G loss: 1.330703]\n",
      "[D loss: 0.914461] [G loss: 1.393558]\n",
      "[D loss: 0.713289] [G loss: 1.361230]\n",
      "[D loss: 0.709904] [G loss: 1.410277]\n",
      "[D loss: 0.622123] [G loss: 1.620647]\n",
      "[D loss: 0.568502] [G loss: 1.978137]\n",
      "[D loss: 0.651057] [G loss: 1.773348]\n",
      "[D loss: 0.951979] [G loss: 1.551116]\n",
      "[D loss: 0.866697] [G loss: 1.511985]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.862149] [G loss: 1.370376]\n",
      "[D loss: 0.792491] [G loss: 1.509633]\n",
      "[D loss: 1.059480] [G loss: 1.577513]\n",
      "[D loss: 0.691377] [G loss: 1.922158]\n",
      "[D loss: 0.737064] [G loss: 1.362115]\n",
      "[D loss: 0.979487] [G loss: 1.277154]\n",
      "[D loss: 0.562699] [G loss: 1.434398]\n",
      "[D loss: 0.861003] [G loss: 1.573669]\n",
      "[D loss: 0.936251] [G loss: 1.495334]\n",
      "[D loss: 0.857610] [G loss: 1.379110]\n",
      "[D loss: 0.682487] [G loss: 1.531113]\n",
      "[D loss: 0.727141] [G loss: 1.475345]\n",
      "[D loss: 0.700908] [G loss: 1.528333]\n",
      "[D loss: 0.666932] [G loss: 1.521706]\n",
      "[D loss: 0.874265] [G loss: 1.599498]\n",
      "[D loss: 0.578725] [G loss: 1.729079]\n",
      "[D loss: 0.775409] [G loss: 1.449686]\n",
      "[D loss: 0.857167] [G loss: 2.021837]\n",
      "[D loss: 0.711534] [G loss: 1.719163]\n",
      "[D loss: 0.733280] [G loss: 1.698553]\n",
      "[D loss: 0.757827] [G loss: 1.295418]\n",
      "[D loss: 0.745654] [G loss: 1.376472]\n",
      "[D loss: 0.936453] [G loss: 1.208416]\n",
      "[D loss: 0.792445] [G loss: 1.485725]\n",
      "[D loss: 1.018400] [G loss: 1.584733]\n",
      "[D loss: 0.654972] [G loss: 1.707839]\n",
      "[D loss: 0.893402] [G loss: 1.645579]\n",
      "[D loss: 1.210178] [G loss: 1.448184]\n",
      "[D loss: 0.791655] [G loss: 1.456902]\n",
      "[D loss: 0.856151] [G loss: 1.393748]\n",
      "[D loss: 0.679437] [G loss: 1.330927]\n",
      "[D loss: 0.906060] [G loss: 1.571724]\n",
      "[D loss: 0.920562] [G loss: 1.307043]\n",
      "[D loss: 0.796689] [G loss: 1.464501]\n",
      "[D loss: 0.761749] [G loss: 1.330874]\n",
      "[D loss: 0.804209] [G loss: 1.386840]\n",
      "[D loss: 0.746758] [G loss: 1.597528]\n",
      "[D loss: 0.803990] [G loss: 1.561182]\n",
      "[D loss: 0.710884] [G loss: 1.612203]\n",
      "[D loss: 0.763005] [G loss: 1.212713]\n",
      "[D loss: 0.717713] [G loss: 1.402673]\n",
      "[D loss: 0.853512] [G loss: 1.466063]\n",
      "[D loss: 0.878399] [G loss: 1.579454]\n",
      "[D loss: 1.136346] [G loss: 1.322255]\n",
      "[D loss: 0.735095] [G loss: 1.455670]\n",
      "[D loss: 0.794459] [G loss: 1.581324]\n",
      "[D loss: 0.749591] [G loss: 1.665101]\n",
      "[D loss: 0.959903] [G loss: 1.509213]\n",
      "[D loss: 0.846906] [G loss: 1.330591]\n",
      "[D loss: 0.781461] [G loss: 1.465581]\n",
      "[D loss: 0.918911] [G loss: 1.542731]\n",
      "[D loss: 0.887867] [G loss: 1.819562]\n",
      "[D loss: 0.843717] [G loss: 1.440676]\n",
      "[D loss: 0.893659] [G loss: 1.504237]\n",
      "[D loss: 0.788830] [G loss: 1.393859]\n",
      "[D loss: 0.752395] [G loss: 1.218627]\n",
      "[D loss: 0.908630] [G loss: 1.472233]\n",
      "[D loss: 0.705526] [G loss: 1.366805]\n",
      "[D loss: 0.927066] [G loss: 1.528573]\n",
      "[D loss: 0.758021] [G loss: 1.589374]\n",
      "[D loss: 0.810339] [G loss: 1.836370]\n",
      "[D loss: 0.879568] [G loss: 1.662994]\n",
      "[D loss: 1.033413] [G loss: 1.435239]\n",
      "[D loss: 0.768014] [G loss: 1.458750]\n",
      "[D loss: 0.934611] [G loss: 1.246079]\n",
      "[D loss: 0.805375] [G loss: 1.645352]\n",
      "[D loss: 0.832369] [G loss: 1.317804]\n",
      "[D loss: 0.878128] [G loss: 1.416746]\n",
      "[D loss: 0.729287] [G loss: 1.471504]\n",
      "[D loss: 0.752062] [G loss: 1.319137]\n",
      "[D loss: 0.757948] [G loss: 1.552835]\n",
      "[D loss: 0.822893] [G loss: 1.515865]\n",
      "[D loss: 0.745593] [G loss: 1.409992]\n",
      "[D loss: 0.731782] [G loss: 1.476580]\n",
      "[D loss: 0.983584] [G loss: 1.420359]\n",
      "[D loss: 0.789179] [G loss: 1.552488]\n",
      "[D loss: 0.665459] [G loss: 1.628267]\n",
      "[D loss: 0.822330] [G loss: 1.508586]\n",
      "[D loss: 0.681951] [G loss: 1.657686]\n",
      "[D loss: 0.977064] [G loss: 1.481473]\n",
      "[D loss: 0.752046] [G loss: 1.510183]\n",
      "[D loss: 0.868817] [G loss: 1.369373]\n",
      "[D loss: 0.832352] [G loss: 1.424653]\n",
      "[D loss: 0.877564] [G loss: 1.560100]\n",
      "[D loss: 0.883771] [G loss: 1.424949]\n",
      "[D loss: 1.059648] [G loss: 1.332855]\n",
      "[D loss: 0.639036] [G loss: 1.379081]\n",
      "[D loss: 1.052377] [G loss: 1.149122]\n",
      "[D loss: 0.812090] [G loss: 1.646189]\n",
      "[D loss: 0.942188] [G loss: 1.440951]\n",
      "[D loss: 0.852004] [G loss: 1.423340]\n",
      "[D loss: 0.778301] [G loss: 1.317051]\n",
      "[D loss: 0.718881] [G loss: 1.298752]\n",
      "[D loss: 0.907012] [G loss: 1.395629]\n",
      "[D loss: 0.727534] [G loss: 1.715299]\n",
      "[D loss: 0.999527] [G loss: 1.388603]\n",
      "[D loss: 0.809211] [G loss: 1.460285]\n",
      "[D loss: 0.814355] [G loss: 1.370591]\n",
      "[D loss: 0.775828] [G loss: 1.542904]\n",
      "[D loss: 1.144477] [G loss: 1.074603]\n",
      "[D loss: 0.630229] [G loss: 1.497686]\n",
      "[D loss: 0.861812] [G loss: 1.665455]\n",
      "[D loss: 1.177962] [G loss: 1.486372]\n",
      "[D loss: 0.653094] [G loss: 1.654403]\n",
      "[D loss: 0.989844] [G loss: 1.604628]\n",
      "[D loss: 0.832275] [G loss: 1.449235]\n",
      "[D loss: 0.810809] [G loss: 1.346467]\n",
      "[D loss: 1.027167] [G loss: 1.164753]\n",
      "[D loss: 0.839922] [G loss: 1.406254]\n",
      "[D loss: 0.711864] [G loss: 1.399582]\n",
      "[D loss: 0.791112] [G loss: 1.524528]\n",
      "[D loss: 0.766180] [G loss: 1.292378]\n",
      "[D loss: 0.681442] [G loss: 1.475050]\n",
      "[D loss: 0.774962] [G loss: 1.321947]\n",
      "[D loss: 0.809966] [G loss: 1.506108]\n",
      "[D loss: 0.928843] [G loss: 1.447052]\n",
      "[D loss: 0.893015] [G loss: 1.388187]\n",
      "[D loss: 0.720958] [G loss: 1.511722]\n",
      "[D loss: 0.868107] [G loss: 1.405567]\n",
      "[D loss: 0.800066] [G loss: 1.282012]\n",
      "[D loss: 0.696200] [G loss: 1.651446]\n",
      "[D loss: 0.811825] [G loss: 1.605415]\n",
      "[D loss: 0.714193] [G loss: 1.495849]\n",
      "[D loss: 0.783164] [G loss: 1.500002]\n",
      "[D loss: 1.031349] [G loss: 1.378644]\n",
      "[D loss: 0.609177] [G loss: 1.652877]\n",
      "[D loss: 0.598890] [G loss: 1.483482]\n",
      "[D loss: 1.096336] [G loss: 1.585276]\n",
      "[D loss: 0.733900] [G loss: 1.640812]\n",
      "[D loss: 0.713600] [G loss: 1.407700]\n",
      "[D loss: 0.865796] [G loss: 1.536258]\n",
      "[D loss: 0.919697] [G loss: 1.515761]\n",
      "[D loss: 0.765947] [G loss: 1.442479]\n",
      "[D loss: 0.658103] [G loss: 1.296919]\n",
      "[D loss: 0.918575] [G loss: 1.438671]\n",
      "[D loss: 0.864403] [G loss: 1.629301]\n",
      "[D loss: 0.685673] [G loss: 1.473482]\n",
      "[D loss: 0.926353] [G loss: 1.319721]\n",
      "[D loss: 0.992292] [G loss: 1.347611]\n",
      "[D loss: 0.861950] [G loss: 1.532142]\n",
      "[D loss: 0.828853] [G loss: 1.581550]\n",
      "[D loss: 0.740539] [G loss: 1.626935]\n",
      "[D loss: 0.938699] [G loss: 1.299809]\n",
      "[D loss: 0.941890] [G loss: 1.434012]\n",
      "[D loss: 0.995697] [G loss: 1.300462]\n",
      "[D loss: 0.888561] [G loss: 1.451389]\n",
      "[D loss: 0.765000] [G loss: 1.475702]\n",
      "[D loss: 0.854648] [G loss: 1.502870]\n",
      "[D loss: 0.876040] [G loss: 1.442101]\n",
      "[D loss: 0.675972] [G loss: 1.399680]\n",
      "[D loss: 1.047117] [G loss: 1.413521]\n",
      "[D loss: 0.874602] [G loss: 1.529110]\n",
      "[D loss: 0.856349] [G loss: 1.446039]\n",
      "[D loss: 1.060348] [G loss: 1.414281]\n",
      "[D loss: 0.830203] [G loss: 1.333507]\n",
      "[D loss: 0.706710] [G loss: 1.302894]\n",
      "[D loss: 1.076288] [G loss: 1.660228]\n",
      "[D loss: 0.698670] [G loss: 1.431886]\n",
      "[D loss: 0.800414] [G loss: 1.348277]\n",
      "[D loss: 0.958289] [G loss: 1.219003]\n",
      "[D loss: 0.874856] [G loss: 1.462797]\n",
      "[D loss: 0.712787] [G loss: 1.503856]\n",
      "[D loss: 1.104904] [G loss: 1.278020]\n",
      "[D loss: 0.957426] [G loss: 1.407438]\n",
      "[D loss: 0.649159] [G loss: 1.574788]\n",
      "[D loss: 1.090366] [G loss: 1.292480]\n",
      "[D loss: 0.896019] [G loss: 1.256754]\n",
      "[D loss: 0.720224] [G loss: 1.437402]\n",
      "[D loss: 0.880562] [G loss: 1.285156]\n",
      "[D loss: 0.810655] [G loss: 1.472799]\n",
      "[D loss: 0.623051] [G loss: 1.511855]\n",
      "[D loss: 0.952979] [G loss: 1.419403]\n",
      "[D loss: 0.947492] [G loss: 1.597820]\n",
      "[D loss: 0.750638] [G loss: 1.397642]\n",
      "[D loss: 0.611421] [G loss: 1.533788]\n",
      "[D loss: 0.897121] [G loss: 1.249908]\n",
      "[D loss: 0.815635] [G loss: 1.491002]\n",
      "[D loss: 0.755486] [G loss: 1.694863]\n",
      "[D loss: 0.894003] [G loss: 1.551345]\n",
      "[D loss: 0.794009] [G loss: 1.486991]\n",
      "[D loss: 0.917577] [G loss: 1.560319]\n",
      "[D loss: 0.745966] [G loss: 1.495692]\n",
      "[D loss: 0.591671] [G loss: 1.721686]\n",
      "[D loss: 0.892879] [G loss: 1.700018]\n",
      "[D loss: 0.963741] [G loss: 1.531867]\n",
      "[D loss: 0.770274] [G loss: 1.342694]\n",
      "[D loss: 0.696389] [G loss: 1.376464]\n",
      "[D loss: 0.877962] [G loss: 1.441394]\n",
      "[D loss: 0.970797] [G loss: 1.559227]\n",
      "[D loss: 0.855500] [G loss: 1.365016]\n",
      "[D loss: 0.833412] [G loss: 1.590865]\n",
      "[D loss: 0.627460] [G loss: 1.459018]\n",
      "[D loss: 0.757544] [G loss: 1.304271]\n",
      "[D loss: 0.842647] [G loss: 1.406389]\n",
      "[D loss: 0.916654] [G loss: 1.365578]\n",
      "[D loss: 0.894713] [G loss: 1.423874]\n",
      "[D loss: 0.782177] [G loss: 1.538510]\n",
      "[D loss: 0.984291] [G loss: 1.360551]\n",
      "[D loss: 0.954887] [G loss: 1.446981]\n",
      "[D loss: 0.807069] [G loss: 1.501922]\n",
      "[D loss: 0.634570] [G loss: 1.425240]\n",
      "[D loss: 0.841227] [G loss: 1.457796]\n",
      "[D loss: 0.845949] [G loss: 1.396180]\n",
      "[D loss: 0.849522] [G loss: 1.401353]\n",
      "[D loss: 0.833221] [G loss: 1.362602]\n",
      "[D loss: 0.797923] [G loss: 1.401802]\n",
      "[D loss: 0.767853] [G loss: 1.336224]\n",
      "[D loss: 0.990431] [G loss: 1.408982]\n",
      "[D loss: 0.827708] [G loss: 1.541118]\n",
      "[D loss: 0.893332] [G loss: 1.495884]\n",
      "[D loss: 0.758350] [G loss: 1.415953]\n",
      "[D loss: 0.698835] [G loss: 1.374805]\n",
      "[D loss: 0.597759] [G loss: 1.583969]\n",
      "[D loss: 0.997509] [G loss: 1.656951]\n",
      "[D loss: 0.784969] [G loss: 1.280494]\n",
      "[D loss: 0.763480] [G loss: 1.420999]\n",
      "[D loss: 0.902736] [G loss: 1.523264]\n",
      "[D loss: 0.845914] [G loss: 1.512880]\n",
      "[D loss: 0.682886] [G loss: 1.548920]\n",
      "[D loss: 0.887618] [G loss: 1.569299]\n",
      "[D loss: 0.774316] [G loss: 1.624446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.840595] [G loss: 1.273210]\n",
      "[D loss: 0.735753] [G loss: 1.432026]\n",
      "[D loss: 0.798905] [G loss: 1.415567]\n",
      "[D loss: 0.878611] [G loss: 1.342686]\n",
      "[D loss: 0.840393] [G loss: 1.503134]\n",
      "[D loss: 0.659599] [G loss: 1.464167]\n",
      "[D loss: 0.826044] [G loss: 1.678342]\n",
      "[D loss: 0.825671] [G loss: 1.576298]\n",
      "[D loss: 0.787428] [G loss: 1.406933]\n",
      "[D loss: 0.490850] [G loss: 1.522106]\n",
      "[D loss: 0.863753] [G loss: 1.420565]\n",
      "[D loss: 0.703951] [G loss: 1.554925]\n",
      "[D loss: 0.829954] [G loss: 1.369649]\n",
      "[D loss: 0.907444] [G loss: 1.430700]\n",
      "[D loss: 1.040214] [G loss: 1.443838]\n",
      "[D loss: 0.774992] [G loss: 1.807755]\n",
      "[D loss: 0.863358] [G loss: 1.365815]\n",
      "[D loss: 0.600648] [G loss: 1.556072]\n",
      "[D loss: 0.783414] [G loss: 1.562149]\n",
      "[D loss: 0.881327] [G loss: 1.485388]\n",
      "[D loss: 0.990654] [G loss: 1.327976]\n",
      "[D loss: 0.979268] [G loss: 1.597954]\n",
      "[D loss: 0.903039] [G loss: 1.453044]\n",
      "[D loss: 0.829655] [G loss: 1.459983]\n",
      "[D loss: 0.804687] [G loss: 1.396573]\n",
      "[D loss: 0.982489] [G loss: 1.532700]\n",
      "[D loss: 0.780621] [G loss: 1.517728]\n",
      "[D loss: 0.662774] [G loss: 1.530509]\n",
      "[D loss: 0.855027] [G loss: 1.531009]\n",
      "[D loss: 0.936943] [G loss: 1.537112]\n",
      "[D loss: 0.908913] [G loss: 1.394649]\n",
      "[D loss: 0.633879] [G loss: 1.560916]\n",
      "[D loss: 0.708772] [G loss: 1.745692]\n",
      "[D loss: 1.124669] [G loss: 1.351592]\n",
      "[D loss: 0.881332] [G loss: 1.450475]\n",
      "[D loss: 0.882416] [G loss: 1.468059]\n",
      "[D loss: 0.948399] [G loss: 1.562659]\n",
      "[D loss: 0.835736] [G loss: 1.470917]\n",
      "[D loss: 0.974555] [G loss: 1.101256]\n",
      "[D loss: 0.884649] [G loss: 1.163564]\n",
      "[D loss: 0.885717] [G loss: 1.458612]\n",
      "[D loss: 0.809443] [G loss: 1.435214]\n",
      "[D loss: 0.763062] [G loss: 1.608313]\n",
      "[D loss: 0.968342] [G loss: 1.738339]\n",
      "[D loss: 1.090986] [G loss: 1.093088]\n",
      "[D loss: 0.730745] [G loss: 1.423703]\n",
      "[D loss: 0.773709] [G loss: 1.420196]\n",
      "[D loss: 0.549217] [G loss: 1.420962]\n",
      "[D loss: 0.819907] [G loss: 1.501583]\n",
      "[D loss: 0.675128] [G loss: 1.591727]\n",
      "[D loss: 1.022363] [G loss: 1.128805]\n",
      "[D loss: 0.942214] [G loss: 1.576559]\n",
      "[D loss: 1.049347] [G loss: 1.422449]\n",
      "[D loss: 1.099920] [G loss: 1.520543]\n",
      "[D loss: 1.135477] [G loss: 1.537795]\n",
      "[D loss: 0.782745] [G loss: 1.211147]\n",
      "[D loss: 0.824744] [G loss: 1.298954]\n",
      "[D loss: 0.986434] [G loss: 1.398535]\n",
      "[D loss: 0.792593] [G loss: 1.360762]\n",
      "[D loss: 0.766573] [G loss: 1.452981]\n",
      "[D loss: 0.781933] [G loss: 1.487820]\n",
      "[D loss: 0.655264] [G loss: 1.607031]\n",
      "[D loss: 0.736231] [G loss: 1.450094]\n",
      "[D loss: 0.925989] [G loss: 1.275187]\n",
      "[D loss: 1.049209] [G loss: 1.325750]\n",
      "[D loss: 0.989207] [G loss: 1.391929]\n",
      "[D loss: 0.771892] [G loss: 1.533298]\n",
      "[D loss: 0.901110] [G loss: 1.439041]\n",
      "[D loss: 0.802427] [G loss: 1.662334]\n",
      "[D loss: 0.815024] [G loss: 1.466576]\n",
      "[D loss: 0.827886] [G loss: 1.500600]\n",
      "[D loss: 0.951064] [G loss: 1.467035]\n",
      "[D loss: 0.713389] [G loss: 1.342482]\n",
      "[D loss: 0.732479] [G loss: 1.411822]\n",
      "[D loss: 0.874824] [G loss: 1.476813]\n",
      "[D loss: 0.730853] [G loss: 1.535764]\n",
      "[D loss: 0.761125] [G loss: 1.393797]\n",
      "[D loss: 0.676860] [G loss: 1.489177]\n",
      "[D loss: 0.966699] [G loss: 1.395622]\n",
      "[D loss: 0.739807] [G loss: 1.493286]\n",
      "[D loss: 1.002869] [G loss: 1.521325]\n",
      "[D loss: 0.599907] [G loss: 1.685437]\n",
      "[D loss: 0.890132] [G loss: 1.429864]\n",
      "[D loss: 0.593783] [G loss: 1.495237]\n",
      "[D loss: 0.832248] [G loss: 1.681834]\n",
      "[D loss: 0.929344] [G loss: 1.710546]\n",
      "[D loss: 0.969101] [G loss: 1.389987]\n",
      "[D loss: 0.777866] [G loss: 1.471014]\n",
      "[D loss: 1.041063] [G loss: 1.206570]\n",
      "[D loss: 0.811048] [G loss: 1.421541]\n",
      "[D loss: 0.990562] [G loss: 1.380656]\n",
      "[D loss: 0.733821] [G loss: 1.506209]\n",
      "[D loss: 0.655201] [G loss: 1.536386]\n",
      "[D loss: 0.910286] [G loss: 1.562734]\n",
      "[D loss: 0.943468] [G loss: 1.361866]\n",
      "[D loss: 0.618625] [G loss: 1.585965]\n",
      "[D loss: 0.688884] [G loss: 1.400915]\n",
      "[D loss: 0.861689] [G loss: 1.321534]\n",
      "[D loss: 0.743693] [G loss: 1.413519]\n",
      "[D loss: 1.036365] [G loss: 1.445642]\n",
      "[D loss: 0.758040] [G loss: 1.557360]\n",
      "[D loss: 0.919304] [G loss: 1.373990]\n",
      "[D loss: 0.983384] [G loss: 1.544474]\n",
      "[D loss: 0.712107] [G loss: 1.700925]\n",
      "[D loss: 0.797992] [G loss: 1.305281]\n",
      "[D loss: 1.043605] [G loss: 1.525241]\n",
      "[D loss: 1.225719] [G loss: 1.829623]\n",
      "[D loss: 0.943778] [G loss: 1.634989]\n",
      "[D loss: 0.872544] [G loss: 1.440361]\n",
      "[D loss: 0.577812] [G loss: 1.408538]\n",
      "[D loss: 0.974746] [G loss: 1.308739]\n",
      "[D loss: 0.882084] [G loss: 1.396580]\n",
      "[D loss: 0.875450] [G loss: 1.478799]\n",
      "[D loss: 0.935407] [G loss: 1.537633]\n",
      "[D loss: 1.033147] [G loss: 1.319561]\n",
      "[D loss: 0.852579] [G loss: 1.607538]\n",
      "[D loss: 0.729779] [G loss: 1.467827]\n",
      "[D loss: 0.803957] [G loss: 1.363833]\n",
      "[D loss: 0.834083] [G loss: 1.511211]\n",
      "[D loss: 1.019594] [G loss: 1.334969]\n",
      "[D loss: 0.781977] [G loss: 1.306779]\n",
      "[D loss: 0.581388] [G loss: 1.394729]\n",
      "[D loss: 1.018409] [G loss: 1.319422]\n",
      "[D loss: 0.859914] [G loss: 1.413783]\n",
      "[D loss: 0.702042] [G loss: 1.581613]\n",
      "[D loss: 0.616251] [G loss: 1.733879]\n",
      "[D loss: 0.892485] [G loss: 1.640171]\n",
      "[D loss: 0.779458] [G loss: 1.475207]\n",
      "[D loss: 0.612994] [G loss: 1.414140]\n",
      "[D loss: 0.931826] [G loss: 1.381542]\n",
      "[D loss: 0.794657] [G loss: 1.533549]\n",
      "[D loss: 0.744639] [G loss: 1.518640]\n",
      "[D loss: 0.905607] [G loss: 1.606654]\n",
      "[D loss: 0.682982] [G loss: 1.450756]\n",
      "[D loss: 0.865558] [G loss: 1.448200]\n",
      "[D loss: 0.779581] [G loss: 1.433137]\n",
      "[D loss: 0.717644] [G loss: 1.601049]\n",
      "[D loss: 0.762523] [G loss: 1.507439]\n",
      "[D loss: 0.674807] [G loss: 1.474565]\n",
      "[D loss: 1.030011] [G loss: 1.436110]\n",
      "[D loss: 0.906753] [G loss: 1.258790]\n",
      "[D loss: 0.943255] [G loss: 1.474326]\n",
      "[D loss: 0.527261] [G loss: 1.653227]\n",
      "[D loss: 0.761489] [G loss: 1.740665]\n",
      "[D loss: 0.873456] [G loss: 1.932816]\n",
      "[D loss: 0.834022] [G loss: 1.710517]\n",
      "[D loss: 0.780850] [G loss: 1.653100]\n",
      "[D loss: 0.724557] [G loss: 1.554799]\n",
      "[D loss: 0.780167] [G loss: 1.342343]\n",
      "[D loss: 0.727304] [G loss: 1.372519]\n",
      "[D loss: 0.884321] [G loss: 1.307987]\n",
      "[D loss: 0.787805] [G loss: 1.512485]\n",
      "[D loss: 0.687732] [G loss: 1.670511]\n",
      "[D loss: 0.625013] [G loss: 1.558156]\n",
      "[D loss: 0.780671] [G loss: 1.516130]\n",
      "[D loss: 0.833064] [G loss: 1.622634]\n",
      "[D loss: 0.786701] [G loss: 1.675936]\n",
      "[D loss: 0.689495] [G loss: 1.488450]\n",
      "[D loss: 0.887490] [G loss: 1.379941]\n",
      "[D loss: 0.963023] [G loss: 1.374988]\n",
      "[D loss: 0.899526] [G loss: 1.556762]\n",
      "[D loss: 0.776814] [G loss: 1.459846]\n",
      "[D loss: 0.825666] [G loss: 1.449304]\n",
      "[D loss: 0.639987] [G loss: 1.509893]\n",
      "[D loss: 1.000448] [G loss: 1.559010]\n",
      "[D loss: 0.748779] [G loss: 1.612666]\n",
      "[D loss: 0.638942] [G loss: 1.618281]\n",
      "[D loss: 0.826214] [G loss: 1.479190]\n",
      "[D loss: 0.812363] [G loss: 1.557829]\n",
      "[D loss: 0.861328] [G loss: 1.469597]\n",
      "[D loss: 0.847604] [G loss: 1.526359]\n",
      "[D loss: 0.819568] [G loss: 1.506934]\n",
      "[D loss: 0.468021] [G loss: 1.735071]\n",
      "[D loss: 0.840875] [G loss: 1.428452]\n",
      "[D loss: 0.963706] [G loss: 1.423556]\n",
      "[D loss: 0.702616] [G loss: 1.600848]\n",
      "[D loss: 0.848337] [G loss: 1.590081]\n",
      "[D loss: 0.973991] [G loss: 1.290570]\n",
      "[D loss: 0.727050] [G loss: 1.514748]\n",
      "[D loss: 0.897178] [G loss: 1.531127]\n",
      "[D loss: 0.620533] [G loss: 1.559322]\n",
      "[D loss: 0.921049] [G loss: 1.431159]\n",
      "[D loss: 0.873117] [G loss: 1.692425]\n",
      "[D loss: 0.705326] [G loss: 1.487380]\n",
      "[D loss: 0.785283] [G loss: 1.328592]\n",
      "[D loss: 0.909890] [G loss: 1.319388]\n",
      "[D loss: 0.765389] [G loss: 1.561766]\n",
      "[D loss: 0.733285] [G loss: 1.372005]\n",
      "[D loss: 0.991789] [G loss: 1.378590]\n",
      "[D loss: 0.767100] [G loss: 1.395392]\n",
      "[D loss: 0.875863] [G loss: 1.600284]\n",
      "[D loss: 0.706781] [G loss: 1.578821]\n",
      "[D loss: 0.568012] [G loss: 1.685550]\n",
      "[D loss: 1.155061] [G loss: 1.214383]\n",
      "[D loss: 0.820647] [G loss: 1.475683]\n",
      "[D loss: 0.690567] [G loss: 1.673168]\n",
      "[D loss: 0.818928] [G loss: 1.423239]\n",
      "[D loss: 0.698528] [G loss: 1.482535]\n",
      "[D loss: 0.707634] [G loss: 1.618356]\n",
      "[D loss: 0.754252] [G loss: 1.469039]\n",
      "[D loss: 0.702946] [G loss: 1.217966]\n",
      "[D loss: 0.942142] [G loss: 1.560125]\n",
      "[D loss: 1.175073] [G loss: 1.407113]\n",
      "[D loss: 0.832000] [G loss: 1.574157]\n",
      "[D loss: 0.837810] [G loss: 1.628219]\n",
      "[D loss: 0.854184] [G loss: 1.585230]\n",
      "[D loss: 0.875893] [G loss: 1.355470]\n",
      "[D loss: 0.824682] [G loss: 1.303035]\n",
      "[D loss: 0.768129] [G loss: 1.471229]\n",
      "[D loss: 0.827731] [G loss: 1.426985]\n",
      "[D loss: 0.662817] [G loss: 1.555498]\n",
      "[D loss: 0.648203] [G loss: 1.381908]\n",
      "[D loss: 0.988765] [G loss: 1.431312]\n",
      "[D loss: 0.768730] [G loss: 1.460990]\n",
      "[D loss: 1.023636] [G loss: 1.334014]\n",
      "[D loss: 0.745889] [G loss: 1.483304]\n",
      "[D loss: 1.018006] [G loss: 1.485984]\n",
      "[D loss: 0.681784] [G loss: 1.553847]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.908669] [G loss: 1.470429]\n",
      "[D loss: 0.839952] [G loss: 1.359290]\n",
      "[D loss: 0.896608] [G loss: 1.484219]\n",
      "[D loss: 0.737517] [G loss: 1.383424]\n",
      "[D loss: 1.010033] [G loss: 1.461754]\n",
      "[D loss: 0.867064] [G loss: 1.357636]\n",
      "[D loss: 0.796541] [G loss: 1.367630]\n",
      "[D loss: 0.634980] [G loss: 1.564274]\n",
      "[D loss: 0.754584] [G loss: 1.591718]\n",
      "[D loss: 0.664843] [G loss: 1.375435]\n",
      "[D loss: 0.717436] [G loss: 1.484451]\n",
      "[D loss: 0.828113] [G loss: 1.333600]\n",
      "[D loss: 1.056292] [G loss: 1.601283]\n",
      "[D loss: 0.764344] [G loss: 1.766376]\n",
      "[D loss: 0.631861] [G loss: 1.390321]\n",
      "[D loss: 0.591205] [G loss: 1.705508]\n",
      "[D loss: 0.844440] [G loss: 1.356538]\n",
      "[D loss: 0.894189] [G loss: 1.663570]\n",
      "[D loss: 0.804135] [G loss: 1.537356]\n",
      "[D loss: 0.932524] [G loss: 1.245218]\n",
      "[D loss: 0.808544] [G loss: 1.478871]\n",
      "[D loss: 0.576472] [G loss: 1.691980]\n",
      "[D loss: 0.771436] [G loss: 1.679672]\n",
      "[D loss: 0.760400] [G loss: 1.536838]\n",
      "[D loss: 0.857097] [G loss: 1.300451]\n",
      "[D loss: 0.857780] [G loss: 1.301959]\n",
      "[D loss: 0.853275] [G loss: 1.647903]\n",
      "[D loss: 1.022449] [G loss: 1.542412]\n",
      "[D loss: 0.926580] [G loss: 1.253623]\n",
      "[D loss: 0.934294] [G loss: 1.222980]\n",
      "[D loss: 0.879994] [G loss: 1.435234]\n",
      "[D loss: 0.866481] [G loss: 1.483754]\n",
      "[D loss: 0.715156] [G loss: 1.655918]\n",
      "[D loss: 0.891325] [G loss: 1.543799]\n",
      "[D loss: 0.993491] [G loss: 1.347041]\n",
      "[D loss: 1.040667] [G loss: 1.445699]\n",
      "[D loss: 0.854465] [G loss: 1.376741]\n",
      "[D loss: 0.792063] [G loss: 1.491973]\n",
      "[D loss: 0.707030] [G loss: 1.539911]\n",
      "[D loss: 0.998688] [G loss: 1.328784]\n",
      "[D loss: 0.702342] [G loss: 1.351972]\n",
      "[D loss: 0.871837] [G loss: 1.546922]\n",
      "[D loss: 0.801731] [G loss: 1.334366]\n",
      "[D loss: 0.901408] [G loss: 1.302849]\n",
      "[D loss: 0.740322] [G loss: 1.511872]\n",
      "[D loss: 0.766514] [G loss: 1.540600]\n",
      "[D loss: 0.818011] [G loss: 1.476504]\n",
      "[D loss: 0.793286] [G loss: 1.484861]\n",
      "[D loss: 0.742746] [G loss: 1.525193]\n",
      "[D loss: 1.000261] [G loss: 1.447793]\n",
      "[D loss: 1.009469] [G loss: 1.326479]\n",
      "[D loss: 0.765101] [G loss: 1.458850]\n",
      "[D loss: 0.900749] [G loss: 1.548707]\n",
      "[D loss: 0.631956] [G loss: 1.543934]\n",
      "[D loss: 0.971510] [G loss: 1.463521]\n",
      "[D loss: 0.797654] [G loss: 1.445285]\n",
      "[D loss: 0.794854] [G loss: 1.413738]\n",
      "[D loss: 0.965033] [G loss: 1.166725]\n",
      "[D loss: 0.604221] [G loss: 1.609843]\n",
      "[D loss: 0.732583] [G loss: 1.526622]\n",
      "[D loss: 0.602599] [G loss: 1.609492]\n",
      "[D loss: 0.900709] [G loss: 1.327269]\n",
      "[D loss: 0.827449] [G loss: 1.438745]\n",
      "[D loss: 0.894334] [G loss: 1.542274]\n",
      "[D loss: 0.835948] [G loss: 1.346484]\n",
      "[D loss: 0.802700] [G loss: 1.660522]\n",
      "[D loss: 0.718601] [G loss: 1.334726]\n",
      "[D loss: 0.907931] [G loss: 1.289413]\n",
      "[D loss: 0.870281] [G loss: 1.455039]\n",
      "[D loss: 0.853927] [G loss: 1.459759]\n",
      "[D loss: 0.829719] [G loss: 1.466870]\n",
      "[D loss: 0.866547] [G loss: 1.498004]\n",
      "[D loss: 0.602252] [G loss: 1.620858]\n",
      "[D loss: 0.959927] [G loss: 1.378301]\n",
      "[D loss: 0.980426] [G loss: 1.492443]\n",
      "[D loss: 0.650759] [G loss: 1.498453]\n",
      "[D loss: 0.757059] [G loss: 1.427324]\n",
      "[D loss: 0.808026] [G loss: 1.328399]\n",
      "[D loss: 0.804953] [G loss: 1.555916]\n",
      "[D loss: 0.758708] [G loss: 1.565113]\n",
      "[D loss: 1.050312] [G loss: 1.542699]\n",
      "[D loss: 0.955335] [G loss: 1.354650]\n",
      "[D loss: 1.218145] [G loss: 1.318821]\n",
      "[D loss: 1.036299] [G loss: 1.362439]\n",
      "[D loss: 0.865123] [G loss: 1.465823]\n",
      "[D loss: 0.804995] [G loss: 1.360551]\n",
      "[D loss: 0.851121] [G loss: 1.416116]\n",
      "[D loss: 0.616904] [G loss: 1.454072]\n",
      "[D loss: 1.007644] [G loss: 1.281183]\n",
      "[D loss: 0.854090] [G loss: 1.650239]\n",
      "[D loss: 0.765213] [G loss: 1.451111]\n",
      "[D loss: 0.626988] [G loss: 1.525966]\n",
      "[D loss: 0.802008] [G loss: 1.594098]\n",
      "[D loss: 0.726468] [G loss: 1.549823]\n",
      "[D loss: 0.563838] [G loss: 1.382402]\n",
      "[D loss: 0.875657] [G loss: 1.220556]\n",
      "[D loss: 0.914356] [G loss: 1.396653]\n",
      "[D loss: 0.540072] [G loss: 1.552439]\n",
      "[D loss: 0.758074] [G loss: 1.416778]\n",
      "[D loss: 0.767209] [G loss: 1.672841]\n",
      "[D loss: 0.889579] [G loss: 1.296118]\n",
      "[D loss: 0.924184] [G loss: 1.448832]\n",
      "[D loss: 0.791280] [G loss: 1.658343]\n",
      "[D loss: 0.743134] [G loss: 1.629547]\n",
      "[D loss: 1.126972] [G loss: 1.384732]\n",
      "[D loss: 0.874385] [G loss: 1.492387]\n",
      "[D loss: 0.794402] [G loss: 1.398577]\n",
      "[D loss: 0.883505] [G loss: 1.573571]\n",
      "[D loss: 0.883944] [G loss: 1.356528]\n",
      "[D loss: 1.047823] [G loss: 1.627177]\n",
      "[D loss: 0.503664] [G loss: 1.735193]\n",
      "[D loss: 0.803484] [G loss: 1.403456]\n",
      "[D loss: 0.880594] [G loss: 1.552374]\n",
      "[D loss: 0.990739] [G loss: 1.485616]\n",
      "[D loss: 0.941405] [G loss: 1.438910]\n",
      "[D loss: 0.916186] [G loss: 1.378644]\n",
      "[D loss: 0.886576] [G loss: 1.527868]\n",
      "[D loss: 0.746197] [G loss: 1.746315]\n",
      "[D loss: 0.716293] [G loss: 1.577305]\n",
      "[D loss: 0.735089] [G loss: 1.500602]\n",
      "[D loss: 0.990497] [G loss: 1.369931]\n",
      "[D loss: 0.550589] [G loss: 1.473383]\n",
      "[D loss: 0.850885] [G loss: 1.322746]\n",
      "[D loss: 0.886427] [G loss: 1.392132]\n",
      "[D loss: 0.760467] [G loss: 1.766470]\n",
      "[D loss: 0.816183] [G loss: 1.564322]\n",
      "[D loss: 0.742274] [G loss: 1.434343]\n",
      "[D loss: 0.569158] [G loss: 1.411079]\n",
      "[D loss: 0.860672] [G loss: 1.570957]\n",
      "[D loss: 0.710231] [G loss: 1.729899]\n",
      "[D loss: 0.921577] [G loss: 1.440912]\n",
      "[D loss: 0.753203] [G loss: 1.581590]\n",
      "[D loss: 0.982643] [G loss: 1.468183]\n",
      "[D loss: 0.829562] [G loss: 1.469971]\n",
      "[D loss: 0.728068] [G loss: 1.646482]\n",
      "[D loss: 1.054404] [G loss: 1.359266]\n",
      "[D loss: 0.868220] [G loss: 1.302316]\n",
      "[D loss: 0.864600] [G loss: 1.359177]\n",
      "[D loss: 0.899583] [G loss: 1.431492]\n",
      "[D loss: 0.920413] [G loss: 1.356816]\n",
      "[D loss: 0.758845] [G loss: 1.529992]\n",
      "[D loss: 0.915213] [G loss: 1.428496]\n",
      "[D loss: 0.793337] [G loss: 1.734927]\n",
      "[D loss: 0.915058] [G loss: 1.375962]\n",
      "[D loss: 0.799601] [G loss: 1.402792]\n",
      "[D loss: 0.677093] [G loss: 1.435000]\n",
      "[D loss: 0.954976] [G loss: 1.340805]\n",
      "[D loss: 0.704709] [G loss: 1.483181]\n",
      "[D loss: 0.667924] [G loss: 1.427351]\n",
      "[D loss: 0.786860] [G loss: 1.276414]\n",
      "[D loss: 0.882414] [G loss: 1.515501]\n",
      "[D loss: 0.834737] [G loss: 1.540406]\n",
      "[D loss: 0.599284] [G loss: 1.515203]\n",
      "[D loss: 0.823058] [G loss: 1.464489]\n",
      "[D loss: 0.638743] [G loss: 1.529006]\n",
      "[D loss: 0.801925] [G loss: 1.591182]\n",
      "[D loss: 0.850766] [G loss: 1.367839]\n",
      "[D loss: 1.046131] [G loss: 1.424535]\n",
      "[D loss: 0.703000] [G loss: 1.611522]\n",
      "[D loss: 0.702013] [G loss: 1.390682]\n",
      "[D loss: 0.929690] [G loss: 1.758242]\n",
      "[D loss: 0.752480] [G loss: 1.529324]\n",
      "[D loss: 0.951825] [G loss: 1.450677]\n",
      "[D loss: 0.807876] [G loss: 1.379014]\n",
      "[D loss: 0.808743] [G loss: 1.448706]\n",
      "[D loss: 0.876692] [G loss: 1.429516]\n",
      "[D loss: 1.026872] [G loss: 1.444827]\n",
      "[D loss: 1.026986] [G loss: 1.312100]\n",
      "[D loss: 0.744157] [G loss: 1.421081]\n",
      "[D loss: 0.820358] [G loss: 1.488972]\n",
      "[D loss: 0.817470] [G loss: 1.572907]\n",
      "[D loss: 1.064694] [G loss: 1.405141]\n",
      "[D loss: 0.746443] [G loss: 1.312919]\n",
      "[D loss: 0.797031] [G loss: 1.338550]\n",
      "[D loss: 0.697831] [G loss: 1.547933]\n",
      "[D loss: 0.438757] [G loss: 1.551813]\n",
      "[D loss: 0.612314] [G loss: 1.495289]\n",
      "[D loss: 0.734670] [G loss: 1.720366]\n",
      "[D loss: 0.885684] [G loss: 1.462102]\n",
      "[D loss: 0.905646] [G loss: 1.422345]\n",
      "[D loss: 0.739272] [G loss: 1.508418]\n",
      "[D loss: 0.714931] [G loss: 1.493137]\n",
      "[D loss: 0.954160] [G loss: 1.320326]\n",
      "[D loss: 0.800374] [G loss: 1.485824]\n",
      "[D loss: 1.009604] [G loss: 1.636562]\n",
      "[D loss: 0.826192] [G loss: 1.507825]\n",
      "[D loss: 0.908210] [G loss: 1.388962]\n",
      "[D loss: 0.997949] [G loss: 1.265437]\n",
      "[D loss: 0.757725] [G loss: 1.455948]\n",
      "[D loss: 0.987804] [G loss: 1.603658]\n",
      "[D loss: 0.677779] [G loss: 1.507920]\n",
      "[D loss: 0.776232] [G loss: 1.619648]\n",
      "[D loss: 0.730758] [G loss: 1.371282]\n",
      "[D loss: 1.102599] [G loss: 1.298110]\n",
      "[D loss: 0.948149] [G loss: 1.473633]\n",
      "[D loss: 0.915084] [G loss: 1.329269]\n",
      "[D loss: 0.848635] [G loss: 1.374104]\n",
      "[D loss: 0.737059] [G loss: 1.453585]\n",
      "[D loss: 0.780363] [G loss: 1.462306]\n",
      "[D loss: 0.776069] [G loss: 1.625248]\n",
      "[D loss: 0.766762] [G loss: 1.666245]\n",
      "[D loss: 0.783125] [G loss: 1.442823]\n",
      "[D loss: 0.620851] [G loss: 1.382905]\n",
      "[D loss: 0.655080] [G loss: 1.299000]\n",
      "[D loss: 0.838505] [G loss: 1.488034]\n",
      "[D loss: 0.739959] [G loss: 1.670407]\n",
      "[D loss: 0.846537] [G loss: 1.331138]\n",
      "[D loss: 0.777420] [G loss: 1.698910]\n",
      "[D loss: 0.630070] [G loss: 1.514966]\n",
      "[D loss: 0.790796] [G loss: 1.517700]\n",
      "[D loss: 0.966991] [G loss: 1.507777]\n",
      "[D loss: 0.900123] [G loss: 1.328687]\n",
      "[D loss: 0.813934] [G loss: 1.340942]\n",
      "[D loss: 0.820554] [G loss: 1.609460]\n",
      "[D loss: 0.872489] [G loss: 1.617740]\n",
      "[D loss: 1.003470] [G loss: 1.344183]\n",
      "[D loss: 0.836850] [G loss: 1.736338]\n",
      "[D loss: 0.791827] [G loss: 1.565351]\n",
      "[D loss: 0.591846] [G loss: 1.602905]\n",
      "[D loss: 0.689084] [G loss: 1.422176]\n",
      "[D loss: 0.810906] [G loss: 1.388094]\n",
      "[D loss: 0.640440] [G loss: 1.488571]\n",
      "[D loss: 0.804253] [G loss: 1.323443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.952289] [G loss: 1.730296]\n",
      "[D loss: 0.974595] [G loss: 1.463720]\n",
      "[D loss: 1.053155] [G loss: 1.333415]\n",
      "[D loss: 1.338701] [G loss: 1.348443]\n",
      "[D loss: 0.662823] [G loss: 1.420627]\n",
      "[D loss: 0.925793] [G loss: 1.503483]\n",
      "[D loss: 1.017459] [G loss: 1.218301]\n",
      "[D loss: 0.763071] [G loss: 1.392327]\n",
      "[D loss: 0.896899] [G loss: 1.429617]\n",
      "[D loss: 0.709856] [G loss: 1.626791]\n",
      "[D loss: 0.937655] [G loss: 1.519560]\n",
      "[D loss: 0.838894] [G loss: 1.565850]\n",
      "[D loss: 0.717293] [G loss: 1.778700]\n",
      "[D loss: 0.819787] [G loss: 1.491749]\n",
      "[D loss: 0.848883] [G loss: 1.498246]\n",
      "[D loss: 0.935453] [G loss: 1.552963]\n",
      "[D loss: 0.926223] [G loss: 1.195656]\n",
      "[D loss: 1.009320] [G loss: 1.477151]\n",
      "[D loss: 0.712557] [G loss: 1.488442]\n",
      "[D loss: 0.926775] [G loss: 1.412933]\n",
      "[D loss: 0.855525] [G loss: 1.438440]\n",
      "[D loss: 1.033342] [G loss: 1.445597]\n",
      "[D loss: 1.001923] [G loss: 1.351905]\n",
      "[D loss: 0.858284] [G loss: 1.308669]\n",
      "[D loss: 0.697400] [G loss: 1.488819]\n",
      "[D loss: 0.572193] [G loss: 1.553893]\n",
      "[D loss: 0.808987] [G loss: 1.587810]\n",
      "[D loss: 0.777219] [G loss: 1.676776]\n",
      "[D loss: 0.763054] [G loss: 1.745347]\n",
      "[D loss: 0.821904] [G loss: 1.574889]\n",
      "[D loss: 1.120611] [G loss: 1.327111]\n",
      "[D loss: 0.766630] [G loss: 1.534334]\n",
      "[D loss: 1.125305] [G loss: 1.402306]\n",
      "[D loss: 0.772518] [G loss: 1.636431]\n",
      "[D loss: 1.100186] [G loss: 1.368780]\n",
      "[D loss: 0.773481] [G loss: 1.508945]\n",
      "[D loss: 0.968694] [G loss: 1.288920]\n",
      "[D loss: 0.876176] [G loss: 1.409797]\n",
      "[D loss: 0.888804] [G loss: 1.498371]\n",
      "[D loss: 0.792711] [G loss: 1.432897]\n",
      "[D loss: 0.694864] [G loss: 1.589569]\n",
      "[D loss: 0.885426] [G loss: 1.330036]\n",
      "[D loss: 0.878553] [G loss: 1.306646]\n",
      "[D loss: 0.921234] [G loss: 1.597085]\n",
      "[D loss: 0.803296] [G loss: 1.487994]\n",
      "[D loss: 1.037251] [G loss: 1.514305]\n",
      "[D loss: 0.845399] [G loss: 1.605547]\n",
      "[D loss: 0.647209] [G loss: 1.502788]\n",
      "[D loss: 0.857344] [G loss: 1.348214]\n",
      "[D loss: 1.053981] [G loss: 1.308288]\n",
      "[D loss: 0.747759] [G loss: 1.419257]\n",
      "[D loss: 0.704467] [G loss: 1.452603]\n",
      "[D loss: 0.871759] [G loss: 1.581262]\n",
      "[D loss: 0.904793] [G loss: 1.454683]\n",
      "[D loss: 0.891832] [G loss: 1.445890]\n",
      "[D loss: 0.902872] [G loss: 1.216311]\n",
      "[D loss: 0.731688] [G loss: 1.486839]\n",
      "[D loss: 0.913184] [G loss: 1.469735]\n",
      "[D loss: 0.776086] [G loss: 1.390773]\n",
      "[D loss: 0.961561] [G loss: 1.563347]\n",
      "[D loss: 0.700864] [G loss: 1.339608]\n",
      "[D loss: 1.039871] [G loss: 1.398017]\n",
      "[D loss: 1.168537] [G loss: 1.460334]\n",
      "[D loss: 0.934924] [G loss: 1.491293]\n",
      "[D loss: 0.659404] [G loss: 1.605945]\n",
      "[D loss: 0.810733] [G loss: 1.391076]\n",
      "[D loss: 0.983210] [G loss: 1.370677]\n",
      "[D loss: 0.529899] [G loss: 1.442949]\n",
      "[D loss: 0.817447] [G loss: 1.424622]\n",
      "[D loss: 0.719447] [G loss: 1.416822]\n",
      "[D loss: 0.993626] [G loss: 1.245929]\n",
      "[D loss: 0.944646] [G loss: 1.402582]\n",
      "[D loss: 0.730871] [G loss: 1.565084]\n",
      "[D loss: 0.772902] [G loss: 1.480609]\n",
      "[D loss: 0.922365] [G loss: 1.536444]\n",
      "[D loss: 0.677109] [G loss: 1.550336]\n",
      "[D loss: 0.785352] [G loss: 1.766340]\n",
      "[D loss: 0.812182] [G loss: 1.652057]\n",
      "[D loss: 0.824605] [G loss: 1.655107]\n",
      "[D loss: 0.809952] [G loss: 1.516356]\n",
      "[D loss: 0.995087] [G loss: 1.370629]\n",
      "[D loss: 0.791376] [G loss: 1.359656]\n",
      "[D loss: 0.913841] [G loss: 1.323578]\n",
      "[D loss: 0.741309] [G loss: 1.552985]\n",
      "[D loss: 0.575299] [G loss: 1.480348]\n",
      "[D loss: 0.685741] [G loss: 1.624067]\n",
      "[D loss: 0.742524] [G loss: 1.440631]\n",
      "[D loss: 0.839599] [G loss: 1.286359]\n",
      "[D loss: 0.810980] [G loss: 1.636301]\n",
      "[D loss: 0.629569] [G loss: 1.639760]\n",
      "[D loss: 0.932412] [G loss: 1.467573]\n",
      "[D loss: 0.663753] [G loss: 1.485685]\n",
      "[D loss: 0.819350] [G loss: 1.644934]\n",
      "[D loss: 0.896163] [G loss: 1.545898]\n",
      "[D loss: 1.186960] [G loss: 1.474095]\n",
      "[D loss: 0.731204] [G loss: 1.336225]\n",
      "[D loss: 0.753035] [G loss: 1.362448]\n",
      "[D loss: 0.850001] [G loss: 1.339106]\n",
      "[D loss: 1.152655] [G loss: 1.311704]\n",
      "[D loss: 0.831774] [G loss: 1.264474]\n",
      "[D loss: 0.744586] [G loss: 1.597481]\n",
      "[D loss: 0.843005] [G loss: 1.464465]\n",
      "[D loss: 1.031237] [G loss: 1.471065]\n",
      "[D loss: 0.819271] [G loss: 1.540928]\n",
      "[D loss: 0.823409] [G loss: 1.193068]\n",
      "[D loss: 0.678270] [G loss: 1.483287]\n",
      "[D loss: 0.729834] [G loss: 1.378058]\n",
      "[D loss: 0.677229] [G loss: 1.557868]\n",
      "[D loss: 0.842260] [G loss: 1.399389]\n",
      "[D loss: 0.947885] [G loss: 1.667091]\n",
      "[D loss: 0.647458] [G loss: 1.426635]\n",
      "[D loss: 0.615238] [G loss: 1.370618]\n",
      "[D loss: 1.169132] [G loss: 1.312571]\n",
      "[D loss: 0.906497] [G loss: 1.344178]\n",
      "[D loss: 0.912200] [G loss: 1.433965]\n",
      "[D loss: 0.790834] [G loss: 1.242150]\n",
      "[D loss: 0.602208] [G loss: 1.615112]\n",
      "[D loss: 0.711123] [G loss: 1.518959]\n",
      "[D loss: 0.768588] [G loss: 1.491848]\n",
      "[D loss: 0.762003] [G loss: 1.630742]\n",
      "[D loss: 0.875247] [G loss: 1.571761]\n",
      "[D loss: 0.940048] [G loss: 1.434026]\n",
      "[D loss: 0.748349] [G loss: 1.692304]\n",
      "[D loss: 0.652553] [G loss: 1.628841]\n",
      "[D loss: 0.977491] [G loss: 1.665132]\n",
      "[D loss: 1.032537] [G loss: 1.289343]\n",
      "[D loss: 0.637328] [G loss: 1.510237]\n",
      "[D loss: 0.717391] [G loss: 1.389841]\n",
      "[D loss: 0.821465] [G loss: 1.439411]\n",
      "[D loss: 0.918515] [G loss: 1.310423]\n",
      "[D loss: 0.907749] [G loss: 1.415973]\n",
      "[D loss: 0.695570] [G loss: 1.850593]\n",
      "[D loss: 0.783964] [G loss: 1.586442]\n",
      "[D loss: 1.141051] [G loss: 1.499878]\n",
      "[D loss: 0.765367] [G loss: 1.589580]\n",
      "[D loss: 0.823871] [G loss: 1.280393]\n",
      "[D loss: 0.729956] [G loss: 1.590748]\n",
      "[D loss: 0.915805] [G loss: 1.679404]\n",
      "[D loss: 0.812950] [G loss: 1.381533]\n",
      "[D loss: 1.010419] [G loss: 1.418785]\n",
      "[D loss: 0.705073] [G loss: 1.363076]\n",
      "[D loss: 0.758054] [G loss: 1.472983]\n",
      "[D loss: 0.792802] [G loss: 1.571465]\n",
      "[D loss: 0.754556] [G loss: 1.565454]\n",
      "[D loss: 0.795483] [G loss: 1.632759]\n",
      "[D loss: 0.840247] [G loss: 1.534423]\n",
      "[D loss: 0.799419] [G loss: 1.418072]\n",
      "[D loss: 0.745613] [G loss: 1.301158]\n",
      "[D loss: 0.722061] [G loss: 1.522580]\n",
      "[D loss: 0.940750] [G loss: 1.524348]\n",
      "[D loss: 0.717978] [G loss: 1.673544]\n",
      "[D loss: 1.074418] [G loss: 1.586459]\n",
      "[D loss: 0.864348] [G loss: 1.365281]\n",
      "[D loss: 0.640256] [G loss: 1.553043]\n",
      "[D loss: 0.782738] [G loss: 1.361388]\n",
      "[D loss: 0.887663] [G loss: 1.515470]\n",
      "[D loss: 0.709032] [G loss: 1.415840]\n",
      "[D loss: 0.844775] [G loss: 1.464285]\n",
      "[D loss: 1.191888] [G loss: 1.489351]\n",
      "[D loss: 0.671560] [G loss: 1.337056]\n",
      "[D loss: 0.965314] [G loss: 1.431629]\n",
      "[D loss: 0.695054] [G loss: 1.526446]\n",
      "[D loss: 0.761027] [G loss: 1.771286]\n",
      "[D loss: 0.652906] [G loss: 1.606476]\n",
      "[D loss: 0.871073] [G loss: 1.259516]\n",
      "[D loss: 0.792987] [G loss: 1.293182]\n",
      "[D loss: 1.018377] [G loss: 1.297670]\n",
      "[D loss: 0.970643] [G loss: 1.478688]\n",
      "[D loss: 0.995792] [G loss: 1.371975]\n",
      "[D loss: 0.848314] [G loss: 1.306044]\n",
      "[D loss: 0.740280] [G loss: 1.433557]\n",
      "[D loss: 0.796013] [G loss: 1.396482]\n",
      "[D loss: 0.928436] [G loss: 1.235802]\n",
      "[D loss: 0.650778] [G loss: 1.549604]\n",
      "[D loss: 0.683817] [G loss: 1.574344]\n",
      "[D loss: 0.846086] [G loss: 1.274704]\n",
      "[D loss: 0.906166] [G loss: 1.622517]\n",
      "[D loss: 0.825812] [G loss: 1.490875]\n",
      "[D loss: 0.814783] [G loss: 1.481993]\n",
      "[D loss: 0.831259] [G loss: 1.336284]\n",
      "[D loss: 0.961223] [G loss: 1.448817]\n",
      "[D loss: 0.771648] [G loss: 1.549583]\n",
      "[D loss: 0.891616] [G loss: 1.432755]\n",
      "[D loss: 0.813927] [G loss: 1.622305]\n",
      "[D loss: 0.796101] [G loss: 1.512040]\n",
      "[D loss: 0.710635] [G loss: 1.303813]\n",
      "[D loss: 0.737377] [G loss: 1.183069]\n",
      "[D loss: 1.047876] [G loss: 1.364771]\n",
      "[D loss: 0.978107] [G loss: 1.494095]\n",
      "[D loss: 0.850717] [G loss: 1.436258]\n",
      "[D loss: 0.816257] [G loss: 1.270796]\n",
      "[D loss: 0.790309] [G loss: 1.496671]\n",
      "[D loss: 0.927552] [G loss: 1.411860]\n",
      "[D loss: 0.741076] [G loss: 1.501075]\n",
      "[D loss: 0.845270] [G loss: 1.597227]\n",
      "[D loss: 0.935672] [G loss: 1.434317]\n",
      "[D loss: 1.129076] [G loss: 1.342390]\n",
      "[D loss: 0.915763] [G loss: 1.288010]\n",
      "[D loss: 0.815096] [G loss: 1.627970]\n",
      "[D loss: 0.739151] [G loss: 1.566669]\n",
      "[D loss: 0.908109] [G loss: 1.508481]\n",
      "[D loss: 0.987564] [G loss: 1.454408]\n",
      "[D loss: 0.855484] [G loss: 1.669902]\n",
      "[D loss: 0.729750] [G loss: 1.254749]\n",
      "[D loss: 0.615306] [G loss: 1.432033]\n",
      "[D loss: 0.691313] [G loss: 1.575340]\n",
      "[D loss: 0.927770] [G loss: 1.474305]\n",
      "[D loss: 0.924117] [G loss: 1.223583]\n",
      "[D loss: 0.637958] [G loss: 1.351288]\n",
      "[D loss: 0.902326] [G loss: 1.473448]\n",
      "[D loss: 1.094413] [G loss: 1.499904]\n",
      "[D loss: 0.889878] [G loss: 1.461110]\n",
      "[D loss: 0.808346] [G loss: 1.420450]\n",
      "[D loss: 0.621693] [G loss: 1.576314]\n",
      "[D loss: 0.777366] [G loss: 1.523875]\n",
      "[D loss: 0.920805] [G loss: 1.475378]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.840453] [G loss: 1.597651]\n",
      "[D loss: 0.755820] [G loss: 1.481607]\n",
      "[D loss: 0.756598] [G loss: 1.587835]\n",
      "[D loss: 0.602452] [G loss: 1.691188]\n",
      "[D loss: 0.761787] [G loss: 1.675628]\n",
      "[D loss: 0.704535] [G loss: 1.474648]\n",
      "[D loss: 0.817666] [G loss: 1.431685]\n",
      "[D loss: 0.724149] [G loss: 1.538085]\n",
      "[D loss: 0.966140] [G loss: 1.349874]\n",
      "[D loss: 0.931896] [G loss: 1.431002]\n",
      "[D loss: 0.819577] [G loss: 1.422514]\n",
      "[D loss: 0.643140] [G loss: 1.640270]\n",
      "[D loss: 0.921563] [G loss: 1.470250]\n",
      "[D loss: 0.677606] [G loss: 1.531947]\n",
      "[D loss: 0.722749] [G loss: 1.739624]\n",
      "[D loss: 0.755712] [G loss: 1.558258]\n",
      "[D loss: 1.070146] [G loss: 1.363068]\n",
      "[D loss: 0.621634] [G loss: 1.713681]\n",
      "[D loss: 1.149040] [G loss: 1.548362]\n",
      "[D loss: 0.764226] [G loss: 1.577502]\n",
      "[D loss: 0.733063] [G loss: 1.549920]\n",
      "[D loss: 0.971131] [G loss: 1.461548]\n",
      "[D loss: 0.936489] [G loss: 1.139862]\n",
      "[D loss: 1.228648] [G loss: 1.534016]\n",
      "[D loss: 0.955645] [G loss: 1.482865]\n",
      "[D loss: 0.757554] [G loss: 1.607705]\n",
      "[D loss: 0.931721] [G loss: 1.313467]\n",
      "[D loss: 0.940660] [G loss: 1.495882]\n",
      "[D loss: 0.807747] [G loss: 1.406577]\n",
      "[D loss: 0.805317] [G loss: 1.460384]\n",
      "[D loss: 0.686258] [G loss: 1.645131]\n",
      "[D loss: 0.859940] [G loss: 1.555896]\n",
      "[D loss: 0.620273] [G loss: 1.246057]\n",
      "[D loss: 0.726337] [G loss: 1.450188]\n",
      "[D loss: 0.969425] [G loss: 1.431203]\n",
      "[D loss: 0.751855] [G loss: 1.397369]\n",
      "[D loss: 0.858643] [G loss: 1.248473]\n",
      "[D loss: 0.850977] [G loss: 1.532257]\n",
      "[D loss: 0.844299] [G loss: 1.552506]\n",
      "[D loss: 0.757017] [G loss: 1.534730]\n",
      "[D loss: 0.844586] [G loss: 1.460110]\n",
      "[D loss: 0.529556] [G loss: 1.479959]\n",
      "[D loss: 0.894344] [G loss: 1.290354]\n",
      "[D loss: 0.925174] [G loss: 1.671745]\n",
      "[D loss: 0.617313] [G loss: 1.545007]\n",
      "[D loss: 0.865005] [G loss: 1.462901]\n",
      "[D loss: 0.988395] [G loss: 1.264749]\n",
      "[D loss: 0.677944] [G loss: 1.467141]\n",
      "[D loss: 0.926933] [G loss: 1.255360]\n",
      "[D loss: 0.826339] [G loss: 1.291786]\n",
      "[D loss: 0.879795] [G loss: 1.363920]\n",
      "[D loss: 0.688656] [G loss: 1.507066]\n",
      "[D loss: 0.886622] [G loss: 1.475277]\n",
      "[D loss: 0.785460] [G loss: 1.626063]\n",
      "[D loss: 0.726685] [G loss: 1.589434]\n",
      "[D loss: 0.760260] [G loss: 1.335239]\n",
      "[D loss: 0.825329] [G loss: 1.536744]\n",
      "[D loss: 0.851262] [G loss: 1.302537]\n",
      "[D loss: 0.851042] [G loss: 1.607890]\n",
      "[D loss: 1.239047] [G loss: 1.344363]\n",
      "[D loss: 1.142517] [G loss: 1.342750]\n",
      "[D loss: 0.879458] [G loss: 1.526427]\n",
      "[D loss: 0.986833] [G loss: 1.693919]\n",
      "[D loss: 0.653045] [G loss: 1.449413]\n",
      "[D loss: 0.979392] [G loss: 1.539807]\n",
      "[D loss: 0.995693] [G loss: 1.276550]\n",
      "[D loss: 0.970527] [G loss: 1.240553]\n",
      "[D loss: 0.623245] [G loss: 1.419019]\n",
      "[D loss: 0.845073] [G loss: 1.408230]\n",
      "[D loss: 0.811315] [G loss: 1.454600]\n",
      "[D loss: 0.887474] [G loss: 1.362497]\n",
      "[D loss: 0.902959] [G loss: 1.377245]\n",
      "[D loss: 0.859832] [G loss: 1.531130]\n",
      "[D loss: 1.025468] [G loss: 1.474242]\n",
      "[D loss: 0.722393] [G loss: 1.248339]\n",
      "[D loss: 0.869305] [G loss: 1.379103]\n",
      "[D loss: 0.950339] [G loss: 1.384496]\n",
      "[D loss: 0.897137] [G loss: 1.305115]\n",
      "[D loss: 0.670692] [G loss: 1.252407]\n",
      "[D loss: 0.837269] [G loss: 1.388695]\n",
      "[D loss: 0.952299] [G loss: 1.251085]\n",
      "[D loss: 0.886034] [G loss: 1.308317]\n",
      "[D loss: 0.836723] [G loss: 1.336236]\n",
      "[D loss: 0.848453] [G loss: 1.372757]\n",
      "[D loss: 0.946910] [G loss: 1.440003]\n",
      "[D loss: 0.847412] [G loss: 1.470136]\n",
      "[D loss: 0.734618] [G loss: 1.299928]\n",
      "[D loss: 0.811768] [G loss: 1.240157]\n",
      "[D loss: 0.710615] [G loss: 1.468037]\n",
      "[D loss: 0.728663] [G loss: 1.325017]\n",
      "[D loss: 0.667905] [G loss: 1.463130]\n",
      "[D loss: 0.736392] [G loss: 1.877232]\n",
      "[D loss: 0.703979] [G loss: 1.463377]\n",
      "[D loss: 0.869934] [G loss: 1.422157]\n",
      "[D loss: 0.802882] [G loss: 1.539231]\n",
      "[D loss: 0.837487] [G loss: 1.430875]\n",
      "[D loss: 1.009571] [G loss: 1.353721]\n",
      "[D loss: 0.919021] [G loss: 1.530111]\n",
      "[D loss: 0.990827] [G loss: 1.422424]\n",
      "[D loss: 0.688525] [G loss: 1.547016]\n",
      "[D loss: 0.742217] [G loss: 1.509085]\n",
      "[D loss: 0.952168] [G loss: 1.214814]\n",
      "[D loss: 1.045005] [G loss: 1.489160]\n",
      "[D loss: 0.761150] [G loss: 1.584166]\n",
      "[D loss: 0.816913] [G loss: 1.446849]\n",
      "[D loss: 0.771712] [G loss: 1.451128]\n",
      "[D loss: 0.689363] [G loss: 1.294805]\n",
      "[D loss: 1.060730] [G loss: 1.328112]\n",
      "[D loss: 0.803084] [G loss: 1.212418]\n",
      "[D loss: 1.011999] [G loss: 1.358776]\n",
      "[D loss: 0.697174] [G loss: 1.399155]\n",
      "[D loss: 0.796280] [G loss: 1.509899]\n",
      "[D loss: 0.724161] [G loss: 1.492866]\n",
      "[D loss: 0.865534] [G loss: 1.417480]\n",
      "[D loss: 0.784012] [G loss: 1.350318]\n",
      "[D loss: 0.881207] [G loss: 1.475820]\n",
      "[D loss: 0.862410] [G loss: 1.439288]\n",
      "[D loss: 0.725218] [G loss: 1.746144]\n",
      "[D loss: 1.059217] [G loss: 1.554073]\n",
      "[D loss: 0.904189] [G loss: 1.454821]\n",
      "[D loss: 0.796539] [G loss: 1.429852]\n",
      "[D loss: 0.930228] [G loss: 1.256688]\n",
      "[D loss: 0.827765] [G loss: 1.426456]\n",
      "[D loss: 1.058209] [G loss: 1.184729]\n",
      "[D loss: 0.884002] [G loss: 1.401116]\n",
      "[D loss: 0.963920] [G loss: 1.327971]\n",
      "[D loss: 0.959587] [G loss: 1.434599]\n",
      "[D loss: 0.763583] [G loss: 1.455848]\n",
      "[D loss: 1.012854] [G loss: 1.701087]\n",
      "[D loss: 0.527222] [G loss: 1.640449]\n",
      "[D loss: 0.919885] [G loss: 1.612604]\n",
      "[D loss: 0.756754] [G loss: 1.531207]\n",
      "[D loss: 0.829424] [G loss: 1.474614]\n",
      "[D loss: 0.833881] [G loss: 1.603682]\n",
      "[D loss: 0.787897] [G loss: 1.289648]\n",
      "[D loss: 0.938944] [G loss: 1.351884]\n",
      "[D loss: 0.945256] [G loss: 1.263834]\n",
      "[D loss: 0.865781] [G loss: 1.352422]\n",
      "[D loss: 0.807455] [G loss: 1.251949]\n",
      "[D loss: 1.002061] [G loss: 1.297190]\n",
      "[D loss: 0.888225] [G loss: 1.516627]\n",
      "[D loss: 0.997936] [G loss: 1.402672]\n",
      "[D loss: 0.905530] [G loss: 1.576871]\n",
      "[D loss: 0.890753] [G loss: 1.411182]\n",
      "[D loss: 0.948648] [G loss: 1.109241]\n",
      "[D loss: 1.088552] [G loss: 1.274328]\n",
      "[D loss: 0.887422] [G loss: 1.488629]\n",
      "[D loss: 0.791742] [G loss: 1.558269]\n",
      "[D loss: 0.886814] [G loss: 1.294387]\n",
      "[D loss: 0.976646] [G loss: 1.469849]\n",
      "[D loss: 0.856136] [G loss: 1.641901]\n",
      "[D loss: 0.866383] [G loss: 1.749222]\n",
      "[D loss: 0.692515] [G loss: 1.411901]\n",
      "[D loss: 0.634611] [G loss: 1.451288]\n",
      "[D loss: 0.699997] [G loss: 1.351816]\n",
      "[D loss: 0.744339] [G loss: 1.689636]\n",
      "[D loss: 1.019247] [G loss: 1.253237]\n",
      "[D loss: 0.941854] [G loss: 1.452982]\n",
      "[D loss: 0.744790] [G loss: 1.310578]\n",
      "[D loss: 0.999951] [G loss: 1.366948]\n",
      "[D loss: 0.946110] [G loss: 1.548520]\n",
      "[D loss: 0.605110] [G loss: 1.530403]\n",
      "[D loss: 0.643397] [G loss: 1.270823]\n",
      "[D loss: 0.795271] [G loss: 1.394019]\n",
      "[D loss: 0.691998] [G loss: 1.617726]\n",
      "[D loss: 0.971350] [G loss: 1.684721]\n",
      "[D loss: 0.952849] [G loss: 1.252315]\n",
      "[D loss: 0.793227] [G loss: 1.328687]\n",
      "[D loss: 1.072379] [G loss: 1.341563]\n",
      "[D loss: 0.933719] [G loss: 1.500681]\n",
      "[D loss: 0.713514] [G loss: 1.397156]\n",
      "[D loss: 0.725795] [G loss: 1.343347]\n",
      "[D loss: 0.654849] [G loss: 1.455121]\n",
      "[D loss: 0.804643] [G loss: 1.575418]\n",
      "[D loss: 0.679248] [G loss: 1.475521]\n",
      "[D loss: 0.783009] [G loss: 1.477188]\n",
      "[D loss: 0.845237] [G loss: 1.734064]\n",
      "[D loss: 0.738310] [G loss: 1.367704]\n",
      "[D loss: 1.036574] [G loss: 1.518673]\n",
      "[D loss: 0.946973] [G loss: 1.653628]\n",
      "[D loss: 0.976175] [G loss: 1.561660]\n",
      "[D loss: 0.803258] [G loss: 1.467787]\n",
      "[D loss: 0.909505] [G loss: 1.408898]\n",
      "[D loss: 0.914110] [G loss: 1.394092]\n",
      "[D loss: 0.801063] [G loss: 1.680623]\n",
      "[D loss: 0.722916] [G loss: 1.433592]\n",
      "[D loss: 0.715847] [G loss: 1.572973]\n",
      "[D loss: 0.824995] [G loss: 1.299794]\n",
      "[D loss: 0.800597] [G loss: 1.158002]\n",
      "[D loss: 1.092623] [G loss: 1.365139]\n",
      "[D loss: 0.895251] [G loss: 1.180107]\n",
      "[D loss: 0.846785] [G loss: 1.607759]\n",
      "[D loss: 0.622959] [G loss: 1.505492]\n",
      "[D loss: 0.782334] [G loss: 1.362544]\n",
      "[D loss: 0.699684] [G loss: 1.477634]\n",
      "[D loss: 0.639302] [G loss: 1.406655]\n",
      "[D loss: 0.775016] [G loss: 1.280113]\n",
      "[D loss: 0.714913] [G loss: 1.430409]\n",
      "[D loss: 0.671605] [G loss: 1.445436]\n",
      "[D loss: 0.993345] [G loss: 1.378594]\n",
      "[D loss: 0.709094] [G loss: 1.424024]\n",
      "[D loss: 1.025261] [G loss: 1.568735]\n",
      "[D loss: 0.719426] [G loss: 1.327402]\n",
      "[D loss: 0.842148] [G loss: 1.439604]\n",
      "[D loss: 0.810473] [G loss: 1.549883]\n",
      "[D loss: 0.862138] [G loss: 1.541973]\n",
      "[D loss: 1.153864] [G loss: 1.174628]\n",
      "[D loss: 0.737854] [G loss: 1.420102]\n",
      "[D loss: 0.844058] [G loss: 1.599785]\n",
      "[D loss: 0.910424] [G loss: 1.463891]\n",
      "[D loss: 0.959267] [G loss: 1.311992]\n",
      "[D loss: 0.750019] [G loss: 1.356712]\n",
      "[D loss: 0.845296] [G loss: 1.529620]\n",
      "[D loss: 0.666798] [G loss: 1.461915]\n",
      "[D loss: 0.903084] [G loss: 1.437466]\n",
      "[D loss: 0.587743] [G loss: 1.381267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.862929] [G loss: 1.608875]\n",
      "[D loss: 0.967257] [G loss: 1.211348]\n",
      "[D loss: 0.898101] [G loss: 1.446509]\n",
      "[D loss: 0.786752] [G loss: 1.284574]\n",
      "[D loss: 0.749879] [G loss: 1.623676]\n",
      "[D loss: 0.785720] [G loss: 1.522972]\n",
      "[D loss: 0.726120] [G loss: 1.604551]\n",
      "[D loss: 1.047765] [G loss: 1.319124]\n",
      "[D loss: 1.110505] [G loss: 1.260466]\n",
      "[D loss: 0.868991] [G loss: 1.147153]\n",
      "[D loss: 0.717438] [G loss: 1.339590]\n",
      "[D loss: 0.855259] [G loss: 1.449365]\n",
      "[D loss: 0.909239] [G loss: 1.421459]\n",
      "[D loss: 0.888247] [G loss: 1.386791]\n",
      "[D loss: 0.842010] [G loss: 1.503604]\n",
      "[D loss: 0.962281] [G loss: 1.490022]\n",
      "[D loss: 0.829600] [G loss: 1.416067]\n",
      "[D loss: 0.567517] [G loss: 1.505424]\n",
      "[D loss: 0.974268] [G loss: 1.540219]\n",
      "[D loss: 0.737690] [G loss: 1.664531]\n",
      "[D loss: 0.753649] [G loss: 1.561720]\n",
      "[D loss: 0.780461] [G loss: 1.509954]\n",
      "[D loss: 0.867378] [G loss: 1.209604]\n",
      "[D loss: 0.749670] [G loss: 1.526124]\n",
      "[D loss: 0.834224] [G loss: 1.493618]\n",
      "[D loss: 0.797187] [G loss: 1.313899]\n",
      "[D loss: 0.942954] [G loss: 1.460249]\n",
      "[D loss: 0.687459] [G loss: 1.566597]\n",
      "[D loss: 0.909953] [G loss: 1.539598]\n",
      "[D loss: 0.835140] [G loss: 1.315461]\n",
      "[D loss: 0.734402] [G loss: 1.536922]\n",
      "[D loss: 0.817445] [G loss: 1.520418]\n",
      "[D loss: 0.615922] [G loss: 1.540607]\n",
      "[D loss: 0.762830] [G loss: 1.503833]\n",
      "[D loss: 0.876969] [G loss: 1.333798]\n",
      "[D loss: 0.928796] [G loss: 1.499648]\n",
      "[D loss: 0.838036] [G loss: 1.746146]\n",
      "[D loss: 0.783092] [G loss: 1.762306]\n",
      "[D loss: 0.636418] [G loss: 1.445317]\n",
      "[D loss: 0.799637] [G loss: 1.539258]\n",
      "[D loss: 0.871875] [G loss: 1.594462]\n",
      "[D loss: 0.685917] [G loss: 1.751681]\n",
      "[D loss: 0.910949] [G loss: 1.438268]\n",
      "[D loss: 0.862246] [G loss: 1.458974]\n",
      "[D loss: 0.919699] [G loss: 1.438452]\n",
      "[D loss: 0.809890] [G loss: 1.415148]\n",
      "[D loss: 0.883544] [G loss: 1.432338]\n",
      "[D loss: 0.800547] [G loss: 1.493450]\n",
      "[D loss: 0.966694] [G loss: 1.828959]\n",
      "[D loss: 0.801779] [G loss: 1.686384]\n",
      "[D loss: 0.767500] [G loss: 1.406847]\n",
      "[D loss: 0.727669] [G loss: 1.349497]\n",
      "[D loss: 0.754378] [G loss: 1.601574]\n",
      "[D loss: 0.785649] [G loss: 1.646269]\n",
      "[D loss: 0.999761] [G loss: 1.697713]\n",
      "[D loss: 0.774802] [G loss: 1.554672]\n",
      "[D loss: 0.683445] [G loss: 1.669371]\n",
      "[D loss: 0.785819] [G loss: 1.480159]\n",
      "[D loss: 0.878348] [G loss: 1.290189]\n",
      "[D loss: 0.697899] [G loss: 1.455303]\n",
      "[D loss: 0.599885] [G loss: 1.585387]\n",
      "[D loss: 0.693236] [G loss: 1.654169]\n",
      "[D loss: 0.647699] [G loss: 1.581833]\n",
      "[D loss: 0.826153] [G loss: 1.473834]\n",
      "[D loss: 0.595145] [G loss: 1.464402]\n",
      "[D loss: 1.068008] [G loss: 1.338229]\n",
      "[D loss: 0.802257] [G loss: 1.540672]\n",
      "[D loss: 0.824458] [G loss: 1.610368]\n",
      "[D loss: 0.661731] [G loss: 1.561306]\n",
      "[D loss: 1.053698] [G loss: 1.408119]\n",
      "[D loss: 0.923580] [G loss: 1.494144]\n",
      "[D loss: 0.744962] [G loss: 1.451057]\n",
      "[D loss: 0.713615] [G loss: 1.498889]\n",
      "[D loss: 0.707581] [G loss: 1.526045]\n",
      "[D loss: 0.803675] [G loss: 1.460452]\n",
      "[D loss: 0.590067] [G loss: 1.422477]\n",
      "[D loss: 0.856843] [G loss: 1.726110]\n",
      "[D loss: 0.629363] [G loss: 1.652729]\n",
      "[D loss: 0.849736] [G loss: 1.737625]\n",
      "[D loss: 0.589610] [G loss: 1.509667]\n",
      "[D loss: 0.632358] [G loss: 1.507634]\n",
      "[D loss: 0.782110] [G loss: 1.454525]\n",
      "[D loss: 0.724255] [G loss: 1.762376]\n",
      "[D loss: 0.783203] [G loss: 1.491927]\n",
      "[D loss: 0.856631] [G loss: 1.685182]\n",
      "[D loss: 1.011421] [G loss: 1.477023]\n",
      "[D loss: 1.097616] [G loss: 1.305202]\n",
      "[D loss: 0.894026] [G loss: 1.466249]\n",
      "[D loss: 0.807297] [G loss: 1.341819]\n",
      "[D loss: 0.748613] [G loss: 1.453599]\n",
      "[D loss: 0.878597] [G loss: 1.539590]\n",
      "[D loss: 0.748482] [G loss: 1.343399]\n",
      "[D loss: 0.922673] [G loss: 1.349672]\n",
      "[D loss: 0.711218] [G loss: 1.506126]\n",
      "[D loss: 0.775683] [G loss: 1.556170]\n",
      "[D loss: 0.806617] [G loss: 1.885737]\n",
      "[D loss: 0.936531] [G loss: 1.410794]\n",
      "[D loss: 0.832768] [G loss: 1.607262]\n",
      "[D loss: 0.808742] [G loss: 1.426695]\n",
      "[D loss: 0.845418] [G loss: 1.406379]\n",
      "[D loss: 0.539653] [G loss: 1.450872]\n",
      "[D loss: 1.134686] [G loss: 1.579967]\n",
      "[D loss: 0.767325] [G loss: 1.418664]\n",
      "[D loss: 0.924204] [G loss: 1.537860]\n",
      "[D loss: 0.691709] [G loss: 1.398573]\n",
      "[D loss: 1.070669] [G loss: 1.418019]\n",
      "[D loss: 0.644269] [G loss: 1.381596]\n",
      "[D loss: 0.834019] [G loss: 1.361451]\n",
      "[D loss: 0.968800] [G loss: 1.773978]\n",
      "[D loss: 1.001655] [G loss: 1.296479]\n",
      "[D loss: 0.638701] [G loss: 1.502460]\n",
      "[D loss: 0.709746] [G loss: 1.285194]\n",
      "[D loss: 0.779795] [G loss: 1.526153]\n",
      "[D loss: 1.066592] [G loss: 1.386994]\n",
      "[D loss: 0.676556] [G loss: 1.473985]\n",
      "[D loss: 1.026578] [G loss: 1.677945]\n",
      "[D loss: 0.688154] [G loss: 1.332710]\n",
      "[D loss: 0.856554] [G loss: 1.439235]\n",
      "[D loss: 0.954744] [G loss: 1.635518]\n",
      "[D loss: 0.906983] [G loss: 1.291411]\n",
      "[D loss: 0.864930] [G loss: 1.527847]\n",
      "[D loss: 0.651691] [G loss: 1.247237]\n",
      "[D loss: 0.846270] [G loss: 1.591673]\n",
      "[D loss: 0.807063] [G loss: 1.260413]\n",
      "[D loss: 0.857735] [G loss: 1.421734]\n",
      "[D loss: 0.927067] [G loss: 1.384698]\n",
      "[D loss: 0.821371] [G loss: 1.344588]\n",
      "[D loss: 0.924733] [G loss: 1.452109]\n",
      "[D loss: 0.753926] [G loss: 1.333106]\n",
      "[D loss: 0.923837] [G loss: 1.205664]\n",
      "[D loss: 0.896186] [G loss: 1.541028]\n",
      "[D loss: 0.984952] [G loss: 1.305694]\n",
      "[D loss: 1.095016] [G loss: 1.273042]\n",
      "[D loss: 0.759676] [G loss: 1.367196]\n",
      "[D loss: 0.883231] [G loss: 1.652929]\n",
      "[D loss: 0.741388] [G loss: 1.639699]\n",
      "[D loss: 0.902951] [G loss: 1.259944]\n",
      "[D loss: 0.803132] [G loss: 1.277716]\n",
      "[D loss: 0.806853] [G loss: 1.374463]\n",
      "[D loss: 0.923248] [G loss: 1.461768]\n",
      "[D loss: 1.146417] [G loss: 1.371394]\n",
      "[D loss: 0.868686] [G loss: 1.643224]\n",
      "[D loss: 0.728351] [G loss: 1.472745]\n",
      "[D loss: 0.881242] [G loss: 1.308696]\n",
      "[D loss: 0.750389] [G loss: 1.464964]\n",
      "[D loss: 0.975031] [G loss: 1.198961]\n",
      "[D loss: 0.996616] [G loss: 1.462587]\n",
      "[D loss: 0.646364] [G loss: 1.452519]\n",
      "[D loss: 0.826079] [G loss: 1.407115]\n",
      "[D loss: 0.660152] [G loss: 1.645138]\n",
      "[D loss: 0.952039] [G loss: 1.506007]\n",
      "[D loss: 0.767627] [G loss: 1.523386]\n",
      "[D loss: 0.933694] [G loss: 1.412192]\n",
      "[D loss: 0.842005] [G loss: 1.299002]\n",
      "[D loss: 0.965525] [G loss: 1.354777]\n",
      "[D loss: 0.664525] [G loss: 1.347425]\n",
      "[D loss: 0.885722] [G loss: 1.526527]\n",
      "[D loss: 0.707837] [G loss: 1.552872]\n",
      "[D loss: 0.876246] [G loss: 1.354817]\n",
      "[D loss: 1.010459] [G loss: 1.476170]\n",
      "[D loss: 0.948387] [G loss: 1.527785]\n",
      "[D loss: 0.980569] [G loss: 1.460086]\n",
      "[D loss: 0.752305] [G loss: 1.344942]\n",
      "[D loss: 0.740866] [G loss: 1.321589]\n",
      "[D loss: 0.783438] [G loss: 1.322604]\n",
      "[D loss: 0.743142] [G loss: 1.545226]\n",
      "[D loss: 0.782023] [G loss: 1.669858]\n",
      "[D loss: 0.895677] [G loss: 1.319758]\n",
      "[D loss: 1.015556] [G loss: 1.426023]\n",
      "[D loss: 0.734859] [G loss: 1.594245]\n",
      "[D loss: 0.666596] [G loss: 1.483240]\n",
      "[D loss: 0.758916] [G loss: 1.662238]\n",
      "[D loss: 0.683292] [G loss: 1.626748]\n",
      "[D loss: 0.669800] [G loss: 1.518857]\n",
      "[D loss: 0.950883] [G loss: 1.581476]\n",
      "[D loss: 0.799834] [G loss: 1.475992]\n",
      "[D loss: 0.790787] [G loss: 1.483716]\n",
      "[D loss: 0.891468] [G loss: 1.522302]\n",
      "[D loss: 0.982732] [G loss: 1.319157]\n",
      "[D loss: 0.738909] [G loss: 1.634837]\n",
      "[D loss: 0.872424] [G loss: 1.515084]\n",
      "[D loss: 0.782372] [G loss: 1.456190]\n",
      "[D loss: 0.729242] [G loss: 1.259997]\n",
      "[D loss: 0.852834] [G loss: 1.422835]\n",
      "[D loss: 1.033708] [G loss: 1.325521]\n",
      "[D loss: 0.820920] [G loss: 1.428810]\n",
      "[D loss: 0.843292] [G loss: 1.704962]\n",
      "[D loss: 0.671865] [G loss: 1.456600]\n",
      "[D loss: 0.837870] [G loss: 1.278828]\n",
      "[D loss: 0.431513] [G loss: 1.535122]\n",
      "[D loss: 0.988324] [G loss: 1.456784]\n",
      "[D loss: 0.828143] [G loss: 1.473114]\n",
      "[D loss: 0.969846] [G loss: 1.250325]\n",
      "[D loss: 0.879791] [G loss: 1.173468]\n",
      "[D loss: 0.886497] [G loss: 1.356022]\n",
      "[D loss: 0.748500] [G loss: 1.529521]\n",
      "[D loss: 0.780063] [G loss: 1.529948]\n",
      "[D loss: 1.019266] [G loss: 1.454748]\n",
      "[D loss: 0.704228] [G loss: 1.592471]\n",
      "[D loss: 1.081111] [G loss: 1.426917]\n",
      "[D loss: 1.146004] [G loss: 1.151343]\n",
      "[D loss: 0.828226] [G loss: 1.316693]\n",
      "[D loss: 0.834708] [G loss: 1.396680]\n",
      "[D loss: 0.874699] [G loss: 1.619739]\n",
      "[D loss: 0.762006] [G loss: 1.586033]\n",
      "[D loss: 0.817189] [G loss: 1.437995]\n",
      "[D loss: 0.818878] [G loss: 1.399691]\n",
      "[D loss: 0.739756] [G loss: 1.441288]\n",
      "[D loss: 0.787378] [G loss: 1.480896]\n",
      "[D loss: 0.829102] [G loss: 1.415557]\n",
      "[D loss: 0.567228] [G loss: 1.504906]\n",
      "[D loss: 0.884850] [G loss: 1.278754]\n",
      "[D loss: 0.690673] [G loss: 1.609804]\n",
      "[D loss: 0.726835] [G loss: 1.643145]\n",
      "[D loss: 0.798418] [G loss: 1.680703]\n",
      "[D loss: 0.800772] [G loss: 1.621169]\n",
      "[D loss: 0.864079] [G loss: 1.579323]\n",
      "[D loss: 0.929367] [G loss: 1.442563]\n",
      "[D loss: 0.815406] [G loss: 1.485100]\n",
      "[D loss: 0.872819] [G loss: 1.565328]\n",
      "[D loss: 1.087404] [G loss: 1.510677]\n",
      "[D loss: 0.856841] [G loss: 1.442463]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.808172] [G loss: 1.445162]\n",
      "[D loss: 1.041739] [G loss: 1.362808]\n",
      "[D loss: 0.859205] [G loss: 1.506760]\n",
      "[D loss: 0.772854] [G loss: 1.577119]\n",
      "[D loss: 0.729593] [G loss: 1.402931]\n",
      "[D loss: 0.949831] [G loss: 1.534333]\n",
      "[D loss: 0.886506] [G loss: 1.430889]\n",
      "[D loss: 0.862869] [G loss: 1.380176]\n",
      "[D loss: 0.678376] [G loss: 1.491782]\n",
      "[D loss: 0.975299] [G loss: 1.532769]\n",
      "[D loss: 0.663831] [G loss: 1.489848]\n",
      "[D loss: 0.982607] [G loss: 1.531794]\n",
      "[D loss: 0.558399] [G loss: 1.479816]\n",
      "[D loss: 0.822245] [G loss: 1.349983]\n",
      "[D loss: 0.850679] [G loss: 1.553694]\n",
      "[D loss: 0.843696] [G loss: 1.444854]\n",
      "[D loss: 0.922118] [G loss: 1.258856]\n",
      "[D loss: 0.971257] [G loss: 1.235421]\n",
      "[D loss: 1.086140] [G loss: 1.245185]\n",
      "[D loss: 0.692710] [G loss: 1.463961]\n",
      "[D loss: 0.771609] [G loss: 1.502584]\n",
      "[D loss: 0.862446] [G loss: 1.707732]\n",
      "[D loss: 0.691044] [G loss: 1.707241]\n",
      "[D loss: 0.986732] [G loss: 1.485985]\n",
      "[D loss: 0.642100] [G loss: 1.576456]\n",
      "[D loss: 0.887207] [G loss: 1.456370]\n",
      "[D loss: 0.811960] [G loss: 1.502359]\n",
      "[D loss: 0.919496] [G loss: 1.632142]\n",
      "[D loss: 0.884483] [G loss: 1.731858]\n",
      "[D loss: 0.827333] [G loss: 1.504759]\n",
      "[D loss: 1.063371] [G loss: 1.262584]\n",
      "[D loss: 0.573214] [G loss: 1.697335]\n",
      "[D loss: 1.081221] [G loss: 1.212761]\n",
      "[D loss: 0.969152] [G loss: 1.195847]\n",
      "[D loss: 0.857348] [G loss: 1.456697]\n",
      "[D loss: 0.755574] [G loss: 1.502761]\n",
      "[D loss: 0.939798] [G loss: 1.248181]\n",
      "[D loss: 1.044995] [G loss: 1.389304]\n",
      "[D loss: 0.814396] [G loss: 1.370666]\n",
      "[D loss: 0.709496] [G loss: 1.587002]\n",
      "[D loss: 0.889591] [G loss: 1.370355]\n",
      "[D loss: 0.789514] [G loss: 1.388486]\n",
      "[D loss: 0.846291] [G loss: 1.394356]\n",
      "[D loss: 0.677402] [G loss: 1.620966]\n",
      "[D loss: 0.848942] [G loss: 1.439988]\n",
      "[D loss: 1.073491] [G loss: 1.352774]\n",
      "[D loss: 0.783458] [G loss: 1.511614]\n",
      "[D loss: 0.884656] [G loss: 1.490735]\n",
      "[D loss: 1.080229] [G loss: 1.391729]\n",
      "[D loss: 0.692151] [G loss: 1.312369]\n",
      "[D loss: 0.875708] [G loss: 1.314841]\n",
      "[D loss: 0.885460] [G loss: 1.501625]\n",
      "[D loss: 0.948821] [G loss: 1.443674]\n",
      "[D loss: 0.702342] [G loss: 1.477227]\n",
      "[D loss: 0.961428] [G loss: 1.516070]\n",
      "[D loss: 1.035797] [G loss: 1.283285]\n",
      "[D loss: 0.868837] [G loss: 1.369885]\n",
      "[D loss: 0.778973] [G loss: 1.641268]\n",
      "[D loss: 0.603973] [G loss: 1.343278]\n",
      "[D loss: 0.724585] [G loss: 1.405399]\n",
      "[D loss: 0.706161] [G loss: 1.186510]\n",
      "[D loss: 0.639159] [G loss: 1.377633]\n",
      "[D loss: 1.127114] [G loss: 1.376069]\n",
      "[D loss: 1.010845] [G loss: 1.370929]\n",
      "[D loss: 0.709683] [G loss: 1.380634]\n",
      "[D loss: 0.925937] [G loss: 1.312259]\n",
      "[D loss: 1.012477] [G loss: 1.261098]\n",
      "[D loss: 0.879744] [G loss: 1.295731]\n",
      "[D loss: 0.731186] [G loss: 1.337804]\n",
      "[D loss: 1.039040] [G loss: 1.381655]\n",
      "[D loss: 0.840234] [G loss: 1.658870]\n",
      "[D loss: 0.779564] [G loss: 1.437714]\n",
      "[D loss: 0.876212] [G loss: 1.536422]\n",
      "[D loss: 0.874766] [G loss: 1.505333]\n",
      "[D loss: 0.624894] [G loss: 1.567495]\n",
      "[D loss: 0.927278] [G loss: 1.543733]\n",
      "[D loss: 0.845418] [G loss: 1.468832]\n",
      "[D loss: 0.781759] [G loss: 1.203569]\n",
      "[D loss: 0.782422] [G loss: 1.530042]\n",
      "[D loss: 0.555872] [G loss: 1.618187]\n",
      "[D loss: 0.887301] [G loss: 1.619332]\n",
      "[D loss: 0.865644] [G loss: 1.449925]\n",
      "[D loss: 1.096771] [G loss: 1.455272]\n",
      "[D loss: 0.709473] [G loss: 1.497032]\n",
      "[D loss: 0.850096] [G loss: 1.612741]\n",
      "[D loss: 1.000002] [G loss: 1.546547]\n",
      "[D loss: 0.846481] [G loss: 1.388838]\n",
      "[D loss: 0.902113] [G loss: 1.327186]\n",
      "[D loss: 0.603505] [G loss: 1.325525]\n",
      "[D loss: 0.759923] [G loss: 1.381943]\n",
      "[D loss: 0.750570] [G loss: 1.407096]\n",
      "[D loss: 0.768549] [G loss: 1.536821]\n",
      "[D loss: 0.644843] [G loss: 1.228256]\n",
      "[D loss: 0.893240] [G loss: 1.312601]\n",
      "[D loss: 0.723814] [G loss: 1.638929]\n",
      "[D loss: 0.754287] [G loss: 1.552122]\n",
      "[D loss: 0.992047] [G loss: 1.407766]\n",
      "[D loss: 0.891361] [G loss: 1.619446]\n",
      "[D loss: 1.001686] [G loss: 1.470451]\n",
      "[D loss: 0.835524] [G loss: 1.659381]\n",
      "[D loss: 0.749260] [G loss: 1.492360]\n",
      "[D loss: 0.904325] [G loss: 1.458214]\n",
      "[D loss: 0.661164] [G loss: 1.487564]\n",
      "[D loss: 0.885517] [G loss: 1.432222]\n",
      "[D loss: 0.733354] [G loss: 1.483285]\n",
      "[D loss: 0.915600] [G loss: 1.344359]\n",
      "[D loss: 0.939075] [G loss: 1.285646]\n",
      "[D loss: 0.906544] [G loss: 1.420960]\n",
      "[D loss: 0.998129] [G loss: 1.334623]\n",
      "[D loss: 0.754275] [G loss: 1.629520]\n",
      "[D loss: 0.520765] [G loss: 1.587693]\n",
      "[D loss: 0.987536] [G loss: 1.398074]\n",
      "[D loss: 0.691324] [G loss: 1.302444]\n",
      "[D loss: 0.950020] [G loss: 1.410288]\n",
      "[D loss: 0.867413] [G loss: 1.444030]\n",
      "[D loss: 0.652667] [G loss: 1.786626]\n",
      "[D loss: 0.949764] [G loss: 1.521132]\n",
      "[D loss: 0.834566] [G loss: 1.611327]\n",
      "[D loss: 0.736521] [G loss: 1.426042]\n",
      "[D loss: 0.601904] [G loss: 1.527219]\n",
      "[D loss: 0.884598] [G loss: 1.401519]\n",
      "[D loss: 0.939954] [G loss: 1.359357]\n",
      "[D loss: 0.814296] [G loss: 1.475938]\n",
      "[D loss: 0.912257] [G loss: 1.555546]\n",
      "[D loss: 0.886896] [G loss: 1.448014]\n",
      "[D loss: 0.936659] [G loss: 1.248025]\n",
      "[D loss: 0.686934] [G loss: 1.505356]\n",
      "epoch:13, g_loss:2757.742919921875,d_loss:1555.721923828125\n",
      "[D loss: 0.907087] [G loss: 1.426702]\n",
      "[D loss: 0.765349] [G loss: 1.370831]\n",
      "[D loss: 0.741675] [G loss: 1.400896]\n",
      "[D loss: 0.782725] [G loss: 1.338143]\n",
      "[D loss: 0.719684] [G loss: 1.553205]\n",
      "[D loss: 0.774077] [G loss: 1.491838]\n",
      "[D loss: 0.889049] [G loss: 1.503604]\n",
      "[D loss: 0.868903] [G loss: 1.333453]\n",
      "[D loss: 0.800632] [G loss: 1.371844]\n",
      "[D loss: 0.919999] [G loss: 1.366088]\n",
      "[D loss: 0.754516] [G loss: 1.359040]\n",
      "[D loss: 0.658222] [G loss: 1.581135]\n",
      "[D loss: 0.883396] [G loss: 1.454775]\n",
      "[D loss: 0.646474] [G loss: 1.323748]\n",
      "[D loss: 0.861485] [G loss: 1.360728]\n",
      "[D loss: 0.888500] [G loss: 1.577615]\n",
      "[D loss: 0.853222] [G loss: 1.551888]\n",
      "[D loss: 0.754566] [G loss: 1.434928]\n",
      "[D loss: 0.982288] [G loss: 1.306993]\n",
      "[D loss: 0.882278] [G loss: 1.458744]\n",
      "[D loss: 0.763221] [G loss: 1.412678]\n",
      "[D loss: 0.911649] [G loss: 1.509444]\n",
      "[D loss: 0.529455] [G loss: 1.628739]\n",
      "[D loss: 0.960899] [G loss: 1.435795]\n",
      "[D loss: 0.770401] [G loss: 1.298736]\n",
      "[D loss: 0.704469] [G loss: 1.453010]\n",
      "[D loss: 0.923273] [G loss: 1.300755]\n",
      "[D loss: 0.783475] [G loss: 1.615643]\n",
      "[D loss: 0.765720] [G loss: 1.482890]\n",
      "[D loss: 0.836155] [G loss: 1.363506]\n",
      "[D loss: 0.932467] [G loss: 1.398425]\n",
      "[D loss: 0.882565] [G loss: 1.238721]\n",
      "[D loss: 0.709262] [G loss: 1.509823]\n",
      "[D loss: 0.655392] [G loss: 1.744212]\n",
      "[D loss: 0.829982] [G loss: 1.538077]\n",
      "[D loss: 0.864504] [G loss: 1.436198]\n",
      "[D loss: 1.073992] [G loss: 1.508692]\n",
      "[D loss: 0.667623] [G loss: 1.418631]\n",
      "[D loss: 0.985054] [G loss: 1.488613]\n",
      "[D loss: 0.766201] [G loss: 1.508529]\n",
      "[D loss: 0.885679] [G loss: 1.570411]\n",
      "[D loss: 0.774700] [G loss: 1.394342]\n",
      "[D loss: 0.857748] [G loss: 1.429674]\n",
      "[D loss: 0.678170] [G loss: 1.631780]\n",
      "[D loss: 0.878630] [G loss: 1.708811]\n",
      "[D loss: 0.693933] [G loss: 1.912680]\n",
      "[D loss: 0.673515] [G loss: 1.581471]\n",
      "[D loss: 0.659651] [G loss: 1.418029]\n",
      "[D loss: 0.883875] [G loss: 1.463270]\n",
      "[D loss: 0.574074] [G loss: 1.644511]\n",
      "[D loss: 0.984731] [G loss: 1.506711]\n",
      "[D loss: 0.723286] [G loss: 1.405958]\n",
      "[D loss: 1.128724] [G loss: 1.295639]\n",
      "[D loss: 0.715636] [G loss: 1.363213]\n",
      "[D loss: 0.690150] [G loss: 1.626511]\n",
      "[D loss: 0.986505] [G loss: 1.622901]\n",
      "[D loss: 0.828935] [G loss: 1.600092]\n",
      "[D loss: 1.111175] [G loss: 1.652202]\n",
      "[D loss: 1.057670] [G loss: 1.347372]\n",
      "[D loss: 0.808101] [G loss: 1.406241]\n",
      "[D loss: 0.984963] [G loss: 1.418138]\n",
      "[D loss: 0.769811] [G loss: 1.501557]\n",
      "[D loss: 0.880054] [G loss: 1.590666]\n",
      "[D loss: 0.465556] [G loss: 1.449453]\n",
      "[D loss: 0.635685] [G loss: 1.635885]\n",
      "[D loss: 0.752247] [G loss: 1.589464]\n",
      "[D loss: 0.704224] [G loss: 1.401373]\n",
      "[D loss: 0.865204] [G loss: 1.450876]\n",
      "[D loss: 0.832950] [G loss: 1.515716]\n",
      "[D loss: 0.754171] [G loss: 1.516647]\n",
      "[D loss: 0.826304] [G loss: 1.274556]\n",
      "[D loss: 0.831346] [G loss: 1.729812]\n",
      "[D loss: 0.915226] [G loss: 1.671807]\n",
      "[D loss: 1.023248] [G loss: 1.513215]\n",
      "[D loss: 0.779708] [G loss: 1.647552]\n",
      "[D loss: 1.050673] [G loss: 1.361709]\n",
      "[D loss: 0.828734] [G loss: 1.581993]\n",
      "[D loss: 0.711101] [G loss: 1.520693]\n",
      "[D loss: 0.864209] [G loss: 1.314290]\n",
      "[D loss: 0.925448] [G loss: 1.334294]\n",
      "[D loss: 0.939134] [G loss: 1.312791]\n",
      "[D loss: 0.994598] [G loss: 1.343806]\n",
      "[D loss: 1.059191] [G loss: 1.346352]\n",
      "[D loss: 0.646842] [G loss: 1.391963]\n",
      "[D loss: 0.752288] [G loss: 1.301748]\n",
      "[D loss: 0.920452] [G loss: 1.319151]\n",
      "[D loss: 0.657297] [G loss: 1.422998]\n",
      "[D loss: 0.914147] [G loss: 1.282886]\n",
      "[D loss: 0.713523] [G loss: 1.599592]\n",
      "[D loss: 0.864227] [G loss: 1.334662]\n",
      "[D loss: 0.872002] [G loss: 1.404002]\n",
      "[D loss: 0.689180] [G loss: 1.466363]\n",
      "[D loss: 0.846234] [G loss: 1.355384]\n",
      "[D loss: 0.847034] [G loss: 1.296369]\n",
      "[D loss: 0.812499] [G loss: 1.446438]\n",
      "[D loss: 0.929496] [G loss: 1.395786]\n",
      "[D loss: 0.741847] [G loss: 1.306825]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.799619] [G loss: 1.454687]\n",
      "[D loss: 0.731668] [G loss: 1.463253]\n",
      "[D loss: 0.840853] [G loss: 1.432366]\n",
      "[D loss: 0.940652] [G loss: 1.437292]\n",
      "[D loss: 0.765449] [G loss: 1.502680]\n",
      "[D loss: 0.741873] [G loss: 1.650100]\n",
      "[D loss: 0.791968] [G loss: 1.321444]\n",
      "[D loss: 1.016801] [G loss: 1.439180]\n",
      "[D loss: 0.974432] [G loss: 1.301318]\n",
      "[D loss: 0.783024] [G loss: 1.582972]\n",
      "[D loss: 0.883475] [G loss: 1.472374]\n",
      "[D loss: 0.873995] [G loss: 1.302915]\n",
      "[D loss: 0.994077] [G loss: 1.376479]\n",
      "[D loss: 0.753315] [G loss: 1.437246]\n",
      "[D loss: 1.057870] [G loss: 1.465063]\n",
      "[D loss: 0.784392] [G loss: 1.421775]\n",
      "[D loss: 0.787278] [G loss: 1.329588]\n",
      "[D loss: 0.653407] [G loss: 1.536634]\n",
      "[D loss: 1.098252] [G loss: 1.260552]\n",
      "[D loss: 0.928644] [G loss: 1.373633]\n",
      "[D loss: 0.761145] [G loss: 1.295630]\n",
      "[D loss: 0.820897] [G loss: 1.113771]\n",
      "[D loss: 0.784706] [G loss: 1.310294]\n",
      "[D loss: 0.731916] [G loss: 1.433968]\n",
      "[D loss: 0.766915] [G loss: 1.385643]\n",
      "[D loss: 0.976817] [G loss: 1.365489]\n",
      "[D loss: 0.782358] [G loss: 1.384348]\n",
      "[D loss: 0.926230] [G loss: 1.427467]\n",
      "[D loss: 0.924487] [G loss: 1.241020]\n",
      "[D loss: 0.845505] [G loss: 1.382061]\n",
      "[D loss: 0.750164] [G loss: 1.398462]\n",
      "[D loss: 0.768301] [G loss: 1.439758]\n",
      "[D loss: 0.766599] [G loss: 1.713168]\n",
      "[D loss: 0.854570] [G loss: 1.520923]\n",
      "[D loss: 0.611572] [G loss: 1.646753]\n",
      "[D loss: 0.833224] [G loss: 1.350578]\n",
      "[D loss: 0.825625] [G loss: 1.380904]\n",
      "[D loss: 0.883188] [G loss: 1.414559]\n",
      "[D loss: 0.736695] [G loss: 1.435838]\n",
      "[D loss: 0.717263] [G loss: 1.579677]\n",
      "[D loss: 1.048514] [G loss: 1.503309]\n",
      "[D loss: 0.883240] [G loss: 1.494708]\n",
      "[D loss: 0.734623] [G loss: 1.454504]\n",
      "[D loss: 0.868173] [G loss: 1.439976]\n",
      "[D loss: 0.683202] [G loss: 1.550260]\n",
      "[D loss: 0.796855] [G loss: 1.408075]\n",
      "[D loss: 0.743156] [G loss: 1.648288]\n",
      "[D loss: 0.827506] [G loss: 1.418675]\n",
      "[D loss: 0.714908] [G loss: 1.521631]\n",
      "[D loss: 0.963229] [G loss: 1.402760]\n",
      "[D loss: 0.696355] [G loss: 1.548447]\n",
      "[D loss: 0.811893] [G loss: 1.452024]\n",
      "[D loss: 0.785743] [G loss: 1.563883]\n",
      "[D loss: 1.050847] [G loss: 1.521094]\n",
      "[D loss: 0.621429] [G loss: 1.493040]\n",
      "[D loss: 0.975248] [G loss: 1.386309]\n",
      "[D loss: 0.706567] [G loss: 1.491417]\n",
      "[D loss: 0.724061] [G loss: 1.514143]\n",
      "[D loss: 0.778406] [G loss: 1.745134]\n",
      "[D loss: 0.802111] [G loss: 1.431463]\n",
      "[D loss: 0.750255] [G loss: 1.729778]\n",
      "[D loss: 0.770364] [G loss: 1.459545]\n",
      "[D loss: 0.988590] [G loss: 1.488943]\n",
      "[D loss: 0.793227] [G loss: 1.475233]\n",
      "[D loss: 0.680869] [G loss: 1.367251]\n",
      "[D loss: 0.889624] [G loss: 1.419286]\n",
      "[D loss: 0.651617] [G loss: 1.468363]\n",
      "[D loss: 0.867543] [G loss: 1.611888]\n",
      "[D loss: 0.729664] [G loss: 1.516815]\n",
      "[D loss: 1.074916] [G loss: 1.373519]\n",
      "[D loss: 0.880246] [G loss: 1.652025]\n",
      "[D loss: 0.708035] [G loss: 1.398899]\n",
      "[D loss: 0.829661] [G loss: 1.359130]\n",
      "[D loss: 0.787097] [G loss: 1.376333]\n",
      "[D loss: 1.024908] [G loss: 1.301468]\n",
      "[D loss: 1.028175] [G loss: 1.525308]\n",
      "[D loss: 0.751974] [G loss: 1.432859]\n",
      "[D loss: 0.712512] [G loss: 1.514284]\n",
      "[D loss: 0.886023] [G loss: 1.549909]\n",
      "[D loss: 0.833290] [G loss: 1.420556]\n",
      "[D loss: 0.777335] [G loss: 1.473221]\n",
      "[D loss: 0.725790] [G loss: 1.596519]\n",
      "[D loss: 0.761262] [G loss: 1.579874]\n",
      "[D loss: 0.834477] [G loss: 1.524011]\n",
      "[D loss: 0.902574] [G loss: 1.415723]\n",
      "[D loss: 0.752136] [G loss: 1.430447]\n",
      "[D loss: 1.034083] [G loss: 1.359743]\n",
      "[D loss: 0.774404] [G loss: 1.558616]\n",
      "[D loss: 1.090225] [G loss: 1.478820]\n",
      "[D loss: 0.850931] [G loss: 1.911050]\n",
      "[D loss: 0.983700] [G loss: 1.526638]\n",
      "[D loss: 0.663845] [G loss: 1.464323]\n",
      "[D loss: 1.060868] [G loss: 1.362165]\n",
      "[D loss: 0.729924] [G loss: 1.631169]\n",
      "[D loss: 0.951182] [G loss: 1.689259]\n",
      "[D loss: 0.957816] [G loss: 1.402240]\n",
      "[D loss: 0.863696] [G loss: 1.386984]\n",
      "[D loss: 0.759238] [G loss: 1.376274]\n",
      "[D loss: 0.758091] [G loss: 1.538813]\n",
      "[D loss: 0.954375] [G loss: 1.490502]\n",
      "[D loss: 0.474721] [G loss: 1.500980]\n",
      "[D loss: 0.742915] [G loss: 1.579515]\n",
      "[D loss: 0.666340] [G loss: 1.526658]\n",
      "[D loss: 0.723304] [G loss: 1.534650]\n",
      "[D loss: 1.019744] [G loss: 1.504652]\n",
      "[D loss: 0.787616] [G loss: 1.721398]\n",
      "[D loss: 0.721186] [G loss: 1.668052]\n",
      "[D loss: 0.669174] [G loss: 1.444351]\n",
      "[D loss: 0.991339] [G loss: 1.555116]\n",
      "[D loss: 0.575391] [G loss: 1.484193]\n",
      "[D loss: 0.595523] [G loss: 1.602919]\n",
      "[D loss: 0.788818] [G loss: 1.368511]\n",
      "[D loss: 0.776124] [G loss: 1.435669]\n",
      "[D loss: 0.518714] [G loss: 1.686488]\n",
      "[D loss: 0.713238] [G loss: 1.576581]\n",
      "[D loss: 0.851498] [G loss: 1.448885]\n",
      "[D loss: 0.901301] [G loss: 1.502345]\n",
      "[D loss: 0.712195] [G loss: 1.503956]\n",
      "[D loss: 0.771435] [G loss: 1.407895]\n",
      "[D loss: 1.160269] [G loss: 1.294667]\n",
      "[D loss: 0.777659] [G loss: 1.499536]\n",
      "[D loss: 0.695578] [G loss: 1.791965]\n",
      "[D loss: 0.880242] [G loss: 1.653976]\n",
      "[D loss: 0.846906] [G loss: 1.551094]\n",
      "[D loss: 0.986470] [G loss: 1.288883]\n",
      "[D loss: 1.025801] [G loss: 1.402206]\n",
      "[D loss: 0.850899] [G loss: 1.347996]\n",
      "[D loss: 1.049219] [G loss: 1.544178]\n",
      "[D loss: 0.902782] [G loss: 1.367171]\n",
      "[D loss: 0.772977] [G loss: 1.504093]\n",
      "[D loss: 0.826241] [G loss: 1.531377]\n",
      "[D loss: 0.928108] [G loss: 1.373524]\n",
      "[D loss: 0.781464] [G loss: 1.564260]\n",
      "[D loss: 0.779761] [G loss: 1.512844]\n",
      "[D loss: 0.667029] [G loss: 1.355264]\n",
      "[D loss: 0.787497] [G loss: 1.240940]\n",
      "[D loss: 0.710458] [G loss: 1.529446]\n",
      "[D loss: 0.962067] [G loss: 1.320428]\n",
      "[D loss: 0.815602] [G loss: 1.453574]\n",
      "[D loss: 0.707158] [G loss: 1.316299]\n",
      "[D loss: 0.565143] [G loss: 1.791048]\n",
      "[D loss: 0.719172] [G loss: 1.383194]\n",
      "[D loss: 1.162504] [G loss: 1.460626]\n",
      "[D loss: 0.791240] [G loss: 1.282627]\n",
      "[D loss: 0.726440] [G loss: 1.490206]\n",
      "[D loss: 0.697944] [G loss: 1.410571]\n",
      "[D loss: 0.890665] [G loss: 1.514946]\n",
      "[D loss: 0.636545] [G loss: 1.599280]\n",
      "[D loss: 0.727025] [G loss: 1.532355]\n",
      "[D loss: 0.841876] [G loss: 1.597501]\n",
      "[D loss: 0.786603] [G loss: 1.341185]\n",
      "[D loss: 0.836980] [G loss: 1.371746]\n",
      "[D loss: 0.900725] [G loss: 1.645231]\n",
      "[D loss: 0.647956] [G loss: 1.476084]\n",
      "[D loss: 0.676498] [G loss: 1.587285]\n",
      "[D loss: 0.802500] [G loss: 1.510907]\n",
      "[D loss: 0.803795] [G loss: 1.568303]\n",
      "[D loss: 0.834035] [G loss: 1.389471]\n",
      "[D loss: 0.629445] [G loss: 1.486528]\n",
      "[D loss: 0.833617] [G loss: 1.275034]\n",
      "[D loss: 0.739503] [G loss: 1.861299]\n",
      "[D loss: 0.874884] [G loss: 1.421432]\n",
      "[D loss: 0.875491] [G loss: 1.598682]\n",
      "[D loss: 0.881037] [G loss: 1.424678]\n",
      "[D loss: 0.900284] [G loss: 1.654773]\n",
      "[D loss: 0.901674] [G loss: 1.717635]\n",
      "[D loss: 0.605371] [G loss: 1.561069]\n",
      "[D loss: 0.765766] [G loss: 1.573399]\n",
      "[D loss: 0.845634] [G loss: 1.613906]\n",
      "[D loss: 0.734497] [G loss: 1.684020]\n",
      "[D loss: 0.983132] [G loss: 1.237498]\n",
      "[D loss: 1.098376] [G loss: 1.629851]\n",
      "[D loss: 0.604995] [G loss: 1.487719]\n",
      "[D loss: 0.733216] [G loss: 1.426545]\n",
      "[D loss: 0.913703] [G loss: 1.448493]\n",
      "[D loss: 1.086218] [G loss: 1.221885]\n",
      "[D loss: 1.075139] [G loss: 1.362419]\n",
      "[D loss: 0.612163] [G loss: 1.395403]\n",
      "[D loss: 0.767316] [G loss: 1.470468]\n",
      "[D loss: 0.757361] [G loss: 1.713737]\n",
      "[D loss: 0.806802] [G loss: 1.664442]\n",
      "[D loss: 0.863360] [G loss: 1.603527]\n",
      "[D loss: 0.987774] [G loss: 1.312796]\n",
      "[D loss: 0.654849] [G loss: 1.520443]\n",
      "[D loss: 0.778024] [G loss: 1.506951]\n",
      "[D loss: 0.771705] [G loss: 1.434398]\n",
      "[D loss: 0.678597] [G loss: 1.430840]\n",
      "[D loss: 0.652866] [G loss: 1.472242]\n",
      "[D loss: 0.812963] [G loss: 1.560357]\n",
      "[D loss: 0.896049] [G loss: 1.587355]\n",
      "[D loss: 0.860897] [G loss: 1.413668]\n",
      "[D loss: 0.708062] [G loss: 1.566869]\n",
      "[D loss: 0.864094] [G loss: 1.587094]\n",
      "[D loss: 0.712301] [G loss: 1.692703]\n",
      "[D loss: 0.743278] [G loss: 1.470655]\n",
      "[D loss: 0.796554] [G loss: 1.450451]\n",
      "[D loss: 0.744342] [G loss: 1.389703]\n",
      "[D loss: 0.594310] [G loss: 1.580646]\n",
      "[D loss: 0.827445] [G loss: 1.331138]\n",
      "[D loss: 0.819204] [G loss: 1.432914]\n",
      "[D loss: 0.698226] [G loss: 1.578260]\n",
      "[D loss: 0.790051] [G loss: 1.613199]\n",
      "[D loss: 1.126806] [G loss: 1.536896]\n",
      "[D loss: 0.850445] [G loss: 1.636373]\n",
      "[D loss: 0.889506] [G loss: 1.444089]\n",
      "[D loss: 0.859026] [G loss: 1.537348]\n",
      "[D loss: 1.067763] [G loss: 1.593729]\n",
      "[D loss: 0.811221] [G loss: 1.436355]\n",
      "[D loss: 0.921925] [G loss: 1.485788]\n",
      "[D loss: 0.764787] [G loss: 1.406816]\n",
      "[D loss: 0.833569] [G loss: 1.355331]\n",
      "[D loss: 1.036892] [G loss: 1.413943]\n",
      "[D loss: 0.984509] [G loss: 1.264588]\n",
      "[D loss: 0.860471] [G loss: 1.609987]\n",
      "[D loss: 1.085076] [G loss: 1.391827]\n",
      "[D loss: 1.095058] [G loss: 1.669368]\n",
      "[D loss: 0.756970] [G loss: 1.464490]\n",
      "[D loss: 0.783735] [G loss: 1.254962]\n",
      "[D loss: 0.860333] [G loss: 1.333766]\n",
      "[D loss: 0.870737] [G loss: 1.242414]\n",
      "[D loss: 0.775614] [G loss: 1.276366]\n",
      "[D loss: 0.783939] [G loss: 1.262709]\n",
      "[D loss: 0.671812] [G loss: 1.544200]\n",
      "[D loss: 0.970403] [G loss: 1.386815]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.755113] [G loss: 1.386496]\n",
      "[D loss: 0.772410] [G loss: 1.350553]\n",
      "[D loss: 0.597901] [G loss: 1.488757]\n",
      "[D loss: 0.981129] [G loss: 1.441715]\n",
      "[D loss: 1.019608] [G loss: 1.438421]\n",
      "[D loss: 0.924751] [G loss: 1.491059]\n",
      "[D loss: 0.731270] [G loss: 1.563312]\n",
      "[D loss: 0.780959] [G loss: 1.448268]\n",
      "[D loss: 1.000249] [G loss: 1.566679]\n",
      "[D loss: 0.882056] [G loss: 1.342942]\n",
      "[D loss: 0.709980] [G loss: 1.515347]\n",
      "[D loss: 0.792066] [G loss: 1.383182]\n",
      "[D loss: 0.695082] [G loss: 1.290581]\n",
      "[D loss: 0.921888] [G loss: 1.425626]\n",
      "[D loss: 0.894514] [G loss: 1.661964]\n",
      "[D loss: 0.505759] [G loss: 1.663304]\n",
      "[D loss: 0.645352] [G loss: 1.625964]\n",
      "[D loss: 0.868650] [G loss: 1.537850]\n",
      "[D loss: 0.914663] [G loss: 1.544172]\n",
      "[D loss: 0.805026] [G loss: 1.405369]\n",
      "[D loss: 0.827581] [G loss: 1.444075]\n",
      "[D loss: 0.984239] [G loss: 1.467627]\n",
      "[D loss: 0.697268] [G loss: 1.487253]\n",
      "[D loss: 0.777136] [G loss: 1.493743]\n",
      "[D loss: 0.771662] [G loss: 1.524862]\n",
      "[D loss: 0.818711] [G loss: 1.551396]\n",
      "[D loss: 0.687014] [G loss: 1.568888]\n",
      "[D loss: 0.924204] [G loss: 1.593348]\n",
      "[D loss: 1.097397] [G loss: 1.543286]\n",
      "[D loss: 1.311096] [G loss: 1.326487]\n",
      "[D loss: 0.791512] [G loss: 1.476893]\n",
      "[D loss: 0.812049] [G loss: 1.432066]\n",
      "[D loss: 0.839985] [G loss: 1.508739]\n",
      "[D loss: 0.897791] [G loss: 1.437580]\n",
      "[D loss: 0.879534] [G loss: 1.486117]\n",
      "[D loss: 0.842381] [G loss: 1.306143]\n",
      "[D loss: 0.838374] [G loss: 1.469777]\n",
      "[D loss: 1.036447] [G loss: 1.391387]\n",
      "[D loss: 0.895082] [G loss: 1.443211]\n",
      "[D loss: 0.790656] [G loss: 1.342773]\n",
      "[D loss: 0.974908] [G loss: 1.315227]\n",
      "[D loss: 0.907096] [G loss: 1.310403]\n",
      "[D loss: 0.917032] [G loss: 1.207984]\n",
      "[D loss: 1.018911] [G loss: 1.189044]\n",
      "[D loss: 0.933663] [G loss: 1.299398]\n",
      "[D loss: 0.647071] [G loss: 1.464807]\n",
      "[D loss: 0.719346] [G loss: 1.382379]\n",
      "[D loss: 0.894937] [G loss: 1.598933]\n",
      "[D loss: 0.882894] [G loss: 1.427134]\n",
      "[D loss: 0.847585] [G loss: 1.220335]\n",
      "[D loss: 0.915630] [G loss: 1.520612]\n",
      "[D loss: 0.680899] [G loss: 1.381476]\n",
      "[D loss: 0.804150] [G loss: 1.538500]\n",
      "[D loss: 0.777732] [G loss: 1.488979]\n",
      "[D loss: 0.684819] [G loss: 1.590784]\n",
      "[D loss: 0.923438] [G loss: 1.397995]\n",
      "[D loss: 0.800879] [G loss: 1.493339]\n",
      "[D loss: 0.654665] [G loss: 1.702266]\n",
      "[D loss: 0.946856] [G loss: 1.499982]\n",
      "[D loss: 0.893860] [G loss: 1.452371]\n",
      "[D loss: 0.828823] [G loss: 1.482799]\n",
      "[D loss: 0.900161] [G loss: 1.556651]\n",
      "[D loss: 1.003553] [G loss: 1.270695]\n",
      "[D loss: 0.900444] [G loss: 1.466132]\n",
      "[D loss: 0.970118] [G loss: 1.262688]\n",
      "[D loss: 0.814453] [G loss: 1.488970]\n",
      "[D loss: 0.906388] [G loss: 1.205537]\n",
      "[D loss: 0.722052] [G loss: 1.412375]\n",
      "[D loss: 0.701490] [G loss: 1.534804]\n",
      "[D loss: 0.934572] [G loss: 1.661822]\n",
      "[D loss: 0.817394] [G loss: 1.525804]\n",
      "[D loss: 0.970833] [G loss: 1.520139]\n",
      "[D loss: 0.733476] [G loss: 1.562828]\n",
      "[D loss: 0.605800] [G loss: 1.664243]\n",
      "[D loss: 0.715754] [G loss: 1.508393]\n",
      "[D loss: 0.918803] [G loss: 1.359791]\n",
      "[D loss: 0.780265] [G loss: 1.503253]\n",
      "[D loss: 0.823159] [G loss: 1.343185]\n",
      "[D loss: 0.675656] [G loss: 1.655551]\n",
      "[D loss: 1.021325] [G loss: 1.389620]\n",
      "[D loss: 0.715805] [G loss: 1.455771]\n",
      "[D loss: 0.741719] [G loss: 1.456024]\n",
      "[D loss: 1.023421] [G loss: 1.455639]\n",
      "[D loss: 0.759475] [G loss: 1.653399]\n",
      "[D loss: 0.873067] [G loss: 1.472624]\n",
      "[D loss: 0.813189] [G loss: 1.699148]\n",
      "[D loss: 0.779471] [G loss: 1.277942]\n",
      "[D loss: 0.684178] [G loss: 1.550091]\n",
      "[D loss: 0.857166] [G loss: 1.543493]\n",
      "[D loss: 0.905760] [G loss: 1.516081]\n",
      "[D loss: 0.981554] [G loss: 1.675370]\n",
      "[D loss: 0.819425] [G loss: 1.465896]\n",
      "[D loss: 0.683721] [G loss: 1.563117]\n",
      "[D loss: 0.819582] [G loss: 1.435874]\n",
      "[D loss: 1.005338] [G loss: 1.320621]\n",
      "[D loss: 0.906486] [G loss: 1.556192]\n",
      "[D loss: 0.754628] [G loss: 1.538963]\n",
      "[D loss: 1.042801] [G loss: 1.395891]\n",
      "[D loss: 0.820186] [G loss: 1.384048]\n",
      "[D loss: 0.878519] [G loss: 1.317855]\n",
      "[D loss: 1.237981] [G loss: 1.331343]\n",
      "[D loss: 0.794509] [G loss: 1.417459]\n",
      "[D loss: 0.885399] [G loss: 1.340691]\n",
      "[D loss: 0.826591] [G loss: 1.281553]\n",
      "[D loss: 0.968782] [G loss: 1.318886]\n",
      "[D loss: 0.857496] [G loss: 1.446700]\n",
      "[D loss: 0.816253] [G loss: 1.528323]\n",
      "[D loss: 0.905584] [G loss: 1.487535]\n",
      "[D loss: 0.813485] [G loss: 1.323086]\n",
      "[D loss: 0.575622] [G loss: 1.413451]\n",
      "[D loss: 1.009167] [G loss: 1.145609]\n",
      "[D loss: 0.707206] [G loss: 1.360372]\n",
      "[D loss: 0.925204] [G loss: 1.276238]\n",
      "[D loss: 0.617495] [G loss: 1.307067]\n",
      "[D loss: 0.994817] [G loss: 1.303242]\n",
      "[D loss: 1.035156] [G loss: 1.375695]\n",
      "[D loss: 0.774548] [G loss: 1.590759]\n",
      "[D loss: 0.713858] [G loss: 1.554007]\n",
      "[D loss: 0.834302] [G loss: 1.381109]\n",
      "[D loss: 0.692748] [G loss: 1.515779]\n",
      "[D loss: 0.996803] [G loss: 1.426869]\n",
      "[D loss: 0.763851] [G loss: 1.121027]\n",
      "[D loss: 0.923312] [G loss: 1.298113]\n",
      "[D loss: 0.740952] [G loss: 1.466280]\n",
      "[D loss: 0.893319] [G loss: 1.357761]\n",
      "[D loss: 0.892233] [G loss: 1.517607]\n",
      "[D loss: 1.014045] [G loss: 1.393993]\n",
      "[D loss: 0.641797] [G loss: 1.601143]\n",
      "[D loss: 1.065623] [G loss: 1.395469]\n",
      "[D loss: 0.744240] [G loss: 1.553416]\n",
      "[D loss: 0.653104] [G loss: 1.593307]\n",
      "[D loss: 0.774976] [G loss: 1.315470]\n",
      "[D loss: 0.820404] [G loss: 1.243337]\n",
      "[D loss: 0.751542] [G loss: 1.384921]\n",
      "[D loss: 0.771836] [G loss: 1.607448]\n",
      "[D loss: 0.728895] [G loss: 1.522227]\n",
      "[D loss: 0.867437] [G loss: 1.400684]\n",
      "[D loss: 0.758035] [G loss: 1.495981]\n",
      "[D loss: 0.949568] [G loss: 1.264133]\n",
      "[D loss: 0.610172] [G loss: 1.373855]\n",
      "[D loss: 0.623489] [G loss: 1.442303]\n",
      "[D loss: 0.650652] [G loss: 1.425036]\n",
      "[D loss: 0.972666] [G loss: 1.311054]\n",
      "[D loss: 0.683543] [G loss: 1.688188]\n",
      "[D loss: 0.842976] [G loss: 1.568110]\n",
      "[D loss: 0.798915] [G loss: 1.561811]\n",
      "[D loss: 0.758353] [G loss: 1.527879]\n",
      "[D loss: 0.904045] [G loss: 1.593554]\n",
      "[D loss: 0.748196] [G loss: 1.500163]\n",
      "[D loss: 0.748580] [G loss: 1.546275]\n",
      "[D loss: 0.814453] [G loss: 1.674972]\n",
      "[D loss: 0.980492] [G loss: 1.529534]\n",
      "[D loss: 0.731910] [G loss: 1.494048]\n",
      "[D loss: 0.843255] [G loss: 1.408098]\n",
      "[D loss: 1.009188] [G loss: 1.334769]\n",
      "[D loss: 1.058417] [G loss: 1.439771]\n",
      "[D loss: 0.801319] [G loss: 1.536399]\n",
      "[D loss: 0.716144] [G loss: 1.394253]\n",
      "[D loss: 0.956410] [G loss: 1.401750]\n",
      "[D loss: 0.750987] [G loss: 1.548025]\n",
      "[D loss: 1.034996] [G loss: 1.429310]\n",
      "[D loss: 0.665890] [G loss: 1.570711]\n",
      "[D loss: 0.691694] [G loss: 1.605893]\n",
      "[D loss: 0.595072] [G loss: 1.622950]\n",
      "[D loss: 1.044081] [G loss: 1.300410]\n",
      "[D loss: 0.749771] [G loss: 1.456651]\n",
      "[D loss: 0.697034] [G loss: 1.606658]\n",
      "[D loss: 0.928094] [G loss: 1.435117]\n",
      "[D loss: 0.898079] [G loss: 1.416684]\n",
      "[D loss: 1.012854] [G loss: 1.438553]\n",
      "[D loss: 0.800944] [G loss: 1.388750]\n",
      "[D loss: 0.729688] [G loss: 1.341658]\n",
      "[D loss: 0.825937] [G loss: 1.379428]\n",
      "[D loss: 0.795614] [G loss: 1.376510]\n",
      "[D loss: 0.679001] [G loss: 1.385212]\n",
      "[D loss: 0.692996] [G loss: 1.674771]\n",
      "[D loss: 0.722933] [G loss: 1.557005]\n",
      "[D loss: 0.754132] [G loss: 1.566252]\n",
      "[D loss: 0.663306] [G loss: 1.636578]\n",
      "[D loss: 0.646889] [G loss: 1.400162]\n",
      "[D loss: 0.753641] [G loss: 1.460251]\n",
      "[D loss: 0.692207] [G loss: 1.572919]\n",
      "[D loss: 0.965852] [G loss: 1.502996]\n",
      "[D loss: 1.056644] [G loss: 1.432234]\n",
      "[D loss: 1.068766] [G loss: 1.637381]\n",
      "[D loss: 0.619437] [G loss: 1.685733]\n",
      "[D loss: 0.744154] [G loss: 1.313639]\n",
      "[D loss: 1.075127] [G loss: 1.356647]\n",
      "[D loss: 0.719289] [G loss: 1.299248]\n",
      "[D loss: 0.627224] [G loss: 1.384796]\n",
      "[D loss: 0.931996] [G loss: 1.404912]\n",
      "[D loss: 1.141945] [G loss: 1.291187]\n",
      "[D loss: 0.640481] [G loss: 1.531616]\n",
      "[D loss: 0.896124] [G loss: 1.507981]\n",
      "[D loss: 1.001046] [G loss: 1.432072]\n",
      "[D loss: 0.776236] [G loss: 1.583786]\n",
      "[D loss: 0.826519] [G loss: 1.394122]\n",
      "[D loss: 0.776937] [G loss: 1.501547]\n",
      "[D loss: 0.795371] [G loss: 1.560631]\n",
      "[D loss: 0.937990] [G loss: 1.631011]\n",
      "[D loss: 0.891647] [G loss: 1.360620]\n",
      "[D loss: 0.830689] [G loss: 1.381074]\n",
      "[D loss: 0.722330] [G loss: 1.570020]\n",
      "[D loss: 0.882272] [G loss: 1.390143]\n",
      "[D loss: 0.781263] [G loss: 1.342316]\n",
      "[D loss: 0.696555] [G loss: 1.432663]\n",
      "[D loss: 0.804454] [G loss: 1.424236]\n",
      "[D loss: 0.645503] [G loss: 1.507465]\n",
      "[D loss: 0.788121] [G loss: 1.397611]\n",
      "[D loss: 1.063264] [G loss: 1.342693]\n",
      "[D loss: 0.871033] [G loss: 1.441470]\n",
      "[D loss: 0.850915] [G loss: 1.798473]\n",
      "[D loss: 0.913637] [G loss: 1.501693]\n",
      "[D loss: 0.836156] [G loss: 1.548840]\n",
      "[D loss: 0.994562] [G loss: 1.353846]\n",
      "[D loss: 0.851428] [G loss: 1.176809]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.785501] [G loss: 1.270899]\n",
      "[D loss: 0.869099] [G loss: 1.290407]\n",
      "[D loss: 0.880755] [G loss: 1.549798]\n",
      "[D loss: 0.904132] [G loss: 1.270542]\n",
      "[D loss: 0.893504] [G loss: 1.314953]\n",
      "[D loss: 0.719541] [G loss: 1.446480]\n",
      "[D loss: 0.631075] [G loss: 1.409096]\n",
      "[D loss: 0.795672] [G loss: 1.334635]\n",
      "[D loss: 0.842496] [G loss: 1.397524]\n",
      "[D loss: 0.913467] [G loss: 1.303565]\n",
      "[D loss: 0.813843] [G loss: 1.448660]\n",
      "[D loss: 0.884975] [G loss: 1.320300]\n",
      "[D loss: 0.767322] [G loss: 1.458967]\n",
      "[D loss: 0.746573] [G loss: 1.317331]\n",
      "[D loss: 1.094817] [G loss: 1.334357]\n",
      "[D loss: 0.839375] [G loss: 1.457188]\n",
      "[D loss: 0.890753] [G loss: 1.758610]\n",
      "[D loss: 0.712707] [G loss: 1.566601]\n",
      "[D loss: 0.668402] [G loss: 1.328352]\n",
      "[D loss: 0.947576] [G loss: 1.216301]\n",
      "[D loss: 1.008021] [G loss: 1.340017]\n",
      "[D loss: 0.701260] [G loss: 1.540761]\n",
      "[D loss: 0.876469] [G loss: 1.514443]\n",
      "[D loss: 0.782015] [G loss: 1.420900]\n",
      "[D loss: 0.831017] [G loss: 1.524632]\n",
      "[D loss: 0.602800] [G loss: 1.421365]\n",
      "[D loss: 0.686045] [G loss: 1.562124]\n",
      "[D loss: 0.718543] [G loss: 1.535025]\n",
      "[D loss: 0.655388] [G loss: 1.603538]\n",
      "[D loss: 0.896300] [G loss: 1.602109]\n",
      "[D loss: 0.959824] [G loss: 1.523771]\n",
      "[D loss: 0.810791] [G loss: 1.653507]\n",
      "[D loss: 0.886801] [G loss: 1.488591]\n",
      "[D loss: 0.764497] [G loss: 1.683604]\n",
      "[D loss: 0.760711] [G loss: 1.337418]\n",
      "[D loss: 0.774363] [G loss: 1.450837]\n",
      "[D loss: 0.729876] [G loss: 1.494597]\n",
      "[D loss: 1.047952] [G loss: 1.352352]\n",
      "[D loss: 0.780785] [G loss: 1.329964]\n",
      "[D loss: 0.763466] [G loss: 1.321486]\n",
      "[D loss: 0.813928] [G loss: 1.632068]\n",
      "[D loss: 0.863749] [G loss: 1.428343]\n",
      "[D loss: 0.822370] [G loss: 1.329841]\n",
      "[D loss: 0.842125] [G loss: 1.426095]\n",
      "[D loss: 0.720369] [G loss: 1.583061]\n",
      "[D loss: 0.837753] [G loss: 1.448831]\n",
      "[D loss: 0.813312] [G loss: 1.494347]\n",
      "[D loss: 0.770096] [G loss: 1.725577]\n",
      "[D loss: 0.621635] [G loss: 1.641681]\n",
      "[D loss: 0.868880] [G loss: 1.604234]\n",
      "[D loss: 0.717678] [G loss: 1.546570]\n",
      "[D loss: 0.620331] [G loss: 1.539183]\n",
      "[D loss: 0.745399] [G loss: 1.488814]\n",
      "[D loss: 0.855160] [G loss: 1.499305]\n",
      "[D loss: 0.801148] [G loss: 1.247001]\n",
      "[D loss: 1.019217] [G loss: 1.476762]\n",
      "[D loss: 0.724516] [G loss: 1.475582]\n",
      "[D loss: 0.754755] [G loss: 1.669743]\n",
      "[D loss: 0.891810] [G loss: 1.774092]\n",
      "[D loss: 0.959962] [G loss: 1.504231]\n",
      "[D loss: 0.756427] [G loss: 1.457437]\n",
      "[D loss: 1.035685] [G loss: 1.561446]\n",
      "[D loss: 0.505961] [G loss: 1.630700]\n",
      "[D loss: 0.602283] [G loss: 1.457639]\n",
      "[D loss: 0.788071] [G loss: 1.553339]\n",
      "[D loss: 0.798864] [G loss: 1.538789]\n",
      "[D loss: 0.815316] [G loss: 1.538111]\n",
      "[D loss: 0.675851] [G loss: 1.719223]\n",
      "[D loss: 0.684077] [G loss: 1.712462]\n",
      "[D loss: 1.080233] [G loss: 1.235886]\n",
      "[D loss: 0.449250] [G loss: 1.391538]\n",
      "[D loss: 0.756023] [G loss: 1.421688]\n",
      "[D loss: 0.865994] [G loss: 1.548704]\n",
      "[D loss: 0.713903] [G loss: 1.604226]\n",
      "[D loss: 0.955397] [G loss: 1.352390]\n",
      "[D loss: 0.801729] [G loss: 1.494708]\n",
      "[D loss: 0.898352] [G loss: 1.276711]\n",
      "[D loss: 0.666742] [G loss: 1.450645]\n",
      "[D loss: 0.688015] [G loss: 1.521903]\n",
      "[D loss: 0.971808] [G loss: 1.384520]\n",
      "[D loss: 0.692975] [G loss: 1.578762]\n",
      "[D loss: 0.761775] [G loss: 1.651242]\n",
      "[D loss: 0.932270] [G loss: 1.346626]\n",
      "[D loss: 0.923172] [G loss: 1.470077]\n",
      "[D loss: 0.870052] [G loss: 1.631556]\n",
      "[D loss: 0.702872] [G loss: 1.519832]\n",
      "[D loss: 0.960468] [G loss: 1.216685]\n",
      "[D loss: 0.807733] [G loss: 1.519753]\n",
      "[D loss: 0.911415] [G loss: 1.578456]\n",
      "[D loss: 0.824881] [G loss: 1.419311]\n",
      "[D loss: 0.841294] [G loss: 1.594131]\n",
      "[D loss: 1.033042] [G loss: 1.406418]\n",
      "[D loss: 0.826519] [G loss: 1.505823]\n",
      "[D loss: 0.828776] [G loss: 1.353214]\n",
      "[D loss: 0.901657] [G loss: 1.567086]\n",
      "[D loss: 0.858733] [G loss: 1.512005]\n",
      "[D loss: 0.796828] [G loss: 1.385143]\n",
      "[D loss: 0.718140] [G loss: 1.538230]\n",
      "[D loss: 0.849350] [G loss: 1.352646]\n",
      "[D loss: 0.926282] [G loss: 1.419581]\n",
      "[D loss: 0.825525] [G loss: 1.401030]\n",
      "[D loss: 0.734837] [G loss: 1.360254]\n",
      "[D loss: 0.807999] [G loss: 1.649211]\n",
      "[D loss: 0.797107] [G loss: 1.665073]\n",
      "[D loss: 0.781512] [G loss: 1.567343]\n",
      "[D loss: 0.790976] [G loss: 1.779163]\n",
      "[D loss: 0.839741] [G loss: 1.526635]\n",
      "[D loss: 0.821519] [G loss: 1.600909]\n",
      "[D loss: 0.849176] [G loss: 1.225940]\n",
      "[D loss: 0.650122] [G loss: 1.490702]\n",
      "[D loss: 0.906489] [G loss: 1.577087]\n",
      "[D loss: 0.729459] [G loss: 1.507987]\n",
      "[D loss: 1.057263] [G loss: 1.558321]\n",
      "[D loss: 0.734385] [G loss: 1.433639]\n",
      "[D loss: 0.891832] [G loss: 1.527551]\n",
      "[D loss: 0.807347] [G loss: 1.363673]\n",
      "[D loss: 0.729064] [G loss: 1.429093]\n",
      "[D loss: 0.779347] [G loss: 1.578185]\n",
      "[D loss: 0.900255] [G loss: 1.511942]\n",
      "[D loss: 0.896324] [G loss: 1.671243]\n",
      "[D loss: 0.989120] [G loss: 1.806462]\n",
      "[D loss: 0.639372] [G loss: 1.476508]\n",
      "[D loss: 0.762634] [G loss: 1.596818]\n",
      "[D loss: 0.951881] [G loss: 1.237433]\n",
      "[D loss: 0.763775] [G loss: 1.352882]\n",
      "[D loss: 0.757738] [G loss: 1.440533]\n",
      "[D loss: 0.801952] [G loss: 1.553460]\n",
      "[D loss: 0.718577] [G loss: 1.610298]\n",
      "[D loss: 0.893550] [G loss: 1.404057]\n",
      "[D loss: 0.814347] [G loss: 1.494496]\n",
      "[D loss: 0.996933] [G loss: 1.503347]\n",
      "[D loss: 0.844837] [G loss: 1.416738]\n",
      "[D loss: 0.977399] [G loss: 1.241354]\n",
      "[D loss: 0.889540] [G loss: 1.309795]\n",
      "[D loss: 0.912673] [G loss: 1.387645]\n",
      "[D loss: 0.895521] [G loss: 1.346220]\n",
      "[D loss: 0.789780] [G loss: 1.328133]\n",
      "[D loss: 0.865696] [G loss: 1.499287]\n",
      "[D loss: 0.891133] [G loss: 1.581162]\n",
      "[D loss: 1.050810] [G loss: 1.444839]\n",
      "[D loss: 0.679259] [G loss: 1.555224]\n",
      "[D loss: 0.927944] [G loss: 1.353451]\n",
      "[D loss: 0.651226] [G loss: 1.487973]\n",
      "[D loss: 0.744750] [G loss: 1.706854]\n",
      "[D loss: 0.869281] [G loss: 1.427319]\n",
      "[D loss: 0.665450] [G loss: 1.476200]\n",
      "[D loss: 0.916818] [G loss: 1.439034]\n",
      "[D loss: 0.780800] [G loss: 1.398837]\n",
      "[D loss: 0.850453] [G loss: 1.381335]\n",
      "[D loss: 0.839742] [G loss: 1.466228]\n",
      "[D loss: 0.658314] [G loss: 1.470953]\n",
      "[D loss: 0.964731] [G loss: 1.567035]\n",
      "[D loss: 0.883640] [G loss: 1.349639]\n",
      "[D loss: 0.780428] [G loss: 1.616123]\n",
      "[D loss: 0.834279] [G loss: 1.657366]\n",
      "[D loss: 0.721033] [G loss: 1.601020]\n",
      "[D loss: 0.768302] [G loss: 1.354979]\n",
      "[D loss: 0.799283] [G loss: 1.386689]\n",
      "[D loss: 0.877260] [G loss: 1.470282]\n",
      "[D loss: 0.744277] [G loss: 1.506949]\n",
      "[D loss: 0.843466] [G loss: 1.358149]\n",
      "[D loss: 0.935651] [G loss: 1.597504]\n",
      "[D loss: 0.770167] [G loss: 1.606677]\n",
      "[D loss: 0.618159] [G loss: 1.313904]\n",
      "[D loss: 0.747659] [G loss: 1.501523]\n",
      "[D loss: 0.895590] [G loss: 1.615648]\n",
      "[D loss: 0.766682] [G loss: 1.764318]\n",
      "[D loss: 0.890708] [G loss: 1.599837]\n",
      "[D loss: 0.715883] [G loss: 1.836395]\n",
      "[D loss: 0.925342] [G loss: 1.674360]\n",
      "[D loss: 0.813767] [G loss: 1.328183]\n",
      "[D loss: 1.040359] [G loss: 1.415910]\n",
      "[D loss: 0.834994] [G loss: 1.621322]\n",
      "[D loss: 0.802747] [G loss: 1.452570]\n",
      "[D loss: 0.971460] [G loss: 1.156596]\n",
      "[D loss: 0.959312] [G loss: 1.441737]\n",
      "[D loss: 0.884764] [G loss: 1.598092]\n",
      "[D loss: 0.750491] [G loss: 1.416974]\n",
      "[D loss: 0.699204] [G loss: 1.334646]\n",
      "[D loss: 0.749710] [G loss: 1.463315]\n",
      "[D loss: 0.696547] [G loss: 1.488545]\n",
      "[D loss: 0.982165] [G loss: 1.381131]\n",
      "[D loss: 0.891898] [G loss: 1.484927]\n",
      "[D loss: 0.659552] [G loss: 1.561713]\n",
      "[D loss: 0.875120] [G loss: 1.347328]\n",
      "[D loss: 0.698595] [G loss: 1.323791]\n",
      "[D loss: 0.680950] [G loss: 1.493140]\n",
      "[D loss: 0.691933] [G loss: 1.531740]\n",
      "[D loss: 0.731298] [G loss: 1.489109]\n",
      "[D loss: 0.666836] [G loss: 1.496262]\n",
      "[D loss: 0.674695] [G loss: 1.641379]\n",
      "[D loss: 0.969931] [G loss: 1.547125]\n",
      "[D loss: 0.753312] [G loss: 1.342545]\n",
      "[D loss: 0.870294] [G loss: 1.511749]\n",
      "[D loss: 0.871821] [G loss: 1.517794]\n",
      "[D loss: 1.171441] [G loss: 1.500433]\n",
      "[D loss: 0.829008] [G loss: 1.469083]\n",
      "[D loss: 0.736519] [G loss: 1.477283]\n",
      "[D loss: 0.855617] [G loss: 1.544338]\n",
      "[D loss: 0.717500] [G loss: 1.524952]\n",
      "[D loss: 0.676839] [G loss: 1.429536]\n",
      "[D loss: 0.860103] [G loss: 1.374528]\n",
      "[D loss: 0.921449] [G loss: 1.382428]\n",
      "[D loss: 0.920949] [G loss: 1.293834]\n",
      "[D loss: 0.729572] [G loss: 1.460525]\n",
      "[D loss: 0.788580] [G loss: 1.405647]\n",
      "[D loss: 0.954393] [G loss: 1.326271]\n",
      "[D loss: 0.938707] [G loss: 1.523362]\n",
      "[D loss: 0.717134] [G loss: 1.629869]\n",
      "[D loss: 0.717715] [G loss: 1.352493]\n",
      "[D loss: 0.771242] [G loss: 1.421410]\n",
      "[D loss: 0.906604] [G loss: 1.418123]\n",
      "[D loss: 0.795342] [G loss: 1.829790]\n",
      "[D loss: 0.575599] [G loss: 1.696497]\n",
      "[D loss: 0.630118] [G loss: 1.481456]\n",
      "[D loss: 0.853294] [G loss: 1.503159]\n",
      "[D loss: 0.765331] [G loss: 1.452028]\n",
      "[D loss: 1.046265] [G loss: 1.731306]\n",
      "[D loss: 0.801204] [G loss: 1.450161]\n",
      "[D loss: 0.755848] [G loss: 1.311366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 1.211244] [G loss: 1.405137]\n",
      "[D loss: 0.984916] [G loss: 1.299547]\n",
      "[D loss: 0.861118] [G loss: 1.464580]\n",
      "[D loss: 1.013917] [G loss: 1.607590]\n",
      "[D loss: 0.632594] [G loss: 1.545502]\n",
      "[D loss: 0.778442] [G loss: 1.397025]\n",
      "[D loss: 0.951220] [G loss: 1.501531]\n",
      "[D loss: 1.123089] [G loss: 1.394151]\n",
      "[D loss: 0.841493] [G loss: 1.368602]\n",
      "[D loss: 0.768380] [G loss: 1.368795]\n",
      "[D loss: 0.595242] [G loss: 1.700014]\n",
      "[D loss: 0.734401] [G loss: 1.545110]\n",
      "[D loss: 1.026729] [G loss: 1.353288]\n",
      "[D loss: 0.929696] [G loss: 1.643429]\n",
      "[D loss: 0.940998] [G loss: 1.349291]\n",
      "[D loss: 0.905285] [G loss: 1.319463]\n",
      "[D loss: 0.875413] [G loss: 1.435004]\n",
      "[D loss: 0.580561] [G loss: 1.466922]\n",
      "[D loss: 0.846734] [G loss: 1.326702]\n",
      "[D loss: 0.782985] [G loss: 1.389780]\n",
      "[D loss: 0.587400] [G loss: 1.578152]\n",
      "[D loss: 0.702058] [G loss: 1.438500]\n",
      "[D loss: 0.939528] [G loss: 1.501178]\n",
      "[D loss: 0.703705] [G loss: 1.485390]\n",
      "[D loss: 0.720173] [G loss: 1.466511]\n",
      "[D loss: 0.783291] [G loss: 1.415735]\n",
      "[D loss: 0.709125] [G loss: 1.409936]\n",
      "[D loss: 0.707366] [G loss: 1.536307]\n",
      "[D loss: 0.846288] [G loss: 1.474313]\n",
      "[D loss: 1.084091] [G loss: 1.634240]\n",
      "[D loss: 0.864946] [G loss: 1.646692]\n",
      "[D loss: 0.906580] [G loss: 1.648149]\n",
      "[D loss: 0.855441] [G loss: 1.336790]\n",
      "[D loss: 0.835390] [G loss: 1.359566]\n",
      "[D loss: 0.792125] [G loss: 1.321833]\n",
      "[D loss: 0.846459] [G loss: 1.371668]\n",
      "[D loss: 0.703746] [G loss: 1.689381]\n",
      "[D loss: 0.684406] [G loss: 1.458643]\n",
      "[D loss: 0.803988] [G loss: 1.622003]\n",
      "[D loss: 0.894922] [G loss: 1.834464]\n",
      "[D loss: 0.943198] [G loss: 1.339092]\n",
      "[D loss: 0.921229] [G loss: 1.470970]\n",
      "[D loss: 0.684639] [G loss: 1.589522]\n",
      "[D loss: 0.586248] [G loss: 1.525643]\n",
      "[D loss: 0.920763] [G loss: 1.380198]\n",
      "[D loss: 0.683733] [G loss: 1.455770]\n",
      "[D loss: 0.826928] [G loss: 1.656188]\n",
      "[D loss: 0.833125] [G loss: 1.644402]\n",
      "[D loss: 0.878376] [G loss: 1.520693]\n",
      "[D loss: 0.963200] [G loss: 1.444231]\n",
      "[D loss: 0.745692] [G loss: 1.501594]\n",
      "[D loss: 0.629602] [G loss: 1.474490]\n",
      "[D loss: 0.843277] [G loss: 1.521885]\n",
      "[D loss: 0.634857] [G loss: 1.560153]\n",
      "[D loss: 0.678481] [G loss: 1.592610]\n",
      "[D loss: 0.661138] [G loss: 1.383311]\n",
      "[D loss: 0.794522] [G loss: 1.724767]\n",
      "[D loss: 0.707664] [G loss: 1.526295]\n",
      "[D loss: 1.016088] [G loss: 1.466758]\n",
      "[D loss: 0.866668] [G loss: 1.589379]\n",
      "[D loss: 0.560113] [G loss: 1.470966]\n",
      "[D loss: 1.400013] [G loss: 1.274390]\n",
      "[D loss: 0.662856] [G loss: 1.702602]\n",
      "[D loss: 0.973370] [G loss: 1.610571]\n",
      "[D loss: 0.687593] [G loss: 1.650277]\n",
      "[D loss: 0.723683] [G loss: 1.363123]\n",
      "[D loss: 0.703286] [G loss: 1.632026]\n",
      "[D loss: 0.969392] [G loss: 1.312948]\n",
      "[D loss: 0.741637] [G loss: 1.314144]\n",
      "[D loss: 0.992465] [G loss: 1.572563]\n",
      "[D loss: 0.798370] [G loss: 1.534445]\n",
      "[D loss: 0.734770] [G loss: 1.915656]\n",
      "[D loss: 0.885398] [G loss: 1.427225]\n",
      "[D loss: 0.707384] [G loss: 1.540114]\n",
      "[D loss: 0.860076] [G loss: 1.399224]\n",
      "[D loss: 0.820218] [G loss: 1.399000]\n",
      "[D loss: 0.818759] [G loss: 1.388792]\n",
      "[D loss: 1.013029] [G loss: 1.358790]\n",
      "[D loss: 0.597176] [G loss: 1.661961]\n",
      "[D loss: 0.721723] [G loss: 1.540352]\n",
      "[D loss: 0.875693] [G loss: 1.540686]\n",
      "[D loss: 0.972143] [G loss: 1.350340]\n",
      "[D loss: 0.859030] [G loss: 1.307286]\n",
      "[D loss: 0.977624] [G loss: 1.338903]\n",
      "[D loss: 1.011217] [G loss: 1.345932]\n",
      "[D loss: 0.928381] [G loss: 1.406584]\n",
      "[D loss: 0.836718] [G loss: 1.422584]\n",
      "[D loss: 0.753205] [G loss: 1.447510]\n",
      "[D loss: 0.800455] [G loss: 1.610207]\n",
      "[D loss: 0.668882] [G loss: 1.450535]\n",
      "[D loss: 0.744582] [G loss: 1.575211]\n",
      "[D loss: 0.988161] [G loss: 1.411639]\n",
      "[D loss: 1.075606] [G loss: 1.355140]\n",
      "[D loss: 0.717524] [G loss: 1.631169]\n",
      "[D loss: 0.907362] [G loss: 1.382726]\n",
      "[D loss: 0.809469] [G loss: 1.528552]\n",
      "[D loss: 0.794945] [G loss: 1.572996]\n",
      "[D loss: 0.710075] [G loss: 1.378869]\n",
      "[D loss: 0.859688] [G loss: 1.502081]\n",
      "[D loss: 0.768030] [G loss: 1.762865]\n",
      "[D loss: 0.882340] [G loss: 1.464127]\n",
      "[D loss: 0.827496] [G loss: 1.465488]\n",
      "[D loss: 0.879093] [G loss: 1.606592]\n",
      "[D loss: 0.926718] [G loss: 1.311715]\n",
      "[D loss: 0.943397] [G loss: 1.575399]\n",
      "[D loss: 0.748671] [G loss: 1.457775]\n",
      "[D loss: 0.990272] [G loss: 1.372191]\n",
      "[D loss: 0.645801] [G loss: 1.609421]\n",
      "[D loss: 0.844517] [G loss: 1.450014]\n",
      "[D loss: 0.954591] [G loss: 1.417830]\n",
      "[D loss: 0.824332] [G loss: 1.559844]\n",
      "[D loss: 0.748860] [G loss: 1.658772]\n",
      "[D loss: 0.667351] [G loss: 1.527002]\n",
      "[D loss: 0.712107] [G loss: 1.660819]\n",
      "[D loss: 0.800217] [G loss: 1.499493]\n",
      "[D loss: 0.665443] [G loss: 1.835568]\n",
      "[D loss: 0.861057] [G loss: 1.504183]\n",
      "[D loss: 0.774524] [G loss: 1.511976]\n",
      "[D loss: 0.805171] [G loss: 1.511760]\n",
      "[D loss: 0.553927] [G loss: 1.739928]\n",
      "[D loss: 0.747994] [G loss: 1.448728]\n",
      "[D loss: 0.759959] [G loss: 1.834777]\n",
      "[D loss: 0.746710] [G loss: 1.594049]\n",
      "[D loss: 1.263090] [G loss: 1.160790]\n",
      "[D loss: 0.890765] [G loss: 1.478033]\n",
      "[D loss: 0.894485] [G loss: 1.660893]\n",
      "[D loss: 0.867421] [G loss: 1.745039]\n",
      "[D loss: 1.027831] [G loss: 1.361089]\n",
      "[D loss: 0.937312] [G loss: 1.546386]\n",
      "[D loss: 0.675138] [G loss: 1.459433]\n",
      "[D loss: 0.981448] [G loss: 1.391405]\n",
      "[D loss: 0.816381] [G loss: 1.326989]\n",
      "[D loss: 0.973636] [G loss: 1.372171]\n",
      "[D loss: 0.824738] [G loss: 1.368319]\n",
      "[D loss: 0.861452] [G loss: 1.516608]\n",
      "[D loss: 0.997504] [G loss: 1.362963]\n",
      "[D loss: 0.757932] [G loss: 1.435836]\n",
      "[D loss: 1.048773] [G loss: 1.318933]\n",
      "[D loss: 0.636693] [G loss: 1.643780]\n",
      "[D loss: 0.940482] [G loss: 1.434252]\n",
      "[D loss: 0.820964] [G loss: 1.407170]\n",
      "[D loss: 0.770076] [G loss: 1.285220]\n",
      "[D loss: 0.791606] [G loss: 1.304177]\n",
      "[D loss: 0.767552] [G loss: 1.529180]\n",
      "[D loss: 0.912817] [G loss: 1.205673]\n",
      "[D loss: 0.793102] [G loss: 1.283792]\n",
      "[D loss: 0.939956] [G loss: 1.523020]\n",
      "[D loss: 0.741745] [G loss: 1.409668]\n",
      "[D loss: 0.785120] [G loss: 1.394361]\n",
      "[D loss: 0.576622] [G loss: 1.661837]\n",
      "[D loss: 0.617406] [G loss: 1.652081]\n",
      "[D loss: 0.723376] [G loss: 1.395842]\n",
      "[D loss: 0.955339] [G loss: 1.237434]\n",
      "[D loss: 1.169635] [G loss: 1.382377]\n",
      "[D loss: 0.715121] [G loss: 1.426363]\n",
      "[D loss: 0.809202] [G loss: 1.695097]\n",
      "[D loss: 0.901505] [G loss: 1.426261]\n",
      "[D loss: 0.703565] [G loss: 1.412297]\n",
      "[D loss: 0.789016] [G loss: 1.619452]\n",
      "[D loss: 1.119135] [G loss: 1.491822]\n",
      "[D loss: 0.790344] [G loss: 1.572400]\n",
      "[D loss: 0.781402] [G loss: 1.418381]\n",
      "[D loss: 0.790033] [G loss: 1.560032]\n",
      "[D loss: 0.760622] [G loss: 1.610022]\n",
      "[D loss: 1.129323] [G loss: 1.404554]\n",
      "[D loss: 0.653677] [G loss: 1.311664]\n",
      "[D loss: 0.769869] [G loss: 1.508941]\n",
      "[D loss: 0.873442] [G loss: 1.483661]\n",
      "[D loss: 1.019161] [G loss: 1.309042]\n",
      "[D loss: 0.767978] [G loss: 1.572924]\n",
      "[D loss: 0.871463] [G loss: 1.490180]\n",
      "[D loss: 0.926118] [G loss: 1.578284]\n",
      "[D loss: 1.066976] [G loss: 1.430195]\n",
      "[D loss: 0.779699] [G loss: 1.467382]\n",
      "[D loss: 0.849775] [G loss: 1.463131]\n",
      "[D loss: 0.831718] [G loss: 1.459492]\n",
      "[D loss: 0.813677] [G loss: 1.224838]\n",
      "[D loss: 0.869035] [G loss: 1.234463]\n",
      "[D loss: 0.591240] [G loss: 1.499006]\n",
      "[D loss: 0.771607] [G loss: 1.434456]\n",
      "[D loss: 0.902938] [G loss: 1.562161]\n",
      "[D loss: 0.700260] [G loss: 1.493934]\n",
      "[D loss: 0.870495] [G loss: 1.481693]\n",
      "[D loss: 0.808748] [G loss: 1.363042]\n",
      "[D loss: 0.760478] [G loss: 1.356033]\n",
      "[D loss: 0.906907] [G loss: 1.526824]\n",
      "[D loss: 0.535252] [G loss: 1.646102]\n",
      "[D loss: 0.662583] [G loss: 1.602659]\n",
      "[D loss: 0.878516] [G loss: 1.393437]\n",
      "[D loss: 0.911734] [G loss: 1.428020]\n",
      "[D loss: 0.707118] [G loss: 1.692410]\n",
      "[D loss: 1.004601] [G loss: 1.482729]\n",
      "[D loss: 0.841688] [G loss: 1.278750]\n",
      "[D loss: 0.584168] [G loss: 1.450253]\n",
      "[D loss: 0.870069] [G loss: 1.368468]\n",
      "[D loss: 0.986843] [G loss: 1.299462]\n",
      "[D loss: 0.888403] [G loss: 1.509646]\n",
      "[D loss: 0.869811] [G loss: 1.388110]\n",
      "[D loss: 0.829442] [G loss: 1.386771]\n",
      "[D loss: 0.951797] [G loss: 1.369784]\n",
      "[D loss: 0.965668] [G loss: 1.585987]\n",
      "[D loss: 0.791420] [G loss: 1.478859]\n",
      "[D loss: 0.830920] [G loss: 1.452038]\n",
      "[D loss: 0.912285] [G loss: 1.385527]\n",
      "[D loss: 0.810031] [G loss: 1.239623]\n",
      "[D loss: 0.834361] [G loss: 1.436266]\n",
      "[D loss: 0.765783] [G loss: 1.683900]\n",
      "[D loss: 0.867897] [G loss: 1.501577]\n",
      "[D loss: 0.893516] [G loss: 1.267128]\n",
      "[D loss: 0.809203] [G loss: 1.184911]\n",
      "[D loss: 0.915281] [G loss: 1.546471]\n",
      "[D loss: 0.856286] [G loss: 1.491445]\n",
      "[D loss: 0.762835] [G loss: 1.398488]\n",
      "[D loss: 0.921716] [G loss: 1.498888]\n",
      "[D loss: 0.710291] [G loss: 1.521746]\n",
      "[D loss: 0.787164] [G loss: 1.493843]\n",
      "[D loss: 0.564526] [G loss: 1.567279]\n",
      "[D loss: 0.783892] [G loss: 1.403038]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.790634] [G loss: 1.587161]\n",
      "[D loss: 0.660846] [G loss: 1.532547]\n",
      "[D loss: 0.928113] [G loss: 1.659447]\n",
      "[D loss: 0.717916] [G loss: 1.398873]\n",
      "[D loss: 0.926447] [G loss: 1.266368]\n",
      "[D loss: 0.640749] [G loss: 1.555211]\n",
      "[D loss: 0.831963] [G loss: 1.721817]\n",
      "[D loss: 0.852243] [G loss: 1.523678]\n",
      "[D loss: 1.092821] [G loss: 1.855941]\n",
      "[D loss: 1.156447] [G loss: 1.715106]\n",
      "[D loss: 0.737849] [G loss: 1.619918]\n",
      "[D loss: 0.752374] [G loss: 1.553723]\n",
      "[D loss: 0.648248] [G loss: 1.770776]\n",
      "[D loss: 0.659830] [G loss: 1.463573]\n",
      "[D loss: 0.868329] [G loss: 1.241115]\n",
      "[D loss: 0.608816] [G loss: 1.742401]\n",
      "[D loss: 0.817503] [G loss: 1.792537]\n",
      "[D loss: 0.716064] [G loss: 1.637749]\n",
      "[D loss: 0.792151] [G loss: 1.592861]\n",
      "[D loss: 0.793515] [G loss: 1.585339]\n",
      "[D loss: 0.794052] [G loss: 1.599277]\n",
      "[D loss: 1.008734] [G loss: 1.665371]\n",
      "[D loss: 0.763152] [G loss: 1.432898]\n",
      "[D loss: 0.883834] [G loss: 1.290367]\n",
      "[D loss: 0.820002] [G loss: 1.374607]\n",
      "[D loss: 0.738602] [G loss: 1.396869]\n",
      "[D loss: 0.962165] [G loss: 1.441499]\n",
      "[D loss: 0.796890] [G loss: 1.282564]\n",
      "[D loss: 0.860540] [G loss: 1.466871]\n",
      "[D loss: 0.772876] [G loss: 1.400756]\n",
      "[D loss: 1.028759] [G loss: 1.471466]\n",
      "[D loss: 0.639255] [G loss: 1.395191]\n",
      "[D loss: 0.692265] [G loss: 1.615654]\n",
      "[D loss: 0.854718] [G loss: 1.517541]\n",
      "[D loss: 0.751798] [G loss: 1.666565]\n",
      "[D loss: 0.650872] [G loss: 1.638503]\n",
      "[D loss: 0.805295] [G loss: 1.414769]\n",
      "[D loss: 1.241802] [G loss: 1.294917]\n",
      "[D loss: 0.749918] [G loss: 1.484143]\n",
      "[D loss: 0.844842] [G loss: 1.432099]\n",
      "[D loss: 0.957173] [G loss: 1.400414]\n",
      "[D loss: 0.833590] [G loss: 1.496265]\n",
      "[D loss: 0.784273] [G loss: 1.286427]\n",
      "[D loss: 0.693958] [G loss: 1.611008]\n",
      "[D loss: 0.862518] [G loss: 1.417793]\n",
      "[D loss: 0.887746] [G loss: 1.554003]\n",
      "[D loss: 0.606205] [G loss: 1.523001]\n",
      "[D loss: 0.560531] [G loss: 1.790823]\n",
      "[D loss: 0.830644] [G loss: 1.781233]\n",
      "[D loss: 0.950512] [G loss: 1.425198]\n",
      "[D loss: 0.917558] [G loss: 1.500216]\n",
      "[D loss: 0.888765] [G loss: 1.291847]\n",
      "[D loss: 1.067459] [G loss: 1.376106]\n",
      "[D loss: 0.916659] [G loss: 1.545216]\n",
      "[D loss: 0.864829] [G loss: 1.544330]\n",
      "[D loss: 0.905340] [G loss: 1.578693]\n",
      "[D loss: 0.806707] [G loss: 1.546032]\n",
      "[D loss: 0.849049] [G loss: 1.381714]\n",
      "[D loss: 0.918273] [G loss: 1.312757]\n",
      "[D loss: 0.623747] [G loss: 1.569092]\n",
      "[D loss: 0.661143] [G loss: 1.532180]\n",
      "[D loss: 0.781021] [G loss: 1.572693]\n",
      "[D loss: 0.769104] [G loss: 1.497248]\n",
      "[D loss: 0.804853] [G loss: 1.362484]\n",
      "[D loss: 0.727818] [G loss: 1.583952]\n",
      "[D loss: 0.791741] [G loss: 1.419434]\n",
      "[D loss: 0.800214] [G loss: 1.358460]\n",
      "[D loss: 0.602334] [G loss: 1.594491]\n",
      "[D loss: 1.056107] [G loss: 1.417849]\n",
      "[D loss: 0.828908] [G loss: 1.376344]\n",
      "[D loss: 0.759628] [G loss: 1.569347]\n",
      "[D loss: 0.984251] [G loss: 1.558491]\n",
      "[D loss: 0.613595] [G loss: 1.508148]\n",
      "[D loss: 0.849721] [G loss: 1.602185]\n",
      "[D loss: 1.103255] [G loss: 1.409306]\n",
      "[D loss: 0.694849] [G loss: 1.536646]\n",
      "[D loss: 0.739750] [G loss: 1.531501]\n",
      "[D loss: 0.744274] [G loss: 1.410312]\n",
      "[D loss: 0.688217] [G loss: 1.625527]\n",
      "[D loss: 0.761536] [G loss: 1.454744]\n",
      "[D loss: 0.938903] [G loss: 1.628022]\n",
      "[D loss: 0.656746] [G loss: 1.674048]\n",
      "[D loss: 0.752025] [G loss: 1.645866]\n",
      "[D loss: 0.690809] [G loss: 1.510077]\n",
      "[D loss: 0.910631] [G loss: 1.649475]\n",
      "[D loss: 0.885174] [G loss: 1.513389]\n",
      "[D loss: 0.680866] [G loss: 1.577846]\n",
      "[D loss: 0.833913] [G loss: 1.352890]\n",
      "[D loss: 0.581732] [G loss: 1.489936]\n",
      "[D loss: 0.991228] [G loss: 1.441941]\n",
      "[D loss: 0.918521] [G loss: 1.389205]\n",
      "[D loss: 0.842698] [G loss: 1.654822]\n",
      "[D loss: 1.113348] [G loss: 1.565313]\n",
      "[D loss: 0.768750] [G loss: 1.620974]\n",
      "[D loss: 0.772303] [G loss: 1.522199]\n",
      "[D loss: 0.715304] [G loss: 1.501697]\n",
      "[D loss: 0.676195] [G loss: 1.449099]\n",
      "[D loss: 0.655309] [G loss: 1.490481]\n",
      "[D loss: 1.007412] [G loss: 1.473819]\n",
      "[D loss: 0.773629] [G loss: 1.353209]\n",
      "[D loss: 0.805068] [G loss: 1.522411]\n",
      "[D loss: 0.756755] [G loss: 1.519025]\n",
      "[D loss: 0.961229] [G loss: 1.391685]\n",
      "[D loss: 0.901729] [G loss: 1.395131]\n",
      "[D loss: 0.799609] [G loss: 1.593260]\n",
      "[D loss: 0.883790] [G loss: 1.527187]\n",
      "[D loss: 0.823020] [G loss: 1.362627]\n",
      "[D loss: 0.785693] [G loss: 1.453405]\n",
      "[D loss: 0.684852] [G loss: 1.668543]\n",
      "[D loss: 1.036103] [G loss: 1.539587]\n",
      "[D loss: 0.796544] [G loss: 1.379959]\n",
      "[D loss: 0.768386] [G loss: 1.498101]\n",
      "[D loss: 0.913997] [G loss: 1.616636]\n",
      "[D loss: 1.043990] [G loss: 1.557984]\n",
      "[D loss: 0.975553] [G loss: 1.665061]\n",
      "[D loss: 0.974904] [G loss: 1.622350]\n",
      "[D loss: 0.707350] [G loss: 1.431524]\n",
      "[D loss: 0.831619] [G loss: 1.238318]\n",
      "[D loss: 0.892150] [G loss: 1.227198]\n",
      "[D loss: 0.902579] [G loss: 1.241159]\n",
      "[D loss: 0.847330] [G loss: 1.700153]\n",
      "[D loss: 0.764603] [G loss: 1.674614]\n",
      "[D loss: 0.630376] [G loss: 1.521143]\n",
      "[D loss: 0.925121] [G loss: 1.274107]\n",
      "[D loss: 0.949487] [G loss: 1.186071]\n",
      "[D loss: 0.677367] [G loss: 1.419200]\n",
      "[D loss: 0.709524] [G loss: 1.782598]\n",
      "[D loss: 0.829925] [G loss: 1.558613]\n",
      "[D loss: 1.021299] [G loss: 1.378790]\n",
      "[D loss: 0.736134] [G loss: 1.470188]\n",
      "[D loss: 0.653377] [G loss: 1.425768]\n",
      "[D loss: 0.869698] [G loss: 1.423076]\n",
      "[D loss: 0.839900] [G loss: 1.466618]\n",
      "[D loss: 0.851874] [G loss: 1.488939]\n",
      "[D loss: 0.580356] [G loss: 1.467451]\n",
      "[D loss: 0.730129] [G loss: 1.396743]\n",
      "[D loss: 0.836933] [G loss: 1.630323]\n",
      "[D loss: 0.651858] [G loss: 1.586343]\n",
      "[D loss: 0.835030] [G loss: 1.494564]\n",
      "[D loss: 0.683308] [G loss: 1.361927]\n",
      "[D loss: 0.653832] [G loss: 1.668636]\n",
      "[D loss: 0.884957] [G loss: 1.588003]\n",
      "[D loss: 0.821035] [G loss: 1.443398]\n",
      "[D loss: 0.734462] [G loss: 1.888426]\n",
      "[D loss: 0.936941] [G loss: 1.680611]\n",
      "[D loss: 0.733847] [G loss: 1.449126]\n",
      "[D loss: 0.659688] [G loss: 1.390177]\n",
      "[D loss: 1.004637] [G loss: 1.443498]\n",
      "[D loss: 0.718455] [G loss: 1.545676]\n",
      "[D loss: 0.965675] [G loss: 1.307147]\n",
      "[D loss: 0.684200] [G loss: 1.701668]\n",
      "[D loss: 0.774560] [G loss: 1.709799]\n",
      "[D loss: 0.786550] [G loss: 1.424442]\n",
      "[D loss: 0.944375] [G loss: 1.437886]\n",
      "[D loss: 1.086189] [G loss: 1.364777]\n",
      "[D loss: 0.697479] [G loss: 1.479795]\n",
      "[D loss: 0.660633] [G loss: 1.368173]\n",
      "[D loss: 0.744651] [G loss: 1.492921]\n",
      "[D loss: 0.825293] [G loss: 1.444553]\n",
      "[D loss: 0.620873] [G loss: 1.575970]\n",
      "[D loss: 0.655691] [G loss: 1.439427]\n",
      "[D loss: 0.799514] [G loss: 1.260861]\n",
      "[D loss: 1.000985] [G loss: 1.560526]\n",
      "[D loss: 0.658102] [G loss: 1.614725]\n",
      "[D loss: 0.668591] [G loss: 1.649113]\n",
      "[D loss: 0.777876] [G loss: 1.683784]\n",
      "[D loss: 0.760667] [G loss: 1.388579]\n",
      "[D loss: 0.863967] [G loss: 1.546587]\n",
      "[D loss: 0.759978] [G loss: 1.370655]\n",
      "[D loss: 1.038179] [G loss: 1.588423]\n",
      "[D loss: 0.871536] [G loss: 1.616685]\n",
      "[D loss: 0.658929] [G loss: 1.479384]\n",
      "[D loss: 0.981630] [G loss: 1.394517]\n",
      "[D loss: 0.811263] [G loss: 1.518042]\n",
      "[D loss: 0.938606] [G loss: 1.339670]\n",
      "[D loss: 0.791030] [G loss: 1.393016]\n",
      "[D loss: 0.665461] [G loss: 1.474396]\n",
      "[D loss: 0.856717] [G loss: 1.444914]\n",
      "[D loss: 0.809237] [G loss: 1.683269]\n",
      "[D loss: 1.109351] [G loss: 1.434389]\n",
      "[D loss: 0.824095] [G loss: 1.710414]\n",
      "[D loss: 0.964680] [G loss: 1.367756]\n",
      "[D loss: 0.858318] [G loss: 1.456521]\n",
      "[D loss: 0.855960] [G loss: 1.391294]\n",
      "[D loss: 0.843954] [G loss: 1.616917]\n",
      "[D loss: 0.957622] [G loss: 1.353676]\n",
      "[D loss: 0.983867] [G loss: 1.448759]\n",
      "[D loss: 0.889617] [G loss: 1.645541]\n",
      "[D loss: 0.818992] [G loss: 1.504534]\n",
      "[D loss: 0.995794] [G loss: 1.325222]\n",
      "[D loss: 0.910577] [G loss: 1.281274]\n",
      "[D loss: 0.984187] [G loss: 1.517414]\n",
      "[D loss: 1.030091] [G loss: 1.351708]\n",
      "[D loss: 0.940523] [G loss: 1.264443]\n",
      "[D loss: 0.704975] [G loss: 1.527713]\n",
      "[D loss: 0.832578] [G loss: 1.385119]\n",
      "[D loss: 0.807272] [G loss: 1.148424]\n",
      "[D loss: 0.823479] [G loss: 1.327678]\n",
      "[D loss: 0.991717] [G loss: 1.209083]\n",
      "[D loss: 0.929420] [G loss: 1.274190]\n",
      "[D loss: 0.790139] [G loss: 1.379480]\n",
      "[D loss: 0.918092] [G loss: 1.334310]\n",
      "[D loss: 0.920485] [G loss: 1.453063]\n",
      "[D loss: 0.722427] [G loss: 1.403485]\n",
      "[D loss: 0.711433] [G loss: 1.541401]\n",
      "[D loss: 1.104446] [G loss: 1.276141]\n",
      "[D loss: 0.901174] [G loss: 1.621712]\n",
      "[D loss: 0.901574] [G loss: 1.392206]\n",
      "[D loss: 0.727605] [G loss: 1.438762]\n",
      "[D loss: 0.789687] [G loss: 1.539444]\n",
      "[D loss: 0.878155] [G loss: 1.442758]\n",
      "[D loss: 0.758577] [G loss: 1.497809]\n",
      "[D loss: 0.952901] [G loss: 1.299749]\n",
      "[D loss: 0.855675] [G loss: 1.322493]\n",
      "[D loss: 0.765646] [G loss: 1.392930]\n",
      "[D loss: 1.099243] [G loss: 1.321224]\n",
      "[D loss: 0.865885] [G loss: 1.471313]\n",
      "[D loss: 0.906331] [G loss: 1.439646]\n",
      "[D loss: 0.678360] [G loss: 1.600904]\n",
      "[D loss: 0.832808] [G loss: 1.448089]\n",
      "[D loss: 1.088401] [G loss: 1.277997]\n",
      "[D loss: 0.660335] [G loss: 1.345235]\n",
      "[D loss: 0.840491] [G loss: 1.370063]\n",
      "[D loss: 0.754772] [G loss: 1.338596]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.740708] [G loss: 1.583297]\n",
      "[D loss: 0.874240] [G loss: 1.496555]\n",
      "[D loss: 0.901769] [G loss: 1.299195]\n",
      "[D loss: 0.907517] [G loss: 1.316062]\n",
      "[D loss: 1.027462] [G loss: 1.146372]\n",
      "[D loss: 0.768488] [G loss: 1.406676]\n",
      "[D loss: 0.817529] [G loss: 1.305892]\n",
      "[D loss: 1.055433] [G loss: 1.439848]\n",
      "[D loss: 0.917921] [G loss: 1.287364]\n",
      "[D loss: 0.884317] [G loss: 1.509266]\n",
      "[D loss: 1.030733] [G loss: 1.428344]\n",
      "[D loss: 0.912609] [G loss: 1.588715]\n",
      "[D loss: 0.831704] [G loss: 1.494942]\n",
      "[D loss: 0.842627] [G loss: 1.313492]\n",
      "[D loss: 0.804006] [G loss: 1.337988]\n",
      "[D loss: 0.932408] [G loss: 1.450048]\n",
      "[D loss: 1.099978] [G loss: 1.197433]\n",
      "[D loss: 0.813265] [G loss: 1.547121]\n",
      "[D loss: 0.658166] [G loss: 1.475777]\n",
      "[D loss: 0.734176] [G loss: 1.721348]\n",
      "[D loss: 0.974208] [G loss: 1.496225]\n",
      "[D loss: 0.761119] [G loss: 1.602306]\n",
      "[D loss: 0.566707] [G loss: 1.447174]\n",
      "[D loss: 0.823157] [G loss: 1.354084]\n",
      "[D loss: 0.871159] [G loss: 1.185596]\n",
      "[D loss: 0.619047] [G loss: 1.439944]\n",
      "[D loss: 0.738364] [G loss: 1.502878]\n",
      "[D loss: 0.931500] [G loss: 1.607266]\n",
      "[D loss: 0.784897] [G loss: 1.340327]\n",
      "[D loss: 0.746455] [G loss: 1.297848]\n",
      "[D loss: 0.630423] [G loss: 1.795141]\n",
      "[D loss: 0.885535] [G loss: 1.356442]\n",
      "[D loss: 0.849184] [G loss: 1.584498]\n",
      "[D loss: 0.760161] [G loss: 1.449255]\n",
      "[D loss: 0.739933] [G loss: 1.610595]\n",
      "[D loss: 0.823606] [G loss: 1.461962]\n",
      "[D loss: 0.808337] [G loss: 1.400653]\n",
      "[D loss: 0.846917] [G loss: 1.621516]\n",
      "[D loss: 0.705126] [G loss: 1.571533]\n",
      "[D loss: 0.706724] [G loss: 1.505523]\n",
      "[D loss: 0.913094] [G loss: 1.413980]\n",
      "[D loss: 0.857385] [G loss: 1.519580]\n",
      "[D loss: 0.785831] [G loss: 1.660705]\n",
      "[D loss: 0.850301] [G loss: 1.531632]\n",
      "[D loss: 0.900535] [G loss: 1.504629]\n",
      "[D loss: 0.942370] [G loss: 1.293516]\n",
      "[D loss: 0.801584] [G loss: 1.324745]\n",
      "[D loss: 0.889021] [G loss: 1.536256]\n",
      "[D loss: 0.840264] [G loss: 1.543640]\n",
      "[D loss: 0.843366] [G loss: 1.770638]\n",
      "[D loss: 0.919225] [G loss: 1.401685]\n",
      "[D loss: 0.730408] [G loss: 1.506905]\n",
      "[D loss: 0.899675] [G loss: 1.282499]\n",
      "[D loss: 0.813806] [G loss: 1.379490]\n",
      "[D loss: 0.681888] [G loss: 1.590593]\n",
      "[D loss: 0.790766] [G loss: 1.344728]\n",
      "[D loss: 0.719509] [G loss: 1.411281]\n",
      "[D loss: 0.665170] [G loss: 1.454740]\n",
      "[D loss: 0.686300] [G loss: 1.472138]\n",
      "[D loss: 0.718797] [G loss: 1.423898]\n",
      "[D loss: 0.671401] [G loss: 1.569313]\n",
      "[D loss: 0.873302] [G loss: 1.513807]\n",
      "[D loss: 0.749083] [G loss: 1.448606]\n",
      "[D loss: 1.107939] [G loss: 1.279794]\n",
      "[D loss: 0.662738] [G loss: 1.698866]\n",
      "[D loss: 0.813058] [G loss: 1.610448]\n",
      "[D loss: 0.800212] [G loss: 1.409183]\n",
      "[D loss: 0.755714] [G loss: 1.492576]\n",
      "[D loss: 0.881031] [G loss: 1.415743]\n",
      "[D loss: 0.769729] [G loss: 1.217325]\n",
      "[D loss: 1.062675] [G loss: 1.488861]\n",
      "[D loss: 0.958968] [G loss: 1.593262]\n",
      "[D loss: 0.946378] [G loss: 1.650783]\n",
      "[D loss: 0.914233] [G loss: 1.594306]\n",
      "[D loss: 0.677737] [G loss: 1.507936]\n",
      "[D loss: 0.582183] [G loss: 1.525326]\n",
      "[D loss: 0.859961] [G loss: 1.454107]\n",
      "[D loss: 0.946190] [G loss: 1.542691]\n",
      "[D loss: 0.854192] [G loss: 1.693371]\n",
      "[D loss: 0.886322] [G loss: 1.421386]\n",
      "[D loss: 0.737134] [G loss: 1.389240]\n",
      "[D loss: 0.710279] [G loss: 1.470317]\n",
      "[D loss: 0.793368] [G loss: 1.393940]\n",
      "[D loss: 0.809245] [G loss: 1.601952]\n",
      "[D loss: 0.812252] [G loss: 1.928055]\n",
      "[D loss: 0.567190] [G loss: 1.658026]\n",
      "[D loss: 0.669490] [G loss: 1.428027]\n",
      "[D loss: 0.754806] [G loss: 1.503551]\n",
      "[D loss: 0.843736] [G loss: 1.487833]\n",
      "[D loss: 0.722137] [G loss: 1.517817]\n",
      "[D loss: 0.775332] [G loss: 1.580284]\n",
      "[D loss: 0.928424] [G loss: 1.568085]\n",
      "[D loss: 1.058909] [G loss: 1.261029]\n",
      "[D loss: 0.690775] [G loss: 1.366642]\n",
      "[D loss: 0.646758] [G loss: 1.455229]\n",
      "[D loss: 0.560690] [G loss: 1.630703]\n",
      "[D loss: 0.619318] [G loss: 1.823528]\n",
      "[D loss: 0.533315] [G loss: 1.838839]\n",
      "[D loss: 0.706023] [G loss: 1.438848]\n",
      "[D loss: 1.042347] [G loss: 1.585368]\n",
      "[D loss: 0.673706] [G loss: 1.406560]\n",
      "[D loss: 0.863500] [G loss: 1.511156]\n",
      "[D loss: 0.899974] [G loss: 1.487124]\n",
      "[D loss: 0.813301] [G loss: 1.417597]\n",
      "[D loss: 0.833821] [G loss: 1.643111]\n",
      "[D loss: 1.007823] [G loss: 1.626828]\n",
      "[D loss: 0.725699] [G loss: 1.418654]\n",
      "[D loss: 0.757226] [G loss: 1.591357]\n",
      "[D loss: 0.809200] [G loss: 1.533688]\n",
      "[D loss: 0.805494] [G loss: 1.605269]\n",
      "[D loss: 0.998651] [G loss: 1.297071]\n",
      "[D loss: 0.772849] [G loss: 1.481127]\n",
      "[D loss: 0.851652] [G loss: 1.526812]\n",
      "[D loss: 1.063302] [G loss: 1.366532]\n",
      "[D loss: 0.751798] [G loss: 1.450621]\n",
      "[D loss: 0.720279] [G loss: 1.253819]\n",
      "[D loss: 0.770033] [G loss: 1.377069]\n",
      "[D loss: 1.018161] [G loss: 1.418679]\n",
      "[D loss: 0.729336] [G loss: 1.374639]\n",
      "[D loss: 0.758275] [G loss: 1.670067]\n",
      "[D loss: 0.652371] [G loss: 1.497630]\n",
      "[D loss: 0.494864] [G loss: 1.634606]\n",
      "[D loss: 0.835284] [G loss: 1.446543]\n",
      "[D loss: 0.940533] [G loss: 1.643709]\n",
      "[D loss: 0.555331] [G loss: 1.793697]\n",
      "[D loss: 0.835629] [G loss: 1.424713]\n",
      "[D loss: 0.701176] [G loss: 1.478736]\n",
      "[D loss: 0.488334] [G loss: 1.678934]\n",
      "[D loss: 0.662089] [G loss: 1.460444]\n",
      "[D loss: 0.895907] [G loss: 1.458697]\n",
      "[D loss: 0.886725] [G loss: 1.620953]\n",
      "[D loss: 0.831226] [G loss: 1.543412]\n",
      "[D loss: 0.944461] [G loss: 1.416325]\n",
      "[D loss: 0.681454] [G loss: 1.428013]\n",
      "[D loss: 0.869876] [G loss: 1.618370]\n",
      "[D loss: 0.870655] [G loss: 1.493988]\n",
      "[D loss: 0.915548] [G loss: 1.486171]\n",
      "[D loss: 1.022181] [G loss: 1.342251]\n",
      "[D loss: 1.000042] [G loss: 1.422622]\n",
      "[D loss: 0.974510] [G loss: 1.462698]\n",
      "[D loss: 0.965902] [G loss: 1.443676]\n",
      "[D loss: 0.802345] [G loss: 1.400971]\n",
      "[D loss: 0.795610] [G loss: 1.541728]\n",
      "[D loss: 0.813062] [G loss: 1.443707]\n",
      "[D loss: 0.870374] [G loss: 1.335549]\n",
      "[D loss: 1.034246] [G loss: 1.136708]\n",
      "[D loss: 0.948187] [G loss: 1.412396]\n",
      "[D loss: 1.016982] [G loss: 1.655535]\n",
      "[D loss: 0.883729] [G loss: 1.440530]\n",
      "[D loss: 0.845833] [G loss: 1.332263]\n",
      "[D loss: 0.646386] [G loss: 1.321337]\n",
      "[D loss: 0.828458] [G loss: 1.399644]\n",
      "[D loss: 1.059031] [G loss: 1.225041]\n",
      "[D loss: 0.934962] [G loss: 1.461286]\n",
      "[D loss: 0.734010] [G loss: 1.634251]\n",
      "[D loss: 0.940930] [G loss: 1.324615]\n",
      "[D loss: 0.885367] [G loss: 1.585679]\n",
      "[D loss: 0.715970] [G loss: 1.669554]\n",
      "[D loss: 0.976158] [G loss: 1.345709]\n",
      "[D loss: 0.754234] [G loss: 1.334419]\n",
      "[D loss: 0.916825] [G loss: 1.477003]\n",
      "[D loss: 0.979372] [G loss: 1.491176]\n",
      "[D loss: 0.854683] [G loss: 1.209831]\n",
      "[D loss: 1.018198] [G loss: 1.362640]\n",
      "[D loss: 0.848236] [G loss: 1.355139]\n",
      "[D loss: 0.928597] [G loss: 1.335846]\n",
      "[D loss: 0.816092] [G loss: 1.473474]\n",
      "[D loss: 0.682684] [G loss: 1.681298]\n",
      "[D loss: 0.936194] [G loss: 1.376430]\n",
      "[D loss: 0.922851] [G loss: 1.366711]\n",
      "[D loss: 1.044120] [G loss: 1.507012]\n",
      "[D loss: 0.818956] [G loss: 1.381844]\n",
      "[D loss: 0.914050] [G loss: 1.407646]\n",
      "[D loss: 0.906135] [G loss: 1.444065]\n",
      "[D loss: 0.605849] [G loss: 1.501969]\n",
      "[D loss: 0.765861] [G loss: 1.653982]\n",
      "[D loss: 0.834819] [G loss: 1.344180]\n",
      "[D loss: 0.782214] [G loss: 1.457171]\n",
      "[D loss: 0.609389] [G loss: 1.658790]\n",
      "[D loss: 0.914008] [G loss: 1.518166]\n",
      "[D loss: 0.675114] [G loss: 1.868754]\n",
      "[D loss: 0.726393] [G loss: 1.451841]\n",
      "[D loss: 0.839858] [G loss: 1.583855]\n",
      "[D loss: 0.673451] [G loss: 1.465368]\n",
      "[D loss: 0.928973] [G loss: 1.513029]\n",
      "[D loss: 0.889052] [G loss: 1.476009]\n",
      "[D loss: 0.806425] [G loss: 1.328242]\n",
      "[D loss: 0.784378] [G loss: 1.544620]\n",
      "[D loss: 0.997012] [G loss: 1.465115]\n",
      "[D loss: 0.841542] [G loss: 1.444641]\n",
      "[D loss: 1.107215] [G loss: 1.009215]\n",
      "[D loss: 0.802217] [G loss: 1.412038]\n",
      "[D loss: 0.717807] [G loss: 1.444230]\n",
      "[D loss: 0.828712] [G loss: 1.495505]\n",
      "[D loss: 0.788508] [G loss: 1.550653]\n",
      "[D loss: 0.766894] [G loss: 1.511283]\n",
      "[D loss: 0.805659] [G loss: 1.704143]\n",
      "[D loss: 0.672540] [G loss: 1.485217]\n",
      "[D loss: 0.892140] [G loss: 1.522030]\n",
      "[D loss: 1.091377] [G loss: 1.335671]\n",
      "[D loss: 0.805310] [G loss: 1.608803]\n",
      "[D loss: 0.850060] [G loss: 1.306971]\n",
      "[D loss: 0.653085] [G loss: 1.508682]\n",
      "[D loss: 0.726015] [G loss: 1.360807]\n",
      "[D loss: 0.750758] [G loss: 1.271261]\n",
      "[D loss: 1.057431] [G loss: 1.509269]\n",
      "[D loss: 0.626679] [G loss: 1.322991]\n",
      "[D loss: 0.557651] [G loss: 1.631177]\n",
      "[D loss: 0.820865] [G loss: 1.527489]\n",
      "[D loss: 0.906682] [G loss: 1.295531]\n",
      "[D loss: 0.765928] [G loss: 1.327128]\n",
      "[D loss: 0.992548] [G loss: 1.541321]\n",
      "[D loss: 0.799682] [G loss: 1.475528]\n",
      "[D loss: 0.694299] [G loss: 1.535491]\n",
      "[D loss: 0.787713] [G loss: 1.416233]\n",
      "[D loss: 0.976566] [G loss: 1.373518]\n",
      "[D loss: 0.971913] [G loss: 1.171573]\n",
      "[D loss: 0.731481] [G loss: 1.389774]\n",
      "[D loss: 0.825876] [G loss: 1.515725]\n",
      "[D loss: 0.892148] [G loss: 1.322675]\n",
      "[D loss: 1.054734] [G loss: 1.473698]\n",
      "[D loss: 0.781531] [G loss: 1.367020]\n",
      "[D loss: 0.611252] [G loss: 1.609239]\n",
      "[D loss: 0.698795] [G loss: 1.406570]\n",
      "[D loss: 0.873543] [G loss: 1.420493]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.722656] [G loss: 1.366260]\n",
      "[D loss: 0.823393] [G loss: 1.653903]\n",
      "[D loss: 0.825131] [G loss: 1.613159]\n",
      "[D loss: 0.789592] [G loss: 1.480248]\n",
      "[D loss: 0.963571] [G loss: 1.256837]\n",
      "[D loss: 0.993735] [G loss: 1.504695]\n",
      "[D loss: 0.964434] [G loss: 1.435751]\n",
      "[D loss: 0.834452] [G loss: 1.318336]\n",
      "[D loss: 0.871319] [G loss: 1.547853]\n",
      "[D loss: 1.008951] [G loss: 1.479358]\n",
      "[D loss: 0.796774] [G loss: 1.500741]\n",
      "[D loss: 0.840965] [G loss: 1.221871]\n",
      "[D loss: 0.715684] [G loss: 1.685062]\n",
      "[D loss: 0.761592] [G loss: 1.310909]\n",
      "[D loss: 0.995802] [G loss: 1.308215]\n",
      "[D loss: 0.692097] [G loss: 1.602910]\n",
      "[D loss: 0.720625] [G loss: 1.474495]\n",
      "[D loss: 0.708413] [G loss: 1.559869]\n",
      "[D loss: 0.820747] [G loss: 1.483292]\n",
      "[D loss: 0.841426] [G loss: 1.327707]\n",
      "[D loss: 0.739707] [G loss: 1.354027]\n",
      "[D loss: 0.720297] [G loss: 1.517722]\n",
      "[D loss: 0.857748] [G loss: 1.606833]\n",
      "[D loss: 0.897230] [G loss: 1.550031]\n",
      "[D loss: 0.989420] [G loss: 1.285808]\n",
      "[D loss: 0.802351] [G loss: 1.262164]\n",
      "[D loss: 0.845188] [G loss: 1.517991]\n",
      "[D loss: 0.822273] [G loss: 1.510834]\n",
      "[D loss: 0.674982] [G loss: 1.680208]\n",
      "[D loss: 0.883158] [G loss: 1.608779]\n",
      "[D loss: 0.938096] [G loss: 1.774043]\n",
      "[D loss: 0.930775] [G loss: 1.239180]\n",
      "[D loss: 0.723518] [G loss: 1.461289]\n",
      "[D loss: 0.852524] [G loss: 1.245401]\n",
      "[D loss: 1.020081] [G loss: 1.392910]\n",
      "[D loss: 0.731273] [G loss: 1.582070]\n",
      "[D loss: 0.847497] [G loss: 1.408091]\n",
      "[D loss: 0.946166] [G loss: 1.646836]\n",
      "[D loss: 0.576167] [G loss: 1.472529]\n",
      "[D loss: 0.859775] [G loss: 1.478466]\n",
      "[D loss: 0.725600] [G loss: 1.487030]\n",
      "[D loss: 0.692101] [G loss: 1.324845]\n",
      "[D loss: 0.847580] [G loss: 1.349547]\n",
      "[D loss: 0.811927] [G loss: 1.543475]\n",
      "[D loss: 1.158841] [G loss: 1.535732]\n",
      "[D loss: 0.887249] [G loss: 1.352499]\n",
      "[D loss: 1.039655] [G loss: 1.263102]\n",
      "[D loss: 0.663255] [G loss: 1.478397]\n",
      "[D loss: 0.901466] [G loss: 1.423759]\n",
      "[D loss: 0.725011] [G loss: 1.382509]\n",
      "[D loss: 0.842906] [G loss: 1.661109]\n",
      "[D loss: 0.856581] [G loss: 1.327148]\n",
      "[D loss: 0.980746] [G loss: 1.386091]\n",
      "[D loss: 0.660957] [G loss: 1.561433]\n",
      "[D loss: 0.987744] [G loss: 1.267675]\n",
      "[D loss: 0.924387] [G loss: 1.774763]\n",
      "[D loss: 0.830991] [G loss: 1.561970]\n",
      "[D loss: 0.746677] [G loss: 1.433630]\n",
      "[D loss: 0.900115] [G loss: 1.358390]\n",
      "[D loss: 0.698407] [G loss: 1.443433]\n",
      "[D loss: 0.836575] [G loss: 1.304017]\n",
      "[D loss: 1.037882] [G loss: 1.156235]\n",
      "[D loss: 0.896544] [G loss: 1.419304]\n",
      "[D loss: 0.769873] [G loss: 1.466002]\n",
      "[D loss: 0.662760] [G loss: 1.510651]\n",
      "[D loss: 0.927654] [G loss: 1.458040]\n",
      "[D loss: 0.967928] [G loss: 1.531319]\n",
      "[D loss: 0.767783] [G loss: 1.545393]\n",
      "[D loss: 0.752083] [G loss: 1.339075]\n",
      "[D loss: 0.624515] [G loss: 1.645469]\n",
      "[D loss: 0.603166] [G loss: 1.553194]\n",
      "[D loss: 0.634988] [G loss: 1.427511]\n",
      "[D loss: 0.685308] [G loss: 1.679446]\n",
      "[D loss: 0.766828] [G loss: 1.454143]\n",
      "[D loss: 0.557620] [G loss: 1.704943]\n",
      "[D loss: 0.916138] [G loss: 1.395376]\n",
      "[D loss: 0.713487] [G loss: 1.633579]\n",
      "[D loss: 0.659752] [G loss: 1.736909]\n",
      "[D loss: 0.772796] [G loss: 1.654253]\n",
      "[D loss: 0.902741] [G loss: 1.400014]\n",
      "[D loss: 1.024654] [G loss: 1.786945]\n",
      "[D loss: 0.925391] [G loss: 1.453725]\n",
      "[D loss: 0.631643] [G loss: 1.429890]\n",
      "[D loss: 0.673395] [G loss: 1.697371]\n",
      "[D loss: 1.083216] [G loss: 1.294849]\n",
      "[D loss: 0.719697] [G loss: 1.468306]\n",
      "[D loss: 1.169150] [G loss: 1.537242]\n",
      "[D loss: 0.905387] [G loss: 1.353092]\n",
      "[D loss: 0.953115] [G loss: 1.329526]\n",
      "[D loss: 0.915384] [G loss: 1.677929]\n",
      "[D loss: 0.900818] [G loss: 1.533345]\n",
      "[D loss: 0.890937] [G loss: 1.424376]\n",
      "[D loss: 0.646800] [G loss: 1.329488]\n",
      "[D loss: 0.786957] [G loss: 1.230310]\n",
      "[D loss: 0.735327] [G loss: 1.353625]\n",
      "[D loss: 0.794019] [G loss: 1.352621]\n",
      "[D loss: 0.895006] [G loss: 1.264281]\n",
      "[D loss: 0.756724] [G loss: 1.335580]\n",
      "[D loss: 1.044751] [G loss: 1.223422]\n",
      "[D loss: 0.790540] [G loss: 1.580684]\n",
      "[D loss: 0.713256] [G loss: 1.398480]\n",
      "[D loss: 0.654066] [G loss: 1.431377]\n",
      "[D loss: 0.839086] [G loss: 1.507766]\n",
      "[D loss: 0.952176] [G loss: 1.316763]\n",
      "[D loss: 0.968375] [G loss: 1.244752]\n",
      "[D loss: 0.657834] [G loss: 1.427576]\n",
      "[D loss: 0.614463] [G loss: 1.509898]\n",
      "[D loss: 0.704528] [G loss: 1.423280]\n",
      "[D loss: 0.796500] [G loss: 1.414655]\n",
      "[D loss: 0.970975] [G loss: 1.380407]\n",
      "[D loss: 0.708473] [G loss: 1.411958]\n",
      "[D loss: 0.924822] [G loss: 1.336134]\n",
      "[D loss: 0.786053] [G loss: 1.407643]\n",
      "[D loss: 0.730141] [G loss: 1.464377]\n",
      "[D loss: 0.848890] [G loss: 1.525320]\n",
      "[D loss: 0.781353] [G loss: 1.561069]\n",
      "[D loss: 1.051306] [G loss: 1.611704]\n",
      "[D loss: 0.832688] [G loss: 1.548395]\n",
      "[D loss: 0.710533] [G loss: 1.452615]\n",
      "[D loss: 0.842718] [G loss: 1.458374]\n",
      "[D loss: 0.647931] [G loss: 1.512710]\n",
      "[D loss: 0.748351] [G loss: 1.586805]\n",
      "[D loss: 0.957062] [G loss: 1.652604]\n",
      "[D loss: 1.077908] [G loss: 1.402488]\n",
      "[D loss: 0.804652] [G loss: 1.461964]\n",
      "[D loss: 0.689628] [G loss: 1.433511]\n",
      "[D loss: 0.820465] [G loss: 1.290304]\n",
      "[D loss: 0.965972] [G loss: 1.328753]\n",
      "[D loss: 0.820142] [G loss: 1.392988]\n",
      "[D loss: 0.806685] [G loss: 1.293292]\n",
      "[D loss: 0.812330] [G loss: 1.483119]\n",
      "[D loss: 0.690750] [G loss: 1.433094]\n",
      "[D loss: 0.812781] [G loss: 1.652808]\n",
      "[D loss: 0.721555] [G loss: 1.673046]\n",
      "[D loss: 0.845525] [G loss: 1.445615]\n",
      "[D loss: 1.076092] [G loss: 1.517009]\n",
      "[D loss: 0.823654] [G loss: 1.454649]\n",
      "[D loss: 0.917954] [G loss: 1.351408]\n",
      "[D loss: 0.802377] [G loss: 1.529989]\n",
      "[D loss: 0.989150] [G loss: 1.133096]\n",
      "[D loss: 0.810178] [G loss: 1.378262]\n",
      "[D loss: 0.678776] [G loss: 1.416546]\n",
      "[D loss: 0.839513] [G loss: 1.541000]\n",
      "[D loss: 0.853990] [G loss: 1.510469]\n",
      "[D loss: 0.717625] [G loss: 1.705642]\n",
      "[D loss: 0.905847] [G loss: 1.216023]\n",
      "[D loss: 0.921369] [G loss: 1.433920]\n",
      "[D loss: 1.061833] [G loss: 1.363085]\n",
      "[D loss: 0.904339] [G loss: 1.443978]\n",
      "[D loss: 0.855991] [G loss: 1.335279]\n",
      "[D loss: 0.827265] [G loss: 1.243758]\n",
      "[D loss: 0.699475] [G loss: 1.535352]\n",
      "[D loss: 0.828927] [G loss: 1.313739]\n",
      "[D loss: 0.894973] [G loss: 1.272964]\n",
      "[D loss: 0.693567] [G loss: 1.329398]\n",
      "[D loss: 0.848894] [G loss: 1.562343]\n",
      "[D loss: 0.723220] [G loss: 1.507729]\n",
      "[D loss: 0.789517] [G loss: 1.474213]\n",
      "[D loss: 0.638631] [G loss: 1.485631]\n",
      "[D loss: 0.714513] [G loss: 1.714842]\n",
      "[D loss: 0.881650] [G loss: 1.517624]\n",
      "[D loss: 0.718957] [G loss: 1.484835]\n",
      "[D loss: 0.822875] [G loss: 1.333215]\n",
      "[D loss: 0.805364] [G loss: 1.781443]\n",
      "[D loss: 0.848810] [G loss: 1.351620]\n",
      "[D loss: 1.064155] [G loss: 1.414130]\n",
      "[D loss: 0.834123] [G loss: 1.693701]\n",
      "[D loss: 0.974870] [G loss: 1.655173]\n",
      "[D loss: 0.849128] [G loss: 1.577407]\n",
      "[D loss: 0.825537] [G loss: 1.539040]\n",
      "[D loss: 0.727450] [G loss: 1.671111]\n",
      "[D loss: 0.656847] [G loss: 1.580230]\n",
      "[D loss: 0.987249] [G loss: 1.397617]\n",
      "[D loss: 0.783696] [G loss: 1.635257]\n",
      "[D loss: 1.148191] [G loss: 1.342109]\n",
      "[D loss: 0.869053] [G loss: 1.371209]\n",
      "[D loss: 0.942751] [G loss: 1.191600]\n",
      "[D loss: 0.729023] [G loss: 1.367848]\n",
      "[D loss: 0.601434] [G loss: 1.519843]\n",
      "[D loss: 0.738277] [G loss: 1.680873]\n",
      "[D loss: 0.925461] [G loss: 1.434046]\n",
      "[D loss: 1.015082] [G loss: 1.488241]\n",
      "[D loss: 0.769687] [G loss: 1.550424]\n",
      "[D loss: 0.949515] [G loss: 1.246871]\n",
      "[D loss: 0.731527] [G loss: 1.478199]\n",
      "[D loss: 0.923583] [G loss: 1.346942]\n",
      "[D loss: 1.018506] [G loss: 1.371229]\n",
      "[D loss: 0.688435] [G loss: 1.557271]\n",
      "[D loss: 0.761853] [G loss: 1.442199]\n",
      "[D loss: 0.916809] [G loss: 1.423082]\n",
      "[D loss: 0.652893] [G loss: 1.478359]\n",
      "[D loss: 0.749073] [G loss: 1.500406]\n",
      "[D loss: 0.716287] [G loss: 1.777802]\n",
      "[D loss: 0.955456] [G loss: 1.494982]\n",
      "[D loss: 0.693892] [G loss: 1.529156]\n",
      "[D loss: 1.055789] [G loss: 1.501056]\n",
      "[D loss: 0.814090] [G loss: 1.637741]\n",
      "[D loss: 0.654231] [G loss: 1.526138]\n",
      "[D loss: 0.787104] [G loss: 1.470243]\n",
      "[D loss: 0.946611] [G loss: 1.468012]\n",
      "[D loss: 0.809009] [G loss: 1.355415]\n",
      "[D loss: 0.864038] [G loss: 1.344658]\n",
      "[D loss: 0.632077] [G loss: 1.670286]\n",
      "[D loss: 0.753500] [G loss: 1.467727]\n",
      "[D loss: 0.853719] [G loss: 1.569906]\n",
      "[D loss: 0.636430] [G loss: 1.323774]\n",
      "[D loss: 0.763747] [G loss: 1.262956]\n",
      "[D loss: 0.949916] [G loss: 1.441143]\n",
      "[D loss: 0.901770] [G loss: 1.497117]\n",
      "[D loss: 0.747899] [G loss: 1.572816]\n",
      "[D loss: 0.858644] [G loss: 1.646710]\n",
      "[D loss: 0.830932] [G loss: 1.447843]\n",
      "[D loss: 0.856209] [G loss: 1.404382]\n",
      "[D loss: 0.923302] [G loss: 1.343556]\n",
      "[D loss: 0.831831] [G loss: 1.581558]\n",
      "[D loss: 0.941714] [G loss: 1.358518]\n",
      "[D loss: 1.068622] [G loss: 1.281799]\n",
      "[D loss: 0.910629] [G loss: 1.535200]\n",
      "[D loss: 0.845677] [G loss: 1.534925]\n",
      "[D loss: 0.892648] [G loss: 1.329346]\n",
      "[D loss: 0.782218] [G loss: 1.518121]\n",
      "[D loss: 0.614599] [G loss: 1.661071]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.656263] [G loss: 1.648955]\n",
      "[D loss: 1.197448] [G loss: 1.375161]\n",
      "[D loss: 0.674513] [G loss: 1.708766]\n",
      "[D loss: 0.861094] [G loss: 1.534828]\n",
      "[D loss: 0.833840] [G loss: 1.557207]\n",
      "[D loss: 1.185923] [G loss: 1.181488]\n",
      "[D loss: 0.844541] [G loss: 1.440535]\n",
      "[D loss: 0.780891] [G loss: 1.474244]\n",
      "[D loss: 0.817142] [G loss: 1.240608]\n",
      "[D loss: 0.705758] [G loss: 1.460085]\n",
      "[D loss: 0.947823] [G loss: 1.438202]\n",
      "[D loss: 0.747249] [G loss: 1.412679]\n",
      "[D loss: 0.840155] [G loss: 1.323685]\n",
      "[D loss: 0.804030] [G loss: 1.458172]\n",
      "[D loss: 1.022437] [G loss: 1.408755]\n",
      "[D loss: 0.950281] [G loss: 1.556395]\n",
      "[D loss: 0.732975] [G loss: 1.784039]\n",
      "[D loss: 0.880502] [G loss: 1.484999]\n",
      "[D loss: 0.988412] [G loss: 1.460756]\n",
      "[D loss: 1.131272] [G loss: 1.261164]\n",
      "[D loss: 0.755143] [G loss: 1.109264]\n",
      "[D loss: 0.910295] [G loss: 1.325035]\n",
      "[D loss: 0.861905] [G loss: 1.453414]\n",
      "[D loss: 0.983753] [G loss: 1.416993]\n",
      "[D loss: 0.764926] [G loss: 1.332027]\n",
      "[D loss: 0.804966] [G loss: 1.503506]\n",
      "[D loss: 0.890154] [G loss: 1.468253]\n",
      "[D loss: 0.768078] [G loss: 1.446339]\n",
      "[D loss: 0.915813] [G loss: 1.541133]\n",
      "[D loss: 0.660719] [G loss: 1.468515]\n",
      "[D loss: 0.899157] [G loss: 1.322371]\n",
      "[D loss: 1.096963] [G loss: 1.324448]\n",
      "[D loss: 0.831547] [G loss: 1.524014]\n",
      "[D loss: 0.812873] [G loss: 1.316477]\n",
      "[D loss: 0.903350] [G loss: 1.318187]\n",
      "[D loss: 0.700560] [G loss: 1.338986]\n",
      "[D loss: 1.052086] [G loss: 1.468325]\n",
      "[D loss: 1.091003] [G loss: 1.656216]\n",
      "[D loss: 1.000799] [G loss: 1.366263]\n",
      "[D loss: 0.920571] [G loss: 1.530661]\n",
      "[D loss: 0.733756] [G loss: 1.304855]\n",
      "[D loss: 1.056660] [G loss: 1.497063]\n",
      "[D loss: 0.812323] [G loss: 1.336756]\n",
      "[D loss: 1.033130] [G loss: 1.215525]\n",
      "[D loss: 0.939273] [G loss: 1.363376]\n",
      "[D loss: 0.913567] [G loss: 1.300388]\n",
      "[D loss: 0.781688] [G loss: 1.394192]\n",
      "[D loss: 0.838627] [G loss: 1.418268]\n",
      "[D loss: 0.927538] [G loss: 1.349091]\n",
      "[D loss: 0.855410] [G loss: 1.321428]\n",
      "[D loss: 0.844530] [G loss: 1.486470]\n",
      "[D loss: 0.667397] [G loss: 1.404059]\n",
      "[D loss: 0.734390] [G loss: 1.375323]\n",
      "[D loss: 0.649572] [G loss: 1.337996]\n",
      "[D loss: 1.038692] [G loss: 1.304571]\n",
      "[D loss: 1.055522] [G loss: 1.360836]\n",
      "[D loss: 0.901068] [G loss: 1.408728]\n",
      "[D loss: 0.769793] [G loss: 1.382087]\n",
      "[D loss: 0.790857] [G loss: 1.596137]\n",
      "[D loss: 0.886857] [G loss: 1.440716]\n",
      "[D loss: 0.858893] [G loss: 1.518636]\n",
      "[D loss: 0.965691] [G loss: 1.402513]\n",
      "[D loss: 1.223009] [G loss: 1.242403]\n",
      "[D loss: 0.901173] [G loss: 1.392738]\n",
      "[D loss: 0.879363] [G loss: 1.239621]\n",
      "[D loss: 0.592113] [G loss: 1.252327]\n",
      "[D loss: 0.739022] [G loss: 1.195688]\n",
      "[D loss: 0.996390] [G loss: 1.309420]\n",
      "[D loss: 0.796320] [G loss: 1.366324]\n",
      "[D loss: 0.976255] [G loss: 1.368287]\n",
      "[D loss: 0.603036] [G loss: 1.323764]\n",
      "[D loss: 0.750891] [G loss: 1.448246]\n",
      "[D loss: 0.835515] [G loss: 1.598312]\n",
      "[D loss: 0.721328] [G loss: 1.376034]\n",
      "[D loss: 0.911985] [G loss: 1.431965]\n",
      "[D loss: 0.785102] [G loss: 1.290300]\n",
      "[D loss: 0.936118] [G loss: 1.547217]\n",
      "[D loss: 0.711227] [G loss: 1.456236]\n",
      "[D loss: 0.988228] [G loss: 1.377026]\n",
      "[D loss: 0.747149] [G loss: 1.594733]\n",
      "[D loss: 1.025112] [G loss: 1.273654]\n",
      "[D loss: 0.949415] [G loss: 1.617839]\n",
      "[D loss: 0.759280] [G loss: 1.376367]\n",
      "[D loss: 0.766356] [G loss: 1.501511]\n",
      "[D loss: 1.171431] [G loss: 1.410013]\n",
      "[D loss: 0.691944] [G loss: 1.429452]\n",
      "[D loss: 0.845780] [G loss: 1.356195]\n",
      "[D loss: 0.930254] [G loss: 1.240110]\n",
      "[D loss: 1.121430] [G loss: 1.234313]\n",
      "[D loss: 0.773696] [G loss: 1.694823]\n",
      "[D loss: 0.887528] [G loss: 1.540955]\n",
      "[D loss: 0.777518] [G loss: 1.546254]\n",
      "[D loss: 0.836477] [G loss: 1.476058]\n",
      "[D loss: 0.546744] [G loss: 1.583861]\n",
      "[D loss: 0.681386] [G loss: 1.603574]\n",
      "[D loss: 0.941097] [G loss: 1.212266]\n",
      "[D loss: 0.878358] [G loss: 1.664067]\n",
      "[D loss: 0.810885] [G loss: 1.651095]\n",
      "[D loss: 0.604611] [G loss: 1.510701]\n",
      "[D loss: 0.719570] [G loss: 1.405819]\n",
      "[D loss: 0.646582] [G loss: 1.258632]\n",
      "[D loss: 0.841334] [G loss: 1.370690]\n",
      "[D loss: 0.627662] [G loss: 1.401756]\n",
      "[D loss: 0.874574] [G loss: 1.499021]\n",
      "[D loss: 0.825644] [G loss: 1.457965]\n",
      "[D loss: 1.081124] [G loss: 1.467723]\n",
      "[D loss: 0.862782] [G loss: 1.362719]\n",
      "[D loss: 0.745625] [G loss: 1.477290]\n",
      "[D loss: 0.878195] [G loss: 1.510624]\n",
      "[D loss: 0.748334] [G loss: 1.555313]\n",
      "[D loss: 0.769042] [G loss: 1.614352]\n",
      "[D loss: 0.870387] [G loss: 1.547564]\n",
      "[D loss: 0.993921] [G loss: 1.295676]\n",
      "[D loss: 0.681138] [G loss: 1.538686]\n",
      "[D loss: 0.916962] [G loss: 1.593471]\n",
      "[D loss: 1.011398] [G loss: 1.313013]\n",
      "[D loss: 0.784006] [G loss: 1.533642]\n",
      "[D loss: 0.902663] [G loss: 1.357208]\n",
      "[D loss: 0.772804] [G loss: 1.683697]\n",
      "[D loss: 1.020680] [G loss: 1.590248]\n",
      "[D loss: 0.769638] [G loss: 1.390597]\n",
      "[D loss: 1.022057] [G loss: 1.535010]\n",
      "[D loss: 0.636976] [G loss: 1.611381]\n",
      "[D loss: 0.678509] [G loss: 1.396128]\n",
      "[D loss: 0.982016] [G loss: 1.348145]\n",
      "[D loss: 0.699685] [G loss: 1.315331]\n",
      "[D loss: 0.774152] [G loss: 1.562460]\n",
      "[D loss: 0.848323] [G loss: 1.521143]\n",
      "[D loss: 0.687876] [G loss: 1.496215]\n",
      "[D loss: 1.018833] [G loss: 1.373060]\n",
      "[D loss: 0.957122] [G loss: 1.405925]\n",
      "[D loss: 0.802360] [G loss: 1.617975]\n",
      "[D loss: 0.884709] [G loss: 1.316257]\n",
      "[D loss: 0.785040] [G loss: 1.617937]\n",
      "[D loss: 0.950820] [G loss: 1.285407]\n",
      "[D loss: 0.839469] [G loss: 1.545643]\n",
      "[D loss: 1.185201] [G loss: 1.248893]\n",
      "[D loss: 0.756702] [G loss: 1.630440]\n",
      "[D loss: 0.673623] [G loss: 1.412691]\n",
      "[D loss: 0.638037] [G loss: 1.374437]\n",
      "[D loss: 0.718499] [G loss: 1.648942]\n",
      "[D loss: 0.904193] [G loss: 1.436383]\n",
      "[D loss: 0.860453] [G loss: 1.494249]\n",
      "[D loss: 0.925325] [G loss: 1.515022]\n",
      "[D loss: 0.834679] [G loss: 1.733776]\n",
      "[D loss: 0.716162] [G loss: 1.526903]\n",
      "[D loss: 0.926035] [G loss: 1.501081]\n",
      "[D loss: 0.798524] [G loss: 1.638309]\n",
      "[D loss: 0.826235] [G loss: 1.562281]\n",
      "[D loss: 0.866244] [G loss: 1.481097]\n",
      "[D loss: 0.923384] [G loss: 1.442054]\n",
      "[D loss: 0.654575] [G loss: 1.718522]\n",
      "[D loss: 0.798450] [G loss: 1.383506]\n",
      "[D loss: 0.870350] [G loss: 1.542164]\n",
      "[D loss: 0.682928] [G loss: 1.375864]\n",
      "[D loss: 0.725584] [G loss: 1.654827]\n",
      "[D loss: 0.957712] [G loss: 1.925753]\n",
      "[D loss: 0.831778] [G loss: 1.506616]\n",
      "[D loss: 0.811794] [G loss: 1.206388]\n",
      "[D loss: 0.811415] [G loss: 1.238693]\n",
      "[D loss: 1.001963] [G loss: 1.311314]\n",
      "[D loss: 0.875589] [G loss: 1.425382]\n",
      "[D loss: 0.824384] [G loss: 1.469524]\n",
      "[D loss: 0.913988] [G loss: 1.616175]\n",
      "[D loss: 0.801823] [G loss: 1.413386]\n",
      "[D loss: 0.802443] [G loss: 1.576748]\n",
      "[D loss: 1.084944] [G loss: 1.388310]\n",
      "[D loss: 0.677063] [G loss: 1.494871]\n",
      "[D loss: 0.852419] [G loss: 1.584228]\n",
      "[D loss: 0.920880] [G loss: 1.408420]\n",
      "[D loss: 0.918630] [G loss: 1.424184]\n",
      "[D loss: 0.605741] [G loss: 1.276676]\n",
      "[D loss: 1.038451] [G loss: 1.378371]\n",
      "[D loss: 0.773429] [G loss: 1.486277]\n",
      "[D loss: 1.291221] [G loss: 1.218665]\n",
      "[D loss: 0.756406] [G loss: 1.519649]\n",
      "[D loss: 0.910410] [G loss: 1.421977]\n",
      "[D loss: 0.717631] [G loss: 1.433268]\n",
      "[D loss: 0.857829] [G loss: 1.656411]\n",
      "[D loss: 0.962352] [G loss: 1.406422]\n",
      "[D loss: 0.799926] [G loss: 1.498797]\n",
      "[D loss: 0.976754] [G loss: 1.489845]\n",
      "[D loss: 0.885894] [G loss: 1.408237]\n",
      "[D loss: 0.977488] [G loss: 1.328987]\n",
      "[D loss: 0.776294] [G loss: 1.395636]\n",
      "[D loss: 0.707196] [G loss: 1.380818]\n",
      "[D loss: 0.997605] [G loss: 1.556172]\n",
      "[D loss: 0.750312] [G loss: 1.511869]\n",
      "[D loss: 1.050407] [G loss: 1.362450]\n",
      "[D loss: 0.761570] [G loss: 1.356473]\n",
      "[D loss: 0.848834] [G loss: 1.492838]\n",
      "[D loss: 0.745743] [G loss: 1.349188]\n",
      "[D loss: 0.706292] [G loss: 1.410489]\n",
      "[D loss: 0.789257] [G loss: 1.486236]\n",
      "[D loss: 1.036396] [G loss: 1.323661]\n",
      "[D loss: 0.794907] [G loss: 1.444069]\n",
      "[D loss: 0.974835] [G loss: 1.300624]\n",
      "[D loss: 1.078607] [G loss: 1.349233]\n",
      "[D loss: 1.079434] [G loss: 1.365828]\n",
      "[D loss: 0.587815] [G loss: 1.576061]\n",
      "[D loss: 0.810396] [G loss: 1.564187]\n",
      "[D loss: 0.721540] [G loss: 1.508782]\n",
      "[D loss: 0.809038] [G loss: 1.546549]\n",
      "[D loss: 0.823888] [G loss: 1.485764]\n",
      "[D loss: 0.539306] [G loss: 1.744146]\n",
      "[D loss: 0.761038] [G loss: 1.360090]\n",
      "[D loss: 0.767712] [G loss: 1.484738]\n",
      "[D loss: 1.241228] [G loss: 1.373501]\n",
      "[D loss: 0.627683] [G loss: 1.510750]\n",
      "[D loss: 0.729662] [G loss: 1.461642]\n",
      "[D loss: 0.970285] [G loss: 1.260983]\n",
      "[D loss: 0.800282] [G loss: 1.363799]\n",
      "[D loss: 0.786915] [G loss: 1.532035]\n",
      "[D loss: 0.800187] [G loss: 1.549847]\n",
      "[D loss: 0.670058] [G loss: 1.523580]\n",
      "[D loss: 0.680819] [G loss: 1.448918]\n",
      "[D loss: 0.918160] [G loss: 1.467665]\n",
      "[D loss: 0.944893] [G loss: 1.395529]\n",
      "[D loss: 0.909526] [G loss: 1.776327]\n",
      "[D loss: 0.702048] [G loss: 1.645925]\n",
      "[D loss: 0.922933] [G loss: 1.312279]\n",
      "[D loss: 0.850721] [G loss: 1.794666]\n",
      "[D loss: 0.959047] [G loss: 1.391767]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.897251] [G loss: 1.530753]\n",
      "[D loss: 0.764099] [G loss: 1.467734]\n",
      "[D loss: 0.722138] [G loss: 1.422037]\n",
      "[D loss: 0.693576] [G loss: 1.437695]\n",
      "[D loss: 0.876386] [G loss: 1.256115]\n",
      "[D loss: 0.803114] [G loss: 1.484683]\n",
      "epoch:14, g_loss:2755.32568359375,d_loss:1552.6180419921875\n",
      "[D loss: 0.659860] [G loss: 1.650594]\n",
      "[D loss: 0.969136] [G loss: 1.387992]\n",
      "[D loss: 0.754252] [G loss: 1.239853]\n",
      "[D loss: 0.918315] [G loss: 1.299499]\n",
      "[D loss: 0.856184] [G loss: 1.311951]\n",
      "[D loss: 1.057447] [G loss: 1.690151]\n",
      "[D loss: 0.985311] [G loss: 1.550948]\n",
      "[D loss: 0.673117] [G loss: 1.477158]\n",
      "[D loss: 0.866222] [G loss: 1.504919]\n",
      "[D loss: 0.886812] [G loss: 1.313331]\n",
      "[D loss: 0.705516] [G loss: 1.576965]\n",
      "[D loss: 0.808069] [G loss: 1.452178]\n",
      "[D loss: 0.831486] [G loss: 1.248528]\n",
      "[D loss: 0.789579] [G loss: 1.775041]\n",
      "[D loss: 0.984357] [G loss: 1.376948]\n",
      "[D loss: 0.741076] [G loss: 1.328721]\n",
      "[D loss: 0.932418] [G loss: 1.340918]\n",
      "[D loss: 0.836511] [G loss: 1.404866]\n",
      "[D loss: 0.946604] [G loss: 1.444702]\n",
      "[D loss: 0.904509] [G loss: 1.372149]\n",
      "[D loss: 0.778215] [G loss: 1.499676]\n",
      "[D loss: 0.704630] [G loss: 1.407836]\n",
      "[D loss: 0.917906] [G loss: 1.496884]\n",
      "[D loss: 1.054611] [G loss: 1.336874]\n",
      "[D loss: 0.773889] [G loss: 1.320401]\n",
      "[D loss: 0.725084] [G loss: 1.297511]\n",
      "[D loss: 0.876237] [G loss: 1.248362]\n",
      "[D loss: 0.796645] [G loss: 1.498639]\n",
      "[D loss: 0.867788] [G loss: 1.433829]\n",
      "[D loss: 0.725161] [G loss: 1.510730]\n",
      "[D loss: 0.775785] [G loss: 1.541856]\n",
      "[D loss: 1.046585] [G loss: 1.429010]\n",
      "[D loss: 0.992407] [G loss: 1.216987]\n",
      "[D loss: 0.690602] [G loss: 1.426250]\n",
      "[D loss: 0.795268] [G loss: 1.360443]\n",
      "[D loss: 0.797884] [G loss: 1.323588]\n",
      "[D loss: 0.768644] [G loss: 1.508454]\n",
      "[D loss: 0.630444] [G loss: 1.589446]\n",
      "[D loss: 0.845804] [G loss: 1.428482]\n",
      "[D loss: 0.787885] [G loss: 1.415612]\n",
      "[D loss: 0.804854] [G loss: 1.561643]\n",
      "[D loss: 0.843806] [G loss: 1.476765]\n",
      "[D loss: 0.719232] [G loss: 1.439399]\n",
      "[D loss: 0.864120] [G loss: 1.472839]\n",
      "[D loss: 0.783882] [G loss: 1.498332]\n",
      "[D loss: 0.594033] [G loss: 1.634509]\n",
      "[D loss: 1.018577] [G loss: 1.590051]\n",
      "[D loss: 1.029309] [G loss: 1.387882]\n",
      "[D loss: 0.834918] [G loss: 1.250065]\n",
      "[D loss: 0.615053] [G loss: 1.665468]\n",
      "[D loss: 0.799040] [G loss: 1.683686]\n",
      "[D loss: 0.777738] [G loss: 1.505382]\n",
      "[D loss: 0.781265] [G loss: 1.467517]\n",
      "[D loss: 0.990832] [G loss: 1.494575]\n",
      "[D loss: 0.783381] [G loss: 1.862832]\n",
      "[D loss: 0.866554] [G loss: 1.491036]\n",
      "[D loss: 0.588114] [G loss: 2.000503]\n",
      "[D loss: 0.767236] [G loss: 1.687434]\n",
      "[D loss: 0.755616] [G loss: 1.660129]\n",
      "[D loss: 0.940187] [G loss: 1.682061]\n",
      "[D loss: 0.945334] [G loss: 1.327045]\n",
      "[D loss: 0.744658] [G loss: 1.546496]\n",
      "[D loss: 0.870884] [G loss: 1.693267]\n",
      "[D loss: 0.740364] [G loss: 1.630861]\n",
      "[D loss: 0.695673] [G loss: 1.603169]\n",
      "[D loss: 0.707481] [G loss: 1.638794]\n",
      "[D loss: 0.821186] [G loss: 1.536119]\n",
      "[D loss: 0.784702] [G loss: 1.513636]\n",
      "[D loss: 0.567854] [G loss: 1.535598]\n",
      "[D loss: 0.993241] [G loss: 1.230376]\n",
      "[D loss: 0.699085] [G loss: 1.443205]\n",
      "[D loss: 0.673957] [G loss: 1.529829]\n",
      "[D loss: 0.852857] [G loss: 1.603099]\n",
      "[D loss: 0.938570] [G loss: 1.569272]\n",
      "[D loss: 0.865863] [G loss: 1.225751]\n",
      "[D loss: 0.788189] [G loss: 1.419081]\n",
      "[D loss: 0.863251] [G loss: 1.257879]\n",
      "[D loss: 0.885173] [G loss: 1.299966]\n",
      "[D loss: 0.715274] [G loss: 1.196818]\n",
      "[D loss: 0.901415] [G loss: 1.379251]\n",
      "[D loss: 0.790387] [G loss: 1.549373]\n",
      "[D loss: 0.672543] [G loss: 1.593549]\n",
      "[D loss: 0.905395] [G loss: 1.385445]\n",
      "[D loss: 0.943443] [G loss: 1.331232]\n",
      "[D loss: 0.960801] [G loss: 1.620437]\n",
      "[D loss: 1.118717] [G loss: 1.380465]\n",
      "[D loss: 0.722088] [G loss: 1.285928]\n",
      "[D loss: 0.766795] [G loss: 1.493180]\n",
      "[D loss: 0.946281] [G loss: 1.365838]\n",
      "[D loss: 0.694419] [G loss: 1.632469]\n",
      "[D loss: 0.693300] [G loss: 1.864584]\n",
      "[D loss: 0.835887] [G loss: 1.372509]\n",
      "[D loss: 0.596051] [G loss: 1.812888]\n",
      "[D loss: 0.771588] [G loss: 1.604015]\n",
      "[D loss: 0.994505] [G loss: 1.446943]\n",
      "[D loss: 0.861710] [G loss: 1.496255]\n",
      "[D loss: 0.689811] [G loss: 1.346891]\n",
      "[D loss: 0.965924] [G loss: 1.506892]\n",
      "[D loss: 0.740588] [G loss: 1.587043]\n",
      "[D loss: 1.147647] [G loss: 1.301341]\n",
      "[D loss: 1.069989] [G loss: 1.412686]\n",
      "[D loss: 0.989955] [G loss: 1.369149]\n",
      "[D loss: 0.879086] [G loss: 1.443742]\n",
      "[D loss: 0.880541] [G loss: 1.433647]\n",
      "[D loss: 0.897684] [G loss: 1.474956]\n",
      "[D loss: 0.975567] [G loss: 1.506999]\n",
      "[D loss: 0.835242] [G loss: 1.265866]\n",
      "[D loss: 0.795626] [G loss: 1.355932]\n",
      "[D loss: 0.873835] [G loss: 1.421457]\n",
      "[D loss: 0.846201] [G loss: 1.427824]\n",
      "[D loss: 0.817562] [G loss: 1.318486]\n",
      "[D loss: 0.860818] [G loss: 1.393468]\n",
      "[D loss: 0.867180] [G loss: 1.504128]\n",
      "[D loss: 0.819697] [G loss: 1.449190]\n",
      "[D loss: 0.820247] [G loss: 1.368777]\n",
      "[D loss: 0.623108] [G loss: 1.398324]\n",
      "[D loss: 0.837683] [G loss: 1.502413]\n",
      "[D loss: 0.803274] [G loss: 1.221860]\n",
      "[D loss: 0.981375] [G loss: 1.539676]\n",
      "[D loss: 1.066241] [G loss: 1.696038]\n",
      "[D loss: 0.864762] [G loss: 1.349450]\n",
      "[D loss: 1.136644] [G loss: 1.073289]\n",
      "[D loss: 0.692150] [G loss: 1.389203]\n",
      "[D loss: 0.970450] [G loss: 1.340115]\n",
      "[D loss: 0.834782] [G loss: 1.421292]\n",
      "[D loss: 0.942486] [G loss: 1.150211]\n",
      "[D loss: 0.760248] [G loss: 1.362773]\n",
      "[D loss: 0.833056] [G loss: 1.387791]\n",
      "[D loss: 0.809241] [G loss: 1.253395]\n",
      "[D loss: 0.934813] [G loss: 1.470638]\n",
      "[D loss: 0.867304] [G loss: 1.429624]\n",
      "[D loss: 0.990969] [G loss: 1.444211]\n",
      "[D loss: 0.883357] [G loss: 1.441077]\n",
      "[D loss: 0.990542] [G loss: 1.258309]\n",
      "[D loss: 0.891000] [G loss: 1.262409]\n",
      "[D loss: 0.980659] [G loss: 1.216149]\n",
      "[D loss: 0.698064] [G loss: 1.310042]\n",
      "[D loss: 0.849656] [G loss: 1.594360]\n",
      "[D loss: 0.800062] [G loss: 1.549942]\n",
      "[D loss: 0.780512] [G loss: 1.840755]\n",
      "[D loss: 0.899939] [G loss: 1.423272]\n",
      "[D loss: 0.772105] [G loss: 1.471546]\n",
      "[D loss: 0.725462] [G loss: 1.459758]\n",
      "[D loss: 0.803234] [G loss: 1.666551]\n",
      "[D loss: 0.740208] [G loss: 1.512382]\n",
      "[D loss: 0.993069] [G loss: 1.619138]\n",
      "[D loss: 0.647426] [G loss: 1.657784]\n",
      "[D loss: 0.943688] [G loss: 1.504280]\n",
      "[D loss: 1.047044] [G loss: 1.354928]\n",
      "[D loss: 0.770558] [G loss: 1.404829]\n",
      "[D loss: 0.935037] [G loss: 1.286844]\n",
      "[D loss: 0.918812] [G loss: 1.373217]\n",
      "[D loss: 0.928325] [G loss: 1.715356]\n",
      "[D loss: 0.982954] [G loss: 1.324797]\n",
      "[D loss: 0.634454] [G loss: 1.812232]\n",
      "[D loss: 0.880254] [G loss: 1.479516]\n",
      "[D loss: 0.743622] [G loss: 1.432699]\n",
      "[D loss: 0.856387] [G loss: 1.348385]\n",
      "[D loss: 0.877815] [G loss: 1.712559]\n",
      "[D loss: 1.038368] [G loss: 1.417387]\n",
      "[D loss: 0.786701] [G loss: 1.536572]\n",
      "[D loss: 0.686158] [G loss: 1.427382]\n",
      "[D loss: 0.771434] [G loss: 1.298005]\n",
      "[D loss: 0.995245] [G loss: 1.259419]\n",
      "[D loss: 0.746294] [G loss: 1.313267]\n",
      "[D loss: 0.903381] [G loss: 1.558668]\n",
      "[D loss: 0.927380] [G loss: 1.580648]\n",
      "[D loss: 0.875863] [G loss: 1.545528]\n",
      "[D loss: 0.975835] [G loss: 1.398893]\n",
      "[D loss: 0.799914] [G loss: 1.490022]\n",
      "[D loss: 0.697012] [G loss: 1.346225]\n",
      "[D loss: 0.670203] [G loss: 1.357470]\n",
      "[D loss: 0.825065] [G loss: 1.569333]\n",
      "[D loss: 0.897353] [G loss: 1.432903]\n",
      "[D loss: 0.768768] [G loss: 1.624090]\n",
      "[D loss: 0.938003] [G loss: 1.663986]\n",
      "[D loss: 0.624571] [G loss: 1.668993]\n",
      "[D loss: 0.779028] [G loss: 1.352935]\n",
      "[D loss: 0.883004] [G loss: 1.406540]\n",
      "[D loss: 0.768316] [G loss: 1.567594]\n",
      "[D loss: 0.823464] [G loss: 1.444132]\n",
      "[D loss: 0.755622] [G loss: 1.516056]\n",
      "[D loss: 0.681848] [G loss: 1.415237]\n",
      "[D loss: 0.914853] [G loss: 1.311996]\n",
      "[D loss: 0.833999] [G loss: 1.293887]\n",
      "[D loss: 0.793856] [G loss: 1.617409]\n",
      "[D loss: 0.731194] [G loss: 1.395471]\n",
      "[D loss: 0.739264] [G loss: 1.417695]\n",
      "[D loss: 0.846577] [G loss: 1.667560]\n",
      "[D loss: 0.604862] [G loss: 1.633632]\n",
      "[D loss: 0.963919] [G loss: 1.663215]\n",
      "[D loss: 0.704514] [G loss: 1.632747]\n",
      "[D loss: 0.958313] [G loss: 1.400426]\n",
      "[D loss: 0.761537] [G loss: 1.415568]\n",
      "[D loss: 0.696447] [G loss: 1.481195]\n",
      "[D loss: 0.692705] [G loss: 1.596899]\n",
      "[D loss: 0.792801] [G loss: 1.406518]\n",
      "[D loss: 0.688085] [G loss: 1.565257]\n",
      "[D loss: 0.797963] [G loss: 1.404934]\n",
      "[D loss: 0.813601] [G loss: 1.258202]\n",
      "[D loss: 0.862602] [G loss: 1.430306]\n",
      "[D loss: 0.694742] [G loss: 1.595236]\n",
      "[D loss: 0.822926] [G loss: 1.778040]\n",
      "[D loss: 0.875668] [G loss: 1.404606]\n",
      "[D loss: 0.820136] [G loss: 1.517230]\n",
      "[D loss: 0.554994] [G loss: 1.683132]\n",
      "[D loss: 0.917529] [G loss: 1.603218]\n",
      "[D loss: 0.600358] [G loss: 1.423067]\n",
      "[D loss: 0.741267] [G loss: 1.442331]\n",
      "[D loss: 0.689805] [G loss: 1.543833]\n",
      "[D loss: 1.015933] [G loss: 1.364096]\n",
      "[D loss: 0.685313] [G loss: 1.375165]\n",
      "[D loss: 0.762059] [G loss: 1.687539]\n",
      "[D loss: 0.706516] [G loss: 1.539352]\n",
      "[D loss: 0.686600] [G loss: 1.657032]\n",
      "[D loss: 0.882302] [G loss: 1.636799]\n",
      "[D loss: 0.593021] [G loss: 1.718004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.616019] [G loss: 1.506721]\n",
      "[D loss: 0.723573] [G loss: 1.645940]\n",
      "[D loss: 0.881662] [G loss: 1.499355]\n",
      "[D loss: 0.623590] [G loss: 1.636922]\n",
      "[D loss: 0.955419] [G loss: 1.534592]\n",
      "[D loss: 0.801542] [G loss: 1.401170]\n",
      "[D loss: 0.752975] [G loss: 1.812795]\n",
      "[D loss: 0.728660] [G loss: 1.958744]\n",
      "[D loss: 0.703902] [G loss: 1.534665]\n",
      "[D loss: 0.958165] [G loss: 1.652782]\n",
      "[D loss: 1.083381] [G loss: 1.607008]\n",
      "[D loss: 0.757104] [G loss: 1.487061]\n",
      "[D loss: 0.732694] [G loss: 1.716241]\n",
      "[D loss: 0.632354] [G loss: 1.459049]\n",
      "[D loss: 0.769162] [G loss: 1.501286]\n",
      "[D loss: 0.741242] [G loss: 1.687851]\n",
      "[D loss: 0.947503] [G loss: 1.496202]\n",
      "[D loss: 0.814660] [G loss: 1.362054]\n",
      "[D loss: 0.626421] [G loss: 1.243548]\n",
      "[D loss: 0.839815] [G loss: 1.355018]\n",
      "[D loss: 0.699507] [G loss: 1.336669]\n",
      "[D loss: 1.025041] [G loss: 1.332721]\n",
      "[D loss: 0.805656] [G loss: 1.669113]\n",
      "[D loss: 0.736566] [G loss: 1.635495]\n",
      "[D loss: 1.002780] [G loss: 1.791528]\n",
      "[D loss: 0.843342] [G loss: 1.640936]\n",
      "[D loss: 0.756644] [G loss: 1.729688]\n",
      "[D loss: 0.765697] [G loss: 1.436941]\n",
      "[D loss: 0.882852] [G loss: 1.541531]\n",
      "[D loss: 0.711134] [G loss: 1.611181]\n",
      "[D loss: 1.108296] [G loss: 1.541020]\n",
      "[D loss: 0.913692] [G loss: 1.453942]\n",
      "[D loss: 0.911015] [G loss: 1.348672]\n",
      "[D loss: 0.696491] [G loss: 1.465732]\n",
      "[D loss: 0.835849] [G loss: 1.567631]\n",
      "[D loss: 0.913772] [G loss: 1.590997]\n",
      "[D loss: 0.739414] [G loss: 1.671797]\n",
      "[D loss: 0.856326] [G loss: 1.269575]\n",
      "[D loss: 0.837970] [G loss: 1.393445]\n",
      "[D loss: 0.911109] [G loss: 1.448971]\n",
      "[D loss: 0.798749] [G loss: 1.494673]\n",
      "[D loss: 0.756167] [G loss: 1.451438]\n",
      "[D loss: 0.972717] [G loss: 1.462550]\n",
      "[D loss: 0.999771] [G loss: 1.285968]\n",
      "[D loss: 0.886708] [G loss: 1.599561]\n",
      "[D loss: 0.833153] [G loss: 1.351618]\n",
      "[D loss: 0.919534] [G loss: 1.336831]\n",
      "[D loss: 0.701627] [G loss: 1.392974]\n",
      "[D loss: 0.878556] [G loss: 1.311870]\n",
      "[D loss: 1.235732] [G loss: 1.415095]\n",
      "[D loss: 0.750272] [G loss: 1.452078]\n",
      "[D loss: 0.829552] [G loss: 1.409041]\n",
      "[D loss: 0.997985] [G loss: 1.401188]\n",
      "[D loss: 1.022167] [G loss: 1.332934]\n",
      "[D loss: 0.792321] [G loss: 1.430931]\n",
      "[D loss: 0.798113] [G loss: 1.314249]\n",
      "[D loss: 0.806533] [G loss: 1.545729]\n",
      "[D loss: 0.558396] [G loss: 1.671548]\n",
      "[D loss: 0.746890] [G loss: 1.516456]\n",
      "[D loss: 0.654463] [G loss: 1.649928]\n",
      "[D loss: 0.654140] [G loss: 1.539058]\n",
      "[D loss: 0.674663] [G loss: 1.605131]\n",
      "[D loss: 0.945573] [G loss: 1.474453]\n",
      "[D loss: 0.730044] [G loss: 1.707069]\n",
      "[D loss: 0.575942] [G loss: 1.471785]\n",
      "[D loss: 0.743299] [G loss: 1.582197]\n",
      "[D loss: 0.811188] [G loss: 1.721616]\n",
      "[D loss: 0.954561] [G loss: 1.398866]\n",
      "[D loss: 0.661004] [G loss: 1.561464]\n",
      "[D loss: 0.957663] [G loss: 1.572848]\n",
      "[D loss: 0.809990] [G loss: 1.578272]\n",
      "[D loss: 0.881900] [G loss: 1.456812]\n",
      "[D loss: 0.886816] [G loss: 1.494107]\n",
      "[D loss: 0.768666] [G loss: 1.530759]\n",
      "[D loss: 0.916627] [G loss: 1.473153]\n",
      "[D loss: 1.026571] [G loss: 1.611393]\n",
      "[D loss: 0.915864] [G loss: 1.605778]\n",
      "[D loss: 0.745648] [G loss: 1.640503]\n",
      "[D loss: 0.696876] [G loss: 1.654628]\n",
      "[D loss: 0.743139] [G loss: 1.581570]\n",
      "[D loss: 1.020722] [G loss: 1.289043]\n",
      "[D loss: 0.824609] [G loss: 1.572811]\n",
      "[D loss: 0.741667] [G loss: 1.349447]\n",
      "[D loss: 1.111156] [G loss: 1.428420]\n",
      "[D loss: 0.827049] [G loss: 1.430483]\n",
      "[D loss: 0.822859] [G loss: 1.490255]\n",
      "[D loss: 0.936736] [G loss: 1.513196]\n",
      "[D loss: 0.722021] [G loss: 1.556186]\n",
      "[D loss: 0.672769] [G loss: 1.588856]\n",
      "[D loss: 0.653524] [G loss: 1.465278]\n",
      "[D loss: 0.931914] [G loss: 1.434478]\n",
      "[D loss: 0.925682] [G loss: 1.681408]\n",
      "[D loss: 0.862012] [G loss: 1.343340]\n",
      "[D loss: 0.892000] [G loss: 1.298182]\n",
      "[D loss: 0.830603] [G loss: 1.608899]\n",
      "[D loss: 0.753140] [G loss: 1.426848]\n",
      "[D loss: 0.767458] [G loss: 1.491732]\n",
      "[D loss: 0.978167] [G loss: 1.427884]\n",
      "[D loss: 0.971874] [G loss: 1.570698]\n",
      "[D loss: 0.662967] [G loss: 1.353113]\n",
      "[D loss: 0.817129] [G loss: 1.288217]\n",
      "[D loss: 0.687007] [G loss: 1.425335]\n",
      "[D loss: 0.812137] [G loss: 1.496585]\n",
      "[D loss: 0.473795] [G loss: 1.909584]\n",
      "[D loss: 0.964205] [G loss: 1.670160]\n",
      "[D loss: 0.814342] [G loss: 1.442444]\n",
      "[D loss: 0.937253] [G loss: 1.438285]\n",
      "[D loss: 0.730953] [G loss: 1.708043]\n",
      "[D loss: 0.971088] [G loss: 1.327714]\n",
      "[D loss: 0.953494] [G loss: 1.351948]\n",
      "[D loss: 0.802887] [G loss: 1.585298]\n",
      "[D loss: 0.708532] [G loss: 1.460323]\n",
      "[D loss: 0.811770] [G loss: 1.551158]\n",
      "[D loss: 0.690148] [G loss: 1.566318]\n",
      "[D loss: 0.919250] [G loss: 1.679986]\n",
      "[D loss: 0.876075] [G loss: 1.484565]\n",
      "[D loss: 0.979673] [G loss: 1.427159]\n",
      "[D loss: 0.930726] [G loss: 1.365512]\n",
      "[D loss: 0.844717] [G loss: 1.412895]\n",
      "[D loss: 1.071588] [G loss: 1.367394]\n",
      "[D loss: 0.879431] [G loss: 1.489640]\n",
      "[D loss: 0.743581] [G loss: 1.621879]\n",
      "[D loss: 0.736361] [G loss: 1.586513]\n",
      "[D loss: 1.069919] [G loss: 1.572589]\n",
      "[D loss: 0.697778] [G loss: 1.356274]\n",
      "[D loss: 0.899722] [G loss: 1.436495]\n",
      "[D loss: 0.764542] [G loss: 1.227915]\n",
      "[D loss: 0.863070] [G loss: 1.588052]\n",
      "[D loss: 0.934024] [G loss: 1.442610]\n",
      "[D loss: 0.878018] [G loss: 1.413728]\n",
      "[D loss: 0.864244] [G loss: 1.269564]\n",
      "[D loss: 0.865851] [G loss: 1.465217]\n",
      "[D loss: 0.761177] [G loss: 1.462215]\n",
      "[D loss: 1.043056] [G loss: 1.421376]\n",
      "[D loss: 1.052113] [G loss: 1.288358]\n",
      "[D loss: 0.880809] [G loss: 1.211029]\n",
      "[D loss: 0.787393] [G loss: 1.326294]\n",
      "[D loss: 0.639748] [G loss: 1.536150]\n",
      "[D loss: 0.751893] [G loss: 1.520094]\n",
      "[D loss: 0.918180] [G loss: 1.327936]\n",
      "[D loss: 0.846662] [G loss: 1.263520]\n",
      "[D loss: 0.865290] [G loss: 1.276644]\n",
      "[D loss: 0.914301] [G loss: 1.538888]\n",
      "[D loss: 0.731578] [G loss: 1.395802]\n",
      "[D loss: 0.909807] [G loss: 1.262252]\n",
      "[D loss: 0.924128] [G loss: 1.366334]\n",
      "[D loss: 1.031935] [G loss: 1.395535]\n",
      "[D loss: 0.563096] [G loss: 1.520504]\n",
      "[D loss: 1.008474] [G loss: 1.269381]\n",
      "[D loss: 1.127678] [G loss: 1.315393]\n",
      "[D loss: 0.765514] [G loss: 1.715253]\n",
      "[D loss: 1.180841] [G loss: 1.394592]\n",
      "[D loss: 0.909736] [G loss: 1.284721]\n",
      "[D loss: 0.838701] [G loss: 1.427835]\n",
      "[D loss: 0.921826] [G loss: 1.534235]\n",
      "[D loss: 0.641483] [G loss: 1.347906]\n",
      "[D loss: 0.735659] [G loss: 1.339723]\n",
      "[D loss: 0.886010] [G loss: 1.327474]\n",
      "[D loss: 1.039243] [G loss: 1.263690]\n",
      "[D loss: 0.725240] [G loss: 1.531514]\n",
      "[D loss: 0.940478] [G loss: 1.612768]\n",
      "[D loss: 0.832789] [G loss: 1.646011]\n",
      "[D loss: 0.676161] [G loss: 1.587560]\n",
      "[D loss: 0.781079] [G loss: 1.629636]\n",
      "[D loss: 0.612084] [G loss: 1.595650]\n",
      "[D loss: 0.936247] [G loss: 1.465619]\n",
      "[D loss: 0.899642] [G loss: 1.593992]\n",
      "[D loss: 0.997119] [G loss: 1.390085]\n",
      "[D loss: 0.829298] [G loss: 1.568269]\n",
      "[D loss: 0.873440] [G loss: 1.438699]\n",
      "[D loss: 0.722080] [G loss: 1.490689]\n",
      "[D loss: 0.698289] [G loss: 1.599601]\n",
      "[D loss: 0.602083] [G loss: 1.412793]\n",
      "[D loss: 0.670519] [G loss: 1.368352]\n",
      "[D loss: 1.130140] [G loss: 1.560881]\n",
      "[D loss: 1.064960] [G loss: 1.517858]\n",
      "[D loss: 0.885898] [G loss: 1.498564]\n",
      "[D loss: 0.843110] [G loss: 1.301125]\n",
      "[D loss: 0.731658] [G loss: 1.563606]\n",
      "[D loss: 0.809772] [G loss: 1.384393]\n",
      "[D loss: 0.799765] [G loss: 1.419069]\n",
      "[D loss: 0.778891] [G loss: 1.331241]\n",
      "[D loss: 0.856259] [G loss: 1.253072]\n",
      "[D loss: 0.670611] [G loss: 1.355292]\n",
      "[D loss: 0.739772] [G loss: 1.225699]\n",
      "[D loss: 0.939012] [G loss: 1.322862]\n",
      "[D loss: 0.902369] [G loss: 1.553410]\n",
      "[D loss: 0.874804] [G loss: 1.604790]\n",
      "[D loss: 0.912046] [G loss: 1.383074]\n",
      "[D loss: 0.875721] [G loss: 1.246462]\n",
      "[D loss: 0.799681] [G loss: 1.403507]\n",
      "[D loss: 0.787205] [G loss: 1.375548]\n",
      "[D loss: 0.966189] [G loss: 1.404712]\n",
      "[D loss: 0.880971] [G loss: 1.656305]\n",
      "[D loss: 0.970078] [G loss: 1.287913]\n",
      "[D loss: 0.775556] [G loss: 1.343835]\n",
      "[D loss: 0.706541] [G loss: 1.450273]\n",
      "[D loss: 0.876675] [G loss: 1.449824]\n",
      "[D loss: 0.812611] [G loss: 1.192841]\n",
      "[D loss: 0.809455] [G loss: 1.279044]\n",
      "[D loss: 0.930067] [G loss: 1.577022]\n",
      "[D loss: 0.959985] [G loss: 1.431179]\n",
      "[D loss: 0.751427] [G loss: 1.491626]\n",
      "[D loss: 0.874652] [G loss: 1.491442]\n",
      "[D loss: 0.848698] [G loss: 1.469088]\n",
      "[D loss: 0.796093] [G loss: 1.409865]\n",
      "[D loss: 0.750534] [G loss: 1.246233]\n",
      "[D loss: 0.868845] [G loss: 1.401149]\n",
      "[D loss: 0.899532] [G loss: 1.317343]\n",
      "[D loss: 0.893774] [G loss: 1.262483]\n",
      "[D loss: 0.826107] [G loss: 1.463307]\n",
      "[D loss: 1.009490] [G loss: 1.516303]\n",
      "[D loss: 0.939359] [G loss: 1.548405]\n",
      "[D loss: 0.693530] [G loss: 1.573336]\n",
      "[D loss: 0.912820] [G loss: 1.392871]\n",
      "[D loss: 0.702438] [G loss: 1.308737]\n",
      "[D loss: 0.955434] [G loss: 1.428513]\n",
      "[D loss: 1.023545] [G loss: 1.529411]\n",
      "[D loss: 0.841066] [G loss: 1.324353]\n",
      "[D loss: 0.878443] [G loss: 1.466473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.779129] [G loss: 1.463934]\n",
      "[D loss: 0.884326] [G loss: 1.416564]\n",
      "[D loss: 0.915204] [G loss: 1.351322]\n",
      "[D loss: 0.772098] [G loss: 1.376119]\n",
      "[D loss: 0.742402] [G loss: 1.412597]\n",
      "[D loss: 0.730085] [G loss: 1.505512]\n",
      "[D loss: 0.841162] [G loss: 1.431409]\n",
      "[D loss: 0.779203] [G loss: 1.465183]\n",
      "[D loss: 0.792135] [G loss: 1.528142]\n",
      "[D loss: 0.753052] [G loss: 1.326960]\n",
      "[D loss: 0.938060] [G loss: 1.575247]\n",
      "[D loss: 0.995208] [G loss: 1.694596]\n",
      "[D loss: 0.987520] [G loss: 1.466165]\n",
      "[D loss: 0.706081] [G loss: 1.426551]\n",
      "[D loss: 1.059107] [G loss: 1.115313]\n",
      "[D loss: 0.672509] [G loss: 1.455202]\n",
      "[D loss: 0.789150] [G loss: 1.211133]\n",
      "[D loss: 0.963261] [G loss: 1.252623]\n",
      "[D loss: 0.702300] [G loss: 1.453471]\n",
      "[D loss: 0.815592] [G loss: 1.610252]\n",
      "[D loss: 0.782304] [G loss: 1.515328]\n",
      "[D loss: 0.745648] [G loss: 1.482570]\n",
      "[D loss: 0.903116] [G loss: 1.372239]\n",
      "[D loss: 0.635862] [G loss: 1.445369]\n",
      "[D loss: 0.880121] [G loss: 1.551306]\n",
      "[D loss: 0.861571] [G loss: 1.332410]\n",
      "[D loss: 0.889856] [G loss: 1.488704]\n",
      "[D loss: 0.851461] [G loss: 1.574594]\n",
      "[D loss: 0.676886] [G loss: 1.398030]\n",
      "[D loss: 0.601923] [G loss: 1.607929]\n",
      "[D loss: 0.773147] [G loss: 1.518707]\n",
      "[D loss: 0.942321] [G loss: 1.597557]\n",
      "[D loss: 0.671024] [G loss: 1.532505]\n",
      "[D loss: 0.844662] [G loss: 1.365919]\n",
      "[D loss: 0.596078] [G loss: 1.461929]\n",
      "[D loss: 0.827271] [G loss: 1.432968]\n",
      "[D loss: 0.704564] [G loss: 1.385666]\n",
      "[D loss: 1.017025] [G loss: 1.391474]\n",
      "[D loss: 0.583821] [G loss: 1.574373]\n",
      "[D loss: 0.961126] [G loss: 1.775550]\n",
      "[D loss: 0.806399] [G loss: 1.349239]\n",
      "[D loss: 0.883953] [G loss: 1.338502]\n",
      "[D loss: 0.962056] [G loss: 1.363638]\n",
      "[D loss: 0.895351] [G loss: 1.431717]\n",
      "[D loss: 0.807688] [G loss: 1.383699]\n",
      "[D loss: 0.922620] [G loss: 1.711159]\n",
      "[D loss: 1.137472] [G loss: 1.388188]\n",
      "[D loss: 0.679590] [G loss: 1.444425]\n",
      "[D loss: 0.918582] [G loss: 1.330584]\n",
      "[D loss: 0.859521] [G loss: 1.288487]\n",
      "[D loss: 0.856049] [G loss: 1.303739]\n",
      "[D loss: 0.732635] [G loss: 1.280344]\n",
      "[D loss: 0.728184] [G loss: 1.505870]\n",
      "[D loss: 0.815098] [G loss: 1.459064]\n",
      "[D loss: 0.973526] [G loss: 1.550943]\n",
      "[D loss: 0.965637] [G loss: 1.364576]\n",
      "[D loss: 0.604602] [G loss: 1.467453]\n",
      "[D loss: 0.813749] [G loss: 1.538812]\n",
      "[D loss: 0.763880] [G loss: 1.529432]\n",
      "[D loss: 0.897324] [G loss: 1.366422]\n",
      "[D loss: 0.802110] [G loss: 1.519478]\n",
      "[D loss: 1.024346] [G loss: 1.491909]\n",
      "[D loss: 0.831017] [G loss: 1.316596]\n",
      "[D loss: 1.015220] [G loss: 1.289130]\n",
      "[D loss: 0.760557] [G loss: 1.353213]\n",
      "[D loss: 0.760692] [G loss: 1.450380]\n",
      "[D loss: 0.924670] [G loss: 1.533880]\n",
      "[D loss: 0.804624] [G loss: 1.427168]\n",
      "[D loss: 1.072514] [G loss: 1.267269]\n",
      "[D loss: 0.955641] [G loss: 1.607558]\n",
      "[D loss: 0.874380] [G loss: 1.350280]\n",
      "[D loss: 0.816124] [G loss: 1.481254]\n",
      "[D loss: 0.854832] [G loss: 1.235466]\n",
      "[D loss: 0.862378] [G loss: 1.569300]\n",
      "[D loss: 0.984168] [G loss: 1.281797]\n",
      "[D loss: 0.844071] [G loss: 1.556554]\n",
      "[D loss: 1.141277] [G loss: 1.453036]\n",
      "[D loss: 0.950403] [G loss: 1.295355]\n",
      "[D loss: 0.864879] [G loss: 1.452931]\n",
      "[D loss: 0.872940] [G loss: 1.288691]\n",
      "[D loss: 0.728999] [G loss: 1.484882]\n",
      "[D loss: 0.767624] [G loss: 1.273500]\n",
      "[D loss: 0.800255] [G loss: 1.635579]\n",
      "[D loss: 0.802730] [G loss: 1.282672]\n",
      "[D loss: 0.747197] [G loss: 1.300541]\n",
      "[D loss: 0.852368] [G loss: 1.437724]\n",
      "[D loss: 0.857224] [G loss: 1.419696]\n",
      "[D loss: 0.954833] [G loss: 1.468802]\n",
      "[D loss: 0.725308] [G loss: 1.439644]\n",
      "[D loss: 0.932385] [G loss: 1.534772]\n",
      "[D loss: 0.631883] [G loss: 1.794521]\n",
      "[D loss: 0.896226] [G loss: 1.466233]\n",
      "[D loss: 1.013511] [G loss: 1.161124]\n",
      "[D loss: 0.475668] [G loss: 1.609042]\n",
      "[D loss: 0.802505] [G loss: 1.346271]\n",
      "[D loss: 0.786067] [G loss: 1.307495]\n",
      "[D loss: 0.857292] [G loss: 1.296691]\n",
      "[D loss: 0.839188] [G loss: 1.726065]\n",
      "[D loss: 0.933161] [G loss: 1.558822]\n",
      "[D loss: 0.743772] [G loss: 1.602395]\n",
      "[D loss: 0.892714] [G loss: 1.274278]\n",
      "[D loss: 0.842316] [G loss: 1.445623]\n",
      "[D loss: 0.887376] [G loss: 1.336352]\n",
      "[D loss: 1.066073] [G loss: 1.330724]\n",
      "[D loss: 0.961299] [G loss: 1.369823]\n",
      "[D loss: 0.733684] [G loss: 1.526519]\n",
      "[D loss: 0.702322] [G loss: 1.514072]\n",
      "[D loss: 0.965490] [G loss: 1.302117]\n",
      "[D loss: 0.784208] [G loss: 1.391463]\n",
      "[D loss: 0.968099] [G loss: 1.472791]\n",
      "[D loss: 0.790623] [G loss: 1.674512]\n",
      "[D loss: 1.098584] [G loss: 1.180747]\n",
      "[D loss: 0.838545] [G loss: 1.311482]\n",
      "[D loss: 0.648340] [G loss: 1.445679]\n",
      "[D loss: 0.759473] [G loss: 1.426513]\n",
      "[D loss: 0.718337] [G loss: 1.403625]\n",
      "[D loss: 0.672891] [G loss: 1.431789]\n",
      "[D loss: 0.873054] [G loss: 1.488633]\n",
      "[D loss: 0.779337] [G loss: 1.467854]\n",
      "[D loss: 0.924178] [G loss: 1.609192]\n",
      "[D loss: 0.499273] [G loss: 1.626308]\n",
      "[D loss: 0.659988] [G loss: 1.526656]\n",
      "[D loss: 0.549418] [G loss: 1.731887]\n",
      "[D loss: 1.004481] [G loss: 1.298913]\n",
      "[D loss: 0.802324] [G loss: 1.710521]\n",
      "[D loss: 0.811252] [G loss: 1.512630]\n",
      "[D loss: 0.794762] [G loss: 1.339723]\n",
      "[D loss: 0.901016] [G loss: 1.620152]\n",
      "[D loss: 0.936473] [G loss: 1.706129]\n",
      "[D loss: 0.781798] [G loss: 1.242315]\n",
      "[D loss: 0.950149] [G loss: 1.341363]\n",
      "[D loss: 0.855771] [G loss: 1.354542]\n",
      "[D loss: 0.705422] [G loss: 1.495440]\n",
      "[D loss: 0.880310] [G loss: 1.540403]\n",
      "[D loss: 0.939773] [G loss: 1.539620]\n",
      "[D loss: 0.891876] [G loss: 1.456192]\n",
      "[D loss: 0.870595] [G loss: 1.420374]\n",
      "[D loss: 0.954726] [G loss: 1.345641]\n",
      "[D loss: 0.776648] [G loss: 1.617185]\n",
      "[D loss: 0.614775] [G loss: 1.418141]\n",
      "[D loss: 0.662442] [G loss: 1.677802]\n",
      "[D loss: 0.786382] [G loss: 1.673136]\n",
      "[D loss: 0.694086] [G loss: 1.421922]\n",
      "[D loss: 0.768253] [G loss: 1.647037]\n",
      "[D loss: 0.934731] [G loss: 1.464463]\n",
      "[D loss: 0.936240] [G loss: 1.398396]\n",
      "[D loss: 0.754570] [G loss: 1.364818]\n",
      "[D loss: 0.965808] [G loss: 1.391666]\n",
      "[D loss: 0.714590] [G loss: 1.537830]\n",
      "[D loss: 0.753003] [G loss: 1.682994]\n",
      "[D loss: 0.565879] [G loss: 1.642984]\n",
      "[D loss: 1.165985] [G loss: 1.410752]\n",
      "[D loss: 1.116397] [G loss: 1.239327]\n",
      "[D loss: 0.886751] [G loss: 1.395733]\n",
      "[D loss: 0.935330] [G loss: 1.446219]\n",
      "[D loss: 0.831434] [G loss: 1.531248]\n",
      "[D loss: 0.759216] [G loss: 1.381257]\n",
      "[D loss: 0.826623] [G loss: 1.556141]\n",
      "[D loss: 0.683846] [G loss: 1.295479]\n",
      "[D loss: 0.668640] [G loss: 1.519766]\n",
      "[D loss: 0.882000] [G loss: 1.361099]\n",
      "[D loss: 0.900702] [G loss: 1.280463]\n",
      "[D loss: 0.759701] [G loss: 1.474255]\n",
      "[D loss: 0.853462] [G loss: 1.420313]\n",
      "[D loss: 0.926034] [G loss: 1.790559]\n",
      "[D loss: 0.771025] [G loss: 1.555443]\n",
      "[D loss: 0.845974] [G loss: 1.562538]\n",
      "[D loss: 0.778768] [G loss: 1.457611]\n",
      "[D loss: 0.992973] [G loss: 1.562636]\n",
      "[D loss: 0.851881] [G loss: 1.359043]\n",
      "[D loss: 0.853794] [G loss: 1.324942]\n",
      "[D loss: 0.905574] [G loss: 1.419332]\n",
      "[D loss: 0.773318] [G loss: 1.529853]\n",
      "[D loss: 0.853080] [G loss: 1.384026]\n",
      "[D loss: 1.061459] [G loss: 1.312790]\n",
      "[D loss: 0.713820] [G loss: 1.561038]\n",
      "[D loss: 0.860450] [G loss: 1.571189]\n",
      "[D loss: 0.769229] [G loss: 1.627235]\n",
      "[D loss: 0.876228] [G loss: 1.612031]\n",
      "[D loss: 0.818807] [G loss: 1.843073]\n",
      "[D loss: 0.808815] [G loss: 1.622896]\n",
      "[D loss: 1.053490] [G loss: 1.553859]\n",
      "[D loss: 0.837116] [G loss: 1.493681]\n",
      "[D loss: 0.770202] [G loss: 1.548662]\n",
      "[D loss: 0.967105] [G loss: 1.441367]\n",
      "[D loss: 0.764806] [G loss: 1.597012]\n",
      "[D loss: 0.970420] [G loss: 1.383792]\n",
      "[D loss: 0.770239] [G loss: 1.435035]\n",
      "[D loss: 0.804784] [G loss: 1.403492]\n",
      "[D loss: 0.936792] [G loss: 1.342293]\n",
      "[D loss: 0.996649] [G loss: 1.419025]\n",
      "[D loss: 0.829657] [G loss: 1.438329]\n",
      "[D loss: 0.972260] [G loss: 1.210522]\n",
      "[D loss: 0.828562] [G loss: 1.357951]\n",
      "[D loss: 0.834761] [G loss: 1.407033]\n",
      "[D loss: 0.748166] [G loss: 1.481118]\n",
      "[D loss: 0.890042] [G loss: 1.659778]\n",
      "[D loss: 0.793453] [G loss: 1.413446]\n",
      "[D loss: 0.788864] [G loss: 1.263828]\n",
      "[D loss: 0.879249] [G loss: 1.253225]\n",
      "[D loss: 0.619672] [G loss: 1.371285]\n",
      "[D loss: 1.159189] [G loss: 1.254682]\n",
      "[D loss: 0.712502] [G loss: 1.726473]\n",
      "[D loss: 0.804502] [G loss: 1.717187]\n",
      "[D loss: 0.897369] [G loss: 1.321727]\n",
      "[D loss: 0.748132] [G loss: 1.438106]\n",
      "[D loss: 1.020752] [G loss: 1.456336]\n",
      "[D loss: 0.780300] [G loss: 1.637999]\n",
      "[D loss: 0.749785] [G loss: 1.508544]\n",
      "[D loss: 0.744972] [G loss: 1.652328]\n",
      "[D loss: 0.786237] [G loss: 1.288832]\n",
      "[D loss: 0.727301] [G loss: 1.365896]\n",
      "[D loss: 0.810120] [G loss: 1.359461]\n",
      "[D loss: 0.770296] [G loss: 1.414757]\n",
      "[D loss: 0.571227] [G loss: 1.585975]\n",
      "[D loss: 0.929632] [G loss: 1.535380]\n",
      "[D loss: 0.843497] [G loss: 1.646459]\n",
      "[D loss: 0.936057] [G loss: 1.470596]\n",
      "[D loss: 1.046249] [G loss: 1.516851]\n",
      "[D loss: 0.806214] [G loss: 1.379264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.912198] [G loss: 1.413777]\n",
      "[D loss: 0.882540] [G loss: 1.459888]\n",
      "[D loss: 0.536440] [G loss: 1.536468]\n",
      "[D loss: 0.978123] [G loss: 1.360146]\n",
      "[D loss: 0.854546] [G loss: 1.589772]\n",
      "[D loss: 0.826285] [G loss: 1.323310]\n",
      "[D loss: 0.834990] [G loss: 1.602139]\n",
      "[D loss: 0.757263] [G loss: 1.644400]\n",
      "[D loss: 0.881038] [G loss: 1.576106]\n",
      "[D loss: 1.093428] [G loss: 1.343710]\n",
      "[D loss: 0.715647] [G loss: 1.534361]\n",
      "[D loss: 0.661105] [G loss: 1.399405]\n",
      "[D loss: 0.767776] [G loss: 1.491593]\n",
      "[D loss: 0.835751] [G loss: 1.472570]\n",
      "[D loss: 0.778276] [G loss: 1.766248]\n",
      "[D loss: 0.930708] [G loss: 1.595403]\n",
      "[D loss: 0.882271] [G loss: 1.726021]\n",
      "[D loss: 0.944109] [G loss: 1.305444]\n",
      "[D loss: 0.712571] [G loss: 1.521981]\n",
      "[D loss: 0.989312] [G loss: 1.365881]\n",
      "[D loss: 0.838194] [G loss: 1.460720]\n",
      "[D loss: 0.719011] [G loss: 1.434274]\n",
      "[D loss: 0.753965] [G loss: 1.794833]\n",
      "[D loss: 1.048349] [G loss: 1.526662]\n",
      "[D loss: 0.833386] [G loss: 1.749709]\n",
      "[D loss: 0.670319] [G loss: 1.685156]\n",
      "[D loss: 0.859547] [G loss: 1.501535]\n",
      "[D loss: 0.961285] [G loss: 1.474588]\n",
      "[D loss: 0.977304] [G loss: 1.275840]\n",
      "[D loss: 1.032869] [G loss: 1.234828]\n",
      "[D loss: 0.690978] [G loss: 1.376724]\n",
      "[D loss: 0.777210] [G loss: 1.476284]\n",
      "[D loss: 1.196815] [G loss: 1.335832]\n",
      "[D loss: 0.862648] [G loss: 1.554393]\n",
      "[D loss: 0.984677] [G loss: 1.447570]\n",
      "[D loss: 0.700385] [G loss: 1.575332]\n",
      "[D loss: 0.852385] [G loss: 1.527707]\n",
      "[D loss: 0.833841] [G loss: 1.590564]\n",
      "[D loss: 0.763507] [G loss: 1.349617]\n",
      "[D loss: 0.877698] [G loss: 1.297829]\n",
      "[D loss: 0.963679] [G loss: 1.348552]\n",
      "[D loss: 0.637684] [G loss: 1.773576]\n",
      "[D loss: 1.203636] [G loss: 1.508909]\n",
      "[D loss: 0.675305] [G loss: 1.677897]\n",
      "[D loss: 0.900709] [G loss: 1.450748]\n",
      "[D loss: 1.111969] [G loss: 1.252484]\n",
      "[D loss: 0.910248] [G loss: 1.670800]\n",
      "[D loss: 0.866311] [G loss: 1.225111]\n",
      "[D loss: 0.933741] [G loss: 1.306384]\n",
      "[D loss: 0.900203] [G loss: 1.472234]\n",
      "[D loss: 0.842165] [G loss: 1.560151]\n",
      "[D loss: 0.801306] [G loss: 1.559769]\n",
      "[D loss: 0.727960] [G loss: 1.394827]\n",
      "[D loss: 0.917081] [G loss: 1.338161]\n",
      "[D loss: 0.801719] [G loss: 1.415014]\n",
      "[D loss: 0.637120] [G loss: 1.503462]\n",
      "[D loss: 0.800495] [G loss: 1.396937]\n",
      "[D loss: 0.779023] [G loss: 1.665587]\n",
      "[D loss: 0.806557] [G loss: 1.375583]\n",
      "[D loss: 0.894251] [G loss: 1.355448]\n",
      "[D loss: 0.933507] [G loss: 1.490328]\n",
      "[D loss: 0.688799] [G loss: 1.602121]\n",
      "[D loss: 0.895374] [G loss: 1.518774]\n",
      "[D loss: 0.822481] [G loss: 1.347359]\n",
      "[D loss: 0.887266] [G loss: 1.464384]\n",
      "[D loss: 0.829538] [G loss: 1.495925]\n",
      "[D loss: 0.879830] [G loss: 1.387025]\n",
      "[D loss: 1.020691] [G loss: 1.525015]\n",
      "[D loss: 0.835338] [G loss: 1.555831]\n",
      "[D loss: 0.786073] [G loss: 1.225712]\n",
      "[D loss: 0.865443] [G loss: 1.303005]\n",
      "[D loss: 0.882315] [G loss: 1.619590]\n",
      "[D loss: 0.958214] [G loss: 1.468364]\n",
      "[D loss: 0.785767] [G loss: 1.630075]\n",
      "[D loss: 0.774502] [G loss: 1.360777]\n",
      "[D loss: 1.035746] [G loss: 1.271824]\n",
      "[D loss: 0.725189] [G loss: 1.441267]\n",
      "[D loss: 0.762611] [G loss: 1.516218]\n",
      "[D loss: 0.776306] [G loss: 1.432128]\n",
      "[D loss: 0.768378] [G loss: 1.303783]\n",
      "[D loss: 0.578733] [G loss: 1.597859]\n",
      "[D loss: 1.260385] [G loss: 1.300323]\n",
      "[D loss: 0.864449] [G loss: 1.441456]\n",
      "[D loss: 0.957755] [G loss: 1.319666]\n",
      "[D loss: 1.030681] [G loss: 1.144848]\n",
      "[D loss: 0.805052] [G loss: 1.463747]\n",
      "[D loss: 1.084285] [G loss: 1.186336]\n",
      "[D loss: 0.871965] [G loss: 1.425081]\n",
      "[D loss: 0.805138] [G loss: 1.241260]\n",
      "[D loss: 0.864472] [G loss: 1.386629]\n",
      "[D loss: 0.827881] [G loss: 1.396984]\n",
      "[D loss: 0.869953] [G loss: 1.379619]\n",
      "[D loss: 0.765431] [G loss: 1.425100]\n",
      "[D loss: 1.060567] [G loss: 1.275378]\n",
      "[D loss: 0.749941] [G loss: 1.279478]\n",
      "[D loss: 1.012766] [G loss: 1.347762]\n",
      "[D loss: 0.708456] [G loss: 1.444168]\n",
      "[D loss: 1.065439] [G loss: 1.122365]\n",
      "[D loss: 0.926497] [G loss: 1.228169]\n",
      "[D loss: 0.982437] [G loss: 1.470233]\n",
      "[D loss: 1.057101] [G loss: 1.360175]\n",
      "[D loss: 0.906475] [G loss: 1.322196]\n",
      "[D loss: 0.941147] [G loss: 1.471677]\n",
      "[D loss: 0.672807] [G loss: 1.538356]\n",
      "[D loss: 0.953610] [G loss: 1.311288]\n",
      "[D loss: 0.901897] [G loss: 1.411206]\n",
      "[D loss: 0.900312] [G loss: 1.318827]\n",
      "[D loss: 0.913093] [G loss: 1.300675]\n",
      "[D loss: 0.919174] [G loss: 1.318423]\n",
      "[D loss: 0.840594] [G loss: 1.257536]\n",
      "[D loss: 0.737721] [G loss: 1.336956]\n",
      "[D loss: 0.876645] [G loss: 1.153291]\n",
      "[D loss: 0.787269] [G loss: 1.289375]\n",
      "[D loss: 1.020327] [G loss: 1.172225]\n",
      "[D loss: 0.838367] [G loss: 1.360782]\n",
      "[D loss: 0.754957] [G loss: 1.557021]\n",
      "[D loss: 0.872149] [G loss: 1.294057]\n",
      "[D loss: 0.834214] [G loss: 1.498943]\n",
      "[D loss: 0.683520] [G loss: 1.396909]\n",
      "[D loss: 0.852608] [G loss: 1.466185]\n",
      "[D loss: 0.771493] [G loss: 1.453148]\n",
      "[D loss: 0.980669] [G loss: 1.670040]\n",
      "[D loss: 0.848402] [G loss: 1.296909]\n",
      "[D loss: 0.746725] [G loss: 1.316440]\n",
      "[D loss: 0.807972] [G loss: 1.397662]\n",
      "[D loss: 0.874168] [G loss: 1.321838]\n",
      "[D loss: 0.864615] [G loss: 1.421662]\n",
      "[D loss: 0.751758] [G loss: 1.394895]\n",
      "[D loss: 0.637722] [G loss: 1.533794]\n",
      "[D loss: 0.709523] [G loss: 1.471911]\n",
      "[D loss: 0.737494] [G loss: 1.577145]\n",
      "[D loss: 0.882684] [G loss: 1.556687]\n",
      "[D loss: 0.847772] [G loss: 1.348875]\n",
      "[D loss: 0.791695] [G loss: 1.397459]\n",
      "[D loss: 0.860699] [G loss: 1.393251]\n",
      "[D loss: 1.002372] [G loss: 1.436119]\n",
      "[D loss: 0.837038] [G loss: 1.375142]\n",
      "[D loss: 0.862426] [G loss: 1.358213]\n",
      "[D loss: 1.051166] [G loss: 1.154495]\n",
      "[D loss: 0.766534] [G loss: 1.265301]\n",
      "[D loss: 0.819546] [G loss: 1.263896]\n",
      "[D loss: 0.675439] [G loss: 1.289712]\n",
      "[D loss: 0.644344] [G loss: 1.421854]\n",
      "[D loss: 1.026806] [G loss: 1.455865]\n",
      "[D loss: 0.841091] [G loss: 1.508214]\n",
      "[D loss: 1.034065] [G loss: 1.449759]\n",
      "[D loss: 0.723609] [G loss: 1.564733]\n",
      "[D loss: 0.839667] [G loss: 1.465329]\n",
      "[D loss: 1.002923] [G loss: 1.358062]\n",
      "[D loss: 0.898534] [G loss: 1.376417]\n",
      "[D loss: 0.803692] [G loss: 1.778496]\n",
      "[D loss: 0.764550] [G loss: 1.408491]\n",
      "[D loss: 0.798265] [G loss: 1.121958]\n",
      "[D loss: 0.714397] [G loss: 1.667994]\n",
      "[D loss: 0.834392] [G loss: 1.723756]\n",
      "[D loss: 0.999129] [G loss: 1.458063]\n",
      "[D loss: 0.594307] [G loss: 1.686483]\n",
      "[D loss: 0.690473] [G loss: 1.446562]\n",
      "[D loss: 1.004256] [G loss: 1.637735]\n",
      "[D loss: 0.910026] [G loss: 1.514328]\n",
      "[D loss: 0.805686] [G loss: 1.385690]\n",
      "[D loss: 0.847815] [G loss: 1.431084]\n",
      "[D loss: 0.851029] [G loss: 1.481129]\n",
      "[D loss: 1.066476] [G loss: 1.420710]\n",
      "[D loss: 1.017133] [G loss: 1.403973]\n",
      "[D loss: 0.766413] [G loss: 1.360033]\n",
      "[D loss: 0.691992] [G loss: 1.682849]\n",
      "[D loss: 1.032838] [G loss: 1.650568]\n",
      "[D loss: 0.757593] [G loss: 1.536285]\n",
      "[D loss: 0.819445] [G loss: 1.488924]\n",
      "[D loss: 0.851256] [G loss: 1.346428]\n",
      "[D loss: 0.741810] [G loss: 1.371793]\n",
      "[D loss: 0.717638] [G loss: 1.216775]\n",
      "[D loss: 0.672989] [G loss: 1.333275]\n",
      "[D loss: 0.983264] [G loss: 1.482667]\n",
      "[D loss: 0.768712] [G loss: 1.700235]\n",
      "[D loss: 0.755968] [G loss: 1.492780]\n",
      "[D loss: 0.730491] [G loss: 1.538059]\n",
      "[D loss: 0.870823] [G loss: 1.575799]\n",
      "[D loss: 0.976630] [G loss: 1.563221]\n",
      "[D loss: 0.910198] [G loss: 1.328836]\n",
      "[D loss: 0.707371] [G loss: 1.554747]\n",
      "[D loss: 0.876828] [G loss: 1.434199]\n",
      "[D loss: 1.101923] [G loss: 1.299187]\n",
      "[D loss: 0.651167] [G loss: 1.543563]\n",
      "[D loss: 0.678959] [G loss: 1.687421]\n",
      "[D loss: 0.828657] [G loss: 1.490754]\n",
      "[D loss: 1.082233] [G loss: 1.409821]\n",
      "[D loss: 0.814584] [G loss: 1.498703]\n",
      "[D loss: 0.783726] [G loss: 1.297550]\n",
      "[D loss: 1.147057] [G loss: 1.191417]\n",
      "[D loss: 1.017479] [G loss: 1.385864]\n",
      "[D loss: 0.863392] [G loss: 1.768104]\n",
      "[D loss: 0.596054] [G loss: 1.451988]\n",
      "[D loss: 1.015597] [G loss: 1.646924]\n",
      "[D loss: 0.785467] [G loss: 1.423414]\n",
      "[D loss: 0.673952] [G loss: 1.515478]\n",
      "[D loss: 0.869980] [G loss: 1.466976]\n",
      "[D loss: 0.636711] [G loss: 1.465286]\n",
      "[D loss: 0.955846] [G loss: 1.544690]\n",
      "[D loss: 0.616367] [G loss: 1.537279]\n",
      "[D loss: 0.773598] [G loss: 1.686483]\n",
      "[D loss: 0.619356] [G loss: 1.801047]\n",
      "[D loss: 0.520650] [G loss: 1.528973]\n",
      "[D loss: 0.769131] [G loss: 1.463398]\n",
      "[D loss: 0.661652] [G loss: 1.561195]\n",
      "[D loss: 0.759715] [G loss: 1.469343]\n",
      "[D loss: 0.797533] [G loss: 1.565742]\n",
      "[D loss: 0.939420] [G loss: 1.404571]\n",
      "[D loss: 0.880673] [G loss: 1.613360]\n",
      "[D loss: 0.695247] [G loss: 1.412157]\n",
      "[D loss: 0.892681] [G loss: 1.362233]\n",
      "[D loss: 0.706837] [G loss: 1.373684]\n",
      "[D loss: 0.754714] [G loss: 1.543160]\n",
      "[D loss: 0.910949] [G loss: 1.299763]\n",
      "[D loss: 0.780188] [G loss: 1.409641]\n",
      "[D loss: 0.810320] [G loss: 1.439735]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.771711] [G loss: 1.340636]\n",
      "[D loss: 0.777619] [G loss: 1.327442]\n",
      "[D loss: 0.924143] [G loss: 1.231874]\n",
      "[D loss: 0.795386] [G loss: 1.374972]\n",
      "[D loss: 0.832672] [G loss: 1.545330]\n",
      "[D loss: 0.823134] [G loss: 1.436036]\n",
      "[D loss: 0.841831] [G loss: 1.519189]\n",
      "[D loss: 0.799201] [G loss: 1.561247]\n",
      "[D loss: 0.905223] [G loss: 1.335343]\n",
      "[D loss: 0.663951] [G loss: 1.516093]\n",
      "[D loss: 0.826762] [G loss: 1.400099]\n",
      "[D loss: 0.811787] [G loss: 1.628708]\n",
      "[D loss: 0.944203] [G loss: 1.399525]\n",
      "[D loss: 0.791816] [G loss: 1.430792]\n",
      "[D loss: 0.900488] [G loss: 1.289584]\n",
      "[D loss: 0.912268] [G loss: 1.296213]\n",
      "[D loss: 0.750917] [G loss: 1.533450]\n",
      "[D loss: 0.582359] [G loss: 1.382835]\n",
      "[D loss: 0.683727] [G loss: 1.616234]\n",
      "[D loss: 0.820354] [G loss: 1.607538]\n",
      "[D loss: 0.952221] [G loss: 1.716235]\n",
      "[D loss: 0.676220] [G loss: 1.596439]\n",
      "[D loss: 0.902833] [G loss: 1.454751]\n",
      "[D loss: 0.932752] [G loss: 1.509927]\n",
      "[D loss: 0.817461] [G loss: 1.468527]\n",
      "[D loss: 0.678203] [G loss: 1.617003]\n",
      "[D loss: 0.941201] [G loss: 1.568998]\n",
      "[D loss: 0.953968] [G loss: 1.287024]\n",
      "[D loss: 0.823100] [G loss: 1.544224]\n",
      "[D loss: 1.047358] [G loss: 1.175950]\n",
      "[D loss: 0.968248] [G loss: 1.308050]\n",
      "[D loss: 0.836222] [G loss: 1.172723]\n",
      "[D loss: 0.885347] [G loss: 1.577355]\n",
      "[D loss: 0.949678] [G loss: 1.634690]\n",
      "[D loss: 0.821302] [G loss: 1.583350]\n",
      "[D loss: 0.873236] [G loss: 1.538337]\n",
      "[D loss: 0.828950] [G loss: 1.456766]\n",
      "[D loss: 0.987613] [G loss: 1.115222]\n",
      "[D loss: 0.867578] [G loss: 1.256042]\n",
      "[D loss: 0.800071] [G loss: 1.413058]\n",
      "[D loss: 0.649427] [G loss: 1.692173]\n",
      "[D loss: 0.414779] [G loss: 1.445743]\n",
      "[D loss: 0.946637] [G loss: 1.566861]\n",
      "[D loss: 1.105058] [G loss: 1.637537]\n",
      "[D loss: 0.906568] [G loss: 1.540363]\n",
      "[D loss: 0.586000] [G loss: 1.545517]\n",
      "[D loss: 0.515209] [G loss: 1.581199]\n",
      "[D loss: 0.792503] [G loss: 1.620781]\n",
      "[D loss: 0.916900] [G loss: 1.560196]\n",
      "[D loss: 0.787461] [G loss: 1.864187]\n",
      "[D loss: 1.078764] [G loss: 1.613978]\n",
      "[D loss: 0.983994] [G loss: 1.467136]\n",
      "[D loss: 1.121975] [G loss: 1.605707]\n",
      "[D loss: 0.982730] [G loss: 1.340445]\n",
      "[D loss: 0.825051] [G loss: 1.539140]\n",
      "[D loss: 0.816820] [G loss: 1.493264]\n",
      "[D loss: 0.700237] [G loss: 1.555743]\n",
      "[D loss: 0.766596] [G loss: 1.276806]\n",
      "[D loss: 0.674888] [G loss: 1.587255]\n",
      "[D loss: 0.986104] [G loss: 1.610342]\n",
      "[D loss: 0.837816] [G loss: 1.630398]\n",
      "[D loss: 0.750127] [G loss: 1.616291]\n",
      "[D loss: 0.890266] [G loss: 1.290782]\n",
      "[D loss: 0.734042] [G loss: 1.310269]\n",
      "[D loss: 0.796928] [G loss: 1.631350]\n",
      "[D loss: 1.138113] [G loss: 1.721702]\n",
      "[D loss: 0.885768] [G loss: 1.533566]\n",
      "[D loss: 0.985075] [G loss: 1.225988]\n",
      "[D loss: 0.941760] [G loss: 1.466933]\n",
      "[D loss: 0.853693] [G loss: 1.488800]\n",
      "[D loss: 0.867614] [G loss: 1.404834]\n",
      "[D loss: 0.910408] [G loss: 1.405927]\n",
      "[D loss: 0.581425] [G loss: 1.604549]\n",
      "[D loss: 0.831979] [G loss: 1.352958]\n",
      "[D loss: 0.905024] [G loss: 1.432143]\n",
      "[D loss: 0.639042] [G loss: 1.677184]\n",
      "[D loss: 0.943050] [G loss: 1.446272]\n",
      "[D loss: 0.810696] [G loss: 1.836917]\n",
      "[D loss: 0.883826] [G loss: 1.294296]\n",
      "[D loss: 0.836290] [G loss: 1.338921]\n",
      "[D loss: 0.798495] [G loss: 1.226343]\n",
      "[D loss: 0.713806] [G loss: 1.686563]\n",
      "[D loss: 0.680906] [G loss: 1.430438]\n",
      "[D loss: 0.918582] [G loss: 1.642064]\n",
      "[D loss: 0.689011] [G loss: 1.610728]\n",
      "[D loss: 0.632399] [G loss: 1.697762]\n",
      "[D loss: 0.926620] [G loss: 1.665689]\n",
      "[D loss: 0.654612] [G loss: 1.522992]\n",
      "[D loss: 1.029012] [G loss: 1.389220]\n",
      "[D loss: 0.811921] [G loss: 1.490749]\n",
      "[D loss: 0.825200] [G loss: 1.492665]\n",
      "[D loss: 1.037961] [G loss: 1.106566]\n",
      "[D loss: 0.782574] [G loss: 1.947040]\n",
      "[D loss: 0.786323] [G loss: 1.724952]\n",
      "[D loss: 0.781846] [G loss: 1.680190]\n",
      "[D loss: 1.105345] [G loss: 1.482673]\n",
      "[D loss: 0.729011] [G loss: 1.499577]\n",
      "[D loss: 0.992715] [G loss: 1.494725]\n",
      "[D loss: 0.828339] [G loss: 1.479805]\n",
      "[D loss: 0.908112] [G loss: 1.589685]\n",
      "[D loss: 0.827513] [G loss: 1.302462]\n",
      "[D loss: 1.072550] [G loss: 1.543704]\n",
      "[D loss: 0.593477] [G loss: 1.621916]\n",
      "[D loss: 0.926907] [G loss: 1.408996]\n",
      "[D loss: 0.862643] [G loss: 1.577480]\n",
      "[D loss: 0.904166] [G loss: 1.449035]\n",
      "[D loss: 0.684643] [G loss: 1.544679]\n",
      "[D loss: 0.996089] [G loss: 1.309697]\n",
      "[D loss: 0.802315] [G loss: 1.475969]\n",
      "[D loss: 0.865007] [G loss: 1.458847]\n",
      "[D loss: 0.829142] [G loss: 1.265036]\n",
      "[D loss: 0.890743] [G loss: 1.412497]\n",
      "[D loss: 0.876595] [G loss: 1.244583]\n",
      "[D loss: 0.960760] [G loss: 1.435284]\n",
      "[D loss: 1.024038] [G loss: 1.287251]\n",
      "[D loss: 0.869494] [G loss: 1.667861]\n",
      "[D loss: 0.948782] [G loss: 1.667168]\n",
      "[D loss: 0.868026] [G loss: 1.512243]\n",
      "[D loss: 0.714314] [G loss: 1.396884]\n",
      "[D loss: 0.623124] [G loss: 1.525918]\n",
      "[D loss: 0.752371] [G loss: 1.649590]\n",
      "[D loss: 0.874049] [G loss: 1.367513]\n",
      "[D loss: 0.804424] [G loss: 1.785932]\n",
      "[D loss: 0.691658] [G loss: 1.636453]\n",
      "[D loss: 0.549065] [G loss: 1.787081]\n",
      "[D loss: 0.927060] [G loss: 1.622439]\n",
      "[D loss: 0.922006] [G loss: 1.451308]\n",
      "[D loss: 0.860940] [G loss: 1.367138]\n",
      "[D loss: 0.705790] [G loss: 1.469610]\n",
      "[D loss: 0.746264] [G loss: 1.910735]\n",
      "[D loss: 1.040865] [G loss: 1.585938]\n",
      "[D loss: 0.863823] [G loss: 1.565662]\n",
      "[D loss: 0.919698] [G loss: 1.547884]\n",
      "[D loss: 0.926787] [G loss: 1.421148]\n",
      "[D loss: 0.661538] [G loss: 1.612174]\n",
      "[D loss: 0.983470] [G loss: 1.313402]\n",
      "[D loss: 0.705800] [G loss: 1.861171]\n",
      "[D loss: 0.904029] [G loss: 1.553036]\n",
      "[D loss: 0.701322] [G loss: 1.746824]\n",
      "[D loss: 0.841475] [G loss: 1.547469]\n",
      "[D loss: 0.896634] [G loss: 1.570570]\n",
      "[D loss: 0.947628] [G loss: 1.462752]\n",
      "[D loss: 0.988994] [G loss: 1.433910]\n",
      "[D loss: 0.875476] [G loss: 1.319479]\n",
      "[D loss: 0.784133] [G loss: 1.498822]\n",
      "[D loss: 0.810623] [G loss: 1.551761]\n",
      "[D loss: 1.005681] [G loss: 1.444583]\n",
      "[D loss: 0.887348] [G loss: 1.337692]\n",
      "[D loss: 0.837903] [G loss: 1.732028]\n",
      "[D loss: 1.167406] [G loss: 1.281690]\n",
      "[D loss: 0.734514] [G loss: 1.458849]\n",
      "[D loss: 0.791808] [G loss: 1.376637]\n",
      "[D loss: 0.857778] [G loss: 1.377520]\n",
      "[D loss: 0.819288] [G loss: 1.401756]\n",
      "[D loss: 0.913183] [G loss: 1.476092]\n",
      "[D loss: 1.061786] [G loss: 1.346842]\n",
      "[D loss: 0.750174] [G loss: 1.359128]\n",
      "[D loss: 0.961671] [G loss: 1.532587]\n",
      "[D loss: 0.886258] [G loss: 1.501209]\n",
      "[D loss: 0.964474] [G loss: 1.557505]\n",
      "[D loss: 0.791889] [G loss: 1.399113]\n",
      "[D loss: 0.765102] [G loss: 1.534282]\n",
      "[D loss: 0.982753] [G loss: 1.304776]\n",
      "[D loss: 0.839849] [G loss: 1.426536]\n",
      "[D loss: 0.664668] [G loss: 1.401456]\n",
      "[D loss: 0.664700] [G loss: 1.583652]\n",
      "[D loss: 1.032690] [G loss: 1.309015]\n",
      "[D loss: 0.679629] [G loss: 1.628509]\n",
      "[D loss: 0.700173] [G loss: 1.381827]\n",
      "[D loss: 0.929450] [G loss: 1.351577]\n",
      "[D loss: 1.057199] [G loss: 1.208802]\n",
      "[D loss: 0.652604] [G loss: 1.496655]\n",
      "[D loss: 0.815343] [G loss: 1.565771]\n",
      "[D loss: 0.838611] [G loss: 1.425686]\n",
      "[D loss: 0.778050] [G loss: 1.519074]\n",
      "[D loss: 0.870695] [G loss: 1.522481]\n",
      "[D loss: 1.055749] [G loss: 1.501168]\n",
      "[D loss: 0.880912] [G loss: 1.367567]\n",
      "[D loss: 0.963976] [G loss: 1.408753]\n",
      "[D loss: 0.730346] [G loss: 1.621414]\n",
      "[D loss: 0.561686] [G loss: 1.338664]\n",
      "[D loss: 0.848300] [G loss: 1.489951]\n",
      "[D loss: 0.851760] [G loss: 1.690695]\n",
      "[D loss: 0.708381] [G loss: 1.631381]\n",
      "[D loss: 0.668566] [G loss: 1.621576]\n",
      "[D loss: 0.617306] [G loss: 1.567086]\n",
      "[D loss: 0.695803] [G loss: 1.782242]\n",
      "[D loss: 0.734799] [G loss: 1.679117]\n",
      "[D loss: 0.892122] [G loss: 1.557199]\n",
      "[D loss: 0.839890] [G loss: 1.481944]\n",
      "[D loss: 1.034996] [G loss: 1.506402]\n",
      "[D loss: 0.761360] [G loss: 1.552858]\n",
      "[D loss: 0.541613] [G loss: 1.734874]\n",
      "[D loss: 0.836880] [G loss: 1.569281]\n",
      "[D loss: 0.819644] [G loss: 1.474614]\n",
      "[D loss: 0.698872] [G loss: 1.481329]\n",
      "[D loss: 0.811619] [G loss: 1.560366]\n",
      "[D loss: 0.974972] [G loss: 1.346531]\n",
      "[D loss: 0.991592] [G loss: 1.502346]\n",
      "[D loss: 0.831839] [G loss: 1.521086]\n",
      "[D loss: 0.825146] [G loss: 1.491233]\n",
      "[D loss: 0.982457] [G loss: 1.267526]\n",
      "[D loss: 0.763258] [G loss: 1.399370]\n",
      "[D loss: 0.751329] [G loss: 1.735248]\n",
      "[D loss: 1.136983] [G loss: 1.618769]\n",
      "[D loss: 0.882929] [G loss: 1.453941]\n",
      "[D loss: 1.011043] [G loss: 1.336522]\n",
      "[D loss: 0.674166] [G loss: 1.170376]\n",
      "[D loss: 0.940053] [G loss: 1.520291]\n",
      "[D loss: 0.834304] [G loss: 1.577995]\n",
      "[D loss: 0.933447] [G loss: 1.585036]\n",
      "[D loss: 0.912671] [G loss: 1.383580]\n",
      "[D loss: 0.874433] [G loss: 1.383688]\n",
      "[D loss: 0.961126] [G loss: 1.237437]\n",
      "[D loss: 1.104918] [G loss: 1.571611]\n",
      "[D loss: 1.049402] [G loss: 1.295393]\n",
      "[D loss: 0.699896] [G loss: 1.365019]\n",
      "[D loss: 0.728796] [G loss: 1.464267]\n",
      "[D loss: 0.722983] [G loss: 1.331129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.835567] [G loss: 1.491762]\n",
      "[D loss: 0.710423] [G loss: 1.474416]\n",
      "[D loss: 0.859490] [G loss: 1.440931]\n",
      "[D loss: 0.733467] [G loss: 1.686990]\n",
      "[D loss: 0.800659] [G loss: 1.404540]\n",
      "[D loss: 0.786956] [G loss: 1.486799]\n",
      "[D loss: 0.861719] [G loss: 1.484638]\n",
      "[D loss: 0.869614] [G loss: 1.254303]\n",
      "[D loss: 0.759591] [G loss: 1.565895]\n",
      "[D loss: 0.885554] [G loss: 1.552887]\n",
      "[D loss: 0.881301] [G loss: 1.557496]\n",
      "[D loss: 0.832510] [G loss: 1.434440]\n",
      "[D loss: 0.799093] [G loss: 1.401539]\n",
      "[D loss: 0.953288] [G loss: 1.358739]\n",
      "[D loss: 0.760469] [G loss: 1.576672]\n",
      "[D loss: 0.911509] [G loss: 1.537066]\n",
      "[D loss: 0.917938] [G loss: 1.294741]\n",
      "[D loss: 0.941060] [G loss: 1.522891]\n",
      "[D loss: 0.944038] [G loss: 1.389003]\n",
      "[D loss: 0.711767] [G loss: 1.395560]\n",
      "[D loss: 0.722614] [G loss: 1.323039]\n",
      "[D loss: 0.767833] [G loss: 1.345023]\n",
      "[D loss: 0.715866] [G loss: 1.515539]\n",
      "[D loss: 1.082052] [G loss: 1.366011]\n",
      "[D loss: 0.779443] [G loss: 1.762153]\n",
      "[D loss: 1.099327] [G loss: 1.384026]\n",
      "[D loss: 0.947544] [G loss: 1.238347]\n",
      "[D loss: 0.867618] [G loss: 1.283296]\n",
      "[D loss: 1.031504] [G loss: 1.543540]\n",
      "[D loss: 0.715594] [G loss: 1.628458]\n",
      "[D loss: 0.630288] [G loss: 1.335541]\n",
      "[D loss: 0.931904] [G loss: 1.471423]\n",
      "[D loss: 0.662349] [G loss: 1.416101]\n",
      "[D loss: 0.681998] [G loss: 1.500846]\n",
      "[D loss: 0.844180] [G loss: 1.498931]\n",
      "[D loss: 0.821907] [G loss: 1.291337]\n",
      "[D loss: 0.879938] [G loss: 1.468582]\n",
      "[D loss: 0.778814] [G loss: 1.336989]\n",
      "[D loss: 0.655008] [G loss: 1.455217]\n",
      "[D loss: 0.849929] [G loss: 1.373337]\n",
      "[D loss: 0.935750] [G loss: 1.533041]\n",
      "[D loss: 0.857306] [G loss: 1.415004]\n",
      "[D loss: 0.719738] [G loss: 1.525004]\n",
      "[D loss: 0.661278] [G loss: 1.296333]\n",
      "[D loss: 0.933471] [G loss: 1.256621]\n",
      "[D loss: 0.829399] [G loss: 1.482519]\n",
      "[D loss: 0.823078] [G loss: 1.702635]\n",
      "[D loss: 0.795965] [G loss: 1.449877]\n",
      "[D loss: 0.741259] [G loss: 1.418094]\n",
      "[D loss: 0.966839] [G loss: 1.738424]\n",
      "[D loss: 0.789070] [G loss: 1.556529]\n",
      "[D loss: 0.906469] [G loss: 1.307449]\n",
      "[D loss: 0.841065] [G loss: 1.388593]\n",
      "[D loss: 0.743056] [G loss: 1.322469]\n",
      "[D loss: 0.886706] [G loss: 1.840141]\n",
      "[D loss: 0.773310] [G loss: 1.568107]\n",
      "[D loss: 0.922248] [G loss: 1.616608]\n",
      "[D loss: 0.750672] [G loss: 1.498948]\n",
      "[D loss: 0.780774] [G loss: 1.650031]\n",
      "[D loss: 0.683372] [G loss: 1.444398]\n",
      "[D loss: 0.842459] [G loss: 1.820593]\n",
      "[D loss: 1.113366] [G loss: 1.392894]\n",
      "[D loss: 1.009932] [G loss: 1.414624]\n",
      "[D loss: 0.937111] [G loss: 1.343372]\n",
      "[D loss: 0.687358] [G loss: 1.612713]\n",
      "[D loss: 0.890864] [G loss: 1.626223]\n",
      "[D loss: 0.909791] [G loss: 1.455888]\n",
      "[D loss: 0.891759] [G loss: 1.336076]\n",
      "[D loss: 0.760823] [G loss: 1.354113]\n",
      "[D loss: 0.997228] [G loss: 1.250463]\n",
      "[D loss: 0.795139] [G loss: 1.437624]\n",
      "[D loss: 0.819783] [G loss: 1.355738]\n",
      "[D loss: 0.740325] [G loss: 1.524035]\n",
      "[D loss: 0.879585] [G loss: 1.393038]\n",
      "[D loss: 0.758590] [G loss: 1.488852]\n",
      "[D loss: 1.052511] [G loss: 1.577317]\n",
      "[D loss: 0.922451] [G loss: 1.543205]\n",
      "[D loss: 0.715999] [G loss: 1.462587]\n",
      "[D loss: 0.924187] [G loss: 1.405115]\n",
      "[D loss: 0.974030] [G loss: 1.468357]\n",
      "[D loss: 0.739118] [G loss: 1.338409]\n",
      "[D loss: 0.896818] [G loss: 1.308994]\n",
      "[D loss: 0.763944] [G loss: 1.573697]\n",
      "[D loss: 0.839523] [G loss: 1.586361]\n",
      "[D loss: 0.874682] [G loss: 1.518199]\n",
      "[D loss: 0.828593] [G loss: 1.778015]\n",
      "[D loss: 0.820910] [G loss: 1.561522]\n",
      "[D loss: 0.807689] [G loss: 1.474219]\n",
      "[D loss: 0.899973] [G loss: 1.368842]\n",
      "[D loss: 0.828526] [G loss: 1.499898]\n",
      "[D loss: 0.622045] [G loss: 1.508207]\n",
      "[D loss: 0.937920] [G loss: 1.438851]\n",
      "[D loss: 0.951898] [G loss: 1.415728]\n",
      "[D loss: 0.617063] [G loss: 1.383719]\n",
      "[D loss: 0.743746] [G loss: 1.395541]\n",
      "[D loss: 0.726225] [G loss: 1.449505]\n",
      "[D loss: 0.901150] [G loss: 1.318216]\n",
      "[D loss: 0.971922] [G loss: 1.307379]\n",
      "[D loss: 0.903604] [G loss: 1.364073]\n",
      "[D loss: 1.000623] [G loss: 1.427031]\n",
      "[D loss: 0.787826] [G loss: 1.517433]\n",
      "[D loss: 1.074265] [G loss: 1.318823]\n",
      "[D loss: 0.810023] [G loss: 1.434608]\n",
      "[D loss: 0.884197] [G loss: 1.325803]\n",
      "[D loss: 0.946772] [G loss: 1.631549]\n",
      "[D loss: 1.026487] [G loss: 1.452439]\n",
      "[D loss: 0.767273] [G loss: 1.489863]\n",
      "[D loss: 0.862062] [G loss: 1.420709]\n",
      "[D loss: 0.739501] [G loss: 1.456585]\n",
      "[D loss: 1.019980] [G loss: 1.329786]\n",
      "[D loss: 0.791283] [G loss: 1.568251]\n",
      "[D loss: 0.806661] [G loss: 1.555461]\n",
      "[D loss: 0.803314] [G loss: 1.478801]\n",
      "[D loss: 0.903329] [G loss: 1.263657]\n",
      "[D loss: 0.784020] [G loss: 1.528145]\n",
      "[D loss: 0.880557] [G loss: 1.398745]\n",
      "[D loss: 0.968557] [G loss: 1.401296]\n",
      "[D loss: 0.897465] [G loss: 1.453051]\n",
      "[D loss: 0.930857] [G loss: 1.489391]\n",
      "[D loss: 1.034282] [G loss: 1.594699]\n",
      "[D loss: 0.881407] [G loss: 1.486399]\n",
      "[D loss: 0.635860] [G loss: 1.487753]\n",
      "[D loss: 0.932912] [G loss: 1.327967]\n",
      "[D loss: 0.858896] [G loss: 1.263034]\n",
      "[D loss: 0.669080] [G loss: 1.501074]\n",
      "[D loss: 0.995295] [G loss: 1.610922]\n",
      "[D loss: 0.820246] [G loss: 1.497964]\n",
      "[D loss: 0.834648] [G loss: 1.363708]\n",
      "[D loss: 0.760636] [G loss: 1.522121]\n",
      "[D loss: 1.104789] [G loss: 1.257451]\n",
      "[D loss: 0.843468] [G loss: 1.477972]\n",
      "[D loss: 0.848387] [G loss: 1.331673]\n",
      "[D loss: 0.878651] [G loss: 1.438286]\n",
      "[D loss: 0.926371] [G loss: 1.265586]\n",
      "[D loss: 1.092460] [G loss: 1.296367]\n",
      "[D loss: 0.840551] [G loss: 1.404329]\n",
      "[D loss: 0.717336] [G loss: 1.286705]\n",
      "[D loss: 0.905290] [G loss: 1.240567]\n",
      "[D loss: 0.755406] [G loss: 1.499387]\n",
      "[D loss: 1.038952] [G loss: 1.447573]\n",
      "[D loss: 0.737858] [G loss: 1.572176]\n",
      "[D loss: 0.757707] [G loss: 1.472331]\n",
      "[D loss: 0.778155] [G loss: 1.263464]\n",
      "[D loss: 1.012186] [G loss: 1.202135]\n",
      "[D loss: 0.833079] [G loss: 1.322248]\n",
      "[D loss: 0.932756] [G loss: 1.270934]\n",
      "[D loss: 0.768580] [G loss: 1.536141]\n",
      "[D loss: 1.020195] [G loss: 1.666643]\n",
      "[D loss: 0.967056] [G loss: 1.288031]\n",
      "[D loss: 0.968704] [G loss: 1.276598]\n",
      "[D loss: 0.646221] [G loss: 1.331434]\n",
      "[D loss: 0.778170] [G loss: 1.457185]\n",
      "[D loss: 0.950494] [G loss: 1.367798]\n",
      "[D loss: 0.795857] [G loss: 1.449504]\n",
      "[D loss: 0.790585] [G loss: 1.398543]\n",
      "[D loss: 0.846423] [G loss: 1.518463]\n",
      "[D loss: 0.859904] [G loss: 1.294682]\n",
      "[D loss: 0.810329] [G loss: 1.539351]\n",
      "[D loss: 0.923598] [G loss: 1.467769]\n",
      "[D loss: 0.734111] [G loss: 1.584586]\n",
      "[D loss: 0.610521] [G loss: 1.441744]\n",
      "[D loss: 0.808468] [G loss: 1.398845]\n",
      "[D loss: 0.863944] [G loss: 1.511469]\n",
      "[D loss: 0.695297] [G loss: 1.755427]\n",
      "[D loss: 0.803138] [G loss: 1.565417]\n",
      "[D loss: 0.768706] [G loss: 1.631364]\n",
      "[D loss: 0.715212] [G loss: 1.424446]\n",
      "[D loss: 0.593956] [G loss: 1.408010]\n",
      "[D loss: 0.864290] [G loss: 1.660025]\n",
      "[D loss: 0.992644] [G loss: 1.523493]\n",
      "[D loss: 0.778827] [G loss: 1.518467]\n",
      "[D loss: 0.881021] [G loss: 1.579503]\n",
      "[D loss: 0.786886] [G loss: 1.539479]\n",
      "[D loss: 0.627460] [G loss: 1.432524]\n",
      "[D loss: 0.704821] [G loss: 1.718190]\n",
      "[D loss: 0.993856] [G loss: 1.317256]\n",
      "[D loss: 0.771077] [G loss: 1.491569]\n",
      "[D loss: 0.814680] [G loss: 1.566557]\n",
      "[D loss: 1.380876] [G loss: 1.368474]\n",
      "[D loss: 0.745631] [G loss: 1.481416]\n",
      "[D loss: 0.846123] [G loss: 1.315999]\n",
      "[D loss: 0.886401] [G loss: 1.403985]\n",
      "[D loss: 0.788119] [G loss: 1.471268]\n",
      "[D loss: 0.754796] [G loss: 1.396262]\n",
      "[D loss: 0.894009] [G loss: 1.368257]\n",
      "[D loss: 0.960495] [G loss: 1.509865]\n",
      "[D loss: 0.806854] [G loss: 1.515734]\n",
      "[D loss: 0.807075] [G loss: 1.478359]\n",
      "[D loss: 0.584493] [G loss: 1.560340]\n",
      "[D loss: 0.695515] [G loss: 1.439465]\n",
      "[D loss: 0.782166] [G loss: 1.398098]\n",
      "[D loss: 1.079599] [G loss: 1.188121]\n",
      "[D loss: 0.970271] [G loss: 1.420794]\n",
      "[D loss: 0.974045] [G loss: 1.652868]\n",
      "[D loss: 0.913545] [G loss: 1.487555]\n",
      "[D loss: 0.942818] [G loss: 1.370896]\n",
      "[D loss: 0.885584] [G loss: 1.283323]\n",
      "[D loss: 0.826073] [G loss: 1.416329]\n",
      "[D loss: 0.888165] [G loss: 1.812041]\n",
      "[D loss: 0.815593] [G loss: 1.632732]\n",
      "[D loss: 0.905829] [G loss: 1.643803]\n",
      "[D loss: 1.001837] [G loss: 1.268389]\n",
      "[D loss: 0.781387] [G loss: 1.403651]\n",
      "[D loss: 0.877553] [G loss: 1.489452]\n",
      "[D loss: 0.735191] [G loss: 1.390128]\n",
      "[D loss: 0.783190] [G loss: 1.388623]\n",
      "[D loss: 0.724918] [G loss: 1.365519]\n",
      "[D loss: 0.817435] [G loss: 1.487754]\n",
      "[D loss: 0.855097] [G loss: 1.528616]\n",
      "[D loss: 0.832857] [G loss: 1.412078]\n",
      "[D loss: 0.718415] [G loss: 1.392668]\n",
      "[D loss: 0.610187] [G loss: 1.577759]\n",
      "[D loss: 0.992319] [G loss: 1.334372]\n",
      "[D loss: 1.031975] [G loss: 1.357256]\n",
      "[D loss: 0.751091] [G loss: 1.585654]\n",
      "[D loss: 0.778753] [G loss: 1.463813]\n",
      "[D loss: 0.858954] [G loss: 1.238917]\n",
      "[D loss: 0.803479] [G loss: 1.274088]\n",
      "[D loss: 0.755044] [G loss: 1.448721]\n",
      "[D loss: 0.677811] [G loss: 1.457496]\n",
      "[D loss: 0.502647] [G loss: 1.868337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.771526] [G loss: 1.587270]\n",
      "[D loss: 0.862972] [G loss: 1.304647]\n",
      "[D loss: 0.575135] [G loss: 1.474216]\n",
      "[D loss: 0.834411] [G loss: 1.621667]\n",
      "[D loss: 0.795094] [G loss: 1.508081]\n",
      "[D loss: 0.911180] [G loss: 1.494024]\n",
      "[D loss: 0.971600] [G loss: 1.602675]\n",
      "[D loss: 0.590591] [G loss: 1.639457]\n",
      "[D loss: 0.627866] [G loss: 1.586060]\n",
      "[D loss: 0.787428] [G loss: 1.336246]\n",
      "[D loss: 0.889653] [G loss: 1.641486]\n",
      "[D loss: 0.978594] [G loss: 1.443470]\n",
      "[D loss: 0.921113] [G loss: 1.342308]\n",
      "[D loss: 0.857091] [G loss: 1.695997]\n",
      "[D loss: 0.706295] [G loss: 1.399382]\n",
      "[D loss: 0.768333] [G loss: 1.715453]\n",
      "[D loss: 0.651460] [G loss: 1.584157]\n",
      "[D loss: 0.767755] [G loss: 1.589681]\n",
      "[D loss: 0.853848] [G loss: 1.406541]\n",
      "[D loss: 0.972119] [G loss: 1.605254]\n",
      "[D loss: 0.794948] [G loss: 1.574669]\n",
      "[D loss: 0.743034] [G loss: 1.594256]\n",
      "[D loss: 0.580071] [G loss: 1.479069]\n",
      "[D loss: 0.889523] [G loss: 1.407484]\n",
      "[D loss: 0.831766] [G loss: 1.447391]\n",
      "[D loss: 0.894206] [G loss: 1.508281]\n",
      "[D loss: 0.697070] [G loss: 1.523369]\n",
      "[D loss: 0.665553] [G loss: 1.588280]\n",
      "[D loss: 1.028388] [G loss: 1.359163]\n",
      "[D loss: 0.793428] [G loss: 1.438214]\n",
      "[D loss: 0.985754] [G loss: 1.343122]\n",
      "[D loss: 0.698433] [G loss: 1.294963]\n",
      "[D loss: 0.786591] [G loss: 1.438035]\n",
      "[D loss: 0.823852] [G loss: 1.323041]\n",
      "[D loss: 0.854409] [G loss: 1.600473]\n",
      "[D loss: 0.719438] [G loss: 1.532917]\n",
      "[D loss: 0.825501] [G loss: 1.458996]\n",
      "[D loss: 1.089674] [G loss: 1.318702]\n",
      "[D loss: 0.937128] [G loss: 1.215819]\n",
      "[D loss: 0.786045] [G loss: 1.562089]\n",
      "[D loss: 0.806788] [G loss: 1.623459]\n",
      "[D loss: 0.701386] [G loss: 1.592375]\n",
      "[D loss: 0.974669] [G loss: 1.436318]\n",
      "[D loss: 0.861328] [G loss: 1.543595]\n",
      "[D loss: 0.691670] [G loss: 1.575260]\n",
      "[D loss: 0.670737] [G loss: 1.229000]\n",
      "[D loss: 0.753027] [G loss: 1.651986]\n",
      "[D loss: 0.855951] [G loss: 1.585680]\n",
      "[D loss: 0.938498] [G loss: 1.492327]\n",
      "[D loss: 0.754752] [G loss: 1.496251]\n",
      "[D loss: 0.884716] [G loss: 1.391145]\n",
      "[D loss: 0.633580] [G loss: 1.309265]\n",
      "[D loss: 0.962483] [G loss: 1.351117]\n",
      "[D loss: 0.908439] [G loss: 1.317005]\n",
      "[D loss: 0.728474] [G loss: 1.243393]\n",
      "[D loss: 0.717448] [G loss: 1.352538]\n",
      "[D loss: 0.896706] [G loss: 1.168579]\n",
      "[D loss: 0.777391] [G loss: 1.349499]\n",
      "[D loss: 0.767125] [G loss: 1.679195]\n",
      "[D loss: 0.714976] [G loss: 1.502493]\n",
      "[D loss: 0.847548] [G loss: 1.571018]\n",
      "[D loss: 0.731323] [G loss: 1.483313]\n",
      "[D loss: 0.949713] [G loss: 1.402321]\n",
      "[D loss: 0.919648] [G loss: 1.469750]\n",
      "[D loss: 0.818341] [G loss: 1.532180]\n",
      "[D loss: 0.863526] [G loss: 1.551864]\n",
      "[D loss: 0.592656] [G loss: 1.672811]\n",
      "[D loss: 0.858327] [G loss: 1.483349]\n",
      "[D loss: 0.783884] [G loss: 1.535523]\n",
      "[D loss: 0.840336] [G loss: 1.481356]\n",
      "[D loss: 1.102286] [G loss: 1.276213]\n",
      "[D loss: 1.085474] [G loss: 1.281185]\n",
      "[D loss: 0.743521] [G loss: 1.483464]\n",
      "[D loss: 1.028355] [G loss: 1.466521]\n",
      "[D loss: 0.764834] [G loss: 1.420548]\n",
      "[D loss: 0.794969] [G loss: 1.291065]\n",
      "[D loss: 0.971317] [G loss: 1.316666]\n",
      "[D loss: 0.825423] [G loss: 1.306225]\n",
      "[D loss: 1.095449] [G loss: 1.485765]\n",
      "[D loss: 0.825153] [G loss: 1.292767]\n",
      "[D loss: 0.945656] [G loss: 1.522095]\n",
      "[D loss: 0.848006] [G loss: 1.490936]\n",
      "[D loss: 1.016162] [G loss: 1.454915]\n",
      "[D loss: 0.800677] [G loss: 1.567012]\n",
      "[D loss: 1.023619] [G loss: 1.370808]\n",
      "[D loss: 0.681415] [G loss: 1.488985]\n",
      "[D loss: 0.898735] [G loss: 1.402470]\n",
      "[D loss: 0.922815] [G loss: 1.430255]\n",
      "[D loss: 0.907397] [G loss: 1.411219]\n",
      "[D loss: 0.795185] [G loss: 1.523818]\n",
      "[D loss: 0.719985] [G loss: 1.580102]\n",
      "[D loss: 0.887696] [G loss: 1.278416]\n",
      "[D loss: 0.977555] [G loss: 1.320629]\n",
      "[D loss: 0.849356] [G loss: 1.196354]\n",
      "[D loss: 1.103733] [G loss: 1.273681]\n",
      "[D loss: 0.968165] [G loss: 1.315915]\n",
      "[D loss: 0.721368] [G loss: 1.594858]\n",
      "[D loss: 0.764604] [G loss: 1.480721]\n",
      "[D loss: 1.014663] [G loss: 1.288059]\n",
      "[D loss: 0.973456] [G loss: 1.512316]\n",
      "[D loss: 0.782466] [G loss: 1.552021]\n",
      "[D loss: 0.739522] [G loss: 1.513177]\n",
      "[D loss: 0.812586] [G loss: 1.446754]\n",
      "[D loss: 0.712505] [G loss: 1.300482]\n",
      "[D loss: 0.762552] [G loss: 1.622089]\n",
      "[D loss: 0.989948] [G loss: 1.442550]\n",
      "[D loss: 0.777477] [G loss: 1.635020]\n",
      "[D loss: 0.747626] [G loss: 1.404597]\n",
      "[D loss: 0.738749] [G loss: 1.431165]\n",
      "[D loss: 0.987054] [G loss: 1.323905]\n",
      "[D loss: 0.902211] [G loss: 1.510700]\n",
      "[D loss: 0.726870] [G loss: 1.294910]\n",
      "[D loss: 0.735479] [G loss: 1.305026]\n",
      "[D loss: 0.645604] [G loss: 1.360720]\n",
      "[D loss: 1.079016] [G loss: 1.303435]\n",
      "[D loss: 0.869091] [G loss: 1.715027]\n",
      "[D loss: 0.805469] [G loss: 1.681967]\n",
      "[D loss: 0.813682] [G loss: 1.761564]\n",
      "[D loss: 0.777344] [G loss: 1.556229]\n",
      "[D loss: 0.905687] [G loss: 1.457846]\n",
      "[D loss: 1.038701] [G loss: 1.334400]\n",
      "[D loss: 0.785978] [G loss: 1.534158]\n",
      "[D loss: 0.820227] [G loss: 1.279546]\n",
      "[D loss: 0.773396] [G loss: 1.478732]\n",
      "[D loss: 0.822182] [G loss: 1.445465]\n",
      "[D loss: 0.799480] [G loss: 1.540841]\n",
      "[D loss: 0.873659] [G loss: 1.439026]\n",
      "[D loss: 0.724611] [G loss: 1.510075]\n",
      "[D loss: 0.638812] [G loss: 1.387430]\n",
      "[D loss: 0.676224] [G loss: 1.488194]\n",
      "[D loss: 0.860259] [G loss: 1.368136]\n",
      "[D loss: 0.957192] [G loss: 1.298628]\n",
      "[D loss: 0.734100] [G loss: 1.726636]\n",
      "[D loss: 0.704927] [G loss: 1.323202]\n",
      "[D loss: 0.807647] [G loss: 1.538357]\n",
      "[D loss: 0.994878] [G loss: 1.252631]\n",
      "[D loss: 0.743260] [G loss: 1.459197]\n",
      "[D loss: 0.901926] [G loss: 1.635125]\n",
      "[D loss: 0.988486] [G loss: 1.738784]\n",
      "[D loss: 1.066040] [G loss: 1.456365]\n",
      "[D loss: 0.889029] [G loss: 1.279365]\n",
      "[D loss: 1.011595] [G loss: 1.371082]\n",
      "[D loss: 0.561571] [G loss: 1.445860]\n",
      "[D loss: 0.755150] [G loss: 1.167811]\n",
      "[D loss: 0.833474] [G loss: 1.496685]\n",
      "[D loss: 0.864969] [G loss: 1.381211]\n",
      "[D loss: 0.651026] [G loss: 1.563403]\n",
      "[D loss: 0.969615] [G loss: 1.580482]\n",
      "[D loss: 0.625451] [G loss: 1.636981]\n",
      "[D loss: 0.981281] [G loss: 1.596521]\n",
      "[D loss: 0.741512] [G loss: 1.471969]\n",
      "[D loss: 0.680029] [G loss: 1.438880]\n",
      "[D loss: 1.051371] [G loss: 1.407498]\n",
      "[D loss: 0.918375] [G loss: 1.382603]\n",
      "[D loss: 0.972858] [G loss: 1.457385]\n",
      "[D loss: 0.870740] [G loss: 1.554260]\n",
      "[D loss: 0.848093] [G loss: 1.609251]\n",
      "[D loss: 0.638661] [G loss: 1.383096]\n",
      "[D loss: 0.870645] [G loss: 1.342212]\n",
      "[D loss: 1.074189] [G loss: 1.238493]\n",
      "[D loss: 0.960677] [G loss: 1.486461]\n",
      "[D loss: 0.790176] [G loss: 1.562182]\n",
      "[D loss: 0.766918] [G loss: 1.665105]\n",
      "[D loss: 1.401145] [G loss: 1.309226]\n",
      "[D loss: 1.047296] [G loss: 1.573339]\n",
      "[D loss: 0.912451] [G loss: 1.586241]\n",
      "[D loss: 0.833835] [G loss: 1.534129]\n",
      "[D loss: 0.809736] [G loss: 1.307880]\n",
      "[D loss: 0.738652] [G loss: 1.331167]\n",
      "[D loss: 0.806725] [G loss: 1.702094]\n",
      "[D loss: 0.829881] [G loss: 1.456142]\n",
      "[D loss: 0.923105] [G loss: 1.451112]\n",
      "[D loss: 0.778872] [G loss: 1.362603]\n",
      "[D loss: 0.934263] [G loss: 1.450702]\n",
      "[D loss: 0.909612] [G loss: 1.421181]\n",
      "[D loss: 0.794233] [G loss: 1.341537]\n",
      "[D loss: 1.057442] [G loss: 1.616868]\n",
      "[D loss: 0.816225] [G loss: 1.482397]\n",
      "[D loss: 0.921690] [G loss: 1.275117]\n",
      "[D loss: 0.848413] [G loss: 1.612616]\n",
      "[D loss: 0.765917] [G loss: 1.403263]\n",
      "[D loss: 1.068124] [G loss: 1.320774]\n",
      "[D loss: 0.848820] [G loss: 1.484297]\n",
      "[D loss: 0.927363] [G loss: 1.373739]\n",
      "[D loss: 1.013074] [G loss: 1.364876]\n",
      "[D loss: 0.886667] [G loss: 1.564396]\n",
      "[D loss: 0.899287] [G loss: 1.305718]\n",
      "[D loss: 0.766112] [G loss: 1.515247]\n",
      "[D loss: 1.039087] [G loss: 1.342653]\n",
      "[D loss: 0.887226] [G loss: 1.282950]\n",
      "[D loss: 0.944810] [G loss: 1.426660]\n",
      "[D loss: 1.127256] [G loss: 1.293327]\n",
      "[D loss: 0.835401] [G loss: 1.460090]\n",
      "[D loss: 0.935279] [G loss: 1.510435]\n",
      "[D loss: 0.668917] [G loss: 1.387807]\n",
      "[D loss: 0.857610] [G loss: 1.392965]\n",
      "[D loss: 0.722218] [G loss: 1.273344]\n",
      "[D loss: 0.793190] [G loss: 1.416459]\n",
      "[D loss: 1.034521] [G loss: 1.408644]\n",
      "[D loss: 0.980113] [G loss: 1.255314]\n",
      "[D loss: 1.198031] [G loss: 1.116441]\n",
      "[D loss: 0.921958] [G loss: 1.281228]\n",
      "[D loss: 0.746688] [G loss: 1.452340]\n",
      "[D loss: 0.857686] [G loss: 1.353160]\n",
      "[D loss: 1.298900] [G loss: 1.124866]\n",
      "[D loss: 0.714604] [G loss: 1.408938]\n",
      "[D loss: 1.023296] [G loss: 1.411252]\n",
      "[D loss: 0.942891] [G loss: 1.383852]\n",
      "[D loss: 1.021940] [G loss: 1.389337]\n",
      "[D loss: 0.895022] [G loss: 1.324358]\n",
      "[D loss: 0.942336] [G loss: 1.236504]\n",
      "[D loss: 0.926234] [G loss: 1.360837]\n",
      "[D loss: 0.762473] [G loss: 1.286654]\n",
      "[D loss: 0.901870] [G loss: 1.446700]\n",
      "[D loss: 0.911231] [G loss: 1.673718]\n",
      "[D loss: 0.999879] [G loss: 1.415780]\n",
      "[D loss: 0.928095] [G loss: 1.384689]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.787756] [G loss: 1.223894]\n",
      "[D loss: 0.776636] [G loss: 1.650498]\n",
      "[D loss: 0.755320] [G loss: 1.350864]\n",
      "[D loss: 1.030183] [G loss: 1.295538]\n",
      "[D loss: 0.591636] [G loss: 1.491033]\n",
      "[D loss: 0.845809] [G loss: 1.464674]\n",
      "[D loss: 0.555391] [G loss: 1.516131]\n",
      "[D loss: 0.941824] [G loss: 1.407490]\n",
      "[D loss: 1.028420] [G loss: 1.421944]\n",
      "[D loss: 1.125287] [G loss: 1.403504]\n",
      "[D loss: 1.021295] [G loss: 1.641258]\n",
      "[D loss: 1.119146] [G loss: 1.355887]\n",
      "[D loss: 0.984257] [G loss: 1.295325]\n",
      "[D loss: 0.729555] [G loss: 1.361237]\n",
      "[D loss: 0.727201] [G loss: 1.432069]\n",
      "[D loss: 0.947086] [G loss: 1.165272]\n",
      "[D loss: 0.924571] [G loss: 1.364602]\n",
      "[D loss: 0.785799] [G loss: 1.437847]\n",
      "[D loss: 0.928286] [G loss: 1.243041]\n",
      "[D loss: 0.764206] [G loss: 1.466624]\n",
      "[D loss: 0.788734] [G loss: 1.355585]\n",
      "[D loss: 1.024564] [G loss: 1.170035]\n",
      "[D loss: 0.812661] [G loss: 1.300646]\n",
      "[D loss: 0.967855] [G loss: 1.238287]\n",
      "[D loss: 0.909273] [G loss: 1.475097]\n",
      "[D loss: 0.864389] [G loss: 1.370889]\n",
      "[D loss: 0.769127] [G loss: 1.407321]\n",
      "[D loss: 0.821023] [G loss: 1.397091]\n",
      "[D loss: 0.687342] [G loss: 1.431449]\n",
      "[D loss: 0.995510] [G loss: 1.388017]\n",
      "[D loss: 0.716840] [G loss: 1.522696]\n",
      "[D loss: 0.823314] [G loss: 1.235899]\n",
      "[D loss: 0.838122] [G loss: 1.327828]\n",
      "[D loss: 0.801045] [G loss: 1.322415]\n",
      "[D loss: 0.995007] [G loss: 1.329152]\n",
      "[D loss: 0.878138] [G loss: 1.324535]\n",
      "[D loss: 0.819505] [G loss: 1.581582]\n",
      "[D loss: 0.881353] [G loss: 1.616286]\n",
      "[D loss: 0.768795] [G loss: 1.340097]\n",
      "[D loss: 0.940468] [G loss: 1.462234]\n",
      "[D loss: 0.643075] [G loss: 1.411445]\n",
      "[D loss: 1.000286] [G loss: 1.316901]\n",
      "[D loss: 0.834305] [G loss: 1.380037]\n",
      "[D loss: 0.871016] [G loss: 1.339435]\n",
      "[D loss: 0.858217] [G loss: 1.410696]\n",
      "[D loss: 0.886097] [G loss: 1.285184]\n",
      "[D loss: 0.606132] [G loss: 1.373135]\n",
      "[D loss: 0.716301] [G loss: 1.322585]\n",
      "[D loss: 0.818248] [G loss: 1.211797]\n",
      "[D loss: 0.731747] [G loss: 1.525172]\n",
      "[D loss: 0.890577] [G loss: 1.529014]\n",
      "[D loss: 0.598419] [G loss: 1.518197]\n",
      "[D loss: 0.653317] [G loss: 1.571984]\n",
      "[D loss: 0.781984] [G loss: 1.681886]\n",
      "[D loss: 0.788458] [G loss: 1.456857]\n",
      "[D loss: 0.941927] [G loss: 1.802252]\n",
      "[D loss: 0.790770] [G loss: 1.303307]\n",
      "[D loss: 0.738766] [G loss: 1.359900]\n",
      "[D loss: 0.719543] [G loss: 1.523840]\n",
      "[D loss: 0.982103] [G loss: 1.527156]\n",
      "[D loss: 0.642826] [G loss: 1.735266]\n",
      "[D loss: 0.716553] [G loss: 1.543506]\n",
      "[D loss: 0.878644] [G loss: 1.565673]\n",
      "[D loss: 0.688601] [G loss: 1.580048]\n",
      "[D loss: 1.008717] [G loss: 1.523881]\n",
      "[D loss: 0.701621] [G loss: 1.613511]\n",
      "[D loss: 0.871438] [G loss: 1.546098]\n",
      "[D loss: 0.936647] [G loss: 1.382209]\n",
      "[D loss: 0.770441] [G loss: 1.291570]\n",
      "[D loss: 0.991027] [G loss: 1.520422]\n",
      "[D loss: 0.758398] [G loss: 1.786758]\n",
      "[D loss: 0.784052] [G loss: 1.771355]\n",
      "[D loss: 0.887130] [G loss: 1.717334]\n",
      "[D loss: 0.816723] [G loss: 1.423053]\n",
      "[D loss: 0.702086] [G loss: 1.372089]\n",
      "[D loss: 0.856450] [G loss: 1.583446]\n",
      "[D loss: 0.697907] [G loss: 1.371563]\n",
      "[D loss: 0.666826] [G loss: 1.485525]\n",
      "[D loss: 0.729814] [G loss: 1.562855]\n",
      "[D loss: 0.576272] [G loss: 1.452701]\n",
      "[D loss: 0.816498] [G loss: 1.503850]\n",
      "[D loss: 0.917880] [G loss: 1.447374]\n",
      "[D loss: 0.930999] [G loss: 1.415563]\n",
      "[D loss: 0.695585] [G loss: 1.461289]\n",
      "[D loss: 0.831021] [G loss: 1.296090]\n",
      "[D loss: 0.935544] [G loss: 1.401325]\n",
      "[D loss: 0.819859] [G loss: 1.352315]\n",
      "[D loss: 0.763770] [G loss: 1.380873]\n",
      "[D loss: 0.721328] [G loss: 1.528088]\n",
      "[D loss: 0.914631] [G loss: 1.690091]\n",
      "[D loss: 0.807025] [G loss: 1.557072]\n",
      "[D loss: 0.665831] [G loss: 1.536205]\n",
      "[D loss: 0.784348] [G loss: 1.459008]\n",
      "[D loss: 0.726319] [G loss: 1.524230]\n",
      "[D loss: 0.656439] [G loss: 1.492682]\n",
      "[D loss: 0.821275] [G loss: 1.420108]\n",
      "[D loss: 0.961847] [G loss: 1.382572]\n",
      "[D loss: 0.934819] [G loss: 1.475514]\n",
      "[D loss: 0.788206] [G loss: 1.675715]\n",
      "[D loss: 0.696398] [G loss: 1.405649]\n",
      "[D loss: 0.850419] [G loss: 1.567104]\n",
      "[D loss: 0.822859] [G loss: 1.659143]\n",
      "[D loss: 0.885078] [G loss: 1.587856]\n",
      "[D loss: 0.752070] [G loss: 1.548184]\n",
      "[D loss: 1.013539] [G loss: 1.350586]\n",
      "[D loss: 0.754749] [G loss: 1.466615]\n",
      "[D loss: 0.750083] [G loss: 1.547374]\n",
      "[D loss: 0.890506] [G loss: 1.511934]\n",
      "[D loss: 0.818009] [G loss: 1.397359]\n",
      "[D loss: 0.831569] [G loss: 1.428825]\n",
      "[D loss: 0.815875] [G loss: 1.545863]\n",
      "[D loss: 1.075965] [G loss: 1.520474]\n",
      "[D loss: 0.990660] [G loss: 1.465903]\n",
      "[D loss: 0.688407] [G loss: 1.391651]\n",
      "[D loss: 0.655799] [G loss: 1.627042]\n",
      "[D loss: 0.932122] [G loss: 1.516171]\n",
      "[D loss: 0.552817] [G loss: 1.514912]\n",
      "[D loss: 1.017658] [G loss: 1.344275]\n",
      "[D loss: 1.048304] [G loss: 1.329479]\n",
      "[D loss: 0.619432] [G loss: 1.515182]\n",
      "[D loss: 0.962528] [G loss: 1.464995]\n",
      "[D loss: 0.995253] [G loss: 1.371849]\n",
      "[D loss: 0.918978] [G loss: 1.542158]\n",
      "[D loss: 0.736767] [G loss: 1.542915]\n",
      "[D loss: 0.836139] [G loss: 1.420904]\n",
      "[D loss: 1.042029] [G loss: 1.347566]\n",
      "[D loss: 0.720626] [G loss: 1.509483]\n",
      "[D loss: 0.971864] [G loss: 1.237376]\n",
      "[D loss: 1.032514] [G loss: 1.380104]\n",
      "[D loss: 0.750365] [G loss: 1.669846]\n",
      "[D loss: 0.721136] [G loss: 1.585900]\n",
      "[D loss: 0.781182] [G loss: 1.606405]\n",
      "[D loss: 0.708830] [G loss: 1.322335]\n",
      "[D loss: 0.851744] [G loss: 1.445139]\n",
      "[D loss: 0.765852] [G loss: 1.461990]\n",
      "[D loss: 0.964060] [G loss: 1.525678]\n",
      "[D loss: 0.621489] [G loss: 1.390340]\n",
      "[D loss: 0.912674] [G loss: 1.399541]\n",
      "[D loss: 0.845972] [G loss: 1.303078]\n",
      "[D loss: 1.030440] [G loss: 1.302728]\n",
      "[D loss: 0.928605] [G loss: 1.398282]\n",
      "[D loss: 0.807486] [G loss: 1.591330]\n",
      "[D loss: 0.653417] [G loss: 1.468958]\n",
      "[D loss: 0.794419] [G loss: 1.243792]\n",
      "[D loss: 0.980660] [G loss: 1.288435]\n",
      "[D loss: 0.803952] [G loss: 1.357610]\n",
      "[D loss: 0.885780] [G loss: 1.690649]\n",
      "[D loss: 0.749873] [G loss: 1.776536]\n",
      "[D loss: 0.882831] [G loss: 1.800001]\n",
      "[D loss: 1.064616] [G loss: 1.380514]\n",
      "[D loss: 0.758847] [G loss: 1.657227]\n",
      "[D loss: 1.101055] [G loss: 1.512909]\n",
      "[D loss: 0.741225] [G loss: 1.406548]\n",
      "[D loss: 0.675938] [G loss: 1.477664]\n",
      "[D loss: 0.924592] [G loss: 1.411846]\n",
      "[D loss: 1.004948] [G loss: 1.438558]\n",
      "[D loss: 0.939697] [G loss: 1.360946]\n",
      "[D loss: 1.034118] [G loss: 1.299534]\n",
      "[D loss: 0.860202] [G loss: 1.552914]\n",
      "[D loss: 0.661963] [G loss: 1.567645]\n",
      "[D loss: 0.758781] [G loss: 1.449616]\n",
      "[D loss: 0.898444] [G loss: 1.452655]\n",
      "[D loss: 0.833403] [G loss: 1.368196]\n",
      "[D loss: 0.676686] [G loss: 1.415774]\n",
      "[D loss: 1.014975] [G loss: 1.489428]\n",
      "[D loss: 0.939301] [G loss: 1.283244]\n",
      "[D loss: 0.850078] [G loss: 1.327869]\n",
      "[D loss: 1.130286] [G loss: 1.408659]\n",
      "[D loss: 0.780056] [G loss: 1.488672]\n",
      "[D loss: 0.941396] [G loss: 1.278718]\n",
      "[D loss: 0.761897] [G loss: 1.492302]\n",
      "[D loss: 0.704873] [G loss: 1.564557]\n",
      "[D loss: 0.971267] [G loss: 1.362820]\n",
      "[D loss: 0.748200] [G loss: 1.452491]\n",
      "[D loss: 0.829292] [G loss: 1.463607]\n",
      "[D loss: 0.900016] [G loss: 1.378544]\n",
      "[D loss: 0.788153] [G loss: 1.266538]\n",
      "[D loss: 0.842034] [G loss: 1.462223]\n",
      "[D loss: 0.752243] [G loss: 1.283827]\n",
      "[D loss: 0.782786] [G loss: 1.343645]\n",
      "[D loss: 0.975729] [G loss: 1.496412]\n",
      "[D loss: 0.770246] [G loss: 1.721789]\n",
      "[D loss: 0.873886] [G loss: 1.169694]\n",
      "[D loss: 0.879206] [G loss: 1.658163]\n",
      "[D loss: 0.899249] [G loss: 1.395900]\n",
      "[D loss: 0.781012] [G loss: 1.480746]\n",
      "[D loss: 1.004631] [G loss: 1.227239]\n",
      "[D loss: 0.794372] [G loss: 1.316747]\n",
      "[D loss: 1.021112] [G loss: 1.582687]\n",
      "[D loss: 0.853547] [G loss: 1.576191]\n",
      "[D loss: 1.020301] [G loss: 1.641871]\n",
      "[D loss: 0.822563] [G loss: 1.381980]\n",
      "[D loss: 0.860616] [G loss: 1.538925]\n",
      "[D loss: 0.680814] [G loss: 1.416227]\n",
      "[D loss: 0.803236] [G loss: 1.227642]\n",
      "[D loss: 1.040245] [G loss: 1.538873]\n",
      "[D loss: 0.776881] [G loss: 1.512959]\n",
      "[D loss: 0.835888] [G loss: 1.207475]\n",
      "[D loss: 0.659921] [G loss: 1.577608]\n",
      "[D loss: 0.726372] [G loss: 1.485519]\n",
      "[D loss: 0.816802] [G loss: 1.275475]\n",
      "[D loss: 0.729719] [G loss: 1.514410]\n",
      "[D loss: 0.860583] [G loss: 1.353510]\n",
      "[D loss: 1.044730] [G loss: 1.409376]\n",
      "[D loss: 0.875553] [G loss: 1.412888]\n",
      "[D loss: 0.726152] [G loss: 1.613820]\n",
      "[D loss: 0.721211] [G loss: 1.563286]\n",
      "[D loss: 0.849117] [G loss: 1.380520]\n",
      "[D loss: 0.897861] [G loss: 1.520417]\n",
      "[D loss: 0.800174] [G loss: 1.648456]\n",
      "[D loss: 0.854814] [G loss: 1.374244]\n",
      "[D loss: 0.787345] [G loss: 1.437522]\n",
      "[D loss: 1.035604] [G loss: 1.337779]\n",
      "[D loss: 1.173987] [G loss: 1.258933]\n",
      "[D loss: 0.623334] [G loss: 1.505942]\n",
      "[D loss: 1.032352] [G loss: 1.469238]\n",
      "[D loss: 0.894732] [G loss: 1.382459]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.844262] [G loss: 1.409158]\n",
      "[D loss: 0.954489] [G loss: 1.215085]\n",
      "[D loss: 0.780699] [G loss: 1.318061]\n",
      "[D loss: 0.714408] [G loss: 1.363220]\n",
      "[D loss: 0.850678] [G loss: 1.503475]\n",
      "[D loss: 0.871912] [G loss: 1.192574]\n",
      "[D loss: 0.960677] [G loss: 1.315417]\n",
      "[D loss: 0.994277] [G loss: 1.478607]\n",
      "[D loss: 0.671887] [G loss: 1.564486]\n",
      "[D loss: 1.047786] [G loss: 1.430677]\n",
      "[D loss: 0.759560] [G loss: 1.651969]\n",
      "[D loss: 0.899940] [G loss: 1.307901]\n",
      "[D loss: 0.921302] [G loss: 1.476436]\n",
      "[D loss: 0.852294] [G loss: 1.310213]\n",
      "[D loss: 0.710730] [G loss: 1.407614]\n",
      "[D loss: 1.055176] [G loss: 1.253811]\n",
      "[D loss: 0.965448] [G loss: 1.222987]\n",
      "[D loss: 0.870225] [G loss: 1.395411]\n",
      "[D loss: 0.831236] [G loss: 1.396444]\n",
      "[D loss: 0.798439] [G loss: 1.390609]\n",
      "[D loss: 0.714032] [G loss: 1.521952]\n",
      "[D loss: 0.815649] [G loss: 1.446184]\n",
      "[D loss: 0.867188] [G loss: 1.328100]\n",
      "[D loss: 0.881549] [G loss: 1.285756]\n",
      "[D loss: 0.621449] [G loss: 1.312319]\n",
      "[D loss: 0.949774] [G loss: 1.480041]\n",
      "[D loss: 0.817708] [G loss: 1.388021]\n",
      "[D loss: 0.749993] [G loss: 1.404637]\n",
      "[D loss: 0.979144] [G loss: 1.354329]\n",
      "[D loss: 1.265671] [G loss: 1.347165]\n",
      "[D loss: 0.891076] [G loss: 1.337693]\n",
      "[D loss: 0.972103] [G loss: 1.259847]\n",
      "[D loss: 0.712116] [G loss: 1.387989]\n",
      "[D loss: 0.842423] [G loss: 1.355910]\n",
      "[D loss: 0.801442] [G loss: 1.164773]\n",
      "[D loss: 0.792696] [G loss: 1.347901]\n",
      "[D loss: 0.777291] [G loss: 1.400590]\n",
      "[D loss: 0.988674] [G loss: 1.516943]\n",
      "[D loss: 0.879924] [G loss: 1.607742]\n",
      "[D loss: 0.924158] [G loss: 1.255802]\n",
      "[D loss: 0.878628] [G loss: 1.459703]\n",
      "[D loss: 0.835211] [G loss: 1.556306]\n",
      "[D loss: 0.908478] [G loss: 1.241798]\n",
      "[D loss: 0.607605] [G loss: 1.466784]\n",
      "[D loss: 0.896505] [G loss: 1.494609]\n",
      "[D loss: 1.136721] [G loss: 1.344811]\n",
      "[D loss: 0.927995] [G loss: 1.253168]\n",
      "[D loss: 0.922344] [G loss: 1.383495]\n",
      "[D loss: 0.900842] [G loss: 1.405186]\n",
      "[D loss: 0.631573] [G loss: 1.297064]\n",
      "[D loss: 0.822144] [G loss: 1.377909]\n",
      "[D loss: 0.720602] [G loss: 1.528065]\n",
      "[D loss: 0.755541] [G loss: 1.634108]\n",
      "[D loss: 0.723794] [G loss: 1.365462]\n",
      "[D loss: 0.857787] [G loss: 1.361757]\n",
      "[D loss: 0.777646] [G loss: 1.436944]\n",
      "[D loss: 0.713590] [G loss: 1.480530]\n",
      "[D loss: 0.856761] [G loss: 1.464270]\n",
      "[D loss: 0.979884] [G loss: 1.424576]\n",
      "[D loss: 0.784156] [G loss: 1.503049]\n",
      "[D loss: 0.978014] [G loss: 1.674449]\n",
      "[D loss: 0.748255] [G loss: 1.393347]\n",
      "[D loss: 0.694438] [G loss: 1.389670]\n",
      "[D loss: 0.932373] [G loss: 1.184088]\n",
      "[D loss: 0.905333] [G loss: 1.402819]\n",
      "[D loss: 0.897608] [G loss: 1.550341]\n",
      "[D loss: 0.911611] [G loss: 1.430449]\n",
      "[D loss: 0.853928] [G loss: 1.437165]\n",
      "[D loss: 1.081399] [G loss: 1.254903]\n",
      "[D loss: 0.790609] [G loss: 1.564087]\n",
      "[D loss: 0.948127] [G loss: 1.560772]\n",
      "[D loss: 0.884983] [G loss: 1.558834]\n",
      "[D loss: 0.805025] [G loss: 1.487873]\n",
      "[D loss: 0.966825] [G loss: 1.323855]\n",
      "[D loss: 0.800229] [G loss: 1.605326]\n",
      "[D loss: 0.771963] [G loss: 1.597604]\n",
      "[D loss: 0.866159] [G loss: 1.402352]\n",
      "[D loss: 0.811050] [G loss: 1.398598]\n",
      "[D loss: 0.797652] [G loss: 1.612198]\n",
      "[D loss: 0.843170] [G loss: 1.538285]\n",
      "[D loss: 0.874027] [G loss: 1.291086]\n",
      "[D loss: 0.942455] [G loss: 1.389874]\n",
      "[D loss: 0.875886] [G loss: 1.450536]\n",
      "[D loss: 0.768662] [G loss: 1.375263]\n",
      "[D loss: 0.956816] [G loss: 1.310219]\n",
      "[D loss: 0.842556] [G loss: 1.432300]\n",
      "[D loss: 0.795646] [G loss: 1.261521]\n",
      "[D loss: 0.974916] [G loss: 1.379102]\n",
      "[D loss: 0.881940] [G loss: 1.278790]\n",
      "[D loss: 0.989176] [G loss: 1.463583]\n",
      "[D loss: 0.930456] [G loss: 1.374223]\n",
      "[D loss: 0.672861] [G loss: 1.530706]\n",
      "[D loss: 0.835587] [G loss: 1.344844]\n",
      "[D loss: 0.739602] [G loss: 1.387017]\n",
      "[D loss: 1.053525] [G loss: 1.552156]\n",
      "[D loss: 1.123947] [G loss: 1.236832]\n",
      "[D loss: 0.896702] [G loss: 1.278997]\n",
      "[D loss: 0.583811] [G loss: 1.334744]\n",
      "[D loss: 0.707052] [G loss: 1.468053]\n",
      "[D loss: 0.734054] [G loss: 1.671856]\n",
      "[D loss: 1.070000] [G loss: 1.508205]\n",
      "[D loss: 0.781496] [G loss: 1.574630]\n",
      "[D loss: 0.821755] [G loss: 1.333820]\n",
      "[D loss: 0.682352] [G loss: 1.657972]\n",
      "[D loss: 1.012514] [G loss: 1.258209]\n",
      "[D loss: 0.837167] [G loss: 1.336924]\n",
      "[D loss: 0.820609] [G loss: 1.350435]\n",
      "[D loss: 0.909074] [G loss: 1.457096]\n",
      "[D loss: 0.579916] [G loss: 1.384290]\n",
      "[D loss: 0.698076] [G loss: 1.413345]\n",
      "[D loss: 0.851963] [G loss: 1.517003]\n",
      "[D loss: 0.983562] [G loss: 1.535441]\n",
      "[D loss: 0.875570] [G loss: 1.391302]\n",
      "[D loss: 0.943837] [G loss: 1.491023]\n",
      "[D loss: 0.916159] [G loss: 1.395449]\n",
      "[D loss: 1.031491] [G loss: 1.236097]\n",
      "[D loss: 0.929121] [G loss: 1.358076]\n",
      "[D loss: 0.846490] [G loss: 1.411620]\n",
      "[D loss: 0.660822] [G loss: 1.343424]\n",
      "[D loss: 0.643419] [G loss: 1.533864]\n",
      "[D loss: 0.862618] [G loss: 1.386359]\n",
      "[D loss: 0.732850] [G loss: 1.374686]\n",
      "[D loss: 0.918734] [G loss: 1.334050]\n",
      "[D loss: 1.065817] [G loss: 1.361944]\n",
      "[D loss: 0.559436] [G loss: 1.552895]\n",
      "[D loss: 0.990148] [G loss: 1.657737]\n",
      "[D loss: 0.736221] [G loss: 1.547387]\n",
      "epoch:15, g_loss:2733.1396484375,d_loss:1577.755859375\n",
      "[D loss: 0.798756] [G loss: 1.457014]\n",
      "[D loss: 0.683615] [G loss: 1.413413]\n",
      "[D loss: 0.669082] [G loss: 1.549858]\n",
      "[D loss: 1.010363] [G loss: 1.704030]\n",
      "[D loss: 0.735529] [G loss: 1.499969]\n",
      "[D loss: 0.682925] [G loss: 1.574963]\n",
      "[D loss: 0.749360] [G loss: 1.399354]\n",
      "[D loss: 0.711173] [G loss: 1.512942]\n",
      "[D loss: 0.607943] [G loss: 1.584100]\n",
      "[D loss: 0.873413] [G loss: 1.629752]\n",
      "[D loss: 0.775550] [G loss: 1.692230]\n",
      "[D loss: 0.842625] [G loss: 1.407760]\n",
      "[D loss: 0.527832] [G loss: 1.682132]\n",
      "[D loss: 0.843280] [G loss: 1.844935]\n",
      "[D loss: 0.975402] [G loss: 1.500585]\n",
      "[D loss: 0.853198] [G loss: 1.647855]\n",
      "[D loss: 0.768825] [G loss: 1.646062]\n",
      "[D loss: 0.790229] [G loss: 1.631134]\n",
      "[D loss: 0.786636] [G loss: 1.650063]\n",
      "[D loss: 0.880867] [G loss: 1.496039]\n",
      "[D loss: 0.749649] [G loss: 1.556959]\n",
      "[D loss: 0.831541] [G loss: 1.793244]\n",
      "[D loss: 0.900208] [G loss: 1.346588]\n",
      "[D loss: 0.901246] [G loss: 1.704980]\n",
      "[D loss: 0.893663] [G loss: 1.422608]\n",
      "[D loss: 0.945986] [G loss: 1.263377]\n",
      "[D loss: 0.913111] [G loss: 1.522666]\n",
      "[D loss: 0.860102] [G loss: 1.586000]\n",
      "[D loss: 1.017421] [G loss: 1.587466]\n",
      "[D loss: 0.838487] [G loss: 1.184265]\n",
      "[D loss: 0.914680] [G loss: 1.253679]\n",
      "[D loss: 0.820438] [G loss: 1.385752]\n",
      "[D loss: 0.622336] [G loss: 1.429449]\n",
      "[D loss: 0.964848] [G loss: 1.397053]\n",
      "[D loss: 0.830347] [G loss: 1.459734]\n",
      "[D loss: 0.957566] [G loss: 1.467502]\n",
      "[D loss: 0.859307] [G loss: 1.809109]\n",
      "[D loss: 0.690997] [G loss: 1.743755]\n",
      "[D loss: 0.860821] [G loss: 1.431181]\n",
      "[D loss: 0.776885] [G loss: 1.392309]\n",
      "[D loss: 0.768724] [G loss: 1.288919]\n",
      "[D loss: 0.718117] [G loss: 1.619362]\n",
      "[D loss: 0.766734] [G loss: 1.363765]\n",
      "[D loss: 0.801344] [G loss: 1.428370]\n",
      "[D loss: 0.729615] [G loss: 1.501709]\n",
      "[D loss: 0.898609] [G loss: 1.723175]\n",
      "[D loss: 1.184299] [G loss: 1.553596]\n",
      "[D loss: 0.851811] [G loss: 1.449369]\n",
      "[D loss: 0.931912] [G loss: 1.313045]\n",
      "[D loss: 0.976217] [G loss: 1.291232]\n",
      "[D loss: 0.932168] [G loss: 1.527724]\n",
      "[D loss: 0.921123] [G loss: 1.481230]\n",
      "[D loss: 0.694331] [G loss: 1.409353]\n",
      "[D loss: 0.850837] [G loss: 1.474990]\n",
      "[D loss: 0.777292] [G loss: 1.450704]\n",
      "[D loss: 0.897035] [G loss: 1.393505]\n",
      "[D loss: 0.684064] [G loss: 1.493083]\n",
      "[D loss: 0.860329] [G loss: 1.430223]\n",
      "[D loss: 0.971409] [G loss: 1.228853]\n",
      "[D loss: 1.021609] [G loss: 1.441082]\n",
      "[D loss: 0.622342] [G loss: 1.394817]\n",
      "[D loss: 0.729801] [G loss: 1.544131]\n",
      "[D loss: 1.080032] [G loss: 1.323169]\n",
      "[D loss: 0.985878] [G loss: 1.274002]\n",
      "[D loss: 0.808451] [G loss: 1.375632]\n",
      "[D loss: 0.718903] [G loss: 1.705165]\n",
      "[D loss: 1.005540] [G loss: 1.459625]\n",
      "[D loss: 0.896725] [G loss: 1.473931]\n",
      "[D loss: 0.711466] [G loss: 1.571259]\n",
      "[D loss: 0.872451] [G loss: 1.400717]\n",
      "[D loss: 0.888672] [G loss: 1.388691]\n",
      "[D loss: 0.712613] [G loss: 1.561848]\n",
      "[D loss: 0.617887] [G loss: 1.381103]\n",
      "[D loss: 0.762393] [G loss: 1.547408]\n",
      "[D loss: 0.774943] [G loss: 1.345974]\n",
      "[D loss: 0.776542] [G loss: 1.362963]\n",
      "[D loss: 0.797791] [G loss: 1.700607]\n",
      "[D loss: 0.773806] [G loss: 1.654778]\n",
      "[D loss: 1.220047] [G loss: 1.413867]\n",
      "[D loss: 0.910801] [G loss: 1.500715]\n",
      "[D loss: 0.810823] [G loss: 1.540089]\n",
      "[D loss: 0.733772] [G loss: 1.480888]\n",
      "[D loss: 0.782583] [G loss: 1.461178]\n",
      "[D loss: 0.779110] [G loss: 1.422862]\n",
      "[D loss: 0.925945] [G loss: 1.659798]\n",
      "[D loss: 0.939611] [G loss: 1.381968]\n",
      "[D loss: 0.946908] [G loss: 1.443822]\n",
      "[D loss: 0.814972] [G loss: 1.228134]\n",
      "[D loss: 0.652496] [G loss: 1.512427]\n",
      "[D loss: 0.859913] [G loss: 1.489597]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.857333] [G loss: 1.419685]\n",
      "[D loss: 1.050117] [G loss: 1.632204]\n",
      "[D loss: 0.802911] [G loss: 1.554413]\n",
      "[D loss: 1.051419] [G loss: 1.378837]\n",
      "[D loss: 0.652302] [G loss: 1.364439]\n",
      "[D loss: 0.928244] [G loss: 1.427654]\n",
      "[D loss: 0.811409] [G loss: 1.356202]\n",
      "[D loss: 0.849562] [G loss: 1.290104]\n",
      "[D loss: 0.767062] [G loss: 1.381697]\n",
      "[D loss: 0.625089] [G loss: 1.731000]\n",
      "[D loss: 1.033369] [G loss: 1.441383]\n",
      "[D loss: 0.701489] [G loss: 1.504585]\n",
      "[D loss: 0.854202] [G loss: 1.254565]\n",
      "[D loss: 1.037691] [G loss: 1.214775]\n",
      "[D loss: 0.841707] [G loss: 1.359426]\n",
      "[D loss: 0.715305] [G loss: 1.532924]\n",
      "[D loss: 0.853762] [G loss: 1.472481]\n",
      "[D loss: 0.569131] [G loss: 1.633151]\n",
      "[D loss: 0.524107] [G loss: 1.434572]\n",
      "[D loss: 0.907247] [G loss: 1.413601]\n",
      "[D loss: 0.929203] [G loss: 1.478350]\n",
      "[D loss: 0.709494] [G loss: 1.627288]\n",
      "[D loss: 0.974287] [G loss: 1.438315]\n",
      "[D loss: 0.885288] [G loss: 1.503431]\n",
      "[D loss: 0.839051] [G loss: 1.475606]\n",
      "[D loss: 0.652872] [G loss: 1.547873]\n",
      "[D loss: 0.898458] [G loss: 1.542396]\n",
      "[D loss: 0.806490] [G loss: 1.729179]\n",
      "[D loss: 0.783741] [G loss: 1.524348]\n",
      "[D loss: 0.801625] [G loss: 1.135806]\n",
      "[D loss: 0.655981] [G loss: 1.613104]\n",
      "[D loss: 0.629376] [G loss: 1.685034]\n",
      "[D loss: 0.828034] [G loss: 1.409237]\n",
      "[D loss: 0.918735] [G loss: 1.530856]\n",
      "[D loss: 0.800285] [G loss: 1.682252]\n",
      "[D loss: 0.916070] [G loss: 1.510192]\n",
      "[D loss: 0.878571] [G loss: 1.509985]\n",
      "[D loss: 0.766086] [G loss: 1.556180]\n",
      "[D loss: 0.787227] [G loss: 1.266976]\n",
      "[D loss: 0.914949] [G loss: 1.547093]\n",
      "[D loss: 0.876135] [G loss: 1.556283]\n",
      "[D loss: 0.840576] [G loss: 1.483490]\n",
      "[D loss: 0.870561] [G loss: 1.563666]\n",
      "[D loss: 0.906965] [G loss: 1.265516]\n",
      "[D loss: 0.847182] [G loss: 1.340797]\n",
      "[D loss: 0.710983] [G loss: 1.451829]\n",
      "[D loss: 1.020955] [G loss: 1.409118]\n",
      "[D loss: 0.863352] [G loss: 1.361820]\n",
      "[D loss: 0.803981] [G loss: 1.378594]\n",
      "[D loss: 0.757293] [G loss: 1.392294]\n",
      "[D loss: 0.998827] [G loss: 1.209116]\n",
      "[D loss: 0.899903] [G loss: 1.241143]\n",
      "[D loss: 0.732983] [G loss: 1.490380]\n",
      "[D loss: 0.957167] [G loss: 1.582603]\n",
      "[D loss: 0.559905] [G loss: 1.674697]\n",
      "[D loss: 0.919433] [G loss: 1.485441]\n",
      "[D loss: 0.679208] [G loss: 1.432719]\n",
      "[D loss: 1.325534] [G loss: 1.309985]\n",
      "[D loss: 0.605363] [G loss: 1.872263]\n",
      "[D loss: 0.959281] [G loss: 1.502760]\n",
      "[D loss: 0.657793] [G loss: 1.620894]\n",
      "[D loss: 0.984190] [G loss: 1.339126]\n",
      "[D loss: 0.815516] [G loss: 1.452811]\n",
      "[D loss: 0.863663] [G loss: 1.730363]\n",
      "[D loss: 0.848819] [G loss: 1.602026]\n",
      "[D loss: 0.743213] [G loss: 1.439181]\n",
      "[D loss: 0.865705] [G loss: 1.433950]\n",
      "[D loss: 0.899199] [G loss: 1.587447]\n",
      "[D loss: 0.831840] [G loss: 1.284324]\n",
      "[D loss: 0.677519] [G loss: 1.467044]\n",
      "[D loss: 0.703133] [G loss: 1.381673]\n",
      "[D loss: 0.693896] [G loss: 1.479902]\n",
      "[D loss: 0.787590] [G loss: 1.442002]\n",
      "[D loss: 0.870568] [G loss: 1.293926]\n",
      "[D loss: 0.734263] [G loss: 1.522314]\n",
      "[D loss: 0.954729] [G loss: 1.549775]\n",
      "[D loss: 0.922170] [G loss: 1.206837]\n",
      "[D loss: 0.684729] [G loss: 1.416577]\n",
      "[D loss: 0.679502] [G loss: 1.514328]\n",
      "[D loss: 0.833070] [G loss: 1.509294]\n",
      "[D loss: 0.847113] [G loss: 1.404180]\n",
      "[D loss: 0.885056] [G loss: 1.513682]\n",
      "[D loss: 0.881865] [G loss: 1.319359]\n",
      "[D loss: 0.708249] [G loss: 1.376253]\n",
      "[D loss: 0.707548] [G loss: 1.449942]\n",
      "[D loss: 0.788390] [G loss: 1.297215]\n",
      "[D loss: 0.700107] [G loss: 1.461859]\n",
      "[D loss: 0.763993] [G loss: 1.423001]\n",
      "[D loss: 0.882556] [G loss: 1.500270]\n",
      "[D loss: 0.874742] [G loss: 1.571759]\n",
      "[D loss: 0.929300] [G loss: 1.550901]\n",
      "[D loss: 0.840952] [G loss: 1.377326]\n",
      "[D loss: 0.741766] [G loss: 1.436061]\n",
      "[D loss: 0.760957] [G loss: 1.410247]\n",
      "[D loss: 0.893282] [G loss: 1.450171]\n",
      "[D loss: 0.957220] [G loss: 1.404910]\n",
      "[D loss: 0.857753] [G loss: 1.439254]\n",
      "[D loss: 0.615182] [G loss: 1.438145]\n",
      "[D loss: 0.987590] [G loss: 1.602657]\n",
      "[D loss: 1.009231] [G loss: 1.446227]\n",
      "[D loss: 0.765434] [G loss: 1.319214]\n",
      "[D loss: 0.577631] [G loss: 1.537060]\n",
      "[D loss: 0.973037] [G loss: 1.290573]\n",
      "[D loss: 0.736418] [G loss: 1.181143]\n",
      "[D loss: 0.964854] [G loss: 1.638664]\n",
      "[D loss: 0.581974] [G loss: 1.923134]\n",
      "[D loss: 0.953896] [G loss: 1.709922]\n",
      "[D loss: 0.967079] [G loss: 1.448665]\n",
      "[D loss: 0.736062] [G loss: 1.608665]\n",
      "[D loss: 0.943808] [G loss: 1.557701]\n",
      "[D loss: 0.934953] [G loss: 1.374108]\n",
      "[D loss: 0.899881] [G loss: 1.600890]\n",
      "[D loss: 0.924368] [G loss: 1.484229]\n",
      "[D loss: 0.883961] [G loss: 1.241852]\n",
      "[D loss: 0.847853] [G loss: 1.443741]\n",
      "[D loss: 0.997117] [G loss: 1.209192]\n",
      "[D loss: 0.935754] [G loss: 1.419817]\n",
      "[D loss: 0.947974] [G loss: 1.398313]\n",
      "[D loss: 0.795047] [G loss: 1.370242]\n",
      "[D loss: 0.808853] [G loss: 1.284548]\n",
      "[D loss: 0.851091] [G loss: 1.374083]\n",
      "[D loss: 0.750816] [G loss: 1.420526]\n",
      "[D loss: 0.576963] [G loss: 1.669568]\n",
      "[D loss: 1.055682] [G loss: 1.531736]\n",
      "[D loss: 0.808945] [G loss: 1.345861]\n",
      "[D loss: 0.879793] [G loss: 1.218916]\n",
      "[D loss: 0.728525] [G loss: 1.396192]\n",
      "[D loss: 0.747689] [G loss: 1.386697]\n",
      "[D loss: 0.902796] [G loss: 1.709416]\n",
      "[D loss: 0.700972] [G loss: 1.524128]\n",
      "[D loss: 0.925919] [G loss: 1.358024]\n",
      "[D loss: 1.033092] [G loss: 1.429286]\n",
      "[D loss: 0.914358] [G loss: 1.197823]\n",
      "[D loss: 0.858206] [G loss: 1.146381]\n",
      "[D loss: 0.505417] [G loss: 1.644777]\n",
      "[D loss: 1.109086] [G loss: 1.344140]\n",
      "[D loss: 1.047346] [G loss: 1.546008]\n",
      "[D loss: 0.902381] [G loss: 1.577048]\n",
      "[D loss: 0.786691] [G loss: 1.801050]\n",
      "[D loss: 0.842738] [G loss: 1.486219]\n",
      "[D loss: 0.928184] [G loss: 1.231468]\n",
      "[D loss: 0.673506] [G loss: 1.338919]\n",
      "[D loss: 0.835837] [G loss: 1.451840]\n",
      "[D loss: 0.962404] [G loss: 1.559873]\n",
      "[D loss: 0.990853] [G loss: 1.346660]\n",
      "[D loss: 0.851308] [G loss: 1.449609]\n",
      "[D loss: 0.853124] [G loss: 1.400477]\n",
      "[D loss: 0.860707] [G loss: 1.208268]\n",
      "[D loss: 0.839890] [G loss: 1.521662]\n",
      "[D loss: 0.814825] [G loss: 1.528977]\n",
      "[D loss: 0.865284] [G loss: 1.613590]\n",
      "[D loss: 1.178979] [G loss: 1.156545]\n",
      "[D loss: 0.798203] [G loss: 1.351797]\n",
      "[D loss: 0.896199] [G loss: 1.379033]\n",
      "[D loss: 0.843236] [G loss: 1.537709]\n",
      "[D loss: 0.857564] [G loss: 1.435643]\n",
      "[D loss: 0.635052] [G loss: 1.641650]\n",
      "[D loss: 0.702362] [G loss: 1.638174]\n",
      "[D loss: 0.768082] [G loss: 1.388100]\n",
      "[D loss: 0.846677] [G loss: 1.401978]\n",
      "[D loss: 1.039352] [G loss: 1.462065]\n",
      "[D loss: 1.108631] [G loss: 1.466940]\n",
      "[D loss: 0.948293] [G loss: 1.394369]\n",
      "[D loss: 0.648428] [G loss: 1.356253]\n",
      "[D loss: 0.979098] [G loss: 1.464128]\n",
      "[D loss: 0.801612] [G loss: 1.502106]\n",
      "[D loss: 0.923594] [G loss: 1.422454]\n",
      "[D loss: 0.845482] [G loss: 1.615324]\n",
      "[D loss: 0.583160] [G loss: 1.469172]\n",
      "[D loss: 0.649104] [G loss: 1.470593]\n",
      "[D loss: 0.889737] [G loss: 1.263591]\n",
      "[D loss: 0.778879] [G loss: 1.488632]\n",
      "[D loss: 0.938236] [G loss: 1.373749]\n",
      "[D loss: 0.781481] [G loss: 1.420725]\n",
      "[D loss: 0.902406] [G loss: 1.441311]\n",
      "[D loss: 0.729489] [G loss: 1.552206]\n",
      "[D loss: 0.752493] [G loss: 1.515164]\n",
      "[D loss: 0.784430] [G loss: 1.428688]\n",
      "[D loss: 1.027377] [G loss: 1.383778]\n",
      "[D loss: 0.856712] [G loss: 1.529039]\n",
      "[D loss: 0.856089] [G loss: 1.345882]\n",
      "[D loss: 0.839028] [G loss: 1.396741]\n",
      "[D loss: 0.882950] [G loss: 1.373973]\n",
      "[D loss: 0.768615] [G loss: 1.372467]\n",
      "[D loss: 0.879474] [G loss: 1.505602]\n",
      "[D loss: 0.819948] [G loss: 1.573814]\n",
      "[D loss: 0.928507] [G loss: 1.414181]\n",
      "[D loss: 0.611463] [G loss: 1.495924]\n",
      "[D loss: 0.789872] [G loss: 1.469768]\n",
      "[D loss: 0.944587] [G loss: 1.506605]\n",
      "[D loss: 0.810972] [G loss: 1.401357]\n",
      "[D loss: 0.629296] [G loss: 1.511689]\n",
      "[D loss: 0.770718] [G loss: 1.601350]\n",
      "[D loss: 0.976146] [G loss: 1.380313]\n",
      "[D loss: 1.075059] [G loss: 1.389000]\n",
      "[D loss: 0.800104] [G loss: 1.699175]\n",
      "[D loss: 0.712674] [G loss: 1.882540]\n",
      "[D loss: 0.682357] [G loss: 1.502616]\n",
      "[D loss: 1.013680] [G loss: 1.451830]\n",
      "[D loss: 0.853444] [G loss: 1.509197]\n",
      "[D loss: 1.087651] [G loss: 1.177430]\n",
      "[D loss: 0.598467] [G loss: 1.677329]\n",
      "[D loss: 0.689382] [G loss: 1.330173]\n",
      "[D loss: 0.793333] [G loss: 1.402682]\n",
      "[D loss: 1.079120] [G loss: 1.374345]\n",
      "[D loss: 0.924675] [G loss: 1.445483]\n",
      "[D loss: 0.982092] [G loss: 1.441287]\n",
      "[D loss: 0.976115] [G loss: 1.312625]\n",
      "[D loss: 0.902065] [G loss: 1.387060]\n",
      "[D loss: 0.887010] [G loss: 1.498567]\n",
      "[D loss: 0.660192] [G loss: 1.543344]\n",
      "[D loss: 0.889436] [G loss: 1.356261]\n",
      "[D loss: 0.708906] [G loss: 1.524706]\n",
      "[D loss: 0.735524] [G loss: 1.405365]\n",
      "[D loss: 1.040304] [G loss: 1.371286]\n",
      "[D loss: 0.744599] [G loss: 1.487601]\n",
      "[D loss: 0.783280] [G loss: 1.662595]\n",
      "[D loss: 1.043527] [G loss: 1.426963]\n",
      "[D loss: 0.738742] [G loss: 1.485711]\n",
      "[D loss: 0.902537] [G loss: 1.396022]\n",
      "[D loss: 0.869934] [G loss: 1.435822]\n",
      "[D loss: 0.626353] [G loss: 1.341063]\n",
      "[D loss: 0.747023] [G loss: 1.338257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.838023] [G loss: 1.450629]\n",
      "[D loss: 1.088847] [G loss: 1.324690]\n",
      "[D loss: 0.877936] [G loss: 1.245940]\n",
      "[D loss: 0.721158] [G loss: 1.566659]\n",
      "[D loss: 0.737350] [G loss: 1.522194]\n",
      "[D loss: 1.000338] [G loss: 1.404003]\n",
      "[D loss: 0.720316] [G loss: 1.578015]\n",
      "[D loss: 0.797443] [G loss: 1.413539]\n",
      "[D loss: 0.699741] [G loss: 1.282885]\n",
      "[D loss: 0.737458] [G loss: 1.446259]\n",
      "[D loss: 0.767017] [G loss: 1.537745]\n",
      "[D loss: 1.098077] [G loss: 1.330841]\n",
      "[D loss: 0.765730] [G loss: 1.476870]\n",
      "[D loss: 0.768314] [G loss: 1.489485]\n",
      "[D loss: 0.821026] [G loss: 1.595891]\n",
      "[D loss: 0.663518] [G loss: 1.445263]\n",
      "[D loss: 0.761265] [G loss: 1.436143]\n",
      "[D loss: 0.673414] [G loss: 1.441768]\n",
      "[D loss: 0.942264] [G loss: 1.507544]\n",
      "[D loss: 0.936234] [G loss: 1.413684]\n",
      "[D loss: 1.039347] [G loss: 1.448147]\n",
      "[D loss: 0.887701] [G loss: 1.270932]\n",
      "[D loss: 0.900427] [G loss: 1.392672]\n",
      "[D loss: 0.834628] [G loss: 1.412020]\n",
      "[D loss: 0.803131] [G loss: 1.553575]\n",
      "[D loss: 0.861217] [G loss: 1.442507]\n",
      "[D loss: 0.955818] [G loss: 1.376769]\n",
      "[D loss: 0.712921] [G loss: 1.402674]\n",
      "[D loss: 0.794824] [G loss: 1.455489]\n",
      "[D loss: 0.806049] [G loss: 1.519794]\n",
      "[D loss: 0.732200] [G loss: 1.494347]\n",
      "[D loss: 0.625166] [G loss: 1.618077]\n",
      "[D loss: 0.860804] [G loss: 1.416594]\n",
      "[D loss: 0.810436] [G loss: 1.365795]\n",
      "[D loss: 0.872595] [G loss: 1.425701]\n",
      "[D loss: 0.956560] [G loss: 1.487138]\n",
      "[D loss: 0.654191] [G loss: 1.600251]\n",
      "[D loss: 0.678602] [G loss: 1.432258]\n",
      "[D loss: 0.792063] [G loss: 1.553789]\n",
      "[D loss: 0.859295] [G loss: 1.424961]\n",
      "[D loss: 0.755247] [G loss: 1.415942]\n",
      "[D loss: 0.730002] [G loss: 1.570609]\n",
      "[D loss: 0.741615] [G loss: 1.579573]\n",
      "[D loss: 1.107720] [G loss: 1.482790]\n",
      "[D loss: 0.893455] [G loss: 1.390897]\n",
      "[D loss: 0.951088] [G loss: 1.343854]\n",
      "[D loss: 0.643128] [G loss: 1.339716]\n",
      "[D loss: 0.977114] [G loss: 1.497354]\n",
      "[D loss: 0.760157] [G loss: 1.501476]\n",
      "[D loss: 0.848570] [G loss: 1.178219]\n",
      "[D loss: 0.774789] [G loss: 1.274360]\n",
      "[D loss: 0.803149] [G loss: 1.334726]\n",
      "[D loss: 0.663537] [G loss: 1.387042]\n",
      "[D loss: 0.771708] [G loss: 1.298956]\n",
      "[D loss: 0.985292] [G loss: 1.512408]\n",
      "[D loss: 0.808686] [G loss: 1.423842]\n",
      "[D loss: 0.797595] [G loss: 1.530176]\n",
      "[D loss: 0.800463] [G loss: 1.426182]\n",
      "[D loss: 0.841934] [G loss: 1.392455]\n",
      "[D loss: 0.712962] [G loss: 1.469634]\n",
      "[D loss: 0.617314] [G loss: 1.621896]\n",
      "[D loss: 0.729754] [G loss: 1.636394]\n",
      "[D loss: 0.806374] [G loss: 1.467877]\n",
      "[D loss: 0.944685] [G loss: 1.500832]\n",
      "[D loss: 0.810923] [G loss: 1.423070]\n",
      "[D loss: 0.798358] [G loss: 1.303702]\n",
      "[D loss: 0.770843] [G loss: 1.415170]\n",
      "[D loss: 0.901052] [G loss: 1.697615]\n",
      "[D loss: 1.112781] [G loss: 1.534850]\n",
      "[D loss: 0.924034] [G loss: 1.607633]\n",
      "[D loss: 1.165881] [G loss: 1.278967]\n",
      "[D loss: 1.006233] [G loss: 1.374664]\n",
      "[D loss: 1.075392] [G loss: 1.536428]\n",
      "[D loss: 0.776793] [G loss: 1.289671]\n",
      "[D loss: 0.738474] [G loss: 1.404163]\n",
      "[D loss: 0.811281] [G loss: 1.322294]\n",
      "[D loss: 0.943078] [G loss: 1.397188]\n",
      "[D loss: 0.922178] [G loss: 1.372113]\n",
      "[D loss: 0.786868] [G loss: 1.378331]\n",
      "[D loss: 0.895800] [G loss: 1.243317]\n",
      "[D loss: 0.880194] [G loss: 1.325841]\n",
      "[D loss: 0.758048] [G loss: 1.566982]\n",
      "[D loss: 0.847479] [G loss: 1.632548]\n",
      "[D loss: 0.678672] [G loss: 1.736232]\n",
      "[D loss: 0.879883] [G loss: 1.467964]\n",
      "[D loss: 0.838203] [G loss: 1.562892]\n",
      "[D loss: 0.619848] [G loss: 1.581470]\n",
      "[D loss: 0.765874] [G loss: 1.401022]\n",
      "[D loss: 0.855782] [G loss: 1.376923]\n",
      "[D loss: 0.918925] [G loss: 1.619188]\n",
      "[D loss: 0.748615] [G loss: 1.546137]\n",
      "[D loss: 0.739163] [G loss: 1.520676]\n",
      "[D loss: 0.752151] [G loss: 1.622781]\n",
      "[D loss: 0.583825] [G loss: 1.567391]\n",
      "[D loss: 0.816194] [G loss: 1.350967]\n",
      "[D loss: 0.791628] [G loss: 1.663278]\n",
      "[D loss: 0.915679] [G loss: 1.555345]\n",
      "[D loss: 1.308317] [G loss: 1.205265]\n",
      "[D loss: 0.950059] [G loss: 1.458055]\n",
      "[D loss: 0.826285] [G loss: 1.460889]\n",
      "[D loss: 0.697363] [G loss: 1.350391]\n",
      "[D loss: 0.709903] [G loss: 1.522288]\n",
      "[D loss: 0.933391] [G loss: 1.298266]\n",
      "[D loss: 0.801514] [G loss: 1.470939]\n",
      "[D loss: 0.735484] [G loss: 1.426594]\n",
      "[D loss: 0.768720] [G loss: 1.649836]\n",
      "[D loss: 0.767726] [G loss: 1.468986]\n",
      "[D loss: 0.636222] [G loss: 1.422796]\n",
      "[D loss: 0.973635] [G loss: 1.251279]\n",
      "[D loss: 1.065304] [G loss: 1.182074]\n",
      "[D loss: 0.824472] [G loss: 1.539416]\n",
      "[D loss: 0.869726] [G loss: 1.531170]\n",
      "[D loss: 0.791447] [G loss: 1.700920]\n",
      "[D loss: 0.988173] [G loss: 1.450529]\n",
      "[D loss: 0.906091] [G loss: 1.342103]\n",
      "[D loss: 0.985537] [G loss: 1.396067]\n",
      "[D loss: 0.641529] [G loss: 1.479344]\n",
      "[D loss: 0.659865] [G loss: 1.594502]\n",
      "[D loss: 0.891582] [G loss: 1.542612]\n",
      "[D loss: 0.734917] [G loss: 1.506547]\n",
      "[D loss: 0.936286] [G loss: 1.496289]\n",
      "[D loss: 0.940228] [G loss: 1.246148]\n",
      "[D loss: 0.941230] [G loss: 1.435544]\n",
      "[D loss: 0.948683] [G loss: 1.280502]\n",
      "[D loss: 0.721358] [G loss: 1.501167]\n",
      "[D loss: 0.756938] [G loss: 1.370049]\n",
      "[D loss: 0.844822] [G loss: 1.321410]\n",
      "[D loss: 0.993047] [G loss: 1.340563]\n",
      "[D loss: 0.826216] [G loss: 1.622581]\n",
      "[D loss: 0.830442] [G loss: 1.301658]\n",
      "[D loss: 0.763140] [G loss: 1.445372]\n",
      "[D loss: 1.075243] [G loss: 1.202397]\n",
      "[D loss: 0.743609] [G loss: 1.432505]\n",
      "[D loss: 0.623081] [G loss: 1.601371]\n",
      "[D loss: 0.783967] [G loss: 1.637558]\n",
      "[D loss: 1.055055] [G loss: 1.532284]\n",
      "[D loss: 0.640324] [G loss: 1.545430]\n",
      "[D loss: 0.590586] [G loss: 1.595788]\n",
      "[D loss: 0.711826] [G loss: 1.562197]\n",
      "[D loss: 0.975448] [G loss: 1.394278]\n",
      "[D loss: 0.621074] [G loss: 1.513128]\n",
      "[D loss: 1.077913] [G loss: 1.484240]\n",
      "[D loss: 0.775360] [G loss: 1.579782]\n",
      "[D loss: 0.978124] [G loss: 1.418011]\n",
      "[D loss: 0.797497] [G loss: 1.603673]\n",
      "[D loss: 0.884966] [G loss: 1.460326]\n",
      "[D loss: 1.030275] [G loss: 1.563488]\n",
      "[D loss: 1.005485] [G loss: 1.346965]\n",
      "[D loss: 0.912348] [G loss: 1.390457]\n",
      "[D loss: 0.896320] [G loss: 1.515040]\n",
      "[D loss: 0.739807] [G loss: 1.552611]\n",
      "[D loss: 0.765031] [G loss: 1.614019]\n",
      "[D loss: 1.067611] [G loss: 1.341791]\n",
      "[D loss: 0.759053] [G loss: 1.596850]\n",
      "[D loss: 0.690686] [G loss: 1.389622]\n",
      "[D loss: 0.726374] [G loss: 1.359187]\n",
      "[D loss: 0.732994] [G loss: 1.522173]\n",
      "[D loss: 0.846664] [G loss: 1.588351]\n",
      "[D loss: 0.747504] [G loss: 1.536928]\n",
      "[D loss: 0.903130] [G loss: 1.391382]\n",
      "[D loss: 0.823863] [G loss: 1.487981]\n",
      "[D loss: 0.669819] [G loss: 1.246398]\n",
      "[D loss: 0.753730] [G loss: 1.539734]\n",
      "[D loss: 1.005465] [G loss: 1.359708]\n",
      "[D loss: 1.054226] [G loss: 1.362470]\n",
      "[D loss: 0.936606] [G loss: 1.710103]\n",
      "[D loss: 1.052608] [G loss: 1.373011]\n",
      "[D loss: 0.787560] [G loss: 1.419689]\n",
      "[D loss: 0.700635] [G loss: 1.431163]\n",
      "[D loss: 0.902465] [G loss: 1.430157]\n",
      "[D loss: 0.718960] [G loss: 1.444414]\n",
      "[D loss: 0.850483] [G loss: 1.560419]\n",
      "[D loss: 0.767555] [G loss: 1.504621]\n",
      "[D loss: 0.631326] [G loss: 1.755863]\n",
      "[D loss: 0.992279] [G loss: 1.555564]\n",
      "[D loss: 0.996513] [G loss: 1.222643]\n",
      "[D loss: 0.848855] [G loss: 1.483204]\n",
      "[D loss: 0.989468] [G loss: 1.583988]\n",
      "[D loss: 0.637067] [G loss: 1.431528]\n",
      "[D loss: 0.726045] [G loss: 1.486960]\n",
      "[D loss: 0.739852] [G loss: 1.551096]\n",
      "[D loss: 1.083106] [G loss: 1.483882]\n",
      "[D loss: 0.853292] [G loss: 1.568931]\n",
      "[D loss: 0.903415] [G loss: 1.132328]\n",
      "[D loss: 0.807887] [G loss: 1.544110]\n",
      "[D loss: 0.985112] [G loss: 1.476563]\n",
      "[D loss: 0.641818] [G loss: 1.309652]\n",
      "[D loss: 1.170234] [G loss: 1.227943]\n",
      "[D loss: 0.907507] [G loss: 1.342915]\n",
      "[D loss: 0.817616] [G loss: 1.329982]\n",
      "[D loss: 0.752426] [G loss: 1.326256]\n",
      "[D loss: 0.741125] [G loss: 1.595027]\n",
      "[D loss: 0.815776] [G loss: 1.395901]\n",
      "[D loss: 0.875529] [G loss: 1.475632]\n",
      "[D loss: 0.694474] [G loss: 1.271541]\n",
      "[D loss: 0.761486] [G loss: 1.337056]\n",
      "[D loss: 0.845947] [G loss: 1.498716]\n",
      "[D loss: 0.917393] [G loss: 1.565029]\n",
      "[D loss: 0.779613] [G loss: 1.574961]\n",
      "[D loss: 0.877238] [G loss: 1.378058]\n",
      "[D loss: 0.561087] [G loss: 1.400719]\n",
      "[D loss: 1.210358] [G loss: 1.384302]\n",
      "[D loss: 0.591072] [G loss: 1.587593]\n",
      "[D loss: 0.794515] [G loss: 1.396559]\n",
      "[D loss: 0.860339] [G loss: 1.511086]\n",
      "[D loss: 0.986535] [G loss: 1.502443]\n",
      "[D loss: 1.044472] [G loss: 1.348608]\n",
      "[D loss: 0.787819] [G loss: 1.281287]\n",
      "[D loss: 0.858621] [G loss: 1.373894]\n",
      "[D loss: 0.925560] [G loss: 1.487699]\n",
      "[D loss: 0.767784] [G loss: 1.319777]\n",
      "[D loss: 0.772277] [G loss: 1.419535]\n",
      "[D loss: 1.032585] [G loss: 1.443396]\n",
      "[D loss: 0.765616] [G loss: 1.397058]\n",
      "[D loss: 0.929979] [G loss: 1.311075]\n",
      "[D loss: 0.829849] [G loss: 1.554572]\n",
      "[D loss: 0.805209] [G loss: 1.561466]\n",
      "[D loss: 0.833450] [G loss: 1.291278]\n",
      "[D loss: 0.778348] [G loss: 1.659816]\n",
      "[D loss: 0.701399] [G loss: 1.370203]\n",
      "[D loss: 0.773525] [G loss: 1.550013]\n",
      "[D loss: 0.862730] [G loss: 1.316423]\n",
      "[D loss: 0.829078] [G loss: 1.334450]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.762420] [G loss: 1.652236]\n",
      "[D loss: 0.697176] [G loss: 1.948909]\n",
      "[D loss: 0.932675] [G loss: 1.463735]\n",
      "[D loss: 0.898660] [G loss: 1.685950]\n",
      "[D loss: 0.880610] [G loss: 1.439481]\n",
      "[D loss: 0.902374] [G loss: 1.309328]\n",
      "[D loss: 1.114701] [G loss: 1.403375]\n",
      "[D loss: 0.883705] [G loss: 1.564677]\n",
      "[D loss: 0.720691] [G loss: 1.435295]\n",
      "[D loss: 0.672528] [G loss: 1.579480]\n",
      "[D loss: 0.961159] [G loss: 1.456319]\n",
      "[D loss: 0.735971] [G loss: 1.588816]\n",
      "[D loss: 0.856097] [G loss: 1.418507]\n",
      "[D loss: 0.686128] [G loss: 1.468514]\n",
      "[D loss: 0.804108] [G loss: 1.278162]\n",
      "[D loss: 1.033843] [G loss: 1.343131]\n",
      "[D loss: 0.789559] [G loss: 1.431908]\n",
      "[D loss: 1.100747] [G loss: 1.303211]\n",
      "[D loss: 0.801553] [G loss: 1.411098]\n",
      "[D loss: 0.885078] [G loss: 1.470214]\n",
      "[D loss: 0.731493] [G loss: 1.560602]\n",
      "[D loss: 0.862424] [G loss: 1.644658]\n",
      "[D loss: 0.799145] [G loss: 1.282315]\n",
      "[D loss: 0.691990] [G loss: 1.430653]\n",
      "[D loss: 0.827079] [G loss: 1.722981]\n",
      "[D loss: 0.791044] [G loss: 1.522606]\n",
      "[D loss: 0.871252] [G loss: 1.512834]\n",
      "[D loss: 0.591409] [G loss: 1.489589]\n",
      "[D loss: 0.825345] [G loss: 1.460513]\n",
      "[D loss: 0.913392] [G loss: 1.580134]\n",
      "[D loss: 0.844836] [G loss: 1.391740]\n",
      "[D loss: 0.819599] [G loss: 1.429552]\n",
      "[D loss: 1.025933] [G loss: 1.375495]\n",
      "[D loss: 0.954337] [G loss: 1.236637]\n",
      "[D loss: 0.886633] [G loss: 1.193998]\n",
      "[D loss: 0.691386] [G loss: 1.406724]\n",
      "[D loss: 0.612621] [G loss: 1.420244]\n",
      "[D loss: 1.091198] [G loss: 1.445162]\n",
      "[D loss: 0.864120] [G loss: 1.523752]\n",
      "[D loss: 0.960663] [G loss: 1.470733]\n",
      "[D loss: 0.572632] [G loss: 1.582797]\n",
      "[D loss: 0.557673] [G loss: 1.498780]\n",
      "[D loss: 0.684162] [G loss: 1.433143]\n",
      "[D loss: 0.862430] [G loss: 1.408840]\n",
      "[D loss: 0.702987] [G loss: 1.396152]\n",
      "[D loss: 0.888353] [G loss: 1.613235]\n",
      "[D loss: 1.170844] [G loss: 1.386676]\n",
      "[D loss: 0.947217] [G loss: 1.625922]\n",
      "[D loss: 0.869562] [G loss: 1.412582]\n",
      "[D loss: 1.006210] [G loss: 1.548916]\n",
      "[D loss: 0.962863] [G loss: 1.440461]\n",
      "[D loss: 0.841543] [G loss: 1.577054]\n",
      "[D loss: 0.985928] [G loss: 1.381318]\n",
      "[D loss: 0.811177] [G loss: 1.284135]\n",
      "[D loss: 0.777257] [G loss: 1.526606]\n",
      "[D loss: 0.891563] [G loss: 1.410075]\n",
      "[D loss: 0.922510] [G loss: 1.189716]\n",
      "[D loss: 0.906618] [G loss: 1.300833]\n",
      "[D loss: 0.844239] [G loss: 1.605753]\n",
      "[D loss: 0.863833] [G loss: 1.309968]\n",
      "[D loss: 1.046666] [G loss: 1.670871]\n",
      "[D loss: 0.865636] [G loss: 1.326619]\n",
      "[D loss: 0.651698] [G loss: 1.458867]\n",
      "[D loss: 0.949232] [G loss: 1.323837]\n",
      "[D loss: 0.933326] [G loss: 1.212685]\n",
      "[D loss: 0.641971] [G loss: 1.434351]\n",
      "[D loss: 0.646191] [G loss: 1.246801]\n",
      "[D loss: 0.748600] [G loss: 1.429864]\n",
      "[D loss: 0.780144] [G loss: 1.681556]\n",
      "[D loss: 0.956273] [G loss: 1.352786]\n",
      "[D loss: 0.709962] [G loss: 1.544745]\n",
      "[D loss: 0.731408] [G loss: 1.289354]\n",
      "[D loss: 0.785864] [G loss: 1.493545]\n",
      "[D loss: 0.658213] [G loss: 1.587088]\n",
      "[D loss: 0.937070] [G loss: 1.436597]\n",
      "[D loss: 1.002264] [G loss: 1.539761]\n",
      "[D loss: 1.045637] [G loss: 1.491723]\n",
      "[D loss: 0.999871] [G loss: 1.351477]\n",
      "[D loss: 0.800316] [G loss: 1.451653]\n",
      "[D loss: 0.725171] [G loss: 1.462774]\n",
      "[D loss: 0.908588] [G loss: 1.322877]\n",
      "[D loss: 0.838501] [G loss: 1.591315]\n",
      "[D loss: 0.747649] [G loss: 1.387694]\n",
      "[D loss: 0.757092] [G loss: 1.280667]\n",
      "[D loss: 0.877809] [G loss: 1.272656]\n",
      "[D loss: 0.741671] [G loss: 1.274482]\n",
      "[D loss: 0.796045] [G loss: 1.301430]\n",
      "[D loss: 0.842628] [G loss: 1.379350]\n",
      "[D loss: 0.837490] [G loss: 1.466864]\n",
      "[D loss: 0.812262] [G loss: 1.492398]\n",
      "[D loss: 0.744685] [G loss: 1.442645]\n",
      "[D loss: 0.917757] [G loss: 1.683056]\n",
      "[D loss: 0.827576] [G loss: 1.390330]\n",
      "[D loss: 0.772808] [G loss: 1.397089]\n",
      "[D loss: 0.723418] [G loss: 1.453435]\n",
      "[D loss: 0.776176] [G loss: 1.403916]\n",
      "[D loss: 0.684310] [G loss: 1.476878]\n",
      "[D loss: 0.613499] [G loss: 1.289440]\n",
      "[D loss: 0.862654] [G loss: 1.620331]\n",
      "[D loss: 0.770709] [G loss: 1.695606]\n",
      "[D loss: 0.873606] [G loss: 1.609187]\n",
      "[D loss: 0.865100] [G loss: 1.461450]\n",
      "[D loss: 0.861431] [G loss: 1.434493]\n",
      "[D loss: 0.985173] [G loss: 1.365451]\n",
      "[D loss: 0.910307] [G loss: 1.492296]\n",
      "[D loss: 0.706567] [G loss: 1.760939]\n",
      "[D loss: 0.796869] [G loss: 1.678241]\n",
      "[D loss: 0.765253] [G loss: 1.497331]\n",
      "[D loss: 0.762143] [G loss: 1.239485]\n",
      "[D loss: 0.899781] [G loss: 1.441240]\n",
      "[D loss: 0.784477] [G loss: 1.560791]\n",
      "[D loss: 0.689584] [G loss: 1.436265]\n",
      "[D loss: 0.697561] [G loss: 1.550150]\n",
      "[D loss: 0.792840] [G loss: 1.614654]\n",
      "[D loss: 0.631157] [G loss: 1.595195]\n",
      "[D loss: 0.906373] [G loss: 1.502955]\n",
      "[D loss: 0.872572] [G loss: 1.453168]\n",
      "[D loss: 0.770187] [G loss: 1.687302]\n",
      "[D loss: 0.704622] [G loss: 1.432840]\n",
      "[D loss: 0.886401] [G loss: 1.689677]\n",
      "[D loss: 0.917275] [G loss: 1.510453]\n",
      "[D loss: 0.729408] [G loss: 1.594786]\n",
      "[D loss: 0.818231] [G loss: 1.724349]\n",
      "[D loss: 0.741194] [G loss: 1.449610]\n",
      "[D loss: 0.887753] [G loss: 1.420912]\n",
      "[D loss: 0.784086] [G loss: 1.460686]\n",
      "[D loss: 0.702975] [G loss: 1.367525]\n",
      "[D loss: 0.781547] [G loss: 1.453481]\n",
      "[D loss: 0.714651] [G loss: 1.597462]\n",
      "[D loss: 0.834696] [G loss: 1.489685]\n",
      "[D loss: 0.947029] [G loss: 1.417986]\n",
      "[D loss: 0.891900] [G loss: 1.319738]\n",
      "[D loss: 0.892884] [G loss: 1.534441]\n",
      "[D loss: 0.790689] [G loss: 1.600633]\n",
      "[D loss: 0.895665] [G loss: 1.720794]\n",
      "[D loss: 0.658785] [G loss: 1.418011]\n",
      "[D loss: 0.751557] [G loss: 1.422908]\n",
      "[D loss: 0.663887] [G loss: 1.443902]\n",
      "[D loss: 0.677692] [G loss: 1.547165]\n",
      "[D loss: 0.715729] [G loss: 1.584444]\n",
      "[D loss: 0.867243] [G loss: 1.686828]\n",
      "[D loss: 0.901022] [G loss: 1.589046]\n",
      "[D loss: 1.119047] [G loss: 1.674894]\n",
      "[D loss: 0.946725] [G loss: 1.382137]\n",
      "[D loss: 0.898333] [G loss: 1.453829]\n",
      "[D loss: 0.811524] [G loss: 1.446358]\n",
      "[D loss: 0.861033] [G loss: 1.476945]\n",
      "[D loss: 0.985493] [G loss: 1.214443]\n",
      "[D loss: 0.982965] [G loss: 1.295294]\n",
      "[D loss: 0.824550] [G loss: 1.493563]\n",
      "[D loss: 0.821823] [G loss: 1.427833]\n",
      "[D loss: 0.749064] [G loss: 1.483969]\n",
      "[D loss: 1.037227] [G loss: 1.382547]\n",
      "[D loss: 0.874358] [G loss: 1.262806]\n",
      "[D loss: 0.752568] [G loss: 1.260087]\n",
      "[D loss: 0.949935] [G loss: 1.487495]\n",
      "[D loss: 0.729323] [G loss: 1.588625]\n",
      "[D loss: 0.630679] [G loss: 1.581774]\n",
      "[D loss: 0.810017] [G loss: 1.555346]\n",
      "[D loss: 0.886746] [G loss: 1.472346]\n",
      "[D loss: 0.993120] [G loss: 1.330793]\n",
      "[D loss: 0.605554] [G loss: 1.659603]\n",
      "[D loss: 0.938181] [G loss: 1.829550]\n",
      "[D loss: 0.847138] [G loss: 1.479559]\n",
      "[D loss: 0.893390] [G loss: 1.418489]\n",
      "[D loss: 0.725050] [G loss: 1.453727]\n",
      "[D loss: 0.782040] [G loss: 1.653445]\n",
      "[D loss: 0.785963] [G loss: 1.721440]\n",
      "[D loss: 0.717180] [G loss: 1.478481]\n",
      "[D loss: 0.823429] [G loss: 1.470305]\n",
      "[D loss: 0.857002] [G loss: 1.553483]\n",
      "[D loss: 0.947836] [G loss: 1.444729]\n",
      "[D loss: 0.910339] [G loss: 1.356895]\n",
      "[D loss: 0.770858] [G loss: 1.365359]\n",
      "[D loss: 0.767754] [G loss: 1.304222]\n",
      "[D loss: 0.609437] [G loss: 1.536170]\n",
      "[D loss: 0.657251] [G loss: 1.358956]\n",
      "[D loss: 0.825751] [G loss: 1.486184]\n",
      "[D loss: 0.802359] [G loss: 1.441916]\n",
      "[D loss: 0.762480] [G loss: 1.617138]\n",
      "[D loss: 0.842383] [G loss: 1.594537]\n",
      "[D loss: 0.630398] [G loss: 1.736953]\n",
      "[D loss: 0.565813] [G loss: 1.748177]\n",
      "[D loss: 0.811107] [G loss: 1.557600]\n",
      "[D loss: 0.667122] [G loss: 1.727529]\n",
      "[D loss: 0.743146] [G loss: 1.593020]\n",
      "[D loss: 1.038602] [G loss: 1.365622]\n",
      "[D loss: 0.752284] [G loss: 1.634745]\n",
      "[D loss: 0.704806] [G loss: 1.360408]\n",
      "[D loss: 0.838534] [G loss: 1.596916]\n",
      "[D loss: 0.920167] [G loss: 1.468317]\n",
      "[D loss: 1.108002] [G loss: 1.469073]\n",
      "[D loss: 0.907611] [G loss: 1.475035]\n",
      "[D loss: 0.744658] [G loss: 1.482046]\n",
      "[D loss: 0.987098] [G loss: 1.420411]\n",
      "[D loss: 0.833719] [G loss: 1.455346]\n",
      "[D loss: 1.043011] [G loss: 1.264710]\n",
      "[D loss: 0.857728] [G loss: 1.528018]\n",
      "[D loss: 0.734702] [G loss: 1.495230]\n",
      "[D loss: 1.070451] [G loss: 1.286090]\n",
      "[D loss: 0.916427] [G loss: 1.415926]\n",
      "[D loss: 1.018207] [G loss: 1.400870]\n",
      "[D loss: 0.762735] [G loss: 1.451792]\n",
      "[D loss: 0.877304] [G loss: 1.295315]\n",
      "[D loss: 0.822299] [G loss: 1.383839]\n",
      "[D loss: 0.685606] [G loss: 1.344544]\n",
      "[D loss: 0.788857] [G loss: 1.430185]\n",
      "[D loss: 0.992450] [G loss: 1.385317]\n",
      "[D loss: 0.771283] [G loss: 1.290123]\n",
      "[D loss: 0.998204] [G loss: 1.395998]\n",
      "[D loss: 0.925778] [G loss: 1.443681]\n",
      "[D loss: 1.091578] [G loss: 1.393787]\n",
      "[D loss: 0.744343] [G loss: 1.382379]\n",
      "[D loss: 0.583944] [G loss: 1.524326]\n",
      "[D loss: 0.738154] [G loss: 1.479138]\n",
      "[D loss: 0.950977] [G loss: 1.481259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.743945] [G loss: 1.568788]\n",
      "[D loss: 0.949646] [G loss: 1.359914]\n",
      "[D loss: 1.016954] [G loss: 1.248231]\n",
      "[D loss: 0.772342] [G loss: 1.390845]\n",
      "[D loss: 0.693673] [G loss: 1.735448]\n",
      "[D loss: 0.883188] [G loss: 1.384414]\n",
      "[D loss: 0.580897] [G loss: 1.526266]\n",
      "[D loss: 1.073692] [G loss: 1.341079]\n",
      "[D loss: 1.037228] [G loss: 1.336901]\n",
      "[D loss: 0.814475] [G loss: 1.404582]\n",
      "[D loss: 0.778492] [G loss: 1.634143]\n",
      "[D loss: 0.835101] [G loss: 1.719817]\n",
      "[D loss: 0.648681] [G loss: 1.483187]\n",
      "[D loss: 0.795321] [G loss: 1.341955]\n",
      "[D loss: 0.993211] [G loss: 1.421885]\n",
      "[D loss: 0.734830] [G loss: 1.349432]\n",
      "[D loss: 0.734734] [G loss: 1.520629]\n",
      "[D loss: 0.725170] [G loss: 1.585022]\n",
      "[D loss: 0.648264] [G loss: 1.457506]\n",
      "[D loss: 0.720735] [G loss: 1.206230]\n",
      "[D loss: 0.888223] [G loss: 1.319043]\n",
      "[D loss: 0.836760] [G loss: 1.562669]\n",
      "[D loss: 0.859462] [G loss: 1.588872]\n",
      "[D loss: 0.816209] [G loss: 1.360917]\n",
      "[D loss: 0.891850] [G loss: 1.106377]\n",
      "[D loss: 0.603754] [G loss: 1.849388]\n",
      "[D loss: 0.809527] [G loss: 1.570450]\n",
      "[D loss: 0.965996] [G loss: 1.471503]\n",
      "[D loss: 0.990486] [G loss: 1.430855]\n",
      "[D loss: 0.846804] [G loss: 1.435897]\n",
      "[D loss: 0.690937] [G loss: 1.582270]\n",
      "[D loss: 0.853664] [G loss: 1.372154]\n",
      "[D loss: 1.126402] [G loss: 1.434268]\n",
      "[D loss: 0.786545] [G loss: 1.469178]\n",
      "[D loss: 0.822649] [G loss: 1.470532]\n",
      "[D loss: 0.681459] [G loss: 1.620597]\n",
      "[D loss: 0.934891] [G loss: 1.610730]\n",
      "[D loss: 0.796999] [G loss: 1.460413]\n",
      "[D loss: 0.822395] [G loss: 1.497930]\n",
      "[D loss: 0.852872] [G loss: 1.514076]\n",
      "[D loss: 0.586065] [G loss: 1.850331]\n",
      "[D loss: 0.630146] [G loss: 1.799451]\n",
      "[D loss: 0.813325] [G loss: 1.543590]\n",
      "[D loss: 0.745980] [G loss: 1.676186]\n",
      "[D loss: 0.795129] [G loss: 1.629964]\n",
      "[D loss: 1.197302] [G loss: 1.203388]\n",
      "[D loss: 0.991119] [G loss: 1.334804]\n",
      "[D loss: 0.992211] [G loss: 1.505743]\n",
      "[D loss: 0.849939] [G loss: 1.557982]\n",
      "[D loss: 0.841248] [G loss: 1.537241]\n",
      "[D loss: 0.701958] [G loss: 1.328312]\n",
      "[D loss: 0.584799] [G loss: 1.262175]\n",
      "[D loss: 0.734210] [G loss: 1.511153]\n",
      "[D loss: 0.792395] [G loss: 1.528245]\n",
      "[D loss: 0.734175] [G loss: 1.612610]\n",
      "[D loss: 0.897992] [G loss: 1.389185]\n",
      "[D loss: 0.782728] [G loss: 1.399558]\n",
      "[D loss: 0.928301] [G loss: 1.510949]\n",
      "[D loss: 0.818032] [G loss: 1.730228]\n",
      "[D loss: 1.041068] [G loss: 1.548147]\n",
      "[D loss: 0.685969] [G loss: 1.631483]\n",
      "[D loss: 0.682873] [G loss: 1.352914]\n",
      "[D loss: 0.824599] [G loss: 1.333539]\n",
      "[D loss: 0.921754] [G loss: 1.627265]\n",
      "[D loss: 0.806365] [G loss: 1.532898]\n",
      "[D loss: 0.762248] [G loss: 1.444969]\n",
      "[D loss: 1.119649] [G loss: 1.418590]\n",
      "[D loss: 0.656666] [G loss: 1.608431]\n",
      "[D loss: 0.815804] [G loss: 1.484405]\n",
      "[D loss: 0.794729] [G loss: 1.514669]\n",
      "[D loss: 0.621921] [G loss: 1.584304]\n",
      "[D loss: 0.724094] [G loss: 1.343699]\n",
      "[D loss: 0.626209] [G loss: 1.667345]\n",
      "[D loss: 0.879534] [G loss: 1.483363]\n",
      "[D loss: 0.811714] [G loss: 1.613799]\n",
      "[D loss: 0.683864] [G loss: 1.764712]\n",
      "[D loss: 0.867818] [G loss: 1.476396]\n",
      "[D loss: 0.917353] [G loss: 1.509743]\n",
      "[D loss: 0.623841] [G loss: 1.359505]\n",
      "[D loss: 0.766459] [G loss: 1.440028]\n",
      "[D loss: 1.057710] [G loss: 1.767373]\n",
      "[D loss: 0.847857] [G loss: 1.496755]\n",
      "[D loss: 0.692906] [G loss: 1.439000]\n",
      "[D loss: 1.072566] [G loss: 1.373301]\n",
      "[D loss: 0.886140] [G loss: 1.539972]\n",
      "[D loss: 0.835201] [G loss: 1.786828]\n",
      "[D loss: 0.665230] [G loss: 1.590688]\n",
      "[D loss: 0.803505] [G loss: 1.559592]\n",
      "[D loss: 0.994779] [G loss: 1.382048]\n",
      "[D loss: 0.753965] [G loss: 1.506404]\n",
      "[D loss: 0.952831] [G loss: 1.422217]\n",
      "[D loss: 0.893027] [G loss: 1.547516]\n",
      "[D loss: 1.023941] [G loss: 1.305183]\n",
      "[D loss: 0.674111] [G loss: 1.890520]\n",
      "[D loss: 1.054963] [G loss: 1.393178]\n",
      "[D loss: 0.891549] [G loss: 1.379575]\n",
      "[D loss: 0.647784] [G loss: 1.740821]\n",
      "[D loss: 0.761202] [G loss: 1.374554]\n",
      "[D loss: 0.953770] [G loss: 1.531460]\n",
      "[D loss: 0.599463] [G loss: 1.662820]\n",
      "[D loss: 0.809440] [G loss: 1.782117]\n",
      "[D loss: 0.613955] [G loss: 1.570952]\n",
      "[D loss: 0.733753] [G loss: 1.409993]\n",
      "[D loss: 0.842776] [G loss: 1.491616]\n",
      "[D loss: 0.858762] [G loss: 1.541548]\n",
      "[D loss: 0.829564] [G loss: 1.353693]\n",
      "[D loss: 0.796546] [G loss: 1.453170]\n",
      "[D loss: 0.761966] [G loss: 1.598630]\n",
      "[D loss: 0.875762] [G loss: 1.391811]\n",
      "[D loss: 0.946766] [G loss: 1.447920]\n",
      "[D loss: 0.908151] [G loss: 1.398247]\n",
      "[D loss: 0.704430] [G loss: 1.560303]\n",
      "[D loss: 0.675955] [G loss: 1.673004]\n",
      "[D loss: 0.810009] [G loss: 1.477583]\n",
      "[D loss: 1.007441] [G loss: 1.409460]\n",
      "[D loss: 0.813255] [G loss: 1.664235]\n",
      "[D loss: 0.939810] [G loss: 1.283631]\n",
      "[D loss: 0.869954] [G loss: 1.440380]\n",
      "[D loss: 0.848163] [G loss: 1.453687]\n",
      "[D loss: 0.905789] [G loss: 1.323754]\n",
      "[D loss: 0.940186] [G loss: 1.299790]\n",
      "[D loss: 0.857198] [G loss: 1.426460]\n",
      "[D loss: 0.810540] [G loss: 1.327692]\n",
      "[D loss: 0.906385] [G loss: 1.473846]\n",
      "[D loss: 0.797295] [G loss: 1.269988]\n",
      "[D loss: 0.943657] [G loss: 1.408294]\n",
      "[D loss: 0.773533] [G loss: 1.227651]\n",
      "[D loss: 0.790810] [G loss: 1.388333]\n",
      "[D loss: 0.844001] [G loss: 1.370980]\n",
      "[D loss: 0.675111] [G loss: 1.507834]\n",
      "[D loss: 1.105260] [G loss: 1.417167]\n",
      "[D loss: 0.782466] [G loss: 1.390111]\n",
      "[D loss: 0.955630] [G loss: 1.307789]\n",
      "[D loss: 0.835659] [G loss: 1.469106]\n",
      "[D loss: 0.926234] [G loss: 1.347647]\n",
      "[D loss: 0.983196] [G loss: 1.509696]\n",
      "[D loss: 0.771051] [G loss: 1.364617]\n",
      "[D loss: 0.834317] [G loss: 1.373292]\n",
      "[D loss: 0.742149] [G loss: 1.377943]\n",
      "[D loss: 0.944147] [G loss: 1.431814]\n",
      "[D loss: 0.643876] [G loss: 1.507282]\n",
      "[D loss: 0.922135] [G loss: 1.337746]\n",
      "[D loss: 0.757660] [G loss: 1.172659]\n",
      "[D loss: 0.893299] [G loss: 1.641849]\n",
      "[D loss: 1.076079] [G loss: 1.432544]\n",
      "[D loss: 0.825078] [G loss: 1.520271]\n",
      "[D loss: 0.841834] [G loss: 1.548278]\n",
      "[D loss: 0.665422] [G loss: 1.444495]\n",
      "[D loss: 0.717835] [G loss: 1.590566]\n",
      "[D loss: 0.986535] [G loss: 1.427457]\n",
      "[D loss: 0.774068] [G loss: 1.581676]\n",
      "[D loss: 0.795270] [G loss: 1.492859]\n",
      "[D loss: 0.747607] [G loss: 1.468556]\n",
      "[D loss: 0.908301] [G loss: 1.374079]\n",
      "[D loss: 0.609783] [G loss: 1.446447]\n",
      "[D loss: 0.837718] [G loss: 1.537028]\n",
      "[D loss: 1.013791] [G loss: 1.433395]\n",
      "[D loss: 0.850654] [G loss: 1.562102]\n",
      "[D loss: 0.807319] [G loss: 1.439650]\n",
      "[D loss: 0.754319] [G loss: 1.452142]\n",
      "[D loss: 0.816103] [G loss: 1.411839]\n",
      "[D loss: 0.906878] [G loss: 1.584160]\n",
      "[D loss: 0.806396] [G loss: 1.459599]\n",
      "[D loss: 0.810284] [G loss: 1.541680]\n",
      "[D loss: 0.590638] [G loss: 1.550551]\n",
      "[D loss: 0.926128] [G loss: 1.415735]\n",
      "[D loss: 0.835622] [G loss: 1.364746]\n",
      "[D loss: 0.878200] [G loss: 1.415135]\n",
      "[D loss: 0.716409] [G loss: 1.403394]\n",
      "[D loss: 0.728201] [G loss: 1.415043]\n",
      "[D loss: 0.614015] [G loss: 1.602429]\n",
      "[D loss: 0.913371] [G loss: 1.552427]\n",
      "[D loss: 0.965052] [G loss: 1.264118]\n",
      "[D loss: 0.868271] [G loss: 1.281217]\n",
      "[D loss: 0.961653] [G loss: 1.488506]\n",
      "[D loss: 0.960094] [G loss: 1.345587]\n",
      "[D loss: 0.831975] [G loss: 1.616318]\n",
      "[D loss: 0.867001] [G loss: 1.500168]\n",
      "[D loss: 0.922699] [G loss: 1.746092]\n",
      "[D loss: 0.884226] [G loss: 1.281443]\n",
      "[D loss: 0.758295] [G loss: 1.481065]\n",
      "[D loss: 0.822368] [G loss: 1.243804]\n",
      "[D loss: 0.829375] [G loss: 1.333627]\n",
      "[D loss: 0.883671] [G loss: 1.427363]\n",
      "[D loss: 1.059466] [G loss: 1.597229]\n",
      "[D loss: 0.742716] [G loss: 1.624992]\n",
      "[D loss: 1.013771] [G loss: 1.653420]\n",
      "[D loss: 0.813736] [G loss: 1.354317]\n",
      "[D loss: 0.799124] [G loss: 1.366139]\n",
      "[D loss: 0.956962] [G loss: 1.443768]\n",
      "[D loss: 0.965322] [G loss: 1.424973]\n",
      "[D loss: 0.836061] [G loss: 1.464190]\n",
      "[D loss: 0.810751] [G loss: 1.378019]\n",
      "[D loss: 0.982400] [G loss: 1.523379]\n",
      "[D loss: 0.824091] [G loss: 1.331870]\n",
      "[D loss: 0.652480] [G loss: 1.529473]\n",
      "[D loss: 0.694216] [G loss: 1.573735]\n",
      "[D loss: 0.886665] [G loss: 1.527684]\n",
      "[D loss: 0.818519] [G loss: 1.318508]\n",
      "[D loss: 0.880877] [G loss: 1.149431]\n",
      "[D loss: 0.618238] [G loss: 1.319619]\n",
      "[D loss: 0.805458] [G loss: 1.491591]\n",
      "[D loss: 0.734090] [G loss: 1.395106]\n",
      "[D loss: 0.774444] [G loss: 1.671057]\n",
      "[D loss: 0.816658] [G loss: 1.579218]\n",
      "[D loss: 1.003112] [G loss: 1.445616]\n",
      "[D loss: 0.992395] [G loss: 1.583117]\n",
      "[D loss: 0.886264] [G loss: 1.451999]\n",
      "[D loss: 0.908729] [G loss: 1.674317]\n",
      "[D loss: 0.904578] [G loss: 1.636140]\n",
      "[D loss: 0.973049] [G loss: 1.517525]\n",
      "[D loss: 0.919679] [G loss: 1.422561]\n",
      "[D loss: 0.798020] [G loss: 1.547086]\n",
      "[D loss: 0.833998] [G loss: 1.327616]\n",
      "[D loss: 0.812801] [G loss: 1.513122]\n",
      "[D loss: 1.014258] [G loss: 1.242713]\n",
      "[D loss: 0.812032] [G loss: 1.292449]\n",
      "[D loss: 0.722310] [G loss: 1.499311]\n",
      "[D loss: 0.944473] [G loss: 1.373005]\n",
      "[D loss: 0.874185] [G loss: 1.502730]\n",
      "[D loss: 0.653449] [G loss: 1.567019]\n",
      "[D loss: 1.048759] [G loss: 1.333012]\n",
      "[D loss: 1.229461] [G loss: 1.154071]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.967610] [G loss: 1.405971]\n",
      "[D loss: 1.089468] [G loss: 1.268175]\n",
      "[D loss: 0.967549] [G loss: 1.194514]\n",
      "[D loss: 0.747748] [G loss: 1.450941]\n",
      "[D loss: 0.854921] [G loss: 1.367594]\n",
      "[D loss: 1.155377] [G loss: 1.490267]\n",
      "[D loss: 0.788761] [G loss: 1.410511]\n",
      "[D loss: 0.964351] [G loss: 1.340368]\n",
      "[D loss: 0.729052] [G loss: 1.279075]\n",
      "[D loss: 0.850767] [G loss: 1.369731]\n",
      "[D loss: 0.607995] [G loss: 1.478961]\n",
      "[D loss: 0.735164] [G loss: 1.379420]\n",
      "[D loss: 0.928053] [G loss: 1.487477]\n",
      "[D loss: 0.896014] [G loss: 1.462255]\n",
      "[D loss: 1.011352] [G loss: 1.256839]\n",
      "[D loss: 0.861523] [G loss: 1.374571]\n",
      "[D loss: 0.876640] [G loss: 1.464043]\n",
      "[D loss: 0.948061] [G loss: 1.720785]\n",
      "[D loss: 0.807236] [G loss: 1.316349]\n",
      "[D loss: 0.826932] [G loss: 1.508995]\n",
      "[D loss: 0.678751] [G loss: 1.441084]\n",
      "[D loss: 0.945592] [G loss: 1.282014]\n",
      "[D loss: 0.775534] [G loss: 1.366037]\n",
      "[D loss: 0.796122] [G loss: 1.346078]\n",
      "[D loss: 0.908437] [G loss: 1.474666]\n",
      "[D loss: 0.592186] [G loss: 1.440219]\n",
      "[D loss: 0.811435] [G loss: 1.507385]\n",
      "[D loss: 0.736399] [G loss: 1.501658]\n",
      "[D loss: 0.757106] [G loss: 1.422861]\n",
      "[D loss: 0.941319] [G loss: 1.628671]\n",
      "[D loss: 0.860255] [G loss: 1.348542]\n",
      "[D loss: 0.732665] [G loss: 1.646760]\n",
      "[D loss: 0.764607] [G loss: 1.371956]\n",
      "[D loss: 0.857406] [G loss: 1.342789]\n",
      "[D loss: 0.928652] [G loss: 1.464331]\n",
      "[D loss: 0.740792] [G loss: 1.426164]\n",
      "[D loss: 0.789563] [G loss: 1.436982]\n",
      "[D loss: 0.906134] [G loss: 1.420836]\n",
      "[D loss: 1.050733] [G loss: 1.391828]\n",
      "[D loss: 0.726795] [G loss: 1.668283]\n",
      "[D loss: 0.995609] [G loss: 1.161594]\n",
      "[D loss: 0.761695] [G loss: 1.419506]\n",
      "[D loss: 0.780433] [G loss: 1.708372]\n",
      "[D loss: 1.082263] [G loss: 1.269196]\n",
      "[D loss: 0.925557] [G loss: 1.424351]\n",
      "[D loss: 0.878177] [G loss: 1.357522]\n",
      "[D loss: 0.857920] [G loss: 1.411471]\n",
      "[D loss: 1.035871] [G loss: 1.169050]\n",
      "[D loss: 0.770029] [G loss: 1.357003]\n",
      "[D loss: 0.832694] [G loss: 1.365043]\n",
      "[D loss: 0.916419] [G loss: 1.340790]\n",
      "[D loss: 0.863948] [G loss: 1.309731]\n",
      "[D loss: 0.985480] [G loss: 1.275476]\n",
      "[D loss: 0.779595] [G loss: 1.353319]\n",
      "[D loss: 0.803877] [G loss: 1.369786]\n",
      "[D loss: 0.832437] [G loss: 1.496106]\n",
      "[D loss: 0.778843] [G loss: 1.312278]\n",
      "[D loss: 0.930569] [G loss: 1.432423]\n",
      "[D loss: 0.933455] [G loss: 1.574993]\n",
      "[D loss: 0.889327] [G loss: 1.386612]\n",
      "[D loss: 1.004007] [G loss: 1.363608]\n",
      "[D loss: 0.767501] [G loss: 1.601675]\n",
      "[D loss: 0.802210] [G loss: 1.300684]\n",
      "[D loss: 1.083766] [G loss: 1.226199]\n",
      "[D loss: 0.745133] [G loss: 1.478564]\n",
      "[D loss: 0.728173] [G loss: 1.606329]\n",
      "[D loss: 0.871922] [G loss: 1.441335]\n",
      "[D loss: 0.893491] [G loss: 1.413882]\n",
      "[D loss: 0.747396] [G loss: 1.577324]\n",
      "[D loss: 0.891354] [G loss: 1.496137]\n",
      "[D loss: 0.798840] [G loss: 1.397706]\n",
      "[D loss: 1.015727] [G loss: 1.234254]\n",
      "[D loss: 0.818599] [G loss: 1.442704]\n",
      "[D loss: 0.904366] [G loss: 1.223066]\n",
      "[D loss: 0.757740] [G loss: 1.573089]\n",
      "[D loss: 0.738934] [G loss: 1.629490]\n",
      "[D loss: 0.783732] [G loss: 1.333796]\n",
      "[D loss: 0.824194] [G loss: 1.254971]\n",
      "[D loss: 0.877632] [G loss: 1.352529]\n",
      "[D loss: 0.767765] [G loss: 1.437945]\n",
      "[D loss: 0.693285] [G loss: 1.504615]\n",
      "[D loss: 0.904692] [G loss: 1.508921]\n",
      "[D loss: 0.676863] [G loss: 1.485845]\n",
      "[D loss: 0.624253] [G loss: 1.486141]\n",
      "[D loss: 1.170346] [G loss: 1.466833]\n",
      "[D loss: 0.971967] [G loss: 1.500072]\n",
      "[D loss: 0.925361] [G loss: 1.282226]\n",
      "[D loss: 1.085826] [G loss: 1.382584]\n",
      "[D loss: 0.864344] [G loss: 1.363435]\n",
      "[D loss: 1.030134] [G loss: 1.526049]\n",
      "[D loss: 0.917679] [G loss: 1.554142]\n",
      "[D loss: 1.012244] [G loss: 1.475454]\n",
      "[D loss: 0.976121] [G loss: 1.505584]\n",
      "[D loss: 0.980447] [G loss: 1.181690]\n",
      "[D loss: 0.971787] [G loss: 1.332308]\n",
      "[D loss: 0.880429] [G loss: 1.536568]\n",
      "[D loss: 0.857357] [G loss: 1.541972]\n",
      "[D loss: 0.746951] [G loss: 1.562100]\n",
      "[D loss: 1.136861] [G loss: 1.251839]\n",
      "[D loss: 0.849831] [G loss: 1.546729]\n",
      "[D loss: 0.749740] [G loss: 1.589996]\n",
      "[D loss: 0.750325] [G loss: 1.229985]\n",
      "[D loss: 0.814547] [G loss: 1.437936]\n",
      "[D loss: 0.900948] [G loss: 1.452273]\n",
      "[D loss: 0.879366] [G loss: 1.261052]\n",
      "[D loss: 0.790161] [G loss: 1.431484]\n",
      "[D loss: 0.799632] [G loss: 1.415481]\n",
      "[D loss: 0.820330] [G loss: 1.453294]\n",
      "[D loss: 0.949553] [G loss: 1.433328]\n",
      "[D loss: 0.944308] [G loss: 1.345071]\n",
      "[D loss: 0.794655] [G loss: 1.405822]\n",
      "[D loss: 0.965784] [G loss: 1.610051]\n",
      "[D loss: 0.743752] [G loss: 1.308211]\n",
      "[D loss: 0.868469] [G loss: 1.405541]\n",
      "[D loss: 0.690830] [G loss: 1.373989]\n",
      "[D loss: 0.706567] [G loss: 1.479857]\n",
      "[D loss: 1.077856] [G loss: 1.539307]\n",
      "[D loss: 1.001726] [G loss: 1.420367]\n",
      "[D loss: 0.818665] [G loss: 1.531905]\n",
      "[D loss: 0.815768] [G loss: 1.547157]\n",
      "[D loss: 0.881593] [G loss: 1.457001]\n",
      "[D loss: 0.885854] [G loss: 1.664749]\n",
      "[D loss: 0.739232] [G loss: 1.546036]\n",
      "[D loss: 0.993029] [G loss: 1.502831]\n",
      "[D loss: 0.783124] [G loss: 1.559080]\n",
      "[D loss: 0.759884] [G loss: 1.611356]\n",
      "[D loss: 1.070318] [G loss: 1.387301]\n",
      "[D loss: 0.798630] [G loss: 1.435740]\n",
      "[D loss: 1.003872] [G loss: 1.258007]\n",
      "[D loss: 0.814083] [G loss: 1.387555]\n",
      "[D loss: 0.877141] [G loss: 1.500736]\n",
      "[D loss: 0.904636] [G loss: 1.358959]\n",
      "[D loss: 0.846543] [G loss: 1.369127]\n",
      "[D loss: 0.964065] [G loss: 1.292531]\n",
      "[D loss: 1.032677] [G loss: 1.357253]\n",
      "[D loss: 0.743402] [G loss: 1.316282]\n",
      "[D loss: 0.833845] [G loss: 1.466381]\n",
      "[D loss: 1.019524] [G loss: 1.524829]\n",
      "[D loss: 1.075090] [G loss: 1.462490]\n",
      "[D loss: 0.806295] [G loss: 1.457333]\n",
      "[D loss: 0.654390] [G loss: 1.443240]\n",
      "[D loss: 1.052044] [G loss: 1.403788]\n",
      "[D loss: 0.776097] [G loss: 1.277857]\n",
      "[D loss: 0.551637] [G loss: 1.505417]\n",
      "[D loss: 0.796795] [G loss: 1.520881]\n",
      "[D loss: 1.071807] [G loss: 1.457022]\n",
      "[D loss: 0.820772] [G loss: 1.300413]\n",
      "[D loss: 0.899523] [G loss: 1.506419]\n",
      "[D loss: 0.859540] [G loss: 1.565023]\n",
      "[D loss: 0.798023] [G loss: 1.516410]\n",
      "[D loss: 0.837884] [G loss: 1.290686]\n",
      "[D loss: 0.862716] [G loss: 1.286630]\n",
      "[D loss: 0.933691] [G loss: 1.239135]\n",
      "[D loss: 0.628234] [G loss: 1.411363]\n",
      "[D loss: 1.146654] [G loss: 1.431221]\n",
      "[D loss: 0.953137] [G loss: 1.292268]\n",
      "[D loss: 0.661938] [G loss: 1.347793]\n",
      "[D loss: 0.806131] [G loss: 1.458637]\n",
      "[D loss: 1.015724] [G loss: 1.400840]\n",
      "[D loss: 0.568949] [G loss: 1.342812]\n",
      "[D loss: 0.804375] [G loss: 1.322980]\n",
      "[D loss: 0.763898] [G loss: 1.660403]\n",
      "[D loss: 0.779429] [G loss: 1.414889]\n",
      "[D loss: 0.626079] [G loss: 1.619071]\n",
      "[D loss: 0.822584] [G loss: 1.395686]\n",
      "[D loss: 0.798489] [G loss: 1.348227]\n",
      "[D loss: 0.938525] [G loss: 1.625096]\n",
      "[D loss: 0.769806] [G loss: 1.268870]\n",
      "[D loss: 1.002214] [G loss: 1.507069]\n",
      "[D loss: 0.798739] [G loss: 1.355284]\n",
      "[D loss: 0.989234] [G loss: 1.448133]\n",
      "[D loss: 0.747244] [G loss: 1.445033]\n",
      "[D loss: 0.914119] [G loss: 1.435758]\n",
      "[D loss: 0.725384] [G loss: 1.447360]\n",
      "[D loss: 0.713718] [G loss: 1.491019]\n",
      "[D loss: 0.866120] [G loss: 1.286202]\n",
      "[D loss: 0.959838] [G loss: 1.652403]\n",
      "[D loss: 0.761441] [G loss: 1.639214]\n",
      "[D loss: 0.812137] [G loss: 1.346090]\n",
      "[D loss: 0.860164] [G loss: 1.374430]\n",
      "[D loss: 0.890346] [G loss: 1.322803]\n",
      "[D loss: 1.096642] [G loss: 1.352087]\n",
      "[D loss: 0.696661] [G loss: 1.369985]\n",
      "[D loss: 1.100967] [G loss: 1.246505]\n",
      "[D loss: 0.946763] [G loss: 1.352508]\n",
      "[D loss: 0.905937] [G loss: 1.367073]\n",
      "[D loss: 0.794023] [G loss: 1.488548]\n",
      "[D loss: 0.867678] [G loss: 1.696131]\n",
      "[D loss: 1.052325] [G loss: 1.321472]\n",
      "[D loss: 0.968003] [G loss: 1.245712]\n",
      "[D loss: 0.925615] [G loss: 1.374515]\n",
      "[D loss: 0.726753] [G loss: 1.324260]\n",
      "[D loss: 0.804639] [G loss: 1.361146]\n",
      "[D loss: 0.722568] [G loss: 1.384911]\n",
      "[D loss: 0.713344] [G loss: 1.490665]\n",
      "[D loss: 0.840782] [G loss: 1.537127]\n",
      "[D loss: 0.627763] [G loss: 1.337969]\n",
      "[D loss: 0.812674] [G loss: 1.498295]\n",
      "[D loss: 0.893816] [G loss: 1.463522]\n",
      "[D loss: 0.941655] [G loss: 1.674772]\n",
      "[D loss: 0.974640] [G loss: 1.374382]\n",
      "[D loss: 0.941837] [G loss: 1.422924]\n",
      "[D loss: 0.969817] [G loss: 1.368926]\n",
      "[D loss: 0.805656] [G loss: 1.485981]\n",
      "[D loss: 0.976987] [G loss: 1.297035]\n",
      "[D loss: 0.999861] [G loss: 1.307890]\n",
      "[D loss: 0.907354] [G loss: 1.405594]\n",
      "[D loss: 0.763840] [G loss: 1.455807]\n",
      "[D loss: 0.959537] [G loss: 1.508017]\n",
      "[D loss: 0.950239] [G loss: 1.470077]\n",
      "[D loss: 0.755092] [G loss: 1.327363]\n",
      "[D loss: 0.784003] [G loss: 1.378317]\n",
      "[D loss: 1.025782] [G loss: 1.380659]\n",
      "[D loss: 0.682023] [G loss: 1.569252]\n",
      "[D loss: 0.813534] [G loss: 1.654868]\n",
      "[D loss: 0.769245] [G loss: 1.598158]\n",
      "[D loss: 0.827023] [G loss: 1.433150]\n",
      "[D loss: 0.811118] [G loss: 1.279454]\n",
      "[D loss: 0.890465] [G loss: 1.385245]\n",
      "[D loss: 0.807983] [G loss: 1.359998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.844895] [G loss: 1.311913]\n",
      "[D loss: 1.041483] [G loss: 1.221742]\n",
      "[D loss: 0.898805] [G loss: 1.370143]\n",
      "[D loss: 0.797756] [G loss: 1.288147]\n",
      "[D loss: 1.059480] [G loss: 1.364339]\n",
      "[D loss: 0.700911] [G loss: 1.458742]\n",
      "[D loss: 0.764475] [G loss: 1.429613]\n",
      "[D loss: 0.883336] [G loss: 1.310361]\n",
      "[D loss: 0.786725] [G loss: 1.504870]\n",
      "[D loss: 0.986472] [G loss: 1.427669]\n",
      "[D loss: 0.807254] [G loss: 1.381538]\n",
      "[D loss: 0.990137] [G loss: 1.392847]\n",
      "[D loss: 0.868319] [G loss: 1.360745]\n",
      "[D loss: 0.990131] [G loss: 1.483846]\n",
      "[D loss: 0.996604] [G loss: 1.501819]\n",
      "[D loss: 0.943215] [G loss: 1.558333]\n",
      "[D loss: 0.800378] [G loss: 1.311832]\n",
      "[D loss: 0.830431] [G loss: 1.653531]\n",
      "[D loss: 1.306586] [G loss: 1.205660]\n",
      "[D loss: 0.912027] [G loss: 1.467988]\n",
      "[D loss: 0.973123] [G loss: 1.171799]\n",
      "[D loss: 0.985359] [G loss: 1.253021]\n",
      "[D loss: 0.724254] [G loss: 1.378687]\n",
      "[D loss: 0.982227] [G loss: 1.434270]\n",
      "[D loss: 0.855281] [G loss: 1.388993]\n",
      "[D loss: 0.833125] [G loss: 1.226618]\n",
      "[D loss: 0.883134] [G loss: 1.423511]\n",
      "[D loss: 0.915920] [G loss: 1.346445]\n",
      "[D loss: 1.077223] [G loss: 1.214959]\n",
      "[D loss: 0.958422] [G loss: 1.343870]\n",
      "[D loss: 0.973458] [G loss: 1.318128]\n",
      "[D loss: 0.932666] [G loss: 1.357987]\n",
      "[D loss: 0.731203] [G loss: 1.172400]\n",
      "[D loss: 0.945961] [G loss: 1.144507]\n",
      "[D loss: 0.782896] [G loss: 1.411271]\n",
      "[D loss: 0.993641] [G loss: 1.391376]\n",
      "[D loss: 0.974423] [G loss: 1.280828]\n",
      "[D loss: 0.946862] [G loss: 1.315881]\n",
      "[D loss: 1.041548] [G loss: 1.389578]\n",
      "[D loss: 0.719945] [G loss: 1.372782]\n",
      "[D loss: 0.813386] [G loss: 1.418465]\n",
      "[D loss: 0.852602] [G loss: 1.518070]\n",
      "[D loss: 0.711866] [G loss: 1.508366]\n",
      "[D loss: 0.663331] [G loss: 1.449521]\n",
      "[D loss: 0.798621] [G loss: 1.488188]\n",
      "[D loss: 0.781068] [G loss: 1.503798]\n",
      "[D loss: 0.954969] [G loss: 1.485147]\n",
      "[D loss: 0.941979] [G loss: 1.507962]\n",
      "[D loss: 0.776123] [G loss: 1.278532]\n",
      "[D loss: 0.899576] [G loss: 1.380178]\n",
      "[D loss: 0.936649] [G loss: 1.268822]\n",
      "[D loss: 0.806386] [G loss: 1.346112]\n",
      "[D loss: 0.953700] [G loss: 1.541701]\n",
      "[D loss: 0.778338] [G loss: 1.514212]\n",
      "[D loss: 0.708389] [G loss: 1.282768]\n",
      "[D loss: 1.060955] [G loss: 1.330414]\n",
      "[D loss: 0.688638] [G loss: 1.451611]\n",
      "[D loss: 0.744029] [G loss: 1.486005]\n",
      "[D loss: 0.652506] [G loss: 1.398434]\n",
      "[D loss: 0.885108] [G loss: 1.350760]\n",
      "[D loss: 0.847799] [G loss: 1.313426]\n",
      "[D loss: 1.099506] [G loss: 1.488662]\n",
      "[D loss: 0.904901] [G loss: 1.476382]\n",
      "[D loss: 0.754231] [G loss: 1.487882]\n",
      "[D loss: 0.955116] [G loss: 1.501663]\n",
      "[D loss: 0.925666] [G loss: 1.450414]\n",
      "[D loss: 0.910937] [G loss: 1.446791]\n",
      "[D loss: 0.736795] [G loss: 1.506922]\n",
      "[D loss: 0.968491] [G loss: 1.381770]\n",
      "[D loss: 0.800686] [G loss: 1.545888]\n",
      "[D loss: 0.670739] [G loss: 1.484368]\n",
      "[D loss: 0.878663] [G loss: 1.492330]\n",
      "[D loss: 0.809258] [G loss: 1.403913]\n",
      "[D loss: 0.668002] [G loss: 1.380881]\n",
      "[D loss: 0.683619] [G loss: 1.488807]\n",
      "[D loss: 0.965445] [G loss: 1.345428]\n",
      "[D loss: 0.840786] [G loss: 1.489214]\n",
      "[D loss: 0.909614] [G loss: 1.528556]\n",
      "[D loss: 0.802040] [G loss: 1.342484]\n",
      "[D loss: 0.966632] [G loss: 1.524361]\n",
      "[D loss: 0.762052] [G loss: 1.353598]\n",
      "[D loss: 0.764103] [G loss: 1.518370]\n",
      "[D loss: 0.779562] [G loss: 1.521187]\n",
      "[D loss: 0.750897] [G loss: 1.432988]\n",
      "[D loss: 1.008216] [G loss: 1.533872]\n",
      "[D loss: 1.102803] [G loss: 1.398089]\n",
      "[D loss: 0.832625] [G loss: 1.396017]\n",
      "[D loss: 0.712783] [G loss: 1.413378]\n",
      "[D loss: 1.009371] [G loss: 1.501475]\n",
      "[D loss: 0.729104] [G loss: 1.446808]\n",
      "[D loss: 0.878501] [G loss: 1.367467]\n",
      "[D loss: 0.722537] [G loss: 1.540787]\n",
      "[D loss: 0.643191] [G loss: 1.521401]\n",
      "[D loss: 0.683384] [G loss: 1.465373]\n",
      "[D loss: 0.944726] [G loss: 1.392389]\n",
      "[D loss: 0.922399] [G loss: 1.542846]\n",
      "[D loss: 0.689215] [G loss: 1.649954]\n",
      "[D loss: 0.692459] [G loss: 1.517938]\n",
      "[D loss: 0.979104] [G loss: 1.397436]\n",
      "[D loss: 0.869942] [G loss: 1.362715]\n",
      "[D loss: 0.896807] [G loss: 1.421843]\n",
      "[D loss: 0.828426] [G loss: 1.395378]\n",
      "[D loss: 1.059731] [G loss: 1.710085]\n",
      "[D loss: 0.581422] [G loss: 1.506060]\n",
      "[D loss: 0.881687] [G loss: 1.409213]\n",
      "[D loss: 0.907746] [G loss: 1.638872]\n",
      "[D loss: 1.061531] [G loss: 1.364965]\n",
      "[D loss: 0.831262] [G loss: 1.355485]\n",
      "[D loss: 0.877702] [G loss: 1.289922]\n",
      "[D loss: 1.007344] [G loss: 1.394191]\n",
      "[D loss: 0.820068] [G loss: 1.337679]\n",
      "[D loss: 1.016829] [G loss: 1.525538]\n",
      "[D loss: 0.809778] [G loss: 1.588768]\n",
      "[D loss: 0.790500] [G loss: 1.698016]\n",
      "[D loss: 1.080368] [G loss: 1.142110]\n",
      "[D loss: 0.779481] [G loss: 1.398307]\n",
      "[D loss: 0.807664] [G loss: 1.278360]\n",
      "[D loss: 0.845259] [G loss: 1.317720]\n",
      "[D loss: 0.801618] [G loss: 1.385985]\n",
      "[D loss: 0.853527] [G loss: 1.562365]\n",
      "[D loss: 0.889622] [G loss: 1.422024]\n",
      "[D loss: 0.936006] [G loss: 1.382450]\n",
      "[D loss: 0.886273] [G loss: 1.397068]\n",
      "[D loss: 0.728190] [G loss: 1.405976]\n",
      "[D loss: 0.787678] [G loss: 1.285886]\n",
      "[D loss: 0.805963] [G loss: 1.407772]\n",
      "[D loss: 0.645967] [G loss: 1.470092]\n",
      "[D loss: 0.893240] [G loss: 1.515140]\n",
      "[D loss: 1.043362] [G loss: 1.213770]\n",
      "[D loss: 0.929165] [G loss: 1.294554]\n",
      "[D loss: 0.994343] [G loss: 1.378551]\n",
      "[D loss: 0.651880] [G loss: 1.477487]\n",
      "[D loss: 0.770157] [G loss: 1.405530]\n",
      "[D loss: 0.966653] [G loss: 1.325157]\n",
      "[D loss: 0.771547] [G loss: 1.327316]\n",
      "[D loss: 1.038260] [G loss: 1.244164]\n",
      "[D loss: 0.747078] [G loss: 1.454368]\n",
      "[D loss: 1.025877] [G loss: 1.518558]\n",
      "[D loss: 0.856702] [G loss: 1.256813]\n",
      "[D loss: 0.842815] [G loss: 1.508072]\n",
      "[D loss: 0.964673] [G loss: 1.379655]\n",
      "[D loss: 0.806293] [G loss: 1.282688]\n",
      "[D loss: 1.080551] [G loss: 1.321972]\n",
      "[D loss: 0.709130] [G loss: 1.530887]\n",
      "[D loss: 0.989796] [G loss: 1.601039]\n",
      "[D loss: 0.898118] [G loss: 1.518448]\n",
      "[D loss: 0.675318] [G loss: 1.404449]\n",
      "[D loss: 1.008760] [G loss: 1.614033]\n",
      "[D loss: 0.657312] [G loss: 1.302241]\n",
      "[D loss: 0.728958] [G loss: 1.406822]\n",
      "[D loss: 0.881808] [G loss: 1.496673]\n",
      "[D loss: 0.690237] [G loss: 1.416074]\n",
      "[D loss: 0.868108] [G loss: 1.303981]\n",
      "[D loss: 0.759372] [G loss: 1.413311]\n",
      "[D loss: 0.552663] [G loss: 1.430217]\n",
      "[D loss: 0.954505] [G loss: 1.431267]\n",
      "[D loss: 0.892274] [G loss: 1.340538]\n",
      "[D loss: 0.912480] [G loss: 1.573061]\n",
      "[D loss: 0.608986] [G loss: 1.577964]\n",
      "[D loss: 1.099569] [G loss: 1.301644]\n",
      "[D loss: 0.855817] [G loss: 1.379502]\n",
      "[D loss: 0.613613] [G loss: 1.664610]\n",
      "[D loss: 0.925796] [G loss: 1.496693]\n",
      "[D loss: 0.850391] [G loss: 1.508685]\n",
      "[D loss: 0.610204] [G loss: 1.438682]\n",
      "[D loss: 0.882562] [G loss: 1.638623]\n",
      "[D loss: 0.943283] [G loss: 1.463289]\n",
      "[D loss: 1.246208] [G loss: 1.433361]\n",
      "[D loss: 0.729194] [G loss: 1.344989]\n",
      "[D loss: 0.893224] [G loss: 1.506800]\n",
      "[D loss: 0.802668] [G loss: 1.518023]\n",
      "[D loss: 0.802229] [G loss: 1.610638]\n",
      "[D loss: 0.637347] [G loss: 1.705379]\n",
      "[D loss: 0.763500] [G loss: 1.523704]\n",
      "[D loss: 0.723495] [G loss: 1.470077]\n",
      "[D loss: 0.877159] [G loss: 1.366326]\n",
      "[D loss: 0.742757] [G loss: 1.539249]\n",
      "[D loss: 1.075556] [G loss: 1.353609]\n",
      "[D loss: 0.783613] [G loss: 1.571272]\n",
      "[D loss: 0.827639] [G loss: 1.216387]\n",
      "[D loss: 0.748877] [G loss: 1.360871]\n",
      "[D loss: 0.627560] [G loss: 1.370262]\n",
      "[D loss: 0.893720] [G loss: 1.405766]\n",
      "[D loss: 0.955522] [G loss: 1.502800]\n",
      "[D loss: 0.835736] [G loss: 1.406685]\n",
      "[D loss: 0.995521] [G loss: 1.375634]\n",
      "[D loss: 1.019485] [G loss: 1.471607]\n",
      "[D loss: 0.807184] [G loss: 1.447012]\n",
      "[D loss: 0.772036] [G loss: 1.406159]\n",
      "[D loss: 0.893934] [G loss: 1.573915]\n",
      "[D loss: 0.574778] [G loss: 1.612195]\n",
      "[D loss: 0.730721] [G loss: 1.463547]\n",
      "[D loss: 1.153870] [G loss: 1.341073]\n",
      "[D loss: 0.752422] [G loss: 1.551778]\n",
      "[D loss: 0.842917] [G loss: 1.579818]\n",
      "[D loss: 0.912418] [G loss: 1.499643]\n",
      "[D loss: 0.730309] [G loss: 1.374222]\n",
      "[D loss: 0.752758] [G loss: 1.600200]\n",
      "[D loss: 0.890984] [G loss: 1.349323]\n",
      "[D loss: 0.740001] [G loss: 1.436352]\n",
      "[D loss: 0.901025] [G loss: 1.619450]\n",
      "[D loss: 0.719184] [G loss: 1.535669]\n",
      "[D loss: 0.897788] [G loss: 1.669397]\n",
      "[D loss: 0.833169] [G loss: 1.496916]\n",
      "[D loss: 0.644468] [G loss: 1.600270]\n",
      "[D loss: 1.169224] [G loss: 1.243134]\n",
      "[D loss: 1.066855] [G loss: 1.338360]\n",
      "[D loss: 0.790145] [G loss: 1.367750]\n",
      "[D loss: 0.936823] [G loss: 1.324580]\n",
      "[D loss: 0.763639] [G loss: 1.423020]\n",
      "[D loss: 0.792594] [G loss: 1.391000]\n",
      "[D loss: 0.614483] [G loss: 1.493959]\n",
      "[D loss: 0.817439] [G loss: 1.579076]\n",
      "[D loss: 0.799937] [G loss: 1.569456]\n",
      "[D loss: 0.626767] [G loss: 1.476962]\n",
      "[D loss: 0.549193] [G loss: 1.339364]\n",
      "[D loss: 0.920973] [G loss: 1.392222]\n",
      "[D loss: 0.641547] [G loss: 1.644205]\n",
      "[D loss: 0.678632] [G loss: 1.580384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.781612] [G loss: 1.592183]\n",
      "[D loss: 1.032649] [G loss: 1.383562]\n",
      "[D loss: 0.671072] [G loss: 1.662694]\n",
      "[D loss: 1.041993] [G loss: 1.466073]\n",
      "[D loss: 0.725272] [G loss: 1.486826]\n",
      "[D loss: 0.763025] [G loss: 1.507612]\n",
      "[D loss: 1.195354] [G loss: 1.261413]\n",
      "[D loss: 0.892155] [G loss: 1.395098]\n",
      "[D loss: 1.052945] [G loss: 1.488436]\n",
      "[D loss: 0.882190] [G loss: 1.603217]\n",
      "[D loss: 0.762323] [G loss: 1.390812]\n",
      "[D loss: 0.796076] [G loss: 1.433049]\n",
      "[D loss: 0.781901] [G loss: 1.371744]\n",
      "[D loss: 0.832539] [G loss: 1.495574]\n",
      "[D loss: 0.930972] [G loss: 1.561530]\n",
      "[D loss: 0.745959] [G loss: 1.600711]\n",
      "[D loss: 0.700150] [G loss: 1.438244]\n",
      "[D loss: 0.765318] [G loss: 1.505125]\n",
      "[D loss: 1.021315] [G loss: 1.303827]\n",
      "[D loss: 0.635148] [G loss: 1.490982]\n",
      "[D loss: 1.060946] [G loss: 1.409585]\n",
      "[D loss: 0.825760] [G loss: 1.328038]\n",
      "[D loss: 0.894286] [G loss: 1.409367]\n",
      "[D loss: 0.694186] [G loss: 1.226290]\n",
      "[D loss: 0.964214] [G loss: 1.471312]\n",
      "[D loss: 0.834886] [G loss: 1.379880]\n",
      "[D loss: 1.009502] [G loss: 1.354157]\n",
      "[D loss: 0.782472] [G loss: 1.703511]\n",
      "[D loss: 0.868674] [G loss: 1.423677]\n",
      "[D loss: 1.076162] [G loss: 1.170291]\n",
      "[D loss: 0.930979] [G loss: 1.498324]\n",
      "[D loss: 0.938673] [G loss: 1.216948]\n",
      "[D loss: 0.816678] [G loss: 1.249923]\n",
      "[D loss: 0.983334] [G loss: 1.374712]\n",
      "[D loss: 0.762073] [G loss: 1.545727]\n",
      "[D loss: 0.782175] [G loss: 1.443490]\n",
      "[D loss: 0.953026] [G loss: 1.512776]\n",
      "[D loss: 0.819394] [G loss: 1.293822]\n",
      "[D loss: 0.923472] [G loss: 1.332243]\n",
      "[D loss: 0.834631] [G loss: 1.506856]\n",
      "[D loss: 0.781687] [G loss: 1.425444]\n",
      "[D loss: 1.040428] [G loss: 1.278075]\n",
      "[D loss: 0.858368] [G loss: 1.431777]\n",
      "[D loss: 0.989507] [G loss: 1.263411]\n",
      "[D loss: 0.834293] [G loss: 1.410722]\n",
      "[D loss: 0.801670] [G loss: 1.444578]\n",
      "[D loss: 0.914551] [G loss: 1.466087]\n",
      "[D loss: 0.714373] [G loss: 1.240017]\n",
      "[D loss: 1.001213] [G loss: 1.491958]\n",
      "[D loss: 0.993711] [G loss: 1.363627]\n",
      "[D loss: 1.050123] [G loss: 1.306886]\n",
      "[D loss: 0.735952] [G loss: 1.293685]\n",
      "[D loss: 0.933811] [G loss: 1.403147]\n",
      "[D loss: 0.721927] [G loss: 1.438319]\n",
      "[D loss: 1.041095] [G loss: 1.382960]\n",
      "[D loss: 0.753566] [G loss: 1.419120]\n",
      "[D loss: 1.064875] [G loss: 1.204400]\n",
      "[D loss: 1.158406] [G loss: 1.284560]\n",
      "[D loss: 0.948663] [G loss: 1.359694]\n",
      "[D loss: 0.943471] [G loss: 1.397901]\n",
      "[D loss: 0.795634] [G loss: 1.415551]\n",
      "[D loss: 0.919233] [G loss: 1.324415]\n",
      "[D loss: 1.007029] [G loss: 1.445430]\n",
      "[D loss: 0.883406] [G loss: 1.298967]\n",
      "[D loss: 0.925700] [G loss: 1.258308]\n",
      "[D loss: 0.856387] [G loss: 1.355082]\n",
      "[D loss: 0.850603] [G loss: 1.290606]\n",
      "[D loss: 0.831410] [G loss: 1.318312]\n",
      "[D loss: 1.040713] [G loss: 1.276817]\n",
      "[D loss: 0.872425] [G loss: 1.286958]\n",
      "[D loss: 0.761612] [G loss: 1.354822]\n",
      "[D loss: 0.751539] [G loss: 1.167942]\n",
      "[D loss: 0.845019] [G loss: 1.397392]\n",
      "[D loss: 0.951893] [G loss: 1.206651]\n",
      "[D loss: 0.770546] [G loss: 1.430387]\n",
      "[D loss: 1.026489] [G loss: 1.388061]\n",
      "[D loss: 0.621952] [G loss: 1.458576]\n",
      "[D loss: 0.900463] [G loss: 1.643584]\n",
      "[D loss: 0.965607] [G loss: 1.359364]\n",
      "[D loss: 0.814682] [G loss: 1.193425]\n",
      "[D loss: 1.069451] [G loss: 1.203560]\n",
      "[D loss: 0.791558] [G loss: 1.137742]\n",
      "[D loss: 0.863822] [G loss: 1.684272]\n",
      "[D loss: 0.713245] [G loss: 1.495610]\n",
      "[D loss: 0.904224] [G loss: 1.369373]\n",
      "[D loss: 0.880480] [G loss: 1.296753]\n",
      "[D loss: 0.730973] [G loss: 1.272508]\n",
      "[D loss: 0.956831] [G loss: 1.408323]\n",
      "[D loss: 0.753900] [G loss: 1.342968]\n",
      "[D loss: 0.979523] [G loss: 1.305315]\n",
      "[D loss: 0.796861] [G loss: 1.497103]\n",
      "[D loss: 1.097080] [G loss: 1.593573]\n",
      "[D loss: 0.601538] [G loss: 1.516563]\n",
      "[D loss: 0.712237] [G loss: 1.389995]\n",
      "[D loss: 1.095321] [G loss: 1.384284]\n",
      "[D loss: 0.731298] [G loss: 1.386667]\n",
      "[D loss: 0.970562] [G loss: 1.357402]\n",
      "[D loss: 0.850319] [G loss: 1.598419]\n",
      "[D loss: 0.811431] [G loss: 1.560735]\n",
      "[D loss: 0.828218] [G loss: 1.439579]\n",
      "[D loss: 0.800803] [G loss: 1.499067]\n",
      "[D loss: 0.936834] [G loss: 1.675194]\n",
      "[D loss: 0.846428] [G loss: 1.517423]\n",
      "[D loss: 0.855090] [G loss: 1.296878]\n",
      "[D loss: 0.946988] [G loss: 1.406822]\n",
      "[D loss: 0.825692] [G loss: 1.192621]\n",
      "[D loss: 0.656970] [G loss: 1.535280]\n",
      "[D loss: 0.730198] [G loss: 1.570437]\n",
      "[D loss: 1.003181] [G loss: 1.629277]\n",
      "[D loss: 1.060174] [G loss: 1.525708]\n",
      "[D loss: 0.915596] [G loss: 1.484051]\n",
      "[D loss: 0.872185] [G loss: 1.475712]\n",
      "[D loss: 0.810087] [G loss: 1.590675]\n",
      "[D loss: 0.799352] [G loss: 1.377676]\n",
      "[D loss: 0.714953] [G loss: 1.317555]\n",
      "[D loss: 0.793646] [G loss: 1.377651]\n",
      "[D loss: 1.020719] [G loss: 1.273865]\n",
      "[D loss: 0.781502] [G loss: 1.422401]\n",
      "[D loss: 0.692453] [G loss: 1.510139]\n",
      "[D loss: 0.880198] [G loss: 1.596075]\n",
      "[D loss: 0.721460] [G loss: 1.606027]\n",
      "[D loss: 0.947554] [G loss: 1.377523]\n",
      "[D loss: 0.979519] [G loss: 1.485840]\n",
      "[D loss: 0.918128] [G loss: 1.391226]\n",
      "[D loss: 0.631634] [G loss: 1.579417]\n",
      "[D loss: 1.017905] [G loss: 1.186476]\n",
      "[D loss: 0.793551] [G loss: 1.546355]\n",
      "[D loss: 0.678325] [G loss: 1.463066]\n",
      "[D loss: 0.684486] [G loss: 1.510388]\n",
      "[D loss: 0.977325] [G loss: 1.591882]\n",
      "[D loss: 0.838263] [G loss: 1.465154]\n",
      "[D loss: 0.687181] [G loss: 1.727546]\n",
      "[D loss: 0.723587] [G loss: 1.529053]\n",
      "[D loss: 1.016675] [G loss: 1.498567]\n",
      "[D loss: 0.926730] [G loss: 1.513514]\n",
      "[D loss: 0.911027] [G loss: 1.643510]\n",
      "[D loss: 0.856152] [G loss: 1.395443]\n",
      "[D loss: 0.907268] [G loss: 1.494798]\n",
      "[D loss: 0.915108] [G loss: 1.402425]\n",
      "[D loss: 0.764675] [G loss: 1.420170]\n",
      "[D loss: 1.073411] [G loss: 1.564967]\n",
      "[D loss: 0.943866] [G loss: 1.302480]\n",
      "[D loss: 0.694558] [G loss: 1.216009]\n",
      "[D loss: 0.816356] [G loss: 1.388642]\n",
      "[D loss: 0.909936] [G loss: 1.454669]\n",
      "[D loss: 0.744538] [G loss: 1.652952]\n",
      "[D loss: 0.791461] [G loss: 1.370985]\n",
      "[D loss: 0.979900] [G loss: 1.423430]\n",
      "[D loss: 0.864853] [G loss: 1.371618]\n",
      "[D loss: 0.832479] [G loss: 1.375164]\n",
      "[D loss: 1.023650] [G loss: 1.335512]\n",
      "[D loss: 0.864920] [G loss: 1.223022]\n",
      "[D loss: 0.970833] [G loss: 1.424048]\n",
      "[D loss: 0.748642] [G loss: 1.727498]\n",
      "[D loss: 0.626023] [G loss: 1.633256]\n",
      "[D loss: 0.766136] [G loss: 1.596704]\n",
      "[D loss: 1.004840] [G loss: 1.397297]\n",
      "[D loss: 0.950508] [G loss: 1.242526]\n",
      "[D loss: 0.972339] [G loss: 1.454112]\n",
      "[D loss: 0.778542] [G loss: 1.551821]\n",
      "[D loss: 0.993644] [G loss: 1.619501]\n",
      "[D loss: 0.906002] [G loss: 1.523471]\n",
      "[D loss: 0.875499] [G loss: 1.354799]\n",
      "[D loss: 0.876090] [G loss: 1.304730]\n",
      "[D loss: 0.787896] [G loss: 1.287246]\n",
      "[D loss: 0.946009] [G loss: 1.310907]\n",
      "[D loss: 0.950353] [G loss: 1.414054]\n",
      "[D loss: 0.991294] [G loss: 1.316253]\n",
      "[D loss: 1.004529] [G loss: 1.308279]\n",
      "[D loss: 0.898709] [G loss: 1.393120]\n",
      "[D loss: 0.851057] [G loss: 1.414654]\n",
      "[D loss: 0.875780] [G loss: 1.289596]\n",
      "[D loss: 0.776504] [G loss: 1.698364]\n",
      "[D loss: 0.974392] [G loss: 1.606009]\n",
      "[D loss: 0.746713] [G loss: 1.513353]\n",
      "[D loss: 0.917628] [G loss: 1.352484]\n",
      "[D loss: 0.667944] [G loss: 1.515744]\n",
      "[D loss: 0.885326] [G loss: 1.331242]\n",
      "[D loss: 0.791612] [G loss: 1.490745]\n",
      "[D loss: 0.582139] [G loss: 1.434411]\n",
      "[D loss: 0.744746] [G loss: 1.342728]\n",
      "[D loss: 0.792841] [G loss: 1.346635]\n",
      "[D loss: 0.842229] [G loss: 1.562832]\n",
      "[D loss: 0.920402] [G loss: 1.421857]\n",
      "[D loss: 0.733407] [G loss: 1.439345]\n",
      "[D loss: 0.922982] [G loss: 1.417345]\n",
      "[D loss: 0.751930] [G loss: 1.274676]\n",
      "[D loss: 0.776634] [G loss: 1.400614]\n",
      "[D loss: 0.973924] [G loss: 1.255125]\n",
      "[D loss: 0.821011] [G loss: 1.373710]\n",
      "[D loss: 0.838484] [G loss: 1.422396]\n",
      "[D loss: 0.997261] [G loss: 1.375170]\n",
      "[D loss: 0.882887] [G loss: 1.508984]\n",
      "[D loss: 0.845275] [G loss: 1.511610]\n",
      "[D loss: 0.828118] [G loss: 1.375834]\n",
      "[D loss: 0.811169] [G loss: 1.425563]\n",
      "[D loss: 0.868952] [G loss: 1.564336]\n",
      "[D loss: 0.741785] [G loss: 1.449624]\n",
      "[D loss: 0.985877] [G loss: 1.400013]\n",
      "[D loss: 0.886947] [G loss: 1.440309]\n",
      "[D loss: 1.105051] [G loss: 1.393362]\n",
      "[D loss: 0.811616] [G loss: 1.338011]\n",
      "[D loss: 0.771956] [G loss: 1.529310]\n",
      "[D loss: 0.878570] [G loss: 1.468053]\n",
      "[D loss: 0.861925] [G loss: 1.460663]\n",
      "[D loss: 0.856772] [G loss: 1.621866]\n",
      "[D loss: 0.912857] [G loss: 1.239926]\n",
      "[D loss: 0.945306] [G loss: 1.403599]\n",
      "[D loss: 0.700681] [G loss: 1.654679]\n",
      "[D loss: 0.612661] [G loss: 1.432126]\n",
      "[D loss: 0.868657] [G loss: 1.312000]\n",
      "[D loss: 0.832158] [G loss: 1.224671]\n",
      "[D loss: 0.740667] [G loss: 1.511128]\n",
      "[D loss: 0.628422] [G loss: 1.637392]\n",
      "[D loss: 0.984866] [G loss: 1.411791]\n",
      "[D loss: 0.789570] [G loss: 1.500073]\n",
      "[D loss: 1.005130] [G loss: 1.426768]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.716478] [G loss: 1.413973]\n",
      "[D loss: 0.675432] [G loss: 1.494882]\n",
      "[D loss: 0.990946] [G loss: 1.369841]\n",
      "[D loss: 0.912437] [G loss: 1.313135]\n",
      "[D loss: 0.799207] [G loss: 1.621941]\n",
      "[D loss: 0.960406] [G loss: 1.186120]\n",
      "[D loss: 0.603500] [G loss: 1.373595]\n",
      "[D loss: 0.823211] [G loss: 1.728418]\n",
      "[D loss: 0.778329] [G loss: 1.366768]\n",
      "[D loss: 1.113628] [G loss: 1.372485]\n",
      "[D loss: 0.816287] [G loss: 1.534482]\n",
      "[D loss: 0.739963] [G loss: 1.607498]\n",
      "[D loss: 0.753778] [G loss: 1.529415]\n",
      "[D loss: 0.936794] [G loss: 1.375965]\n",
      "[D loss: 0.962167] [G loss: 1.418255]\n",
      "[D loss: 0.752219] [G loss: 1.479571]\n",
      "[D loss: 0.810471] [G loss: 1.577141]\n",
      "[D loss: 0.925690] [G loss: 1.382709]\n",
      "[D loss: 0.893988] [G loss: 1.537644]\n",
      "[D loss: 0.989147] [G loss: 1.434572]\n",
      "[D loss: 0.901953] [G loss: 1.392456]\n",
      "[D loss: 0.788882] [G loss: 1.344796]\n",
      "[D loss: 0.872040] [G loss: 1.482133]\n",
      "[D loss: 0.912355] [G loss: 1.394675]\n",
      "[D loss: 0.831000] [G loss: 1.527987]\n",
      "[D loss: 0.868765] [G loss: 1.521823]\n",
      "[D loss: 0.850281] [G loss: 1.227995]\n",
      "[D loss: 0.900684] [G loss: 1.141095]\n",
      "[D loss: 0.693136] [G loss: 1.463092]\n",
      "[D loss: 0.939489] [G loss: 1.417624]\n",
      "[D loss: 0.831140] [G loss: 1.604551]\n",
      "[D loss: 0.956274] [G loss: 1.468621]\n",
      "[D loss: 0.755235] [G loss: 1.592347]\n",
      "[D loss: 0.877843] [G loss: 1.233263]\n",
      "[D loss: 0.779401] [G loss: 1.368142]\n",
      "[D loss: 1.047636] [G loss: 1.103958]\n",
      "[D loss: 0.973941] [G loss: 1.446675]\n",
      "[D loss: 0.710420] [G loss: 1.464894]\n",
      "[D loss: 0.939035] [G loss: 1.380978]\n",
      "[D loss: 0.685558] [G loss: 1.467575]\n",
      "[D loss: 0.820803] [G loss: 1.368854]\n",
      "[D loss: 1.196374] [G loss: 1.523582]\n",
      "[D loss: 0.732544] [G loss: 1.590565]\n",
      "[D loss: 0.903785] [G loss: 1.515454]\n",
      "[D loss: 0.714160] [G loss: 1.481217]\n",
      "[D loss: 0.766531] [G loss: 1.692083]\n",
      "[D loss: 0.799040] [G loss: 1.347877]\n",
      "[D loss: 0.851428] [G loss: 1.416201]\n",
      "[D loss: 1.019721] [G loss: 1.308545]\n",
      "[D loss: 0.985096] [G loss: 1.159485]\n",
      "[D loss: 1.163155] [G loss: 1.151483]\n",
      "[D loss: 0.897341] [G loss: 1.403703]\n",
      "[D loss: 0.810918] [G loss: 1.337619]\n",
      "[D loss: 0.912787] [G loss: 1.645500]\n",
      "[D loss: 0.797251] [G loss: 1.635320]\n",
      "[D loss: 0.522255] [G loss: 1.435352]\n",
      "[D loss: 0.862895] [G loss: 1.602608]\n",
      "[D loss: 0.753142] [G loss: 1.532825]\n",
      "[D loss: 1.011114] [G loss: 1.336194]\n",
      "[D loss: 0.961730] [G loss: 1.463801]\n",
      "[D loss: 0.872581] [G loss: 1.410718]\n",
      "[D loss: 0.834357] [G loss: 1.418714]\n",
      "[D loss: 0.809847] [G loss: 1.400135]\n",
      "[D loss: 0.643905] [G loss: 1.585290]\n",
      "[D loss: 0.793235] [G loss: 1.433759]\n",
      "[D loss: 1.016337] [G loss: 1.479269]\n",
      "[D loss: 0.962803] [G loss: 1.319551]\n",
      "[D loss: 1.063109] [G loss: 1.166023]\n",
      "[D loss: 1.008876] [G loss: 1.363184]\n",
      "[D loss: 1.102861] [G loss: 1.428125]\n",
      "[D loss: 1.018703] [G loss: 1.385894]\n",
      "[D loss: 0.821008] [G loss: 1.394354]\n",
      "[D loss: 0.828382] [G loss: 1.419831]\n",
      "[D loss: 0.726887] [G loss: 1.455298]\n",
      "[D loss: 0.871252] [G loss: 1.693524]\n",
      "[D loss: 0.683083] [G loss: 1.415098]\n",
      "[D loss: 0.872885] [G loss: 1.260692]\n",
      "[D loss: 0.838082] [G loss: 1.263673]\n",
      "[D loss: 0.819026] [G loss: 1.467509]\n",
      "[D loss: 0.928937] [G loss: 1.495044]\n",
      "[D loss: 0.882121] [G loss: 1.238039]\n",
      "[D loss: 0.878106] [G loss: 1.309219]\n",
      "[D loss: 0.804260] [G loss: 1.470323]\n",
      "[D loss: 0.987050] [G loss: 1.265411]\n",
      "[D loss: 0.841888] [G loss: 1.334890]\n",
      "[D loss: 0.798417] [G loss: 1.371103]\n",
      "[D loss: 0.854024] [G loss: 1.305035]\n",
      "[D loss: 0.749846] [G loss: 1.368953]\n",
      "[D loss: 1.058576] [G loss: 1.339426]\n",
      "[D loss: 0.825438] [G loss: 1.513576]\n",
      "[D loss: 0.876236] [G loss: 1.506381]\n",
      "[D loss: 0.779764] [G loss: 1.484418]\n",
      "[D loss: 0.957346] [G loss: 1.467657]\n",
      "[D loss: 0.797050] [G loss: 1.444303]\n",
      "[D loss: 0.967995] [G loss: 1.382445]\n",
      "[D loss: 0.908341] [G loss: 1.097376]\n",
      "[D loss: 0.938263] [G loss: 1.423459]\n",
      "[D loss: 0.986942] [G loss: 1.457437]\n",
      "[D loss: 0.722760] [G loss: 1.607969]\n",
      "[D loss: 0.861993] [G loss: 1.235386]\n",
      "[D loss: 0.768186] [G loss: 1.346003]\n",
      "[D loss: 0.787995] [G loss: 1.505071]\n",
      "[D loss: 0.706745] [G loss: 1.482199]\n",
      "[D loss: 0.923606] [G loss: 1.428605]\n",
      "[D loss: 0.816847] [G loss: 1.343548]\n",
      "[D loss: 0.628858] [G loss: 1.602341]\n",
      "[D loss: 0.921774] [G loss: 1.451745]\n",
      "[D loss: 0.709959] [G loss: 1.544242]\n",
      "[D loss: 0.726321] [G loss: 1.432666]\n",
      "[D loss: 0.710251] [G loss: 1.389428]\n",
      "[D loss: 1.039022] [G loss: 1.464465]\n",
      "[D loss: 0.651282] [G loss: 1.622941]\n",
      "[D loss: 0.805985] [G loss: 1.342515]\n",
      "[D loss: 0.737039] [G loss: 1.452279]\n",
      "[D loss: 0.908380] [G loss: 1.393724]\n",
      "[D loss: 0.967364] [G loss: 1.387826]\n",
      "[D loss: 0.812026] [G loss: 1.387673]\n",
      "[D loss: 0.720858] [G loss: 1.407293]\n",
      "[D loss: 0.951148] [G loss: 1.556570]\n",
      "[D loss: 0.799348] [G loss: 1.488918]\n",
      "[D loss: 0.863466] [G loss: 1.507800]\n",
      "[D loss: 0.986166] [G loss: 1.247852]\n",
      "[D loss: 0.646752] [G loss: 1.538980]\n",
      "[D loss: 1.142653] [G loss: 1.474427]\n",
      "[D loss: 0.744252] [G loss: 1.412786]\n",
      "[D loss: 0.807871] [G loss: 1.401138]\n",
      "[D loss: 0.994794] [G loss: 1.289682]\n",
      "[D loss: 0.805656] [G loss: 1.342110]\n",
      "[D loss: 0.962205] [G loss: 1.643519]\n",
      "[D loss: 0.679116] [G loss: 1.438782]\n",
      "[D loss: 0.737850] [G loss: 1.461021]\n",
      "[D loss: 1.016606] [G loss: 1.404259]\n",
      "[D loss: 0.842745] [G loss: 1.391158]\n",
      "[D loss: 1.373077] [G loss: 1.377061]\n",
      "[D loss: 0.823025] [G loss: 1.458625]\n",
      "[D loss: 0.900214] [G loss: 1.364548]\n",
      "[D loss: 0.885115] [G loss: 1.369181]\n",
      "[D loss: 0.878520] [G loss: 1.270449]\n",
      "[D loss: 0.807101] [G loss: 1.670797]\n",
      "[D loss: 0.914081] [G loss: 1.358291]\n",
      "[D loss: 0.928597] [G loss: 1.437054]\n",
      "[D loss: 0.887956] [G loss: 1.273832]\n",
      "[D loss: 0.931246] [G loss: 1.360216]\n",
      "[D loss: 0.965251] [G loss: 1.270099]\n",
      "[D loss: 1.221285] [G loss: 1.537772]\n",
      "[D loss: 0.895378] [G loss: 1.348918]\n",
      "[D loss: 0.847560] [G loss: 1.233533]\n",
      "[D loss: 0.704587] [G loss: 1.364189]\n",
      "[D loss: 1.104685] [G loss: 1.348090]\n",
      "[D loss: 0.836256] [G loss: 1.357337]\n",
      "[D loss: 0.861677] [G loss: 1.222968]\n",
      "[D loss: 0.592545] [G loss: 1.518152]\n",
      "[D loss: 0.820371] [G loss: 1.318699]\n",
      "[D loss: 1.175780] [G loss: 1.341345]\n",
      "[D loss: 0.847262] [G loss: 1.468975]\n",
      "[D loss: 0.980893] [G loss: 1.322803]\n",
      "[D loss: 0.969293] [G loss: 1.374730]\n",
      "[D loss: 0.937268] [G loss: 1.370468]\n",
      "[D loss: 0.875498] [G loss: 1.319661]\n",
      "[D loss: 0.888409] [G loss: 1.644371]\n",
      "[D loss: 0.850892] [G loss: 1.420393]\n",
      "[D loss: 0.811318] [G loss: 1.488588]\n",
      "[D loss: 1.173770] [G loss: 1.188652]\n",
      "[D loss: 0.990650] [G loss: 1.365388]\n",
      "[D loss: 0.906937] [G loss: 1.382260]\n",
      "[D loss: 0.831479] [G loss: 1.532245]\n",
      "[D loss: 0.768420] [G loss: 1.438364]\n",
      "[D loss: 0.873087] [G loss: 1.280284]\n",
      "[D loss: 1.129860] [G loss: 1.337491]\n",
      "[D loss: 0.803009] [G loss: 1.295116]\n",
      "[D loss: 0.880589] [G loss: 1.289838]\n",
      "[D loss: 0.607218] [G loss: 1.602706]\n",
      "[D loss: 0.873270] [G loss: 1.387300]\n",
      "[D loss: 0.637197] [G loss: 1.505574]\n",
      "[D loss: 0.840282] [G loss: 1.418267]\n",
      "[D loss: 0.710742] [G loss: 1.476932]\n",
      "[D loss: 0.946571] [G loss: 1.414111]\n",
      "[D loss: 0.746137] [G loss: 1.363540]\n",
      "[D loss: 0.718220] [G loss: 1.541868]\n",
      "[D loss: 0.793372] [G loss: 1.518110]\n",
      "[D loss: 0.962931] [G loss: 1.476397]\n",
      "[D loss: 0.844460] [G loss: 1.494127]\n",
      "[D loss: 0.939803] [G loss: 1.558658]\n",
      "[D loss: 0.891334] [G loss: 1.354634]\n",
      "[D loss: 0.864487] [G loss: 1.437383]\n",
      "[D loss: 0.680689] [G loss: 1.443496]\n",
      "[D loss: 0.760592] [G loss: 1.564903]\n",
      "[D loss: 0.918963] [G loss: 1.333197]\n",
      "[D loss: 0.865711] [G loss: 1.380824]\n",
      "[D loss: 0.873140] [G loss: 1.501273]\n",
      "[D loss: 0.906587] [G loss: 1.446907]\n",
      "[D loss: 1.044856] [G loss: 1.552234]\n",
      "[D loss: 0.866494] [G loss: 1.563756]\n",
      "[D loss: 0.971908] [G loss: 1.399264]\n",
      "[D loss: 0.865885] [G loss: 1.336624]\n",
      "[D loss: 0.840827] [G loss: 1.492402]\n",
      "[D loss: 0.823378] [G loss: 1.610291]\n",
      "[D loss: 0.690173] [G loss: 1.442493]\n",
      "[D loss: 0.894726] [G loss: 1.278014]\n",
      "[D loss: 0.842343] [G loss: 1.560569]\n",
      "[D loss: 0.902261] [G loss: 1.328133]\n",
      "[D loss: 0.806198] [G loss: 1.373111]\n",
      "[D loss: 1.095336] [G loss: 1.322329]\n",
      "[D loss: 1.182297] [G loss: 1.315487]\n",
      "[D loss: 0.789398] [G loss: 1.263131]\n",
      "[D loss: 0.733473] [G loss: 1.320816]\n",
      "[D loss: 0.884188] [G loss: 1.337016]\n",
      "[D loss: 0.666014] [G loss: 1.386765]\n",
      "[D loss: 1.154734] [G loss: 1.265405]\n",
      "[D loss: 0.643490] [G loss: 1.514403]\n",
      "[D loss: 0.949041] [G loss: 1.225853]\n",
      "[D loss: 0.699906] [G loss: 1.412565]\n",
      "[D loss: 0.832349] [G loss: 1.516917]\n",
      "[D loss: 0.672243] [G loss: 1.416395]\n",
      "[D loss: 0.771598] [G loss: 1.531217]\n",
      "[D loss: 0.899130] [G loss: 1.366143]\n",
      "[D loss: 0.905678] [G loss: 1.632004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 1.073214] [G loss: 1.351153]\n",
      "[D loss: 0.783058] [G loss: 1.475857]\n",
      "[D loss: 0.895385] [G loss: 1.555466]\n",
      "[D loss: 1.049916] [G loss: 1.497486]\n",
      "[D loss: 0.606175] [G loss: 1.589234]\n",
      "[D loss: 0.741714] [G loss: 1.300404]\n",
      "[D loss: 0.743528] [G loss: 1.620962]\n",
      "[D loss: 0.710104] [G loss: 1.458956]\n",
      "[D loss: 0.986169] [G loss: 1.274473]\n",
      "[D loss: 0.818182] [G loss: 1.303813]\n",
      "[D loss: 0.905154] [G loss: 1.361280]\n",
      "[D loss: 0.994884] [G loss: 1.444549]\n",
      "[D loss: 0.871648] [G loss: 1.554425]\n",
      "[D loss: 0.994336] [G loss: 1.604153]\n",
      "[D loss: 0.883415] [G loss: 1.337386]\n",
      "[D loss: 0.911090] [G loss: 1.378400]\n",
      "[D loss: 1.012859] [G loss: 1.180063]\n",
      "[D loss: 1.056967] [G loss: 1.396877]\n",
      "[D loss: 0.875066] [G loss: 1.381791]\n",
      "[D loss: 0.871147] [G loss: 1.584502]\n",
      "[D loss: 0.926558] [G loss: 1.526055]\n",
      "[D loss: 0.764807] [G loss: 1.685822]\n",
      "[D loss: 0.742002] [G loss: 1.681146]\n",
      "[D loss: 0.880064] [G loss: 1.285271]\n",
      "[D loss: 0.858804] [G loss: 1.019050]\n",
      "[D loss: 0.902616] [G loss: 1.217558]\n",
      "[D loss: 0.816513] [G loss: 1.370832]\n",
      "epoch:16, g_loss:2710.105224609375,d_loss:1587.6390380859375\n",
      "[D loss: 0.773270] [G loss: 1.484774]\n",
      "[D loss: 0.853099] [G loss: 1.325078]\n",
      "[D loss: 0.814999] [G loss: 1.384451]\n",
      "[D loss: 0.635924] [G loss: 1.641215]\n",
      "[D loss: 0.774007] [G loss: 1.429148]\n",
      "[D loss: 0.806356] [G loss: 1.612537]\n",
      "[D loss: 0.568618] [G loss: 1.532697]\n",
      "[D loss: 1.027030] [G loss: 1.504523]\n",
      "[D loss: 0.799790] [G loss: 1.542626]\n",
      "[D loss: 0.755795] [G loss: 1.613383]\n",
      "[D loss: 1.125601] [G loss: 1.398440]\n",
      "[D loss: 0.704160] [G loss: 1.570459]\n",
      "[D loss: 0.828964] [G loss: 1.562809]\n",
      "[D loss: 0.887635] [G loss: 1.664410]\n",
      "[D loss: 1.009027] [G loss: 1.620843]\n",
      "[D loss: 0.883094] [G loss: 1.419911]\n",
      "[D loss: 0.701310] [G loss: 1.513090]\n",
      "[D loss: 0.861459] [G loss: 1.440366]\n",
      "[D loss: 1.065067] [G loss: 1.592545]\n",
      "[D loss: 0.631776] [G loss: 1.468692]\n",
      "[D loss: 0.951117] [G loss: 1.326654]\n",
      "[D loss: 0.785169] [G loss: 1.320075]\n",
      "[D loss: 0.813453] [G loss: 1.482119]\n",
      "[D loss: 0.876184] [G loss: 1.662379]\n",
      "[D loss: 0.645526] [G loss: 1.467010]\n",
      "[D loss: 0.800964] [G loss: 1.429655]\n",
      "[D loss: 0.962318] [G loss: 1.371752]\n",
      "[D loss: 0.591850] [G loss: 1.520197]\n",
      "[D loss: 0.731237] [G loss: 1.502989]\n",
      "[D loss: 0.882679] [G loss: 1.319491]\n",
      "[D loss: 0.601252] [G loss: 1.375704]\n",
      "[D loss: 0.909097] [G loss: 1.370854]\n",
      "[D loss: 0.974271] [G loss: 1.261802]\n",
      "[D loss: 0.835898] [G loss: 1.562788]\n",
      "[D loss: 0.689197] [G loss: 1.658368]\n",
      "[D loss: 0.824099] [G loss: 1.504512]\n",
      "[D loss: 0.854572] [G loss: 1.424922]\n",
      "[D loss: 0.927195] [G loss: 1.496434]\n",
      "[D loss: 0.942108] [G loss: 1.267921]\n",
      "[D loss: 0.842120] [G loss: 1.557132]\n",
      "[D loss: 0.617181] [G loss: 1.149314]\n",
      "[D loss: 0.907703] [G loss: 1.538207]\n",
      "[D loss: 0.817063] [G loss: 1.552862]\n",
      "[D loss: 0.797618] [G loss: 1.473967]\n",
      "[D loss: 1.085724] [G loss: 1.373914]\n",
      "[D loss: 0.863783] [G loss: 1.606789]\n",
      "[D loss: 0.664960] [G loss: 1.416972]\n",
      "[D loss: 0.978786] [G loss: 1.504037]\n",
      "[D loss: 1.082202] [G loss: 1.335881]\n",
      "[D loss: 0.842112] [G loss: 1.657587]\n",
      "[D loss: 0.769038] [G loss: 1.553482]\n",
      "[D loss: 0.754578] [G loss: 1.337411]\n",
      "[D loss: 0.741480] [G loss: 1.378051]\n",
      "[D loss: 0.745688] [G loss: 1.311166]\n",
      "[D loss: 0.787468] [G loss: 1.438906]\n",
      "[D loss: 0.742871] [G loss: 1.502512]\n",
      "[D loss: 0.938999] [G loss: 1.376231]\n",
      "[D loss: 0.810779] [G loss: 1.641949]\n",
      "[D loss: 0.969490] [G loss: 1.427163]\n",
      "[D loss: 0.873987] [G loss: 1.294820]\n",
      "[D loss: 0.819398] [G loss: 1.428700]\n",
      "[D loss: 0.803339] [G loss: 1.588901]\n",
      "[D loss: 0.668565] [G loss: 1.616127]\n",
      "[D loss: 0.656824] [G loss: 1.486075]\n",
      "[D loss: 0.772732] [G loss: 1.623464]\n",
      "[D loss: 0.869080] [G loss: 1.716817]\n",
      "[D loss: 0.758728] [G loss: 1.434214]\n",
      "[D loss: 0.646348] [G loss: 1.441353]\n",
      "[D loss: 0.854207] [G loss: 1.274204]\n",
      "[D loss: 0.969743] [G loss: 1.542017]\n",
      "[D loss: 1.135943] [G loss: 1.553453]\n",
      "[D loss: 0.804572] [G loss: 1.332143]\n",
      "[D loss: 0.992772] [G loss: 1.393955]\n",
      "[D loss: 0.799759] [G loss: 1.389583]\n",
      "[D loss: 0.927471] [G loss: 1.211459]\n",
      "[D loss: 0.797612] [G loss: 1.483920]\n",
      "[D loss: 0.875612] [G loss: 1.664944]\n",
      "[D loss: 0.983628] [G loss: 1.434186]\n",
      "[D loss: 1.141986] [G loss: 1.264339]\n",
      "[D loss: 0.814929] [G loss: 1.534863]\n",
      "[D loss: 0.842910] [G loss: 1.199034]\n",
      "[D loss: 0.901524] [G loss: 1.313309]\n",
      "[D loss: 0.759444] [G loss: 1.279198]\n",
      "[D loss: 0.624677] [G loss: 1.642672]\n",
      "[D loss: 0.784429] [G loss: 1.546609]\n",
      "[D loss: 0.809089] [G loss: 1.580875]\n",
      "[D loss: 0.945356] [G loss: 1.436885]\n",
      "[D loss: 0.972490] [G loss: 1.252832]\n",
      "[D loss: 0.992368] [G loss: 1.544470]\n",
      "[D loss: 0.716685] [G loss: 1.636303]\n",
      "[D loss: 0.793839] [G loss: 1.612912]\n",
      "[D loss: 0.989947] [G loss: 1.292629]\n",
      "[D loss: 0.758890] [G loss: 1.585434]\n",
      "[D loss: 0.886996] [G loss: 1.426009]\n",
      "[D loss: 0.928096] [G loss: 1.427799]\n",
      "[D loss: 0.703107] [G loss: 1.486485]\n",
      "[D loss: 0.574533] [G loss: 1.449945]\n",
      "[D loss: 0.955135] [G loss: 1.476765]\n",
      "[D loss: 0.940999] [G loss: 1.277138]\n",
      "[D loss: 0.782686] [G loss: 1.498877]\n",
      "[D loss: 1.016889] [G loss: 1.330023]\n",
      "[D loss: 0.900111] [G loss: 1.403939]\n",
      "[D loss: 0.904655] [G loss: 1.215781]\n",
      "[D loss: 0.843891] [G loss: 1.325123]\n",
      "[D loss: 0.909697] [G loss: 1.577216]\n",
      "[D loss: 0.972043] [G loss: 1.280139]\n",
      "[D loss: 0.634021] [G loss: 1.462460]\n",
      "[D loss: 0.616731] [G loss: 1.522618]\n",
      "[D loss: 0.974823] [G loss: 1.052062]\n",
      "[D loss: 0.858967] [G loss: 1.560483]\n",
      "[D loss: 0.814946] [G loss: 1.709256]\n",
      "[D loss: 0.726384] [G loss: 1.523677]\n",
      "[D loss: 0.757295] [G loss: 1.548891]\n",
      "[D loss: 1.102288] [G loss: 1.255350]\n",
      "[D loss: 0.715222] [G loss: 1.559783]\n",
      "[D loss: 1.027821] [G loss: 1.382179]\n",
      "[D loss: 0.802324] [G loss: 1.362916]\n",
      "[D loss: 0.731812] [G loss: 1.472033]\n",
      "[D loss: 0.755151] [G loss: 1.767805]\n",
      "[D loss: 0.811289] [G loss: 1.586262]\n",
      "[D loss: 0.632728] [G loss: 1.662633]\n",
      "[D loss: 0.963979] [G loss: 1.523252]\n",
      "[D loss: 0.825706] [G loss: 1.668048]\n",
      "[D loss: 0.655873] [G loss: 1.675878]\n",
      "[D loss: 0.712327] [G loss: 1.557892]\n",
      "[D loss: 0.878600] [G loss: 1.378224]\n",
      "[D loss: 0.929719] [G loss: 1.363503]\n",
      "[D loss: 0.673274] [G loss: 1.531760]\n",
      "[D loss: 0.748663] [G loss: 1.446011]\n",
      "[D loss: 0.910966] [G loss: 1.744010]\n",
      "[D loss: 1.068403] [G loss: 1.281889]\n",
      "[D loss: 0.711424] [G loss: 1.619089]\n",
      "[D loss: 0.870890] [G loss: 1.383590]\n",
      "[D loss: 0.768272] [G loss: 1.228329]\n",
      "[D loss: 0.771713] [G loss: 1.319322]\n",
      "[D loss: 1.001107] [G loss: 1.464993]\n",
      "[D loss: 0.871953] [G loss: 1.573436]\n",
      "[D loss: 0.780498] [G loss: 1.747332]\n",
      "[D loss: 0.707363] [G loss: 1.526757]\n",
      "[D loss: 0.837484] [G loss: 1.450127]\n",
      "[D loss: 0.664040] [G loss: 1.563637]\n",
      "[D loss: 0.766178] [G loss: 1.513615]\n",
      "[D loss: 0.642483] [G loss: 1.628839]\n",
      "[D loss: 0.833766] [G loss: 1.414756]\n",
      "[D loss: 0.861596] [G loss: 1.564439]\n",
      "[D loss: 0.966924] [G loss: 1.765033]\n",
      "[D loss: 0.692978] [G loss: 1.461210]\n",
      "[D loss: 0.869816] [G loss: 1.400573]\n",
      "[D loss: 0.886197] [G loss: 1.485889]\n",
      "[D loss: 1.035293] [G loss: 1.632895]\n",
      "[D loss: 0.715099] [G loss: 1.402137]\n",
      "[D loss: 0.862551] [G loss: 1.472415]\n",
      "[D loss: 1.173077] [G loss: 1.270473]\n",
      "[D loss: 0.790218] [G loss: 1.473687]\n",
      "[D loss: 0.953767] [G loss: 1.298720]\n",
      "[D loss: 0.849765] [G loss: 1.677868]\n",
      "[D loss: 1.022340] [G loss: 1.646087]\n",
      "[D loss: 0.804996] [G loss: 1.409420]\n",
      "[D loss: 0.890116] [G loss: 1.473312]\n",
      "[D loss: 0.874023] [G loss: 1.141797]\n",
      "[D loss: 0.767693] [G loss: 1.302357]\n",
      "[D loss: 0.618457] [G loss: 1.468690]\n",
      "[D loss: 0.727793] [G loss: 1.318561]\n",
      "[D loss: 0.744951] [G loss: 1.498435]\n",
      "[D loss: 0.813287] [G loss: 1.253332]\n",
      "[D loss: 1.097817] [G loss: 1.250705]\n",
      "[D loss: 0.576400] [G loss: 1.744438]\n",
      "[D loss: 0.895227] [G loss: 1.470247]\n",
      "[D loss: 1.034142] [G loss: 1.588020]\n",
      "[D loss: 0.877303] [G loss: 1.399783]\n",
      "[D loss: 1.004683] [G loss: 1.332489]\n",
      "[D loss: 0.612696] [G loss: 1.592194]\n",
      "[D loss: 0.827576] [G loss: 1.407131]\n",
      "[D loss: 0.834858] [G loss: 1.258871]\n",
      "[D loss: 0.778318] [G loss: 1.528312]\n",
      "[D loss: 0.655951] [G loss: 1.295375]\n",
      "[D loss: 0.623536] [G loss: 1.489242]\n",
      "[D loss: 0.816761] [G loss: 1.390952]\n",
      "[D loss: 0.850261] [G loss: 1.668256]\n",
      "[D loss: 0.968546] [G loss: 1.567138]\n",
      "[D loss: 0.878484] [G loss: 1.579161]\n",
      "[D loss: 0.870427] [G loss: 1.554145]\n",
      "[D loss: 0.785532] [G loss: 1.533062]\n",
      "[D loss: 0.835008] [G loss: 1.538746]\n",
      "[D loss: 1.030935] [G loss: 1.129448]\n",
      "[D loss: 0.657208] [G loss: 1.516391]\n",
      "[D loss: 0.743498] [G loss: 1.347089]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.680354] [G loss: 1.255424]\n",
      "[D loss: 1.066394] [G loss: 1.483053]\n",
      "[D loss: 0.825627] [G loss: 1.333265]\n",
      "[D loss: 0.722976] [G loss: 1.508348]\n",
      "[D loss: 0.727131] [G loss: 1.420395]\n",
      "[D loss: 0.867287] [G loss: 1.737810]\n",
      "[D loss: 0.787304] [G loss: 1.633982]\n",
      "[D loss: 0.967977] [G loss: 1.114809]\n",
      "[D loss: 0.812619] [G loss: 1.394958]\n",
      "[D loss: 0.845158] [G loss: 1.785157]\n",
      "[D loss: 0.827183] [G loss: 1.461929]\n",
      "[D loss: 0.944499] [G loss: 1.551897]\n",
      "[D loss: 0.990332] [G loss: 1.377654]\n",
      "[D loss: 0.959220] [G loss: 1.576604]\n",
      "[D loss: 0.893705] [G loss: 1.485440]\n",
      "[D loss: 0.722520] [G loss: 1.701441]\n",
      "[D loss: 0.836859] [G loss: 1.330987]\n",
      "[D loss: 0.945817] [G loss: 1.219023]\n",
      "[D loss: 0.733582] [G loss: 1.317233]\n",
      "[D loss: 0.684810] [G loss: 1.514485]\n",
      "[D loss: 0.905570] [G loss: 1.514597]\n",
      "[D loss: 1.028214] [G loss: 1.471088]\n",
      "[D loss: 0.741721] [G loss: 1.597216]\n",
      "[D loss: 1.030048] [G loss: 1.420688]\n",
      "[D loss: 0.670143] [G loss: 1.429787]\n",
      "[D loss: 0.792614] [G loss: 1.289613]\n",
      "[D loss: 0.617651] [G loss: 1.709827]\n",
      "[D loss: 1.021898] [G loss: 1.588794]\n",
      "[D loss: 0.999339] [G loss: 1.408719]\n",
      "[D loss: 0.725788] [G loss: 1.410375]\n",
      "[D loss: 0.913021] [G loss: 1.635831]\n",
      "[D loss: 0.930165] [G loss: 1.531392]\n",
      "[D loss: 0.840411] [G loss: 1.368632]\n",
      "[D loss: 0.796971] [G loss: 1.421231]\n",
      "[D loss: 0.897294] [G loss: 1.255824]\n",
      "[D loss: 0.825541] [G loss: 1.402258]\n",
      "[D loss: 0.896482] [G loss: 1.486536]\n",
      "[D loss: 0.604968] [G loss: 1.391158]\n",
      "[D loss: 0.573066] [G loss: 1.655467]\n",
      "[D loss: 0.816708] [G loss: 1.406222]\n",
      "[D loss: 0.631605] [G loss: 1.444739]\n",
      "[D loss: 0.902975] [G loss: 1.617804]\n",
      "[D loss: 0.715325] [G loss: 1.421690]\n",
      "[D loss: 0.798122] [G loss: 1.671256]\n",
      "[D loss: 0.686758] [G loss: 1.605761]\n",
      "[D loss: 0.874100] [G loss: 1.446056]\n",
      "[D loss: 0.947734] [G loss: 1.382203]\n",
      "[D loss: 0.652792] [G loss: 1.423445]\n",
      "[D loss: 0.770876] [G loss: 1.493634]\n",
      "[D loss: 0.782320] [G loss: 1.840907]\n",
      "[D loss: 0.904977] [G loss: 1.458596]\n",
      "[D loss: 0.937801] [G loss: 1.482730]\n",
      "[D loss: 0.872679] [G loss: 1.311438]\n",
      "[D loss: 0.768603] [G loss: 1.363592]\n",
      "[D loss: 0.827591] [G loss: 1.439067]\n",
      "[D loss: 0.903814] [G loss: 1.779019]\n",
      "[D loss: 0.789549] [G loss: 1.545021]\n",
      "[D loss: 0.950955] [G loss: 1.310718]\n",
      "[D loss: 0.783250] [G loss: 1.458752]\n",
      "[D loss: 0.891474] [G loss: 1.491620]\n",
      "[D loss: 0.730433] [G loss: 1.240498]\n",
      "[D loss: 0.748068] [G loss: 1.616075]\n",
      "[D loss: 0.726303] [G loss: 1.427488]\n",
      "[D loss: 0.850083] [G loss: 1.553561]\n",
      "[D loss: 0.732156] [G loss: 1.472812]\n",
      "[D loss: 0.640951] [G loss: 1.569168]\n",
      "[D loss: 0.979321] [G loss: 1.347413]\n",
      "[D loss: 0.954137] [G loss: 1.218342]\n",
      "[D loss: 0.698497] [G loss: 1.540258]\n",
      "[D loss: 0.751535] [G loss: 1.724883]\n",
      "[D loss: 0.677397] [G loss: 1.591639]\n",
      "[D loss: 0.755337] [G loss: 1.548077]\n",
      "[D loss: 0.556840] [G loss: 1.696210]\n",
      "[D loss: 0.929331] [G loss: 1.762846]\n",
      "[D loss: 0.832098] [G loss: 1.395024]\n",
      "[D loss: 1.061662] [G loss: 1.581888]\n",
      "[D loss: 1.156284] [G loss: 1.182360]\n",
      "[D loss: 0.722354] [G loss: 1.445224]\n",
      "[D loss: 0.826900] [G loss: 1.399341]\n",
      "[D loss: 0.826509] [G loss: 1.434794]\n",
      "[D loss: 0.794168] [G loss: 1.531366]\n",
      "[D loss: 0.809565] [G loss: 1.535923]\n",
      "[D loss: 0.884117] [G loss: 1.480550]\n",
      "[D loss: 0.698965] [G loss: 1.452590]\n",
      "[D loss: 0.956735] [G loss: 1.353527]\n",
      "[D loss: 0.955779] [G loss: 1.350259]\n",
      "[D loss: 0.881577] [G loss: 1.353372]\n",
      "[D loss: 0.742787] [G loss: 1.274940]\n",
      "[D loss: 0.684782] [G loss: 1.411769]\n",
      "[D loss: 0.972687] [G loss: 1.125776]\n",
      "[D loss: 0.854967] [G loss: 1.367936]\n",
      "[D loss: 0.675869] [G loss: 1.639292]\n",
      "[D loss: 0.841588] [G loss: 1.478835]\n",
      "[D loss: 0.833925] [G loss: 1.342720]\n",
      "[D loss: 0.959757] [G loss: 1.498954]\n",
      "[D loss: 0.844989] [G loss: 1.546506]\n",
      "[D loss: 0.918853] [G loss: 1.439098]\n",
      "[D loss: 0.825490] [G loss: 1.511193]\n",
      "[D loss: 0.891039] [G loss: 1.305402]\n",
      "[D loss: 0.807736] [G loss: 1.492534]\n",
      "[D loss: 1.037499] [G loss: 1.527702]\n",
      "[D loss: 0.930116] [G loss: 1.336264]\n",
      "[D loss: 0.799838] [G loss: 1.251849]\n",
      "[D loss: 0.707787] [G loss: 1.417783]\n",
      "[D loss: 0.906197] [G loss: 1.327428]\n",
      "[D loss: 0.795002] [G loss: 1.474817]\n",
      "[D loss: 0.940060] [G loss: 1.338369]\n",
      "[D loss: 0.971950] [G loss: 1.406300]\n",
      "[D loss: 0.880679] [G loss: 1.544215]\n",
      "[D loss: 0.906387] [G loss: 1.510844]\n",
      "[D loss: 0.670298] [G loss: 1.637799]\n",
      "[D loss: 0.672605] [G loss: 1.620927]\n",
      "[D loss: 0.784421] [G loss: 1.448750]\n",
      "[D loss: 0.654354] [G loss: 1.587486]\n",
      "[D loss: 0.828339] [G loss: 1.344729]\n",
      "[D loss: 0.800656] [G loss: 1.393639]\n",
      "[D loss: 1.058141] [G loss: 1.674669]\n",
      "[D loss: 0.826486] [G loss: 1.699029]\n",
      "[D loss: 0.943375] [G loss: 1.459567]\n",
      "[D loss: 0.853902] [G loss: 1.167138]\n",
      "[D loss: 0.789621] [G loss: 1.436524]\n",
      "[D loss: 0.912082] [G loss: 1.301416]\n",
      "[D loss: 0.759894] [G loss: 1.316690]\n",
      "[D loss: 0.949059] [G loss: 1.417379]\n",
      "[D loss: 0.811678] [G loss: 1.339020]\n",
      "[D loss: 1.079528] [G loss: 1.232720]\n",
      "[D loss: 0.682733] [G loss: 1.593043]\n",
      "[D loss: 0.587233] [G loss: 1.581407]\n",
      "[D loss: 0.764037] [G loss: 1.329679]\n",
      "[D loss: 0.758105] [G loss: 1.346029]\n",
      "[D loss: 0.711951] [G loss: 1.562472]\n",
      "[D loss: 0.876456] [G loss: 1.367682]\n",
      "[D loss: 0.668595] [G loss: 1.494300]\n",
      "[D loss: 0.762233] [G loss: 1.261373]\n",
      "[D loss: 0.755120] [G loss: 1.577616]\n",
      "[D loss: 0.866389] [G loss: 1.619353]\n",
      "[D loss: 0.629354] [G loss: 1.495837]\n",
      "[D loss: 0.887889] [G loss: 1.660474]\n",
      "[D loss: 0.815601] [G loss: 1.410503]\n",
      "[D loss: 0.570264] [G loss: 1.529320]\n",
      "[D loss: 0.742662] [G loss: 1.386024]\n",
      "[D loss: 0.686080] [G loss: 1.572451]\n",
      "[D loss: 0.854524] [G loss: 1.655428]\n",
      "[D loss: 0.789739] [G loss: 1.613542]\n",
      "[D loss: 0.855210] [G loss: 1.764839]\n",
      "[D loss: 0.967052] [G loss: 1.566839]\n",
      "[D loss: 0.689888] [G loss: 1.463622]\n",
      "[D loss: 0.936051] [G loss: 1.375123]\n",
      "[D loss: 1.039763] [G loss: 1.066555]\n",
      "[D loss: 0.842125] [G loss: 1.359590]\n",
      "[D loss: 0.711039] [G loss: 1.726230]\n",
      "[D loss: 0.876337] [G loss: 1.727930]\n",
      "[D loss: 0.738189] [G loss: 1.393196]\n",
      "[D loss: 0.829605] [G loss: 1.597858]\n",
      "[D loss: 0.710282] [G loss: 1.701875]\n",
      "[D loss: 0.882508] [G loss: 1.294762]\n",
      "[D loss: 0.735122] [G loss: 1.584500]\n",
      "[D loss: 0.866874] [G loss: 1.698216]\n",
      "[D loss: 0.753693] [G loss: 1.599083]\n",
      "[D loss: 0.830962] [G loss: 1.279205]\n",
      "[D loss: 0.679382] [G loss: 1.569826]\n",
      "[D loss: 1.057405] [G loss: 1.508573]\n",
      "[D loss: 0.773461] [G loss: 1.690779]\n",
      "[D loss: 0.742526] [G loss: 1.543076]\n",
      "[D loss: 0.840560] [G loss: 1.472172]\n",
      "[D loss: 1.154279] [G loss: 1.203023]\n",
      "[D loss: 0.609437] [G loss: 1.509313]\n",
      "[D loss: 1.029456] [G loss: 1.471227]\n",
      "[D loss: 0.755706] [G loss: 1.537197]\n",
      "[D loss: 0.683146] [G loss: 1.563481]\n",
      "[D loss: 0.681611] [G loss: 1.449312]\n",
      "[D loss: 0.933756] [G loss: 1.274665]\n",
      "[D loss: 0.798901] [G loss: 1.374534]\n",
      "[D loss: 0.980058] [G loss: 1.458439]\n",
      "[D loss: 0.769351] [G loss: 1.435610]\n",
      "[D loss: 0.853580] [G loss: 1.309858]\n",
      "[D loss: 0.832717] [G loss: 1.325730]\n",
      "[D loss: 0.752098] [G loss: 1.492803]\n",
      "[D loss: 0.765176] [G loss: 1.407349]\n",
      "[D loss: 0.690547] [G loss: 1.489331]\n",
      "[D loss: 0.786547] [G loss: 1.625395]\n",
      "[D loss: 0.633973] [G loss: 1.656104]\n",
      "[D loss: 1.003390] [G loss: 1.407049]\n",
      "[D loss: 0.936817] [G loss: 1.564777]\n",
      "[D loss: 0.837514] [G loss: 1.435655]\n",
      "[D loss: 1.111175] [G loss: 1.538923]\n",
      "[D loss: 0.820496] [G loss: 1.484143]\n",
      "[D loss: 0.797534] [G loss: 1.528793]\n",
      "[D loss: 0.893243] [G loss: 1.544514]\n",
      "[D loss: 0.870871] [G loss: 1.389733]\n",
      "[D loss: 0.858872] [G loss: 1.339800]\n",
      "[D loss: 0.736171] [G loss: 1.294507]\n",
      "[D loss: 0.991053] [G loss: 1.378973]\n",
      "[D loss: 0.843380] [G loss: 1.476620]\n",
      "[D loss: 0.904385] [G loss: 1.544221]\n",
      "[D loss: 0.809882] [G loss: 1.306402]\n",
      "[D loss: 0.822114] [G loss: 1.446549]\n",
      "[D loss: 0.952651] [G loss: 1.505192]\n",
      "[D loss: 0.776312] [G loss: 1.590128]\n",
      "[D loss: 0.817687] [G loss: 1.280455]\n",
      "[D loss: 1.061332] [G loss: 1.359798]\n",
      "[D loss: 0.836939] [G loss: 1.341159]\n",
      "[D loss: 0.770037] [G loss: 1.491389]\n",
      "[D loss: 0.816451] [G loss: 1.487055]\n",
      "[D loss: 0.852130] [G loss: 1.433943]\n",
      "[D loss: 0.793591] [G loss: 1.465062]\n",
      "[D loss: 0.606723] [G loss: 1.659476]\n",
      "[D loss: 0.820546] [G loss: 1.390674]\n",
      "[D loss: 0.720613] [G loss: 1.468568]\n",
      "[D loss: 0.930320] [G loss: 1.419888]\n",
      "[D loss: 0.597103] [G loss: 1.721441]\n",
      "[D loss: 0.675863] [G loss: 1.521766]\n",
      "[D loss: 0.766005] [G loss: 1.661863]\n",
      "[D loss: 0.835705] [G loss: 1.695517]\n",
      "[D loss: 0.767160] [G loss: 1.604068]\n",
      "[D loss: 0.773244] [G loss: 1.367638]\n",
      "[D loss: 0.621111] [G loss: 1.867382]\n",
      "[D loss: 1.000008] [G loss: 1.554369]\n",
      "[D loss: 0.693138] [G loss: 1.504655]\n",
      "[D loss: 0.860763] [G loss: 1.443497]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.614717] [G loss: 1.497922]\n",
      "[D loss: 0.612274] [G loss: 1.743179]\n",
      "[D loss: 0.868366] [G loss: 1.574922]\n",
      "[D loss: 0.831775] [G loss: 1.391507]\n",
      "[D loss: 0.904053] [G loss: 1.403766]\n",
      "[D loss: 0.655800] [G loss: 1.514399]\n",
      "[D loss: 0.729442] [G loss: 1.516071]\n",
      "[D loss: 0.850597] [G loss: 1.667539]\n",
      "[D loss: 0.786869] [G loss: 1.580061]\n",
      "[D loss: 0.843643] [G loss: 1.441521]\n",
      "[D loss: 0.710749] [G loss: 1.546867]\n",
      "[D loss: 0.945996] [G loss: 1.570093]\n",
      "[D loss: 0.690549] [G loss: 1.548884]\n",
      "[D loss: 0.792451] [G loss: 1.713995]\n",
      "[D loss: 1.238658] [G loss: 1.473456]\n",
      "[D loss: 0.930773] [G loss: 1.652940]\n",
      "[D loss: 0.822904] [G loss: 1.448755]\n",
      "[D loss: 0.922878] [G loss: 1.488317]\n",
      "[D loss: 0.900221] [G loss: 1.328820]\n",
      "[D loss: 0.783774] [G loss: 1.547200]\n",
      "[D loss: 0.762919] [G loss: 1.332296]\n",
      "[D loss: 0.853915] [G loss: 1.324073]\n",
      "[D loss: 0.772333] [G loss: 1.334856]\n",
      "[D loss: 0.795451] [G loss: 1.666374]\n",
      "[D loss: 0.925774] [G loss: 1.856299]\n",
      "[D loss: 0.801458] [G loss: 1.813221]\n",
      "[D loss: 0.763628] [G loss: 1.573774]\n",
      "[D loss: 0.703993] [G loss: 1.547789]\n",
      "[D loss: 0.759580] [G loss: 1.507552]\n",
      "[D loss: 0.665621] [G loss: 1.624337]\n",
      "[D loss: 0.768949] [G loss: 1.642161]\n",
      "[D loss: 0.758136] [G loss: 1.312429]\n",
      "[D loss: 0.804783] [G loss: 1.446641]\n",
      "[D loss: 0.812695] [G loss: 1.721602]\n",
      "[D loss: 0.771114] [G loss: 1.340031]\n",
      "[D loss: 0.594096] [G loss: 1.533674]\n",
      "[D loss: 0.756578] [G loss: 1.623760]\n",
      "[D loss: 0.752283] [G loss: 1.604454]\n",
      "[D loss: 1.099385] [G loss: 1.527360]\n",
      "[D loss: 0.940362] [G loss: 1.367013]\n",
      "[D loss: 0.798370] [G loss: 1.622804]\n",
      "[D loss: 0.667696] [G loss: 1.332560]\n",
      "[D loss: 0.784729] [G loss: 1.630045]\n",
      "[D loss: 0.762546] [G loss: 1.674714]\n",
      "[D loss: 0.807738] [G loss: 1.426526]\n",
      "[D loss: 0.834209] [G loss: 1.564382]\n",
      "[D loss: 0.702505] [G loss: 1.503196]\n",
      "[D loss: 0.728315] [G loss: 1.509525]\n",
      "[D loss: 0.764854] [G loss: 1.673392]\n",
      "[D loss: 0.826407] [G loss: 1.636767]\n",
      "[D loss: 0.902636] [G loss: 1.582123]\n",
      "[D loss: 0.956113] [G loss: 1.472019]\n",
      "[D loss: 0.902732] [G loss: 1.259731]\n",
      "[D loss: 0.636466] [G loss: 1.419349]\n",
      "[D loss: 0.846926] [G loss: 1.503235]\n",
      "[D loss: 0.914943] [G loss: 1.380066]\n",
      "[D loss: 0.838997] [G loss: 1.304680]\n",
      "[D loss: 0.761368] [G loss: 1.676851]\n",
      "[D loss: 0.749387] [G loss: 1.587859]\n",
      "[D loss: 0.857277] [G loss: 1.552617]\n",
      "[D loss: 0.811722] [G loss: 1.540354]\n",
      "[D loss: 0.796686] [G loss: 1.390831]\n",
      "[D loss: 1.032354] [G loss: 1.834205]\n",
      "[D loss: 0.766750] [G loss: 1.557677]\n",
      "[D loss: 0.728881] [G loss: 1.571527]\n",
      "[D loss: 0.897185] [G loss: 1.448949]\n",
      "[D loss: 0.792869] [G loss: 1.624446]\n",
      "[D loss: 1.072594] [G loss: 1.332699]\n",
      "[D loss: 0.700146] [G loss: 1.535126]\n",
      "[D loss: 0.799108] [G loss: 1.403278]\n",
      "[D loss: 0.904963] [G loss: 1.274226]\n",
      "[D loss: 0.827738] [G loss: 1.491216]\n",
      "[D loss: 0.679188] [G loss: 1.613271]\n",
      "[D loss: 0.727026] [G loss: 1.498390]\n",
      "[D loss: 0.847639] [G loss: 1.501415]\n",
      "[D loss: 0.902941] [G loss: 1.506439]\n",
      "[D loss: 0.921006] [G loss: 1.522085]\n",
      "[D loss: 0.928309] [G loss: 1.405752]\n",
      "[D loss: 0.922903] [G loss: 1.532379]\n",
      "[D loss: 0.853506] [G loss: 1.552886]\n",
      "[D loss: 1.027664] [G loss: 1.753957]\n",
      "[D loss: 0.944284] [G loss: 1.593428]\n",
      "[D loss: 0.985741] [G loss: 1.361004]\n",
      "[D loss: 0.851172] [G loss: 1.505717]\n",
      "[D loss: 0.735709] [G loss: 1.278649]\n",
      "[D loss: 0.686494] [G loss: 1.546566]\n",
      "[D loss: 0.694735] [G loss: 1.260689]\n",
      "[D loss: 0.718024] [G loss: 1.500129]\n",
      "[D loss: 0.586077] [G loss: 1.702242]\n",
      "[D loss: 0.642898] [G loss: 1.559890]\n",
      "[D loss: 0.704748] [G loss: 1.589784]\n",
      "[D loss: 0.829086] [G loss: 1.442916]\n",
      "[D loss: 0.900400] [G loss: 1.472218]\n",
      "[D loss: 0.648458] [G loss: 1.807851]\n",
      "[D loss: 0.930362] [G loss: 1.468116]\n",
      "[D loss: 0.692569] [G loss: 1.839444]\n",
      "[D loss: 0.781285] [G loss: 1.407261]\n",
      "[D loss: 0.818776] [G loss: 1.621786]\n",
      "[D loss: 0.680237] [G loss: 1.637046]\n",
      "[D loss: 0.815315] [G loss: 1.399352]\n",
      "[D loss: 0.852615] [G loss: 1.602329]\n",
      "[D loss: 0.773608] [G loss: 1.639028]\n",
      "[D loss: 0.747502] [G loss: 1.494578]\n",
      "[D loss: 0.752918] [G loss: 1.425497]\n",
      "[D loss: 0.976041] [G loss: 1.263373]\n",
      "[D loss: 0.693135] [G loss: 1.590321]\n",
      "[D loss: 0.800134] [G loss: 1.303568]\n",
      "[D loss: 0.691879] [G loss: 1.564245]\n",
      "[D loss: 0.708063] [G loss: 1.690464]\n",
      "[D loss: 0.939292] [G loss: 1.485798]\n",
      "[D loss: 0.753546] [G loss: 1.526353]\n",
      "[D loss: 0.952783] [G loss: 1.599258]\n",
      "[D loss: 0.804293] [G loss: 1.196890]\n",
      "[D loss: 0.669273] [G loss: 1.480585]\n",
      "[D loss: 0.799499] [G loss: 1.590187]\n",
      "[D loss: 1.060954] [G loss: 1.538728]\n",
      "[D loss: 0.771148] [G loss: 1.682322]\n",
      "[D loss: 1.099257] [G loss: 1.482660]\n",
      "[D loss: 0.933326] [G loss: 1.445824]\n",
      "[D loss: 0.787807] [G loss: 1.429002]\n",
      "[D loss: 0.922369] [G loss: 1.576016]\n",
      "[D loss: 0.828667] [G loss: 1.418331]\n",
      "[D loss: 1.047390] [G loss: 1.351306]\n",
      "[D loss: 0.805739] [G loss: 1.433735]\n",
      "[D loss: 1.158275] [G loss: 1.333328]\n",
      "[D loss: 0.689053] [G loss: 1.410852]\n",
      "[D loss: 0.966949] [G loss: 1.566864]\n",
      "[D loss: 1.166070] [G loss: 1.462179]\n",
      "[D loss: 0.922307] [G loss: 1.464140]\n",
      "[D loss: 0.692449] [G loss: 1.262486]\n",
      "[D loss: 0.723489] [G loss: 1.441462]\n",
      "[D loss: 0.893389] [G loss: 1.230653]\n",
      "[D loss: 0.947164] [G loss: 1.538479]\n",
      "[D loss: 0.874939] [G loss: 1.181410]\n",
      "[D loss: 0.766372] [G loss: 1.591048]\n",
      "[D loss: 0.821416] [G loss: 1.701887]\n",
      "[D loss: 1.052196] [G loss: 1.480839]\n",
      "[D loss: 0.743229] [G loss: 1.403683]\n",
      "[D loss: 0.925232] [G loss: 1.472767]\n",
      "[D loss: 0.944224] [G loss: 1.351109]\n",
      "[D loss: 0.968503] [G loss: 1.269453]\n",
      "[D loss: 0.789514] [G loss: 1.569275]\n",
      "[D loss: 0.735709] [G loss: 1.447998]\n",
      "[D loss: 0.754896] [G loss: 1.468617]\n",
      "[D loss: 0.939124] [G loss: 1.695251]\n",
      "[D loss: 0.885744] [G loss: 1.580440]\n",
      "[D loss: 0.942697] [G loss: 1.510175]\n",
      "[D loss: 0.965054] [G loss: 1.352660]\n",
      "[D loss: 0.964423] [G loss: 1.502549]\n",
      "[D loss: 0.721864] [G loss: 1.498743]\n",
      "[D loss: 0.854743] [G loss: 1.519438]\n",
      "[D loss: 0.697238] [G loss: 1.480789]\n",
      "[D loss: 0.812123] [G loss: 1.446951]\n",
      "[D loss: 0.639834] [G loss: 1.467889]\n",
      "[D loss: 0.841950] [G loss: 1.533127]\n",
      "[D loss: 0.718175] [G loss: 1.771918]\n",
      "[D loss: 0.674775] [G loss: 1.765491]\n",
      "[D loss: 0.554615] [G loss: 1.523399]\n",
      "[D loss: 0.627266] [G loss: 1.455372]\n",
      "[D loss: 1.011050] [G loss: 1.653368]\n",
      "[D loss: 0.858043] [G loss: 1.562450]\n",
      "[D loss: 0.805262] [G loss: 1.438635]\n",
      "[D loss: 0.843414] [G loss: 1.495503]\n",
      "[D loss: 0.907509] [G loss: 1.413760]\n",
      "[D loss: 1.059705] [G loss: 1.225446]\n",
      "[D loss: 0.793979] [G loss: 1.281415]\n",
      "[D loss: 0.977345] [G loss: 1.130163]\n",
      "[D loss: 0.921967] [G loss: 1.599607]\n",
      "[D loss: 0.908174] [G loss: 1.596356]\n",
      "[D loss: 0.807103] [G loss: 1.394702]\n",
      "[D loss: 0.780859] [G loss: 1.344605]\n",
      "[D loss: 0.774455] [G loss: 1.409308]\n",
      "[D loss: 0.560442] [G loss: 1.553116]\n",
      "[D loss: 0.832356] [G loss: 1.475592]\n",
      "[D loss: 0.878815] [G loss: 1.609801]\n",
      "[D loss: 0.860397] [G loss: 1.738223]\n",
      "[D loss: 0.681602] [G loss: 1.475044]\n",
      "[D loss: 0.618485] [G loss: 1.767332]\n",
      "[D loss: 0.781776] [G loss: 1.698101]\n",
      "[D loss: 1.124692] [G loss: 1.503044]\n",
      "[D loss: 0.782837] [G loss: 1.356924]\n",
      "[D loss: 0.847288] [G loss: 1.400492]\n",
      "[D loss: 0.752970] [G loss: 1.432815]\n",
      "[D loss: 0.764393] [G loss: 1.770577]\n",
      "[D loss: 0.770206] [G loss: 1.332589]\n",
      "[D loss: 0.826226] [G loss: 1.437950]\n",
      "[D loss: 1.025163] [G loss: 1.406022]\n",
      "[D loss: 0.695115] [G loss: 1.342099]\n",
      "[D loss: 0.971978] [G loss: 1.535186]\n",
      "[D loss: 0.829325] [G loss: 1.424221]\n",
      "[D loss: 0.980196] [G loss: 1.358995]\n",
      "[D loss: 0.793318] [G loss: 1.386031]\n",
      "[D loss: 0.770489] [G loss: 1.554635]\n",
      "[D loss: 0.767589] [G loss: 1.470011]\n",
      "[D loss: 0.944032] [G loss: 1.438237]\n",
      "[D loss: 1.037218] [G loss: 1.418983]\n",
      "[D loss: 0.859237] [G loss: 1.387560]\n",
      "[D loss: 0.759581] [G loss: 1.546605]\n",
      "[D loss: 0.912242] [G loss: 1.399798]\n",
      "[D loss: 0.809018] [G loss: 1.330904]\n",
      "[D loss: 0.806417] [G loss: 1.320065]\n",
      "[D loss: 0.711887] [G loss: 1.419820]\n",
      "[D loss: 0.974499] [G loss: 1.517622]\n",
      "[D loss: 0.939066] [G loss: 1.316876]\n",
      "[D loss: 0.842864] [G loss: 1.385017]\n",
      "[D loss: 0.826462] [G loss: 1.672998]\n",
      "[D loss: 0.627285] [G loss: 1.590532]\n",
      "[D loss: 0.956222] [G loss: 1.430011]\n",
      "[D loss: 1.069662] [G loss: 1.398227]\n",
      "[D loss: 0.709304] [G loss: 1.403154]\n",
      "[D loss: 0.746554] [G loss: 1.405527]\n",
      "[D loss: 0.853862] [G loss: 1.299987]\n",
      "[D loss: 0.832669] [G loss: 1.457743]\n",
      "[D loss: 0.721662] [G loss: 1.596686]\n",
      "[D loss: 0.839873] [G loss: 1.451261]\n",
      "[D loss: 1.069966] [G loss: 1.246123]\n",
      "[D loss: 0.848588] [G loss: 1.526017]\n",
      "[D loss: 1.063636] [G loss: 1.265758]\n",
      "[D loss: 0.838136] [G loss: 1.326310]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.713121] [G loss: 1.506714]\n",
      "[D loss: 0.654509] [G loss: 1.320433]\n",
      "[D loss: 0.880965] [G loss: 1.446600]\n",
      "[D loss: 0.750995] [G loss: 1.434633]\n",
      "[D loss: 0.578373] [G loss: 1.638519]\n",
      "[D loss: 0.933966] [G loss: 1.452335]\n",
      "[D loss: 0.533771] [G loss: 1.595689]\n",
      "[D loss: 0.755064] [G loss: 1.736946]\n",
      "[D loss: 0.950244] [G loss: 1.434087]\n",
      "[D loss: 0.960385] [G loss: 1.562481]\n",
      "[D loss: 0.657689] [G loss: 1.872904]\n",
      "[D loss: 0.688190] [G loss: 1.436914]\n",
      "[D loss: 0.727012] [G loss: 1.739685]\n",
      "[D loss: 0.683105] [G loss: 1.306840]\n",
      "[D loss: 0.874153] [G loss: 1.311860]\n",
      "[D loss: 0.844651] [G loss: 1.375830]\n",
      "[D loss: 0.749633] [G loss: 1.786491]\n",
      "[D loss: 0.899089] [G loss: 1.604733]\n",
      "[D loss: 0.694980] [G loss: 1.538148]\n",
      "[D loss: 0.797404] [G loss: 1.437260]\n",
      "[D loss: 0.808174] [G loss: 1.390332]\n",
      "[D loss: 0.851945] [G loss: 1.378552]\n",
      "[D loss: 0.775315] [G loss: 1.627424]\n",
      "[D loss: 0.957174] [G loss: 1.445683]\n",
      "[D loss: 0.937346] [G loss: 1.463978]\n",
      "[D loss: 0.926675] [G loss: 1.300702]\n",
      "[D loss: 0.759359] [G loss: 1.493054]\n",
      "[D loss: 0.659912] [G loss: 1.414291]\n",
      "[D loss: 0.811456] [G loss: 1.573444]\n",
      "[D loss: 0.859505] [G loss: 1.684478]\n",
      "[D loss: 0.772526] [G loss: 1.615564]\n",
      "[D loss: 0.864704] [G loss: 1.690302]\n",
      "[D loss: 0.891865] [G loss: 1.777722]\n",
      "[D loss: 0.976120] [G loss: 1.304758]\n",
      "[D loss: 0.855413] [G loss: 1.392882]\n",
      "[D loss: 0.689723] [G loss: 1.734437]\n",
      "[D loss: 0.558531] [G loss: 1.727274]\n",
      "[D loss: 0.816478] [G loss: 1.556468]\n",
      "[D loss: 1.025048] [G loss: 1.447439]\n",
      "[D loss: 0.918889] [G loss: 1.185489]\n",
      "[D loss: 0.740599] [G loss: 1.421486]\n",
      "[D loss: 0.836922] [G loss: 1.547589]\n",
      "[D loss: 0.849474] [G loss: 1.518343]\n",
      "[D loss: 0.933257] [G loss: 1.252952]\n",
      "[D loss: 0.840129] [G loss: 1.413512]\n",
      "[D loss: 0.790586] [G loss: 1.413899]\n",
      "[D loss: 0.712028] [G loss: 1.469728]\n",
      "[D loss: 1.048108] [G loss: 1.428226]\n",
      "[D loss: 0.812673] [G loss: 1.503018]\n",
      "[D loss: 0.975157] [G loss: 1.605584]\n",
      "[D loss: 0.731507] [G loss: 1.819461]\n",
      "[D loss: 0.909711] [G loss: 1.690045]\n",
      "[D loss: 0.837678] [G loss: 1.529495]\n",
      "[D loss: 0.917508] [G loss: 1.517895]\n",
      "[D loss: 0.776994] [G loss: 1.546050]\n",
      "[D loss: 0.720451] [G loss: 1.450261]\n",
      "[D loss: 0.667195] [G loss: 1.493914]\n",
      "[D loss: 0.731510] [G loss: 1.592282]\n",
      "[D loss: 0.855870] [G loss: 1.747890]\n",
      "[D loss: 0.857313] [G loss: 1.348202]\n",
      "[D loss: 0.681852] [G loss: 1.719761]\n",
      "[D loss: 0.984065] [G loss: 1.546812]\n",
      "[D loss: 0.697632] [G loss: 1.493755]\n",
      "[D loss: 0.830967] [G loss: 1.415236]\n",
      "[D loss: 0.696625] [G loss: 1.526236]\n",
      "[D loss: 0.905473] [G loss: 1.359898]\n",
      "[D loss: 0.922646] [G loss: 1.315121]\n",
      "[D loss: 0.999115] [G loss: 1.520022]\n",
      "[D loss: 0.911301] [G loss: 1.425074]\n",
      "[D loss: 0.790624] [G loss: 1.600427]\n",
      "[D loss: 0.850809] [G loss: 1.802067]\n",
      "[D loss: 0.986786] [G loss: 1.510994]\n",
      "[D loss: 0.902810] [G loss: 1.376090]\n",
      "[D loss: 0.869848] [G loss: 1.432699]\n",
      "[D loss: 0.843970] [G loss: 1.407797]\n",
      "[D loss: 0.809539] [G loss: 1.409854]\n",
      "[D loss: 1.112062] [G loss: 1.468966]\n",
      "[D loss: 0.763564] [G loss: 1.536062]\n",
      "[D loss: 0.818886] [G loss: 1.324895]\n",
      "[D loss: 0.874847] [G loss: 1.295707]\n",
      "[D loss: 0.866947] [G loss: 1.283720]\n",
      "[D loss: 0.788747] [G loss: 1.361614]\n",
      "[D loss: 0.978945] [G loss: 1.249115]\n",
      "[D loss: 0.771607] [G loss: 1.306135]\n",
      "[D loss: 0.847821] [G loss: 1.645992]\n",
      "[D loss: 0.880839] [G loss: 1.506418]\n",
      "[D loss: 0.858996] [G loss: 1.303829]\n",
      "[D loss: 0.757674] [G loss: 1.546462]\n",
      "[D loss: 0.790215] [G loss: 1.673912]\n",
      "[D loss: 0.679766] [G loss: 1.459506]\n",
      "[D loss: 0.730299] [G loss: 1.448179]\n",
      "[D loss: 1.004218] [G loss: 1.337877]\n",
      "[D loss: 0.826696] [G loss: 1.506210]\n",
      "[D loss: 0.951362] [G loss: 1.616233]\n",
      "[D loss: 0.754378] [G loss: 1.465635]\n",
      "[D loss: 0.966904] [G loss: 1.453174]\n",
      "[D loss: 0.940205] [G loss: 1.617866]\n",
      "[D loss: 0.799645] [G loss: 1.634032]\n",
      "[D loss: 0.777853] [G loss: 1.472719]\n",
      "[D loss: 0.814921] [G loss: 1.443772]\n",
      "[D loss: 0.557844] [G loss: 1.427735]\n",
      "[D loss: 0.781333] [G loss: 1.314028]\n",
      "[D loss: 0.883645] [G loss: 1.251180]\n",
      "[D loss: 1.056568] [G loss: 1.469182]\n",
      "[D loss: 0.886193] [G loss: 1.587756]\n",
      "[D loss: 0.917065] [G loss: 1.678234]\n",
      "[D loss: 0.711942] [G loss: 1.601876]\n",
      "[D loss: 0.775812] [G loss: 1.457035]\n",
      "[D loss: 0.806629] [G loss: 1.416579]\n",
      "[D loss: 0.710829] [G loss: 1.434367]\n",
      "[D loss: 1.124841] [G loss: 1.301853]\n",
      "[D loss: 0.632038] [G loss: 1.364037]\n",
      "[D loss: 0.891762] [G loss: 1.392842]\n",
      "[D loss: 0.895032] [G loss: 1.293411]\n",
      "[D loss: 0.816601] [G loss: 1.500073]\n",
      "[D loss: 0.854506] [G loss: 1.356866]\n",
      "[D loss: 0.780947] [G loss: 1.386206]\n",
      "[D loss: 0.836431] [G loss: 1.413970]\n",
      "[D loss: 0.815884] [G loss: 1.495360]\n",
      "[D loss: 1.092733] [G loss: 1.210593]\n",
      "[D loss: 0.703940] [G loss: 1.473658]\n",
      "[D loss: 1.040587] [G loss: 1.600164]\n",
      "[D loss: 0.766312] [G loss: 1.454568]\n",
      "[D loss: 0.901807] [G loss: 1.703778]\n",
      "[D loss: 0.773999] [G loss: 1.539431]\n",
      "[D loss: 0.929958] [G loss: 1.354882]\n",
      "[D loss: 0.929395] [G loss: 1.206187]\n",
      "[D loss: 1.079396] [G loss: 1.288061]\n",
      "[D loss: 0.955523] [G loss: 1.382943]\n",
      "[D loss: 0.555658] [G loss: 1.544094]\n",
      "[D loss: 0.620367] [G loss: 1.458108]\n",
      "[D loss: 0.636810] [G loss: 1.420177]\n",
      "[D loss: 0.693472] [G loss: 1.469826]\n",
      "[D loss: 1.191909] [G loss: 1.457036]\n",
      "[D loss: 0.929913] [G loss: 1.237979]\n",
      "[D loss: 0.937739] [G loss: 1.488402]\n",
      "[D loss: 0.912115] [G loss: 1.370897]\n",
      "[D loss: 0.785254] [G loss: 1.427236]\n",
      "[D loss: 0.746838] [G loss: 1.478479]\n",
      "[D loss: 1.048114] [G loss: 1.486350]\n",
      "[D loss: 0.848877] [G loss: 1.430869]\n",
      "[D loss: 0.694546] [G loss: 1.359189]\n",
      "[D loss: 0.757835] [G loss: 1.323957]\n",
      "[D loss: 1.041808] [G loss: 1.424728]\n",
      "[D loss: 0.816900] [G loss: 1.564804]\n",
      "[D loss: 0.869899] [G loss: 1.509012]\n",
      "[D loss: 1.054778] [G loss: 1.293287]\n",
      "[D loss: 0.610916] [G loss: 1.230217]\n",
      "[D loss: 0.910659] [G loss: 1.473629]\n",
      "[D loss: 0.680881] [G loss: 1.638241]\n",
      "[D loss: 0.774821] [G loss: 1.670893]\n",
      "[D loss: 0.694099] [G loss: 1.566128]\n",
      "[D loss: 0.822246] [G loss: 1.332104]\n",
      "[D loss: 0.631783] [G loss: 1.463163]\n",
      "[D loss: 0.537014] [G loss: 1.505312]\n",
      "[D loss: 1.068371] [G loss: 1.297700]\n",
      "[D loss: 0.810468] [G loss: 1.402844]\n",
      "[D loss: 0.729238] [G loss: 1.780983]\n",
      "[D loss: 0.726808] [G loss: 1.489031]\n",
      "[D loss: 0.856095] [G loss: 1.344589]\n",
      "[D loss: 1.053773] [G loss: 1.331701]\n",
      "[D loss: 0.711470] [G loss: 1.546983]\n",
      "[D loss: 0.797661] [G loss: 1.534584]\n",
      "[D loss: 0.844308] [G loss: 1.486753]\n",
      "[D loss: 0.827631] [G loss: 1.340459]\n",
      "[D loss: 0.964525] [G loss: 1.310741]\n",
      "[D loss: 0.893703] [G loss: 1.445791]\n",
      "[D loss: 0.950318] [G loss: 1.453819]\n",
      "[D loss: 0.890076] [G loss: 1.323456]\n",
      "[D loss: 0.819734] [G loss: 1.546034]\n",
      "[D loss: 0.918660] [G loss: 1.544907]\n",
      "[D loss: 0.538601] [G loss: 1.525329]\n",
      "[D loss: 0.941234] [G loss: 1.419073]\n",
      "[D loss: 1.167384] [G loss: 1.288522]\n",
      "[D loss: 0.697078] [G loss: 1.418015]\n",
      "[D loss: 1.105037] [G loss: 1.422419]\n",
      "[D loss: 0.761034] [G loss: 1.441863]\n",
      "[D loss: 0.916532] [G loss: 1.389544]\n",
      "[D loss: 1.121901] [G loss: 1.324291]\n",
      "[D loss: 0.825505] [G loss: 1.276959]\n",
      "[D loss: 0.938896] [G loss: 1.501097]\n",
      "[D loss: 1.021161] [G loss: 1.223185]\n",
      "[D loss: 0.805086] [G loss: 1.309839]\n",
      "[D loss: 0.856278] [G loss: 1.414307]\n",
      "[D loss: 0.968165] [G loss: 1.320325]\n",
      "[D loss: 0.552185] [G loss: 1.491717]\n",
      "[D loss: 0.735452] [G loss: 1.610314]\n",
      "[D loss: 0.722649] [G loss: 1.414233]\n",
      "[D loss: 0.926661] [G loss: 1.381177]\n",
      "[D loss: 0.767935] [G loss: 1.467898]\n",
      "[D loss: 0.876180] [G loss: 1.278163]\n",
      "[D loss: 0.886539] [G loss: 1.221872]\n",
      "[D loss: 0.960123] [G loss: 1.552739]\n",
      "[D loss: 0.680070] [G loss: 1.705392]\n",
      "[D loss: 0.745582] [G loss: 1.519187]\n",
      "[D loss: 0.682917] [G loss: 1.579632]\n",
      "[D loss: 1.077185] [G loss: 1.535038]\n",
      "[D loss: 0.803953] [G loss: 1.411697]\n",
      "[D loss: 0.663504] [G loss: 1.456329]\n",
      "[D loss: 1.012078] [G loss: 1.459836]\n",
      "[D loss: 1.098522] [G loss: 1.677824]\n",
      "[D loss: 0.731841] [G loss: 1.399159]\n",
      "[D loss: 0.920978] [G loss: 1.244939]\n",
      "[D loss: 1.090481] [G loss: 1.423006]\n",
      "[D loss: 0.781732] [G loss: 1.315850]\n",
      "[D loss: 0.781596] [G loss: 1.355178]\n",
      "[D loss: 0.919972] [G loss: 1.297486]\n",
      "[D loss: 0.574909] [G loss: 1.421590]\n",
      "[D loss: 1.077953] [G loss: 1.168256]\n",
      "[D loss: 0.753049] [G loss: 1.504102]\n",
      "[D loss: 1.046112] [G loss: 1.547969]\n",
      "[D loss: 0.951356] [G loss: 1.260558]\n",
      "[D loss: 0.730894] [G loss: 1.423951]\n",
      "[D loss: 0.870769] [G loss: 1.303452]\n",
      "[D loss: 0.845642] [G loss: 1.670357]\n",
      "[D loss: 0.680782] [G loss: 1.618913]\n",
      "[D loss: 0.895489] [G loss: 1.499515]\n",
      "[D loss: 0.922106] [G loss: 1.578434]\n",
      "[D loss: 0.872701] [G loss: 1.506856]\n",
      "[D loss: 0.902127] [G loss: 1.211145]\n",
      "[D loss: 0.888002] [G loss: 1.485824]\n",
      "[D loss: 0.921075] [G loss: 1.550052]\n",
      "[D loss: 0.547784] [G loss: 1.400700]\n",
      "[D loss: 1.026402] [G loss: 1.184965]\n",
      "[D loss: 0.984878] [G loss: 1.388192]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.769425] [G loss: 1.534916]\n",
      "[D loss: 0.984564] [G loss: 1.454450]\n",
      "[D loss: 1.052006] [G loss: 1.233581]\n",
      "[D loss: 0.924595] [G loss: 1.388235]\n",
      "[D loss: 0.710566] [G loss: 1.492822]\n",
      "[D loss: 0.710846] [G loss: 1.294713]\n",
      "[D loss: 0.837625] [G loss: 1.346493]\n",
      "[D loss: 0.888303] [G loss: 1.304020]\n",
      "[D loss: 0.904502] [G loss: 1.424570]\n",
      "[D loss: 0.725029] [G loss: 1.512027]\n",
      "[D loss: 0.622668] [G loss: 1.583295]\n",
      "[D loss: 0.785940] [G loss: 1.453029]\n",
      "[D loss: 0.954582] [G loss: 1.591821]\n",
      "[D loss: 0.628633] [G loss: 1.767187]\n",
      "[D loss: 1.006301] [G loss: 1.829047]\n",
      "[D loss: 1.033569] [G loss: 1.383590]\n",
      "[D loss: 0.733936] [G loss: 1.463362]\n",
      "[D loss: 0.729171] [G loss: 1.465417]\n",
      "[D loss: 0.898482] [G loss: 1.401458]\n",
      "[D loss: 0.732228] [G loss: 1.481181]\n",
      "[D loss: 0.655718] [G loss: 1.429356]\n",
      "[D loss: 0.900911] [G loss: 1.419038]\n",
      "[D loss: 0.889970] [G loss: 1.433282]\n",
      "[D loss: 0.952554] [G loss: 1.417474]\n",
      "[D loss: 0.663817] [G loss: 1.431619]\n",
      "[D loss: 0.793088] [G loss: 1.553330]\n",
      "[D loss: 1.167013] [G loss: 1.371086]\n",
      "[D loss: 0.939042] [G loss: 1.496414]\n",
      "[D loss: 0.856966] [G loss: 1.817409]\n",
      "[D loss: 0.737686] [G loss: 1.366300]\n",
      "[D loss: 0.742451] [G loss: 1.518829]\n",
      "[D loss: 0.993099] [G loss: 1.349947]\n",
      "[D loss: 1.060505] [G loss: 1.317473]\n",
      "[D loss: 0.911502] [G loss: 1.412292]\n",
      "[D loss: 0.717664] [G loss: 1.409280]\n",
      "[D loss: 0.816401] [G loss: 1.269251]\n",
      "[D loss: 0.918239] [G loss: 1.487532]\n",
      "[D loss: 0.747726] [G loss: 1.244089]\n",
      "[D loss: 1.180748] [G loss: 1.332011]\n",
      "[D loss: 0.928208] [G loss: 1.328732]\n",
      "[D loss: 0.748395] [G loss: 1.332320]\n",
      "[D loss: 1.037093] [G loss: 1.227583]\n",
      "[D loss: 0.779494] [G loss: 1.440619]\n",
      "[D loss: 1.018397] [G loss: 1.300160]\n",
      "[D loss: 0.878554] [G loss: 1.199318]\n",
      "[D loss: 0.943321] [G loss: 1.211810]\n",
      "[D loss: 0.963256] [G loss: 1.279183]\n",
      "[D loss: 0.865627] [G loss: 1.472919]\n",
      "[D loss: 0.775012] [G loss: 1.636939]\n",
      "[D loss: 0.866427] [G loss: 1.466083]\n",
      "[D loss: 0.802838] [G loss: 1.566051]\n",
      "[D loss: 1.047847] [G loss: 1.429951]\n",
      "[D loss: 0.996156] [G loss: 1.370030]\n",
      "[D loss: 0.773999] [G loss: 1.471081]\n",
      "[D loss: 0.887387] [G loss: 1.662029]\n",
      "[D loss: 0.791984] [G loss: 1.536092]\n",
      "[D loss: 0.771288] [G loss: 1.739979]\n",
      "[D loss: 0.942320] [G loss: 1.380489]\n",
      "[D loss: 0.817588] [G loss: 1.417068]\n",
      "[D loss: 0.915377] [G loss: 1.421421]\n",
      "[D loss: 0.818928] [G loss: 1.236384]\n",
      "[D loss: 0.846300] [G loss: 1.439921]\n",
      "[D loss: 0.954928] [G loss: 1.367021]\n",
      "[D loss: 0.778244] [G loss: 1.447906]\n",
      "[D loss: 0.928227] [G loss: 1.573005]\n",
      "[D loss: 1.005766] [G loss: 1.645461]\n",
      "[D loss: 0.779875] [G loss: 1.352004]\n",
      "[D loss: 0.792984] [G loss: 1.547668]\n",
      "[D loss: 0.824231] [G loss: 1.357674]\n",
      "[D loss: 0.834906] [G loss: 1.375801]\n",
      "[D loss: 0.727803] [G loss: 1.570263]\n",
      "[D loss: 0.904600] [G loss: 1.381590]\n",
      "[D loss: 0.802086] [G loss: 1.359700]\n",
      "[D loss: 0.747228] [G loss: 1.291141]\n",
      "[D loss: 0.732230] [G loss: 1.614478]\n",
      "[D loss: 0.892899] [G loss: 1.324194]\n",
      "[D loss: 0.731071] [G loss: 1.414649]\n",
      "[D loss: 1.147515] [G loss: 1.217877]\n",
      "[D loss: 0.968003] [G loss: 1.285985]\n",
      "[D loss: 0.688363] [G loss: 1.511537]\n",
      "[D loss: 0.825825] [G loss: 1.594247]\n",
      "[D loss: 0.813903] [G loss: 1.584399]\n",
      "[D loss: 0.894344] [G loss: 1.342032]\n",
      "[D loss: 0.874289] [G loss: 1.640882]\n",
      "[D loss: 0.900053] [G loss: 1.405359]\n",
      "[D loss: 0.804695] [G loss: 1.289539]\n",
      "[D loss: 0.853614] [G loss: 1.514907]\n",
      "[D loss: 0.846914] [G loss: 1.353712]\n",
      "[D loss: 0.964797] [G loss: 1.473169]\n",
      "[D loss: 0.919126] [G loss: 1.379725]\n",
      "[D loss: 0.734504] [G loss: 1.424844]\n",
      "[D loss: 0.783172] [G loss: 1.595657]\n",
      "[D loss: 1.027411] [G loss: 1.429957]\n",
      "[D loss: 0.980561] [G loss: 1.398686]\n",
      "[D loss: 1.272839] [G loss: 1.440786]\n",
      "[D loss: 0.607970] [G loss: 1.539262]\n",
      "[D loss: 0.882953] [G loss: 1.406047]\n",
      "[D loss: 0.757012] [G loss: 1.296766]\n",
      "[D loss: 0.920057] [G loss: 1.347924]\n",
      "[D loss: 0.724148] [G loss: 1.426713]\n",
      "[D loss: 0.812684] [G loss: 1.681723]\n",
      "[D loss: 0.736757] [G loss: 1.499090]\n",
      "[D loss: 0.800417] [G loss: 1.436738]\n",
      "[D loss: 0.877317] [G loss: 1.481630]\n",
      "[D loss: 0.951733] [G loss: 1.486280]\n",
      "[D loss: 0.853641] [G loss: 1.549039]\n",
      "[D loss: 0.882364] [G loss: 1.439080]\n",
      "[D loss: 0.734593] [G loss: 1.444568]\n",
      "[D loss: 0.664434] [G loss: 1.460961]\n",
      "[D loss: 0.790396] [G loss: 1.505637]\n",
      "[D loss: 0.957871] [G loss: 1.444232]\n",
      "[D loss: 0.879499] [G loss: 1.314669]\n",
      "[D loss: 0.909973] [G loss: 1.374171]\n",
      "[D loss: 0.748512] [G loss: 1.581859]\n",
      "[D loss: 0.676755] [G loss: 1.413415]\n",
      "[D loss: 0.827288] [G loss: 1.434434]\n",
      "[D loss: 0.750559] [G loss: 1.399428]\n",
      "[D loss: 0.766775] [G loss: 1.507980]\n",
      "[D loss: 0.961016] [G loss: 1.407376]\n",
      "[D loss: 0.950239] [G loss: 1.159839]\n",
      "[D loss: 0.776674] [G loss: 1.564754]\n",
      "[D loss: 0.730836] [G loss: 1.616922]\n",
      "[D loss: 0.935750] [G loss: 1.485492]\n",
      "[D loss: 0.984357] [G loss: 1.483999]\n",
      "[D loss: 0.707730] [G loss: 1.348048]\n",
      "[D loss: 0.703943] [G loss: 1.502752]\n",
      "[D loss: 0.614126] [G loss: 1.386514]\n",
      "[D loss: 0.769711] [G loss: 1.450419]\n",
      "[D loss: 0.870935] [G loss: 1.485084]\n",
      "[D loss: 0.986518] [G loss: 1.694600]\n",
      "[D loss: 1.115774] [G loss: 1.414390]\n",
      "[D loss: 0.571827] [G loss: 1.505667]\n",
      "[D loss: 0.761062] [G loss: 1.744936]\n",
      "[D loss: 1.022186] [G loss: 1.453934]\n",
      "[D loss: 0.931486] [G loss: 1.209025]\n",
      "[D loss: 0.880033] [G loss: 1.301611]\n",
      "[D loss: 0.766487] [G loss: 1.409839]\n",
      "[D loss: 0.879900] [G loss: 1.639020]\n",
      "[D loss: 0.927393] [G loss: 1.584981]\n",
      "[D loss: 0.577360] [G loss: 1.425207]\n",
      "[D loss: 0.906445] [G loss: 1.414786]\n",
      "[D loss: 0.763799] [G loss: 1.437307]\n",
      "[D loss: 1.011783] [G loss: 1.308291]\n",
      "[D loss: 0.772811] [G loss: 1.425393]\n",
      "[D loss: 0.658505] [G loss: 1.460126]\n",
      "[D loss: 0.699693] [G loss: 1.479764]\n",
      "[D loss: 0.874675] [G loss: 1.631418]\n",
      "[D loss: 0.810574] [G loss: 1.670471]\n",
      "[D loss: 0.867795] [G loss: 1.267331]\n",
      "[D loss: 0.813578] [G loss: 1.448726]\n",
      "[D loss: 0.862047] [G loss: 1.402538]\n",
      "[D loss: 0.657184] [G loss: 1.396151]\n",
      "[D loss: 0.736189] [G loss: 1.692691]\n",
      "[D loss: 0.775359] [G loss: 1.629494]\n",
      "[D loss: 0.650582] [G loss: 1.793947]\n",
      "[D loss: 0.628588] [G loss: 1.638633]\n",
      "[D loss: 0.954220] [G loss: 1.444262]\n",
      "[D loss: 0.642816] [G loss: 1.588975]\n",
      "[D loss: 0.800598] [G loss: 1.554249]\n",
      "[D loss: 0.749255] [G loss: 1.448148]\n",
      "[D loss: 0.682770] [G loss: 1.446306]\n",
      "[D loss: 0.618487] [G loss: 1.270072]\n",
      "[D loss: 0.831658] [G loss: 1.665808]\n",
      "[D loss: 0.945471] [G loss: 1.376900]\n",
      "[D loss: 0.684147] [G loss: 1.711433]\n",
      "[D loss: 0.779105] [G loss: 1.557452]\n",
      "[D loss: 0.754608] [G loss: 1.631672]\n",
      "[D loss: 0.567755] [G loss: 1.441839]\n",
      "[D loss: 0.649752] [G loss: 1.553028]\n",
      "[D loss: 0.849787] [G loss: 1.632826]\n",
      "[D loss: 0.878860] [G loss: 1.608783]\n",
      "[D loss: 0.977332] [G loss: 1.630350]\n",
      "[D loss: 0.862819] [G loss: 1.556249]\n",
      "[D loss: 0.931000] [G loss: 1.548197]\n",
      "[D loss: 0.828150] [G loss: 1.460400]\n",
      "[D loss: 0.689694] [G loss: 1.581465]\n",
      "[D loss: 0.794859] [G loss: 1.355641]\n",
      "[D loss: 0.743666] [G loss: 1.493920]\n",
      "[D loss: 0.980166] [G loss: 1.468066]\n",
      "[D loss: 0.769699] [G loss: 1.614651]\n",
      "[D loss: 0.932468] [G loss: 1.550003]\n",
      "[D loss: 0.861032] [G loss: 1.350439]\n",
      "[D loss: 0.930858] [G loss: 1.605289]\n",
      "[D loss: 0.818939] [G loss: 1.565760]\n",
      "[D loss: 0.821831] [G loss: 1.390114]\n",
      "[D loss: 0.746248] [G loss: 1.414015]\n",
      "[D loss: 0.679377] [G loss: 1.670387]\n",
      "[D loss: 1.258125] [G loss: 1.430752]\n",
      "[D loss: 1.076871] [G loss: 1.301140]\n",
      "[D loss: 0.762784] [G loss: 1.421715]\n",
      "[D loss: 0.838138] [G loss: 1.437811]\n",
      "[D loss: 0.764800] [G loss: 1.407486]\n",
      "[D loss: 0.747397] [G loss: 1.396155]\n",
      "[D loss: 0.853341] [G loss: 1.653027]\n",
      "[D loss: 0.815516] [G loss: 1.443237]\n",
      "[D loss: 0.921175] [G loss: 1.555537]\n",
      "[D loss: 0.894508] [G loss: 1.694447]\n",
      "[D loss: 0.910136] [G loss: 1.157305]\n",
      "[D loss: 0.763206] [G loss: 1.432526]\n",
      "[D loss: 0.613382] [G loss: 1.503161]\n",
      "[D loss: 0.744015] [G loss: 1.582314]\n",
      "[D loss: 0.989859] [G loss: 1.388969]\n",
      "[D loss: 0.891106] [G loss: 1.416008]\n",
      "[D loss: 0.820767] [G loss: 1.420874]\n",
      "[D loss: 0.806218] [G loss: 1.525901]\n",
      "[D loss: 0.738537] [G loss: 1.407491]\n",
      "[D loss: 0.949776] [G loss: 1.409555]\n",
      "[D loss: 0.840907] [G loss: 1.448729]\n",
      "[D loss: 0.852816] [G loss: 1.335649]\n",
      "[D loss: 1.084258] [G loss: 1.481348]\n",
      "[D loss: 0.757046] [G loss: 1.418706]\n",
      "[D loss: 0.614627] [G loss: 1.513485]\n",
      "[D loss: 1.123684] [G loss: 1.499722]\n",
      "[D loss: 0.834106] [G loss: 1.363857]\n",
      "[D loss: 1.006098] [G loss: 1.346939]\n",
      "[D loss: 0.862119] [G loss: 1.437154]\n",
      "[D loss: 0.771072] [G loss: 1.433359]\n",
      "[D loss: 0.744345] [G loss: 1.555800]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.686420] [G loss: 1.439580]\n",
      "[D loss: 1.002222] [G loss: 1.747013]\n",
      "[D loss: 0.764231] [G loss: 1.685400]\n",
      "[D loss: 0.765694] [G loss: 1.655112]\n",
      "[D loss: 1.071275] [G loss: 1.304406]\n",
      "[D loss: 0.758760] [G loss: 1.417618]\n",
      "[D loss: 0.748579] [G loss: 1.360654]\n",
      "[D loss: 0.796823] [G loss: 1.465339]\n",
      "[D loss: 0.717767] [G loss: 1.464519]\n",
      "[D loss: 0.665384] [G loss: 1.423140]\n",
      "[D loss: 0.731216] [G loss: 1.490985]\n",
      "[D loss: 0.838596] [G loss: 1.510896]\n",
      "[D loss: 0.763238] [G loss: 1.452297]\n",
      "[D loss: 0.776744] [G loss: 1.403538]\n",
      "[D loss: 0.746450] [G loss: 1.496023]\n",
      "[D loss: 0.516883] [G loss: 1.508717]\n",
      "[D loss: 0.857391] [G loss: 1.519835]\n",
      "[D loss: 0.858943] [G loss: 1.426221]\n",
      "[D loss: 0.844584] [G loss: 1.450180]\n",
      "[D loss: 0.616603] [G loss: 1.809195]\n",
      "[D loss: 0.785569] [G loss: 1.937245]\n",
      "[D loss: 0.804637] [G loss: 1.975701]\n",
      "[D loss: 0.657376] [G loss: 1.758663]\n",
      "[D loss: 0.606875] [G loss: 1.651166]\n",
      "[D loss: 0.842327] [G loss: 1.406134]\n",
      "[D loss: 0.985087] [G loss: 1.463674]\n",
      "[D loss: 0.732143] [G loss: 1.446923]\n",
      "[D loss: 0.570787] [G loss: 1.442787]\n",
      "[D loss: 0.739187] [G loss: 1.505485]\n",
      "[D loss: 1.077062] [G loss: 1.099521]\n",
      "[D loss: 0.636886] [G loss: 1.563300]\n",
      "[D loss: 0.689671] [G loss: 1.480094]\n",
      "[D loss: 0.948266] [G loss: 1.585530]\n",
      "[D loss: 0.836653] [G loss: 1.462458]\n",
      "[D loss: 0.847430] [G loss: 1.412099]\n",
      "[D loss: 0.834050] [G loss: 1.492524]\n",
      "[D loss: 0.666122] [G loss: 1.623411]\n",
      "[D loss: 0.868237] [G loss: 1.736598]\n",
      "[D loss: 0.760583] [G loss: 1.238937]\n",
      "[D loss: 0.647156] [G loss: 1.544386]\n",
      "[D loss: 0.741318] [G loss: 1.644043]\n",
      "[D loss: 0.871288] [G loss: 1.587759]\n",
      "[D loss: 0.908597] [G loss: 1.583697]\n",
      "[D loss: 0.881131] [G loss: 1.495736]\n",
      "[D loss: 0.810351] [G loss: 1.477864]\n",
      "[D loss: 0.747677] [G loss: 1.337230]\n",
      "[D loss: 0.820810] [G loss: 1.476361]\n",
      "[D loss: 0.765406] [G loss: 1.512559]\n",
      "[D loss: 0.604784] [G loss: 1.406557]\n",
      "[D loss: 0.570217] [G loss: 1.561762]\n",
      "[D loss: 0.739070] [G loss: 1.401246]\n",
      "[D loss: 0.889167] [G loss: 1.451257]\n",
      "[D loss: 0.834772] [G loss: 1.962901]\n",
      "[D loss: 0.912886] [G loss: 1.463762]\n",
      "[D loss: 0.608917] [G loss: 1.779540]\n",
      "[D loss: 0.966762] [G loss: 1.498684]\n",
      "[D loss: 0.836293] [G loss: 1.638384]\n",
      "[D loss: 0.729738] [G loss: 1.499322]\n",
      "[D loss: 0.671376] [G loss: 1.639278]\n",
      "[D loss: 0.849221] [G loss: 1.667189]\n",
      "[D loss: 0.821855] [G loss: 1.563440]\n",
      "[D loss: 1.006123] [G loss: 1.474387]\n",
      "[D loss: 0.869683] [G loss: 1.320494]\n",
      "[D loss: 0.802020] [G loss: 1.560284]\n",
      "[D loss: 0.753722] [G loss: 1.396888]\n",
      "[D loss: 0.927945] [G loss: 1.331729]\n",
      "[D loss: 0.838916] [G loss: 1.440523]\n",
      "[D loss: 0.707564] [G loss: 1.616528]\n",
      "[D loss: 0.947193] [G loss: 1.468392]\n",
      "[D loss: 0.881505] [G loss: 1.440798]\n",
      "[D loss: 0.832236] [G loss: 1.774036]\n",
      "[D loss: 0.880271] [G loss: 1.647279]\n",
      "[D loss: 0.736897] [G loss: 1.399279]\n",
      "[D loss: 0.715718] [G loss: 1.460197]\n",
      "[D loss: 0.777337] [G loss: 1.410872]\n",
      "[D loss: 0.695910] [G loss: 1.561011]\n",
      "[D loss: 0.809925] [G loss: 1.464721]\n",
      "[D loss: 0.731564] [G loss: 1.599803]\n",
      "[D loss: 0.697774] [G loss: 1.484974]\n",
      "[D loss: 1.153187] [G loss: 1.513004]\n",
      "[D loss: 0.765976] [G loss: 1.520450]\n",
      "[D loss: 0.990508] [G loss: 1.480932]\n",
      "[D loss: 0.885441] [G loss: 1.403798]\n",
      "[D loss: 0.814169] [G loss: 1.529419]\n",
      "[D loss: 0.686998] [G loss: 1.562760]\n",
      "[D loss: 0.718181] [G loss: 1.530865]\n",
      "[D loss: 0.866796] [G loss: 1.521936]\n",
      "[D loss: 0.798021] [G loss: 1.422667]\n",
      "[D loss: 0.697133] [G loss: 1.656702]\n",
      "[D loss: 0.830676] [G loss: 1.401249]\n",
      "[D loss: 0.802809] [G loss: 1.483486]\n",
      "[D loss: 0.965542] [G loss: 1.667261]\n",
      "[D loss: 0.754059] [G loss: 1.391349]\n",
      "[D loss: 0.850089] [G loss: 1.559985]\n",
      "[D loss: 0.888920] [G loss: 1.418822]\n",
      "[D loss: 1.031537] [G loss: 1.393295]\n",
      "[D loss: 0.934480] [G loss: 1.311450]\n",
      "[D loss: 0.656275] [G loss: 1.525095]\n",
      "[D loss: 0.910720] [G loss: 1.390148]\n",
      "[D loss: 0.900555] [G loss: 1.568515]\n",
      "[D loss: 0.749810] [G loss: 1.709312]\n",
      "[D loss: 0.665152] [G loss: 1.664865]\n",
      "[D loss: 0.995431] [G loss: 1.472011]\n",
      "[D loss: 0.879932] [G loss: 1.494864]\n",
      "[D loss: 0.676900] [G loss: 1.531508]\n",
      "[D loss: 0.969749] [G loss: 1.392960]\n",
      "[D loss: 0.797719] [G loss: 1.551933]\n",
      "[D loss: 0.766203] [G loss: 1.481170]\n",
      "[D loss: 0.895022] [G loss: 1.362013]\n",
      "[D loss: 0.712994] [G loss: 1.408278]\n",
      "[D loss: 0.826846] [G loss: 1.425622]\n",
      "[D loss: 0.741809] [G loss: 1.464234]\n",
      "[D loss: 0.904751] [G loss: 1.682029]\n",
      "[D loss: 0.768418] [G loss: 1.451318]\n",
      "[D loss: 0.725106] [G loss: 1.636344]\n",
      "[D loss: 0.669856] [G loss: 1.727133]\n",
      "[D loss: 0.735580] [G loss: 1.505504]\n",
      "[D loss: 0.722073] [G loss: 1.344056]\n",
      "[D loss: 1.008188] [G loss: 1.383630]\n",
      "[D loss: 0.535394] [G loss: 1.433729]\n",
      "[D loss: 0.809174] [G loss: 1.697137]\n",
      "[D loss: 0.838661] [G loss: 1.431455]\n",
      "[D loss: 0.825545] [G loss: 1.489311]\n",
      "[D loss: 0.743390] [G loss: 1.518322]\n",
      "[D loss: 0.579945] [G loss: 1.636201]\n",
      "[D loss: 0.601665] [G loss: 1.521695]\n",
      "[D loss: 0.931276] [G loss: 1.419834]\n",
      "[D loss: 0.785578] [G loss: 1.530242]\n",
      "[D loss: 0.896129] [G loss: 1.700102]\n",
      "[D loss: 0.736785] [G loss: 1.803252]\n",
      "[D loss: 0.757633] [G loss: 2.049875]\n",
      "[D loss: 0.763926] [G loss: 1.521569]\n",
      "[D loss: 0.789036] [G loss: 1.512469]\n",
      "[D loss: 0.738502] [G loss: 1.578730]\n",
      "[D loss: 0.712719] [G loss: 1.475809]\n",
      "[D loss: 1.106939] [G loss: 1.842874]\n",
      "[D loss: 0.842877] [G loss: 1.646040]\n",
      "[D loss: 0.733634] [G loss: 1.596371]\n",
      "[D loss: 0.794574] [G loss: 1.605909]\n",
      "[D loss: 0.824573] [G loss: 1.490782]\n",
      "[D loss: 0.712809] [G loss: 1.653082]\n",
      "[D loss: 0.722687] [G loss: 1.675463]\n",
      "[D loss: 0.829918] [G loss: 1.813474]\n",
      "[D loss: 0.830998] [G loss: 1.737031]\n",
      "[D loss: 0.812578] [G loss: 1.694739]\n",
      "[D loss: 0.862828] [G loss: 1.495427]\n",
      "[D loss: 0.882612] [G loss: 1.546548]\n",
      "[D loss: 0.877199] [G loss: 1.563503]\n",
      "[D loss: 1.232041] [G loss: 1.391801]\n",
      "[D loss: 0.701498] [G loss: 1.571368]\n",
      "[D loss: 0.769018] [G loss: 1.564554]\n",
      "[D loss: 0.685140] [G loss: 1.679318]\n",
      "[D loss: 0.809565] [G loss: 1.735327]\n",
      "[D loss: 0.828743] [G loss: 1.758699]\n",
      "[D loss: 0.767325] [G loss: 1.480401]\n",
      "[D loss: 1.026779] [G loss: 1.199726]\n",
      "[D loss: 0.986633] [G loss: 1.402312]\n",
      "[D loss: 0.828250] [G loss: 1.486545]\n",
      "[D loss: 0.795729] [G loss: 1.862801]\n",
      "[D loss: 0.842214] [G loss: 1.773782]\n",
      "[D loss: 0.885834] [G loss: 1.534414]\n",
      "[D loss: 0.984099] [G loss: 1.333766]\n",
      "[D loss: 0.627351] [G loss: 1.458851]\n",
      "[D loss: 0.935337] [G loss: 1.373136]\n",
      "[D loss: 0.876143] [G loss: 1.401099]\n",
      "[D loss: 0.711180] [G loss: 1.547162]\n",
      "[D loss: 0.641157] [G loss: 1.760099]\n",
      "[D loss: 0.757015] [G loss: 1.547007]\n",
      "[D loss: 0.587031] [G loss: 1.626513]\n",
      "[D loss: 0.604362] [G loss: 1.785753]\n",
      "[D loss: 0.797531] [G loss: 1.488134]\n",
      "[D loss: 0.753584] [G loss: 1.478267]\n",
      "[D loss: 0.767097] [G loss: 1.565489]\n",
      "[D loss: 0.831968] [G loss: 1.494816]\n",
      "[D loss: 0.985257] [G loss: 1.309054]\n",
      "[D loss: 0.763877] [G loss: 1.448354]\n",
      "[D loss: 0.941159] [G loss: 1.332321]\n",
      "[D loss: 0.780837] [G loss: 1.477868]\n",
      "[D loss: 0.952613] [G loss: 1.460363]\n",
      "[D loss: 0.782517] [G loss: 1.312929]\n",
      "[D loss: 0.972115] [G loss: 1.490779]\n",
      "[D loss: 1.029522] [G loss: 1.491645]\n",
      "[D loss: 0.570984] [G loss: 1.575910]\n",
      "[D loss: 0.762748] [G loss: 1.464705]\n",
      "[D loss: 1.035535] [G loss: 1.465906]\n",
      "[D loss: 0.824284] [G loss: 1.532717]\n",
      "[D loss: 0.518578] [G loss: 1.611989]\n",
      "[D loss: 0.788463] [G loss: 1.361079]\n",
      "[D loss: 0.793324] [G loss: 1.594328]\n",
      "[D loss: 0.796625] [G loss: 1.603680]\n",
      "[D loss: 0.787505] [G loss: 1.602953]\n",
      "[D loss: 0.992161] [G loss: 1.492510]\n",
      "[D loss: 0.821695] [G loss: 1.346006]\n",
      "[D loss: 0.800370] [G loss: 1.419139]\n",
      "[D loss: 0.938976] [G loss: 1.417302]\n",
      "[D loss: 0.763005] [G loss: 1.578470]\n",
      "[D loss: 0.781113] [G loss: 1.363671]\n",
      "[D loss: 0.721196] [G loss: 1.366453]\n",
      "[D loss: 0.690072] [G loss: 1.501397]\n",
      "[D loss: 0.823122] [G loss: 1.454880]\n",
      "[D loss: 0.882150] [G loss: 1.493578]\n",
      "[D loss: 0.885869] [G loss: 1.585087]\n",
      "[D loss: 0.854366] [G loss: 1.647125]\n",
      "[D loss: 0.701987] [G loss: 1.456732]\n",
      "[D loss: 0.641143] [G loss: 1.498958]\n",
      "[D loss: 0.840518] [G loss: 1.613603]\n",
      "[D loss: 0.910118] [G loss: 1.540318]\n",
      "[D loss: 0.909370] [G loss: 1.431117]\n",
      "[D loss: 0.877985] [G loss: 1.461884]\n",
      "[D loss: 0.827397] [G loss: 1.573517]\n",
      "[D loss: 0.827327] [G loss: 1.372854]\n",
      "[D loss: 0.830830] [G loss: 1.368445]\n",
      "[D loss: 0.754418] [G loss: 1.350233]\n",
      "[D loss: 0.893077] [G loss: 1.512367]\n",
      "[D loss: 0.988582] [G loss: 1.361126]\n",
      "[D loss: 0.868032] [G loss: 1.431440]\n",
      "[D loss: 0.690286] [G loss: 1.496258]\n",
      "[D loss: 0.772151] [G loss: 1.454944]\n",
      "[D loss: 0.787456] [G loss: 1.529047]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.807526] [G loss: 1.442570]\n",
      "[D loss: 0.882811] [G loss: 1.242079]\n",
      "[D loss: 0.641273] [G loss: 1.503489]\n",
      "[D loss: 0.712044] [G loss: 1.524920]\n",
      "[D loss: 0.839740] [G loss: 1.406829]\n",
      "[D loss: 0.966350] [G loss: 1.472739]\n",
      "[D loss: 0.833778] [G loss: 1.473732]\n",
      "[D loss: 1.066360] [G loss: 1.386026]\n",
      "[D loss: 0.983449] [G loss: 1.187186]\n",
      "[D loss: 0.674303] [G loss: 1.521121]\n",
      "[D loss: 0.774722] [G loss: 1.390182]\n",
      "[D loss: 1.130143] [G loss: 1.452359]\n",
      "[D loss: 0.778662] [G loss: 1.505122]\n",
      "[D loss: 0.820170] [G loss: 1.607201]\n",
      "[D loss: 0.567449] [G loss: 1.561849]\n",
      "[D loss: 0.794274] [G loss: 1.425328]\n",
      "[D loss: 0.810614] [G loss: 1.537995]\n",
      "[D loss: 0.945503] [G loss: 1.244463]\n",
      "[D loss: 0.822898] [G loss: 1.215667]\n",
      "[D loss: 1.112404] [G loss: 1.485597]\n",
      "[D loss: 0.977177] [G loss: 1.547297]\n",
      "[D loss: 0.767940] [G loss: 1.329830]\n",
      "[D loss: 0.928679] [G loss: 1.466451]\n",
      "[D loss: 0.763109] [G loss: 1.480473]\n",
      "[D loss: 1.045448] [G loss: 1.420619]\n",
      "[D loss: 0.878285] [G loss: 1.275126]\n",
      "[D loss: 0.861970] [G loss: 1.614692]\n",
      "[D loss: 0.900890] [G loss: 1.503067]\n",
      "[D loss: 0.712873] [G loss: 1.489118]\n",
      "[D loss: 0.780890] [G loss: 1.390448]\n",
      "[D loss: 0.981472] [G loss: 1.445796]\n",
      "[D loss: 0.891484] [G loss: 1.765671]\n",
      "[D loss: 0.794353] [G loss: 1.324485]\n",
      "[D loss: 0.835115] [G loss: 1.374592]\n",
      "[D loss: 0.716031] [G loss: 1.392390]\n",
      "[D loss: 0.673751] [G loss: 1.380825]\n",
      "[D loss: 0.805499] [G loss: 1.411535]\n",
      "[D loss: 0.904821] [G loss: 1.472090]\n",
      "[D loss: 1.083118] [G loss: 1.178982]\n",
      "[D loss: 0.688199] [G loss: 1.459161]\n",
      "[D loss: 0.752331] [G loss: 1.414947]\n",
      "[D loss: 1.241864] [G loss: 1.188815]\n",
      "[D loss: 0.838276] [G loss: 1.520254]\n",
      "[D loss: 1.153050] [G loss: 1.555286]\n",
      "[D loss: 0.658794] [G loss: 1.511623]\n",
      "[D loss: 0.906125] [G loss: 1.423001]\n",
      "[D loss: 0.845900] [G loss: 1.326928]\n",
      "[D loss: 0.893661] [G loss: 1.205557]\n",
      "[D loss: 1.091963] [G loss: 1.356264]\n",
      "[D loss: 0.781505] [G loss: 1.588264]\n",
      "[D loss: 0.827024] [G loss: 1.545308]\n",
      "[D loss: 0.925464] [G loss: 1.417212]\n",
      "[D loss: 0.774933] [G loss: 1.475107]\n",
      "[D loss: 0.812125] [G loss: 1.419269]\n",
      "[D loss: 0.788045] [G loss: 1.423321]\n",
      "[D loss: 0.949421] [G loss: 1.071812]\n",
      "[D loss: 0.880449] [G loss: 1.387514]\n",
      "[D loss: 0.978747] [G loss: 1.264314]\n",
      "[D loss: 0.820334] [G loss: 1.488880]\n",
      "[D loss: 0.677570] [G loss: 1.503389]\n",
      "[D loss: 0.526705] [G loss: 1.642446]\n",
      "[D loss: 0.904662] [G loss: 1.610098]\n",
      "[D loss: 0.996389] [G loss: 1.482643]\n",
      "[D loss: 0.899668] [G loss: 1.375884]\n",
      "[D loss: 0.951667] [G loss: 1.392741]\n",
      "[D loss: 0.868202] [G loss: 1.421101]\n",
      "[D loss: 0.877602] [G loss: 1.406943]\n",
      "[D loss: 0.851382] [G loss: 1.318245]\n",
      "[D loss: 0.864370] [G loss: 1.411433]\n",
      "[D loss: 0.756231] [G loss: 1.379333]\n",
      "[D loss: 0.990245] [G loss: 1.243822]\n",
      "[D loss: 0.751850] [G loss: 1.473506]\n",
      "[D loss: 0.827481] [G loss: 1.635017]\n",
      "[D loss: 0.900459] [G loss: 1.592675]\n",
      "[D loss: 0.796473] [G loss: 1.346636]\n",
      "[D loss: 0.793786] [G loss: 1.375406]\n",
      "[D loss: 0.875466] [G loss: 1.388643]\n",
      "[D loss: 0.762763] [G loss: 1.385164]\n",
      "[D loss: 0.831667] [G loss: 1.451289]\n",
      "[D loss: 0.790788] [G loss: 1.345219]\n",
      "[D loss: 0.699792] [G loss: 1.328709]\n",
      "[D loss: 0.686444] [G loss: 1.353485]\n",
      "[D loss: 0.939729] [G loss: 1.589205]\n",
      "[D loss: 0.785602] [G loss: 1.662506]\n",
      "[D loss: 0.999035] [G loss: 1.435943]\n",
      "[D loss: 1.063175] [G loss: 1.249761]\n",
      "[D loss: 0.871668] [G loss: 1.267047]\n",
      "[D loss: 0.886250] [G loss: 1.450004]\n",
      "[D loss: 0.651775] [G loss: 1.364654]\n",
      "[D loss: 0.860024] [G loss: 1.291369]\n",
      "[D loss: 0.639276] [G loss: 1.584038]\n",
      "[D loss: 0.825320] [G loss: 1.326931]\n",
      "[D loss: 0.760594] [G loss: 1.532381]\n",
      "[D loss: 0.839136] [G loss: 1.493438]\n",
      "[D loss: 0.869752] [G loss: 1.387902]\n",
      "[D loss: 0.869147] [G loss: 1.409201]\n",
      "[D loss: 0.750391] [G loss: 1.395515]\n",
      "[D loss: 0.766873] [G loss: 1.302553]\n",
      "[D loss: 0.840147] [G loss: 1.362350]\n",
      "[D loss: 0.649920] [G loss: 1.658816]\n",
      "[D loss: 1.149955] [G loss: 1.425748]\n",
      "[D loss: 0.920443] [G loss: 1.512372]\n",
      "[D loss: 0.736010] [G loss: 1.516236]\n",
      "[D loss: 0.981972] [G loss: 1.372802]\n",
      "[D loss: 0.794037] [G loss: 1.308189]\n",
      "[D loss: 0.998521] [G loss: 1.777769]\n",
      "[D loss: 0.643333] [G loss: 1.342621]\n",
      "[D loss: 0.759271] [G loss: 1.405440]\n",
      "[D loss: 1.018206] [G loss: 1.366848]\n",
      "[D loss: 0.934734] [G loss: 1.174333]\n",
      "[D loss: 0.874017] [G loss: 1.571165]\n",
      "[D loss: 0.739192] [G loss: 1.737860]\n",
      "[D loss: 0.934197] [G loss: 1.309809]\n",
      "[D loss: 0.909494] [G loss: 1.580383]\n",
      "[D loss: 0.779146] [G loss: 1.525938]\n",
      "[D loss: 0.949621] [G loss: 1.570437]\n",
      "[D loss: 0.734246] [G loss: 1.581585]\n",
      "[D loss: 0.824340] [G loss: 1.624796]\n",
      "[D loss: 0.758587] [G loss: 1.773657]\n",
      "[D loss: 1.016145] [G loss: 1.257130]\n",
      "[D loss: 1.051136] [G loss: 1.471952]\n",
      "[D loss: 0.855887] [G loss: 1.398353]\n",
      "[D loss: 0.687402] [G loss: 1.487694]\n",
      "[D loss: 0.956216] [G loss: 1.402127]\n",
      "[D loss: 1.024765] [G loss: 1.221162]\n",
      "[D loss: 0.938373] [G loss: 1.416519]\n",
      "[D loss: 0.759310] [G loss: 1.427457]\n",
      "[D loss: 0.976883] [G loss: 1.396925]\n",
      "[D loss: 0.792952] [G loss: 1.344097]\n",
      "[D loss: 0.793756] [G loss: 1.339903]\n",
      "[D loss: 0.959845] [G loss: 1.419922]\n",
      "[D loss: 0.893611] [G loss: 1.341581]\n",
      "[D loss: 0.865197] [G loss: 1.302121]\n",
      "[D loss: 0.745231] [G loss: 1.535565]\n",
      "[D loss: 0.984575] [G loss: 1.410710]\n",
      "[D loss: 0.785230] [G loss: 1.525573]\n",
      "[D loss: 0.752119] [G loss: 1.441995]\n",
      "[D loss: 0.788654] [G loss: 1.488981]\n",
      "[D loss: 0.855430] [G loss: 1.506869]\n",
      "[D loss: 0.786678] [G loss: 1.300382]\n",
      "[D loss: 0.747289] [G loss: 1.506465]\n",
      "[D loss: 0.881396] [G loss: 1.417942]\n",
      "[D loss: 0.731594] [G loss: 1.452141]\n",
      "[D loss: 0.674769] [G loss: 1.568747]\n",
      "[D loss: 0.905251] [G loss: 1.419231]\n",
      "[D loss: 0.657300] [G loss: 1.632523]\n",
      "[D loss: 0.822769] [G loss: 1.350510]\n",
      "[D loss: 1.019004] [G loss: 1.427304]\n",
      "[D loss: 0.830275] [G loss: 1.410228]\n",
      "[D loss: 0.742754] [G loss: 1.480448]\n",
      "[D loss: 0.669685] [G loss: 1.556642]\n",
      "[D loss: 0.790487] [G loss: 1.618973]\n",
      "[D loss: 0.951773] [G loss: 1.540724]\n",
      "[D loss: 0.922626] [G loss: 1.588839]\n",
      "[D loss: 0.617274] [G loss: 1.585872]\n",
      "[D loss: 0.941069] [G loss: 1.411674]\n",
      "[D loss: 0.771565] [G loss: 1.355796]\n",
      "[D loss: 0.818882] [G loss: 1.219191]\n",
      "[D loss: 0.980252] [G loss: 1.254544]\n",
      "[D loss: 0.761639] [G loss: 1.626275]\n",
      "[D loss: 0.761528] [G loss: 1.284209]\n",
      "[D loss: 0.866739] [G loss: 1.319574]\n",
      "[D loss: 0.835400] [G loss: 1.646473]\n",
      "[D loss: 0.703371] [G loss: 1.390863]\n",
      "[D loss: 0.987290] [G loss: 1.486556]\n",
      "[D loss: 0.818576] [G loss: 1.551030]\n",
      "[D loss: 0.849975] [G loss: 1.429465]\n",
      "[D loss: 0.763733] [G loss: 1.446314]\n",
      "[D loss: 0.962133] [G loss: 1.440991]\n",
      "[D loss: 0.954016] [G loss: 1.564760]\n",
      "[D loss: 0.883432] [G loss: 1.547996]\n",
      "[D loss: 0.920737] [G loss: 1.341512]\n",
      "[D loss: 0.782704] [G loss: 1.434496]\n",
      "[D loss: 0.653960] [G loss: 1.455500]\n",
      "[D loss: 0.674589] [G loss: 1.633672]\n",
      "[D loss: 0.770369] [G loss: 1.612418]\n",
      "[D loss: 0.722016] [G loss: 1.684610]\n",
      "[D loss: 0.868455] [G loss: 1.491741]\n",
      "[D loss: 0.893736] [G loss: 1.346671]\n",
      "[D loss: 0.711280] [G loss: 1.474260]\n",
      "[D loss: 0.829218] [G loss: 1.374858]\n",
      "[D loss: 0.824975] [G loss: 1.394196]\n",
      "[D loss: 0.834788] [G loss: 1.635968]\n",
      "[D loss: 0.895015] [G loss: 1.494802]\n",
      "[D loss: 0.983926] [G loss: 1.403114]\n",
      "[D loss: 0.928545] [G loss: 1.263845]\n",
      "[D loss: 0.800651] [G loss: 1.434724]\n",
      "[D loss: 1.046764] [G loss: 1.178302]\n",
      "[D loss: 0.679747] [G loss: 1.165420]\n",
      "[D loss: 0.761312] [G loss: 1.421247]\n",
      "[D loss: 0.845346] [G loss: 1.377766]\n",
      "[D loss: 0.883914] [G loss: 1.421464]\n",
      "[D loss: 1.168122] [G loss: 1.296728]\n",
      "[D loss: 1.000765] [G loss: 1.357074]\n",
      "[D loss: 0.703089] [G loss: 1.521949]\n",
      "[D loss: 0.979688] [G loss: 1.403352]\n",
      "[D loss: 0.796179] [G loss: 1.466527]\n",
      "[D loss: 0.726260] [G loss: 1.456568]\n",
      "[D loss: 0.782045] [G loss: 1.504329]\n",
      "[D loss: 0.830403] [G loss: 1.769643]\n",
      "[D loss: 0.911262] [G loss: 1.560170]\n",
      "[D loss: 0.868663] [G loss: 1.467759]\n",
      "[D loss: 0.840828] [G loss: 1.405369]\n",
      "[D loss: 0.863194] [G loss: 1.528372]\n",
      "[D loss: 0.802782] [G loss: 1.288114]\n",
      "[D loss: 0.810655] [G loss: 1.299377]\n",
      "[D loss: 0.745506] [G loss: 1.527188]\n",
      "[D loss: 0.971677] [G loss: 1.370274]\n",
      "[D loss: 0.721882] [G loss: 1.365788]\n",
      "[D loss: 0.868276] [G loss: 1.561907]\n",
      "[D loss: 0.847018] [G loss: 1.360559]\n",
      "[D loss: 1.069539] [G loss: 1.464042]\n",
      "[D loss: 1.003229] [G loss: 1.473296]\n",
      "[D loss: 0.587074] [G loss: 1.540120]\n",
      "[D loss: 0.898197] [G loss: 1.490824]\n",
      "[D loss: 0.972702] [G loss: 1.468421]\n",
      "[D loss: 0.876314] [G loss: 1.218542]\n",
      "[D loss: 0.797855] [G loss: 1.594530]\n",
      "[D loss: 0.925187] [G loss: 1.190884]\n",
      "[D loss: 0.842050] [G loss: 1.344426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 1.047194] [G loss: 1.376817]\n",
      "[D loss: 0.713411] [G loss: 1.618908]\n",
      "[D loss: 0.811905] [G loss: 1.420709]\n",
      "[D loss: 0.762697] [G loss: 1.562847]\n",
      "[D loss: 0.734453] [G loss: 1.571812]\n",
      "[D loss: 0.712028] [G loss: 1.460498]\n",
      "[D loss: 0.714038] [G loss: 1.402786]\n",
      "[D loss: 0.832300] [G loss: 1.345534]\n",
      "[D loss: 0.808241] [G loss: 1.504565]\n",
      "[D loss: 0.985680] [G loss: 1.542047]\n",
      "[D loss: 1.002893] [G loss: 1.525410]\n",
      "[D loss: 0.830074] [G loss: 1.370886]\n",
      "[D loss: 0.781782] [G loss: 1.368743]\n",
      "[D loss: 0.883067] [G loss: 1.285707]\n",
      "[D loss: 0.658468] [G loss: 1.337118]\n",
      "[D loss: 1.025671] [G loss: 1.327920]\n",
      "[D loss: 0.936818] [G loss: 1.693062]\n",
      "[D loss: 0.799817] [G loss: 1.501164]\n",
      "[D loss: 0.817577] [G loss: 1.327404]\n",
      "[D loss: 0.958743] [G loss: 1.380216]\n",
      "[D loss: 1.042732] [G loss: 1.263398]\n",
      "[D loss: 0.616049] [G loss: 1.604438]\n",
      "[D loss: 0.770133] [G loss: 1.451987]\n",
      "[D loss: 0.751878] [G loss: 1.573479]\n",
      "[D loss: 0.793893] [G loss: 1.506660]\n",
      "[D loss: 0.865304] [G loss: 1.432704]\n",
      "[D loss: 0.666203] [G loss: 1.464288]\n",
      "[D loss: 0.982197] [G loss: 1.495705]\n",
      "[D loss: 0.703379] [G loss: 1.656627]\n",
      "[D loss: 0.578334] [G loss: 1.743144]\n",
      "[D loss: 0.814293] [G loss: 1.538876]\n",
      "[D loss: 0.831884] [G loss: 1.493162]\n",
      "[D loss: 0.743129] [G loss: 1.465363]\n",
      "[D loss: 0.893114] [G loss: 1.518510]\n",
      "[D loss: 0.705276] [G loss: 1.561337]\n",
      "[D loss: 0.768408] [G loss: 1.370435]\n",
      "[D loss: 0.915677] [G loss: 1.370917]\n",
      "[D loss: 0.796101] [G loss: 1.596224]\n",
      "[D loss: 1.030471] [G loss: 1.402738]\n",
      "[D loss: 0.787172] [G loss: 1.484997]\n",
      "[D loss: 0.842489] [G loss: 1.344361]\n",
      "[D loss: 0.910763] [G loss: 1.483743]\n",
      "[D loss: 0.899191] [G loss: 1.355679]\n",
      "[D loss: 0.752606] [G loss: 1.520710]\n",
      "[D loss: 0.728726] [G loss: 1.532998]\n",
      "[D loss: 0.897310] [G loss: 1.373057]\n",
      "[D loss: 1.000038] [G loss: 1.564799]\n",
      "[D loss: 0.789632] [G loss: 1.372851]\n",
      "[D loss: 0.649366] [G loss: 1.503091]\n",
      "[D loss: 0.987817] [G loss: 1.572724]\n",
      "[D loss: 0.886429] [G loss: 1.500380]\n",
      "[D loss: 0.817080] [G loss: 1.438227]\n",
      "[D loss: 0.871265] [G loss: 1.260922]\n",
      "[D loss: 0.845880] [G loss: 1.306146]\n",
      "[D loss: 0.895618] [G loss: 1.212855]\n",
      "[D loss: 0.993548] [G loss: 1.514068]\n",
      "[D loss: 0.869478] [G loss: 1.196570]\n",
      "[D loss: 0.813452] [G loss: 1.565177]\n",
      "[D loss: 0.924221] [G loss: 1.500242]\n",
      "[D loss: 0.771372] [G loss: 1.506840]\n",
      "[D loss: 0.945131] [G loss: 1.619917]\n",
      "[D loss: 0.866182] [G loss: 1.453915]\n",
      "[D loss: 0.823918] [G loss: 1.397843]\n",
      "[D loss: 0.706764] [G loss: 1.455670]\n",
      "[D loss: 0.747435] [G loss: 1.433461]\n",
      "[D loss: 0.834706] [G loss: 1.321661]\n",
      "[D loss: 0.784802] [G loss: 1.337464]\n",
      "[D loss: 0.972020] [G loss: 1.445164]\n",
      "[D loss: 0.620950] [G loss: 1.457153]\n",
      "[D loss: 0.909371] [G loss: 1.310970]\n",
      "[D loss: 0.797122] [G loss: 1.530019]\n",
      "[D loss: 0.952179] [G loss: 1.195325]\n",
      "[D loss: 0.870094] [G loss: 1.466282]\n",
      "[D loss: 0.797537] [G loss: 1.422407]\n",
      "[D loss: 0.788536] [G loss: 1.445921]\n",
      "[D loss: 0.971585] [G loss: 1.459522]\n",
      "[D loss: 0.908401] [G loss: 1.519891]\n",
      "[D loss: 0.698746] [G loss: 1.622618]\n",
      "[D loss: 1.070330] [G loss: 1.305247]\n",
      "[D loss: 0.549366] [G loss: 1.622051]\n",
      "[D loss: 0.830036] [G loss: 1.597536]\n",
      "[D loss: 0.827721] [G loss: 1.307007]\n",
      "[D loss: 0.982503] [G loss: 1.149207]\n",
      "[D loss: 0.802945] [G loss: 1.576547]\n",
      "[D loss: 0.752114] [G loss: 1.317197]\n",
      "[D loss: 0.841461] [G loss: 1.535869]\n",
      "[D loss: 0.777025] [G loss: 1.390305]\n",
      "[D loss: 0.731865] [G loss: 1.470892]\n",
      "[D loss: 0.978850] [G loss: 1.410971]\n",
      "[D loss: 0.733638] [G loss: 1.469614]\n",
      "[D loss: 0.904536] [G loss: 1.216556]\n",
      "[D loss: 0.882559] [G loss: 1.408767]\n",
      "[D loss: 1.013643] [G loss: 1.142658]\n",
      "[D loss: 0.545547] [G loss: 1.688378]\n",
      "[D loss: 0.944573] [G loss: 1.433316]\n",
      "[D loss: 0.944994] [G loss: 1.305095]\n",
      "[D loss: 1.071457] [G loss: 1.287892]\n",
      "[D loss: 1.034973] [G loss: 1.597918]\n",
      "[D loss: 0.737612] [G loss: 1.417303]\n",
      "[D loss: 0.747297] [G loss: 1.414791]\n",
      "[D loss: 0.757252] [G loss: 1.375214]\n",
      "[D loss: 0.875767] [G loss: 1.404363]\n",
      "[D loss: 0.843273] [G loss: 1.434937]\n",
      "[D loss: 1.005685] [G loss: 1.513949]\n",
      "[D loss: 0.856897] [G loss: 1.373315]\n",
      "[D loss: 0.869351] [G loss: 1.190732]\n",
      "[D loss: 0.767568] [G loss: 1.252389]\n",
      "[D loss: 0.833261] [G loss: 1.374501]\n",
      "[D loss: 0.969189] [G loss: 1.584834]\n",
      "[D loss: 0.866240] [G loss: 1.399609]\n",
      "[D loss: 0.948933] [G loss: 1.659729]\n",
      "[D loss: 1.057482] [G loss: 1.358495]\n",
      "[D loss: 0.647915] [G loss: 1.457253]\n",
      "[D loss: 0.996743] [G loss: 1.331006]\n",
      "[D loss: 0.914711] [G loss: 1.384182]\n",
      "[D loss: 0.713561] [G loss: 1.352321]\n",
      "[D loss: 0.784404] [G loss: 1.480872]\n",
      "[D loss: 0.933345] [G loss: 1.251427]\n",
      "[D loss: 0.775308] [G loss: 1.330520]\n",
      "[D loss: 0.893433] [G loss: 1.325248]\n",
      "[D loss: 0.959753] [G loss: 1.297419]\n",
      "[D loss: 0.900321] [G loss: 1.486562]\n",
      "[D loss: 0.858685] [G loss: 1.333382]\n",
      "[D loss: 0.940137] [G loss: 1.407070]\n",
      "[D loss: 0.805137] [G loss: 1.215034]\n",
      "[D loss: 0.896092] [G loss: 1.248070]\n",
      "[D loss: 0.952327] [G loss: 1.548997]\n",
      "[D loss: 0.853608] [G loss: 1.579211]\n",
      "[D loss: 0.930845] [G loss: 1.350825]\n",
      "[D loss: 0.669123] [G loss: 1.391716]\n",
      "[D loss: 0.846472] [G loss: 1.312726]\n",
      "[D loss: 1.051966] [G loss: 1.278189]\n",
      "[D loss: 0.839167] [G loss: 1.325787]\n",
      "[D loss: 0.814782] [G loss: 1.346895]\n",
      "[D loss: 0.676011] [G loss: 1.272906]\n",
      "[D loss: 0.632344] [G loss: 1.538244]\n",
      "[D loss: 0.836432] [G loss: 1.442142]\n",
      "[D loss: 0.629327] [G loss: 1.434181]\n",
      "[D loss: 1.096592] [G loss: 1.365873]\n",
      "[D loss: 0.789776] [G loss: 1.580236]\n",
      "[D loss: 0.870425] [G loss: 1.598670]\n",
      "[D loss: 0.784385] [G loss: 1.528414]\n",
      "[D loss: 0.872997] [G loss: 1.569391]\n",
      "[D loss: 0.982576] [G loss: 1.513773]\n",
      "[D loss: 0.829698] [G loss: 1.388052]\n",
      "[D loss: 0.902538] [G loss: 1.621492]\n",
      "[D loss: 0.756579] [G loss: 1.338473]\n",
      "[D loss: 0.996470] [G loss: 1.161775]\n",
      "[D loss: 0.893260] [G loss: 1.219587]\n",
      "[D loss: 0.753579] [G loss: 1.314192]\n",
      "[D loss: 0.553002] [G loss: 1.478879]\n",
      "[D loss: 0.863017] [G loss: 1.561943]\n",
      "[D loss: 0.648630] [G loss: 1.839234]\n",
      "[D loss: 0.645220] [G loss: 1.626370]\n",
      "[D loss: 0.857747] [G loss: 1.460090]\n",
      "[D loss: 0.998302] [G loss: 1.537990]\n",
      "[D loss: 0.717687] [G loss: 1.591158]\n",
      "[D loss: 0.937768] [G loss: 1.323824]\n",
      "[D loss: 0.826570] [G loss: 1.563191]\n",
      "[D loss: 0.796506] [G loss: 1.424088]\n",
      "[D loss: 0.921319] [G loss: 1.415962]\n",
      "[D loss: 0.673685] [G loss: 1.476836]\n",
      "[D loss: 0.621503] [G loss: 1.526904]\n",
      "[D loss: 0.851537] [G loss: 1.446060]\n",
      "[D loss: 0.944842] [G loss: 1.266056]\n",
      "[D loss: 1.026949] [G loss: 1.271020]\n",
      "[D loss: 0.951758] [G loss: 1.431510]\n",
      "[D loss: 0.765257] [G loss: 1.475206]\n",
      "[D loss: 0.889554] [G loss: 1.379212]\n",
      "[D loss: 0.778479] [G loss: 1.713676]\n",
      "[D loss: 0.829167] [G loss: 1.356870]\n",
      "[D loss: 0.792814] [G loss: 1.454617]\n",
      "[D loss: 0.995230] [G loss: 1.507114]\n",
      "[D loss: 0.901987] [G loss: 1.269863]\n",
      "[D loss: 0.789663] [G loss: 1.399652]\n",
      "[D loss: 1.031997] [G loss: 1.503910]\n",
      "[D loss: 0.923965] [G loss: 1.324866]\n",
      "[D loss: 1.027973] [G loss: 1.293073]\n",
      "[D loss: 0.686897] [G loss: 1.623144]\n",
      "[D loss: 0.993603] [G loss: 1.266492]\n",
      "[D loss: 0.896182] [G loss: 1.265463]\n",
      "[D loss: 0.682666] [G loss: 1.509906]\n",
      "[D loss: 0.914442] [G loss: 1.585778]\n",
      "[D loss: 1.000781] [G loss: 1.367315]\n",
      "[D loss: 0.936860] [G loss: 1.551427]\n",
      "[D loss: 0.862932] [G loss: 1.440905]\n",
      "[D loss: 0.857471] [G loss: 1.379450]\n",
      "[D loss: 0.795534] [G loss: 1.403699]\n",
      "[D loss: 0.625615] [G loss: 1.428526]\n",
      "[D loss: 0.958054] [G loss: 1.320178]\n",
      "[D loss: 0.806449] [G loss: 1.537064]\n",
      "[D loss: 0.768475] [G loss: 1.462013]\n",
      "[D loss: 0.903192] [G loss: 1.717823]\n",
      "[D loss: 0.871561] [G loss: 1.267938]\n",
      "[D loss: 0.950855] [G loss: 1.374523]\n",
      "[D loss: 0.887068] [G loss: 1.518383]\n",
      "[D loss: 0.883301] [G loss: 1.518948]\n",
      "[D loss: 0.778284] [G loss: 1.442438]\n",
      "[D loss: 0.673036] [G loss: 1.528884]\n",
      "[D loss: 0.812037] [G loss: 1.418506]\n",
      "[D loss: 0.813134] [G loss: 1.237108]\n",
      "[D loss: 0.879644] [G loss: 1.374764]\n",
      "[D loss: 0.772079] [G loss: 1.441897]\n",
      "[D loss: 0.700806] [G loss: 1.635389]\n",
      "[D loss: 0.898361] [G loss: 1.716578]\n",
      "[D loss: 0.869367] [G loss: 1.358646]\n",
      "[D loss: 0.533886] [G loss: 1.487051]\n",
      "[D loss: 0.883083] [G loss: 1.557508]\n",
      "[D loss: 0.881536] [G loss: 1.322212]\n",
      "[D loss: 1.002485] [G loss: 1.429235]\n",
      "[D loss: 0.808369] [G loss: 1.669351]\n",
      "[D loss: 0.863926] [G loss: 1.331416]\n",
      "[D loss: 0.855405] [G loss: 1.604329]\n",
      "[D loss: 0.876152] [G loss: 1.310885]\n",
      "[D loss: 0.751267] [G loss: 1.312370]\n",
      "[D loss: 0.925195] [G loss: 1.258417]\n",
      "[D loss: 0.750442] [G loss: 1.413016]\n",
      "[D loss: 0.921149] [G loss: 1.560428]\n",
      "[D loss: 0.906307] [G loss: 1.737430]\n",
      "[D loss: 1.007261] [G loss: 1.392058]\n",
      "[D loss: 0.775292] [G loss: 1.336234]\n",
      "[D loss: 0.783442] [G loss: 1.310876]\n",
      "[D loss: 0.830091] [G loss: 1.526048]\n",
      "[D loss: 0.899786] [G loss: 1.377635]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.695747] [G loss: 1.368115]\n",
      "[D loss: 0.813103] [G loss: 1.388508]\n",
      "[D loss: 0.961512] [G loss: 1.381881]\n",
      "[D loss: 1.033015] [G loss: 1.393386]\n",
      "[D loss: 0.963809] [G loss: 1.375312]\n",
      "[D loss: 0.858079] [G loss: 1.353929]\n",
      "[D loss: 0.906447] [G loss: 1.506132]\n",
      "[D loss: 0.765148] [G loss: 1.267094]\n",
      "[D loss: 0.815881] [G loss: 1.430308]\n",
      "[D loss: 0.987329] [G loss: 1.439069]\n",
      "[D loss: 0.834853] [G loss: 1.565300]\n",
      "[D loss: 0.993389] [G loss: 1.330175]\n",
      "[D loss: 1.317584] [G loss: 1.146886]\n",
      "[D loss: 0.845901] [G loss: 1.541344]\n",
      "[D loss: 0.772063] [G loss: 1.586972]\n",
      "[D loss: 0.819236] [G loss: 1.364960]\n",
      "[D loss: 0.926776] [G loss: 1.407816]\n",
      "[D loss: 0.965054] [G loss: 1.304182]\n",
      "[D loss: 0.659183] [G loss: 1.358232]\n",
      "[D loss: 0.720819] [G loss: 1.362782]\n",
      "[D loss: 0.806278] [G loss: 1.247542]\n",
      "[D loss: 1.016173] [G loss: 1.499775]\n",
      "[D loss: 0.871828] [G loss: 1.449224]\n",
      "[D loss: 0.857231] [G loss: 1.637870]\n",
      "[D loss: 0.782990] [G loss: 1.390872]\n",
      "[D loss: 0.783737] [G loss: 1.425185]\n",
      "[D loss: 0.992528] [G loss: 1.242994]\n",
      "[D loss: 0.666198] [G loss: 1.403701]\n",
      "[D loss: 0.922506] [G loss: 1.380208]\n",
      "[D loss: 0.682625] [G loss: 1.445326]\n",
      "[D loss: 0.870662] [G loss: 1.429350]\n",
      "[D loss: 0.890077] [G loss: 1.371055]\n",
      "[D loss: 0.668383] [G loss: 1.405550]\n",
      "[D loss: 0.869076] [G loss: 1.422050]\n",
      "[D loss: 0.717462] [G loss: 1.319124]\n",
      "[D loss: 0.833865] [G loss: 1.538664]\n",
      "[D loss: 0.818673] [G loss: 1.486811]\n",
      "[D loss: 0.829981] [G loss: 1.486260]\n",
      "[D loss: 0.849313] [G loss: 1.477013]\n",
      "[D loss: 0.639558] [G loss: 1.503423]\n",
      "[D loss: 0.923713] [G loss: 1.431129]\n",
      "[D loss: 0.570932] [G loss: 1.526385]\n",
      "[D loss: 1.151660] [G loss: 1.263971]\n",
      "[D loss: 1.071749] [G loss: 1.602301]\n",
      "[D loss: 0.750437] [G loss: 1.694916]\n",
      "[D loss: 0.727369] [G loss: 1.572809]\n",
      "[D loss: 0.844830] [G loss: 1.535490]\n",
      "[D loss: 0.709233] [G loss: 1.524749]\n",
      "[D loss: 0.713629] [G loss: 1.567926]\n",
      "[D loss: 0.734384] [G loss: 1.489908]\n",
      "[D loss: 0.902107] [G loss: 1.583251]\n",
      "[D loss: 0.708854] [G loss: 1.843039]\n",
      "[D loss: 0.691035] [G loss: 1.548961]\n",
      "[D loss: 0.758290] [G loss: 1.339824]\n",
      "[D loss: 0.867735] [G loss: 1.420176]\n",
      "[D loss: 0.740607] [G loss: 1.429175]\n",
      "[D loss: 0.749919] [G loss: 1.847757]\n",
      "[D loss: 0.996262] [G loss: 1.560399]\n",
      "[D loss: 0.829782] [G loss: 1.518193]\n",
      "[D loss: 0.623284] [G loss: 1.590537]\n",
      "[D loss: 0.985872] [G loss: 1.655042]\n",
      "[D loss: 1.148914] [G loss: 1.346548]\n",
      "[D loss: 0.811173] [G loss: 1.646860]\n",
      "[D loss: 0.995058] [G loss: 1.379614]\n",
      "[D loss: 0.802329] [G loss: 1.469030]\n",
      "[D loss: 0.733167] [G loss: 1.562129]\n",
      "[D loss: 0.930022] [G loss: 1.382763]\n",
      "[D loss: 0.872544] [G loss: 1.427683]\n",
      "[D loss: 1.117566] [G loss: 1.383528]\n",
      "[D loss: 0.895985] [G loss: 1.419307]\n",
      "[D loss: 0.774601] [G loss: 1.470164]\n",
      "[D loss: 0.901160] [G loss: 1.349067]\n",
      "[D loss: 0.968205] [G loss: 1.262580]\n",
      "[D loss: 0.734470] [G loss: 1.391658]\n",
      "[D loss: 0.800019] [G loss: 1.498703]\n",
      "[D loss: 0.821215] [G loss: 1.325155]\n",
      "[D loss: 0.805464] [G loss: 1.525167]\n",
      "[D loss: 0.857998] [G loss: 1.553274]\n",
      "[D loss: 0.816413] [G loss: 1.322335]\n",
      "[D loss: 0.867935] [G loss: 1.257652]\n",
      "[D loss: 0.779928] [G loss: 1.538612]\n",
      "[D loss: 0.732289] [G loss: 1.230801]\n",
      "[D loss: 0.807269] [G loss: 1.318868]\n",
      "[D loss: 0.923840] [G loss: 1.196275]\n",
      "[D loss: 0.725631] [G loss: 1.481989]\n",
      "[D loss: 0.878459] [G loss: 1.281605]\n",
      "[D loss: 0.955422] [G loss: 1.335407]\n",
      "[D loss: 0.812713] [G loss: 1.392158]\n",
      "[D loss: 0.904910] [G loss: 1.423670]\n",
      "[D loss: 0.841999] [G loss: 1.281678]\n",
      "[D loss: 0.861057] [G loss: 1.255790]\n",
      "[D loss: 0.788459] [G loss: 1.333653]\n",
      "[D loss: 0.899040] [G loss: 1.571319]\n",
      "[D loss: 0.948889] [G loss: 1.433239]\n",
      "[D loss: 0.815519] [G loss: 1.304158]\n",
      "[D loss: 0.785613] [G loss: 1.374175]\n",
      "[D loss: 1.021175] [G loss: 1.344893]\n",
      "[D loss: 0.956804] [G loss: 1.464528]\n",
      "[D loss: 0.695252] [G loss: 1.566547]\n",
      "[D loss: 0.806409] [G loss: 1.584814]\n",
      "[D loss: 1.115239] [G loss: 1.268950]\n",
      "[D loss: 0.602085] [G loss: 1.491883]\n",
      "[D loss: 0.744351] [G loss: 1.461974]\n",
      "[D loss: 0.797315] [G loss: 1.499623]\n",
      "[D loss: 0.797438] [G loss: 1.439001]\n",
      "[D loss: 0.964325] [G loss: 1.343747]\n",
      "[D loss: 0.624632] [G loss: 1.619045]\n",
      "[D loss: 0.822933] [G loss: 1.523188]\n",
      "[D loss: 0.987874] [G loss: 1.347064]\n",
      "[D loss: 0.928344] [G loss: 1.419158]\n",
      "[D loss: 0.915384] [G loss: 1.585462]\n",
      "[D loss: 1.296648] [G loss: 1.250344]\n",
      "[D loss: 0.714212] [G loss: 1.367088]\n",
      "[D loss: 0.823204] [G loss: 1.338403]\n",
      "[D loss: 0.621155] [G loss: 1.237779]\n",
      "[D loss: 0.875140] [G loss: 1.258985]\n",
      "[D loss: 0.900936] [G loss: 1.360947]\n",
      "[D loss: 0.850481] [G loss: 1.332496]\n",
      "[D loss: 0.661963] [G loss: 1.730127]\n",
      "[D loss: 0.936307] [G loss: 1.183921]\n",
      "[D loss: 0.994786] [G loss: 1.523627]\n",
      "[D loss: 0.858103] [G loss: 1.562003]\n",
      "[D loss: 0.902339] [G loss: 1.422946]\n",
      "[D loss: 0.807273] [G loss: 1.543357]\n",
      "[D loss: 0.937338] [G loss: 1.352827]\n",
      "[D loss: 1.126099] [G loss: 1.466140]\n",
      "[D loss: 1.041160] [G loss: 1.453822]\n",
      "[D loss: 0.840197] [G loss: 1.558714]\n",
      "[D loss: 0.694800] [G loss: 1.585557]\n",
      "[D loss: 0.733094] [G loss: 1.294468]\n",
      "[D loss: 0.813019] [G loss: 1.405513]\n",
      "[D loss: 0.654559] [G loss: 1.362902]\n",
      "[D loss: 0.663591] [G loss: 1.342062]\n",
      "[D loss: 1.070065] [G loss: 1.391961]\n",
      "[D loss: 1.140002] [G loss: 1.528292]\n",
      "[D loss: 0.951063] [G loss: 1.287549]\n",
      "[D loss: 0.850306] [G loss: 1.625946]\n",
      "[D loss: 0.794973] [G loss: 1.474019]\n",
      "[D loss: 0.736609] [G loss: 1.352709]\n",
      "[D loss: 0.907232] [G loss: 1.279300]\n",
      "[D loss: 0.888955] [G loss: 1.475080]\n",
      "[D loss: 0.764198] [G loss: 1.717661]\n",
      "[D loss: 0.843315] [G loss: 1.739734]\n",
      "epoch:17, g_loss:2750.084716796875,d_loss:1563.78369140625\n",
      "[D loss: 0.746343] [G loss: 1.437145]\n",
      "[D loss: 1.020334] [G loss: 1.348400]\n",
      "[D loss: 0.813209] [G loss: 1.541291]\n",
      "[D loss: 0.850596] [G loss: 1.136131]\n",
      "[D loss: 0.823311] [G loss: 1.383495]\n",
      "[D loss: 0.692071] [G loss: 1.250684]\n",
      "[D loss: 0.925850] [G loss: 1.210832]\n",
      "[D loss: 0.900272] [G loss: 1.478036]\n",
      "[D loss: 0.730827] [G loss: 1.419230]\n",
      "[D loss: 0.779114] [G loss: 1.393853]\n",
      "[D loss: 0.842835] [G loss: 1.466256]\n",
      "[D loss: 0.915935] [G loss: 1.492232]\n",
      "[D loss: 0.883520] [G loss: 1.441513]\n",
      "[D loss: 0.741207] [G loss: 1.441464]\n",
      "[D loss: 0.977113] [G loss: 1.464372]\n",
      "[D loss: 0.875930] [G loss: 1.538746]\n",
      "[D loss: 0.798761] [G loss: 1.550582]\n",
      "[D loss: 0.651972] [G loss: 1.391996]\n",
      "[D loss: 0.736939] [G loss: 1.539178]\n",
      "[D loss: 0.670476] [G loss: 1.417258]\n",
      "[D loss: 0.969914] [G loss: 1.236156]\n",
      "[D loss: 0.799614] [G loss: 1.371127]\n",
      "[D loss: 0.754417] [G loss: 1.344769]\n",
      "[D loss: 0.747046] [G loss: 1.445688]\n",
      "[D loss: 0.676250] [G loss: 1.636343]\n",
      "[D loss: 0.653036] [G loss: 1.472739]\n",
      "[D loss: 0.987419] [G loss: 1.552299]\n",
      "[D loss: 1.076785] [G loss: 1.424528]\n",
      "[D loss: 0.910937] [G loss: 1.448354]\n",
      "[D loss: 1.014899] [G loss: 1.346606]\n",
      "[D loss: 1.007198] [G loss: 1.447873]\n",
      "[D loss: 0.870573] [G loss: 1.533847]\n",
      "[D loss: 0.855291] [G loss: 1.380905]\n",
      "[D loss: 0.848778] [G loss: 1.308774]\n",
      "[D loss: 1.127833] [G loss: 1.318992]\n",
      "[D loss: 0.880451] [G loss: 1.308106]\n",
      "[D loss: 0.779917] [G loss: 1.253998]\n",
      "[D loss: 1.017158] [G loss: 1.313516]\n",
      "[D loss: 0.770562] [G loss: 1.318817]\n",
      "[D loss: 0.889920] [G loss: 1.623390]\n",
      "[D loss: 0.681608] [G loss: 1.374816]\n",
      "[D loss: 0.668228] [G loss: 1.575709]\n",
      "[D loss: 0.719651] [G loss: 1.366037]\n",
      "[D loss: 0.939069] [G loss: 1.537024]\n",
      "[D loss: 0.789247] [G loss: 1.487863]\n",
      "[D loss: 0.724247] [G loss: 1.560521]\n",
      "[D loss: 0.648989] [G loss: 1.415032]\n",
      "[D loss: 0.928110] [G loss: 1.360868]\n",
      "[D loss: 1.003950] [G loss: 1.281779]\n",
      "[D loss: 0.941945] [G loss: 1.396109]\n",
      "[D loss: 0.621733] [G loss: 1.565768]\n",
      "[D loss: 0.959409] [G loss: 1.337674]\n",
      "[D loss: 0.786556] [G loss: 1.550067]\n",
      "[D loss: 0.832434] [G loss: 1.395205]\n",
      "[D loss: 0.680178] [G loss: 1.649645]\n",
      "[D loss: 0.745626] [G loss: 1.551574]\n",
      "[D loss: 0.781929] [G loss: 1.663884]\n",
      "[D loss: 0.868972] [G loss: 1.385306]\n",
      "[D loss: 0.880290] [G loss: 1.568487]\n",
      "[D loss: 0.877391] [G loss: 1.793066]\n",
      "[D loss: 0.749569] [G loss: 1.536918]\n",
      "[D loss: 0.824890] [G loss: 1.427838]\n",
      "[D loss: 0.848780] [G loss: 1.290728]\n",
      "[D loss: 0.869173] [G loss: 1.433358]\n",
      "[D loss: 0.863569] [G loss: 1.508832]\n",
      "[D loss: 0.675329] [G loss: 1.873200]\n",
      "[D loss: 0.950403] [G loss: 1.486130]\n",
      "[D loss: 1.004076] [G loss: 1.457043]\n",
      "[D loss: 0.828622] [G loss: 1.508300]\n",
      "[D loss: 0.784193] [G loss: 1.413079]\n",
      "[D loss: 1.116539] [G loss: 1.891546]\n",
      "[D loss: 0.658911] [G loss: 1.375555]\n",
      "[D loss: 0.996263] [G loss: 1.202610]\n",
      "[D loss: 0.799575] [G loss: 1.519629]\n",
      "[D loss: 0.744429] [G loss: 1.562974]\n",
      "[D loss: 0.932485] [G loss: 1.263608]\n",
      "[D loss: 0.561985] [G loss: 1.426100]\n",
      "[D loss: 0.787876] [G loss: 1.356201]\n",
      "[D loss: 0.890711] [G loss: 1.570012]\n",
      "[D loss: 0.849900] [G loss: 1.575609]\n",
      "[D loss: 0.958541] [G loss: 1.533443]\n",
      "[D loss: 0.944898] [G loss: 1.305902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.907189] [G loss: 1.330706]\n",
      "[D loss: 0.916511] [G loss: 1.613625]\n",
      "[D loss: 0.692492] [G loss: 1.543094]\n",
      "[D loss: 0.815870] [G loss: 1.465565]\n",
      "[D loss: 0.794578] [G loss: 1.376655]\n",
      "[D loss: 0.807774] [G loss: 1.454841]\n",
      "[D loss: 0.937672] [G loss: 1.401865]\n",
      "[D loss: 0.918921] [G loss: 1.460678]\n",
      "[D loss: 0.916629] [G loss: 1.428520]\n",
      "[D loss: 0.755965] [G loss: 1.554011]\n",
      "[D loss: 0.781038] [G loss: 1.529760]\n",
      "[D loss: 0.854796] [G loss: 1.488095]\n",
      "[D loss: 0.753696] [G loss: 1.282641]\n",
      "[D loss: 0.812201] [G loss: 1.366052]\n",
      "[D loss: 0.677830] [G loss: 1.361687]\n",
      "[D loss: 0.827611] [G loss: 1.592226]\n",
      "[D loss: 0.680011] [G loss: 1.882511]\n",
      "[D loss: 0.628924] [G loss: 1.634813]\n",
      "[D loss: 0.763515] [G loss: 1.731341]\n",
      "[D loss: 0.876414] [G loss: 1.802086]\n",
      "[D loss: 0.968764] [G loss: 1.535840]\n",
      "[D loss: 1.031218] [G loss: 1.515528]\n",
      "[D loss: 0.977792] [G loss: 1.436785]\n",
      "[D loss: 0.819102] [G loss: 1.414161]\n",
      "[D loss: 0.741203] [G loss: 1.618661]\n",
      "[D loss: 0.849920] [G loss: 1.349047]\n",
      "[D loss: 0.953959] [G loss: 1.555981]\n",
      "[D loss: 0.821817] [G loss: 1.382072]\n",
      "[D loss: 0.947631] [G loss: 1.588050]\n",
      "[D loss: 0.909172] [G loss: 1.293197]\n",
      "[D loss: 0.740089] [G loss: 1.445557]\n",
      "[D loss: 1.283695] [G loss: 1.495559]\n",
      "[D loss: 1.134982] [G loss: 1.338206]\n",
      "[D loss: 0.886857] [G loss: 1.405210]\n",
      "[D loss: 0.829038] [G loss: 1.306211]\n",
      "[D loss: 0.884860] [G loss: 1.404872]\n",
      "[D loss: 0.874235] [G loss: 1.339514]\n",
      "[D loss: 0.806462] [G loss: 1.272296]\n",
      "[D loss: 0.791267] [G loss: 1.541720]\n",
      "[D loss: 0.736716] [G loss: 1.353477]\n",
      "[D loss: 0.982014] [G loss: 1.301415]\n",
      "[D loss: 0.853859] [G loss: 1.440545]\n",
      "[D loss: 0.909180] [G loss: 1.428333]\n",
      "[D loss: 0.807645] [G loss: 1.517059]\n",
      "[D loss: 0.746688] [G loss: 1.317390]\n",
      "[D loss: 0.900659] [G loss: 1.404913]\n",
      "[D loss: 0.867351] [G loss: 1.326414]\n",
      "[D loss: 0.774639] [G loss: 1.348015]\n",
      "[D loss: 0.819043] [G loss: 1.524476]\n",
      "[D loss: 0.619310] [G loss: 1.602780]\n",
      "[D loss: 0.899206] [G loss: 1.709916]\n",
      "[D loss: 0.884173] [G loss: 1.255416]\n",
      "[D loss: 0.798378] [G loss: 1.309213]\n",
      "[D loss: 0.764470] [G loss: 1.474290]\n",
      "[D loss: 0.917109] [G loss: 1.273403]\n",
      "[D loss: 0.911441] [G loss: 1.484903]\n",
      "[D loss: 0.890098] [G loss: 1.397591]\n",
      "[D loss: 0.860421] [G loss: 1.375496]\n",
      "[D loss: 0.826014] [G loss: 1.526498]\n",
      "[D loss: 0.850306] [G loss: 1.321714]\n",
      "[D loss: 0.771120] [G loss: 1.508686]\n",
      "[D loss: 0.708073] [G loss: 1.371632]\n",
      "[D loss: 1.126157] [G loss: 1.521088]\n",
      "[D loss: 0.649658] [G loss: 1.379835]\n",
      "[D loss: 0.774979] [G loss: 1.201263]\n",
      "[D loss: 0.702569] [G loss: 1.431975]\n",
      "[D loss: 0.853473] [G loss: 1.414195]\n",
      "[D loss: 0.898689] [G loss: 1.215735]\n",
      "[D loss: 1.086097] [G loss: 1.266227]\n",
      "[D loss: 0.754461] [G loss: 1.339451]\n",
      "[D loss: 0.627672] [G loss: 1.719575]\n",
      "[D loss: 0.708348] [G loss: 1.536856]\n",
      "[D loss: 0.821205] [G loss: 1.428399]\n",
      "[D loss: 0.841340] [G loss: 1.499749]\n",
      "[D loss: 0.739231] [G loss: 1.546279]\n",
      "[D loss: 0.811492] [G loss: 1.380933]\n",
      "[D loss: 0.956099] [G loss: 1.359802]\n",
      "[D loss: 0.806989] [G loss: 1.588024]\n",
      "[D loss: 0.928189] [G loss: 1.493379]\n",
      "[D loss: 0.758999] [G loss: 1.578964]\n",
      "[D loss: 0.738559] [G loss: 1.454676]\n",
      "[D loss: 0.846191] [G loss: 1.391748]\n",
      "[D loss: 0.796021] [G loss: 1.329664]\n",
      "[D loss: 0.955324] [G loss: 1.509827]\n",
      "[D loss: 0.770009] [G loss: 1.548379]\n",
      "[D loss: 0.845777] [G loss: 1.627167]\n",
      "[D loss: 1.062095] [G loss: 1.586112]\n",
      "[D loss: 0.969755] [G loss: 1.308178]\n",
      "[D loss: 0.778063] [G loss: 1.470425]\n",
      "[D loss: 0.948359] [G loss: 1.308262]\n",
      "[D loss: 0.922358] [G loss: 1.371860]\n",
      "[D loss: 0.867897] [G loss: 1.633776]\n",
      "[D loss: 0.639848] [G loss: 1.548375]\n",
      "[D loss: 0.833790] [G loss: 1.397857]\n",
      "[D loss: 0.743806] [G loss: 1.511455]\n",
      "[D loss: 0.780489] [G loss: 1.672321]\n",
      "[D loss: 0.945114] [G loss: 1.175612]\n",
      "[D loss: 0.734419] [G loss: 1.445291]\n",
      "[D loss: 0.637534] [G loss: 1.463039]\n",
      "[D loss: 0.979193] [G loss: 1.677939]\n",
      "[D loss: 1.028334] [G loss: 1.439694]\n",
      "[D loss: 0.749865] [G loss: 1.463866]\n",
      "[D loss: 0.812491] [G loss: 1.491631]\n",
      "[D loss: 0.785207] [G loss: 1.560110]\n",
      "[D loss: 0.811019] [G loss: 1.428292]\n",
      "[D loss: 0.856391] [G loss: 1.636224]\n",
      "[D loss: 0.929317] [G loss: 1.454958]\n",
      "[D loss: 0.794248] [G loss: 1.377335]\n",
      "[D loss: 1.063058] [G loss: 1.287220]\n",
      "[D loss: 0.891634] [G loss: 1.382975]\n",
      "[D loss: 0.899518] [G loss: 1.542766]\n",
      "[D loss: 0.799096] [G loss: 1.499137]\n",
      "[D loss: 0.647252] [G loss: 1.513072]\n",
      "[D loss: 0.692726] [G loss: 1.460618]\n",
      "[D loss: 0.764583] [G loss: 1.638108]\n",
      "[D loss: 1.018650] [G loss: 1.360355]\n",
      "[D loss: 0.966560] [G loss: 1.313656]\n",
      "[D loss: 0.954206] [G loss: 1.316506]\n",
      "[D loss: 0.854646] [G loss: 1.426694]\n",
      "[D loss: 0.853117] [G loss: 1.450726]\n",
      "[D loss: 0.976650] [G loss: 1.185610]\n",
      "[D loss: 0.707181] [G loss: 1.404629]\n",
      "[D loss: 0.799740] [G loss: 1.434103]\n",
      "[D loss: 0.901122] [G loss: 1.349234]\n",
      "[D loss: 0.826953] [G loss: 1.348927]\n",
      "[D loss: 0.620299] [G loss: 1.370317]\n",
      "[D loss: 0.935277] [G loss: 1.334063]\n",
      "[D loss: 0.722933] [G loss: 1.430071]\n",
      "[D loss: 0.805985] [G loss: 1.589858]\n",
      "[D loss: 0.816511] [G loss: 1.599428]\n",
      "[D loss: 0.947711] [G loss: 1.484975]\n",
      "[D loss: 0.844129] [G loss: 1.462203]\n",
      "[D loss: 0.866397] [G loss: 1.667380]\n",
      "[D loss: 0.761580] [G loss: 1.253149]\n",
      "[D loss: 0.795654] [G loss: 1.422655]\n",
      "[D loss: 1.123952] [G loss: 1.293885]\n",
      "[D loss: 0.786097] [G loss: 1.403312]\n",
      "[D loss: 0.867012] [G loss: 1.469166]\n",
      "[D loss: 0.703354] [G loss: 1.549254]\n",
      "[D loss: 0.648357] [G loss: 1.703259]\n",
      "[D loss: 0.696936] [G loss: 1.361122]\n",
      "[D loss: 0.899034] [G loss: 1.436781]\n",
      "[D loss: 1.045334] [G loss: 1.147449]\n",
      "[D loss: 0.694823] [G loss: 1.606459]\n",
      "[D loss: 0.933648] [G loss: 1.341617]\n",
      "[D loss: 0.770123] [G loss: 1.444689]\n",
      "[D loss: 0.785995] [G loss: 1.219816]\n",
      "[D loss: 0.772955] [G loss: 1.398720]\n",
      "[D loss: 0.790490] [G loss: 1.622179]\n",
      "[D loss: 0.795805] [G loss: 1.458722]\n",
      "[D loss: 0.644154] [G loss: 1.503155]\n",
      "[D loss: 0.990060] [G loss: 1.418597]\n",
      "[D loss: 0.878521] [G loss: 1.536091]\n",
      "[D loss: 0.895441] [G loss: 1.376622]\n",
      "[D loss: 0.655112] [G loss: 1.350632]\n",
      "[D loss: 0.714882] [G loss: 1.509617]\n",
      "[D loss: 0.752558] [G loss: 1.581502]\n",
      "[D loss: 0.746263] [G loss: 1.521042]\n",
      "[D loss: 0.976525] [G loss: 1.582242]\n",
      "[D loss: 0.769147] [G loss: 1.620811]\n",
      "[D loss: 1.013716] [G loss: 1.277655]\n",
      "[D loss: 1.038977] [G loss: 1.361753]\n",
      "[D loss: 0.711642] [G loss: 1.438355]\n",
      "[D loss: 0.938579] [G loss: 1.385101]\n",
      "[D loss: 0.690851] [G loss: 1.563298]\n",
      "[D loss: 0.899208] [G loss: 1.515838]\n",
      "[D loss: 0.769698] [G loss: 1.702916]\n",
      "[D loss: 0.738377] [G loss: 1.690584]\n",
      "[D loss: 0.768762] [G loss: 1.462301]\n",
      "[D loss: 1.120268] [G loss: 1.552028]\n",
      "[D loss: 0.791113] [G loss: 1.463849]\n",
      "[D loss: 0.784248] [G loss: 1.312534]\n",
      "[D loss: 1.018935] [G loss: 1.141538]\n",
      "[D loss: 0.950060] [G loss: 1.248785]\n",
      "[D loss: 0.885744] [G loss: 1.426613]\n",
      "[D loss: 0.831530] [G loss: 1.550473]\n",
      "[D loss: 0.762325] [G loss: 1.558258]\n",
      "[D loss: 0.965037] [G loss: 1.441537]\n",
      "[D loss: 0.906472] [G loss: 1.138548]\n",
      "[D loss: 0.620461] [G loss: 1.357005]\n",
      "[D loss: 0.757413] [G loss: 1.608428]\n",
      "[D loss: 0.885679] [G loss: 1.763249]\n",
      "[D loss: 0.866521] [G loss: 1.398112]\n",
      "[D loss: 0.945989] [G loss: 1.445321]\n",
      "[D loss: 0.839746] [G loss: 1.509881]\n",
      "[D loss: 0.651823] [G loss: 1.714383]\n",
      "[D loss: 0.680413] [G loss: 1.578276]\n",
      "[D loss: 1.010759] [G loss: 1.409851]\n",
      "[D loss: 0.798241] [G loss: 1.443618]\n",
      "[D loss: 1.001395] [G loss: 1.567967]\n",
      "[D loss: 0.596029] [G loss: 1.598368]\n",
      "[D loss: 0.799950] [G loss: 1.397400]\n",
      "[D loss: 0.581791] [G loss: 1.570414]\n",
      "[D loss: 0.842249] [G loss: 1.783005]\n",
      "[D loss: 0.823504] [G loss: 1.530696]\n",
      "[D loss: 0.716344] [G loss: 1.689438]\n",
      "[D loss: 0.698642] [G loss: 1.509157]\n",
      "[D loss: 0.660026] [G loss: 1.540635]\n",
      "[D loss: 1.046558] [G loss: 1.468824]\n",
      "[D loss: 0.924612] [G loss: 1.609298]\n",
      "[D loss: 0.745430] [G loss: 1.603708]\n",
      "[D loss: 0.768365] [G loss: 1.568793]\n",
      "[D loss: 0.983846] [G loss: 1.470805]\n",
      "[D loss: 0.879810] [G loss: 1.391407]\n",
      "[D loss: 0.829180] [G loss: 1.713004]\n",
      "[D loss: 0.802824] [G loss: 1.287143]\n",
      "[D loss: 0.844549] [G loss: 1.270055]\n",
      "[D loss: 0.712246] [G loss: 1.371227]\n",
      "[D loss: 0.697822] [G loss: 1.376325]\n",
      "[D loss: 0.843237] [G loss: 1.638950]\n",
      "[D loss: 0.747128] [G loss: 1.614396]\n",
      "[D loss: 0.852880] [G loss: 1.342243]\n",
      "[D loss: 0.863480] [G loss: 1.718089]\n",
      "[D loss: 0.759087] [G loss: 1.561260]\n",
      "[D loss: 0.965908] [G loss: 1.442510]\n",
      "[D loss: 0.980057] [G loss: 1.510433]\n",
      "[D loss: 0.812477] [G loss: 1.429249]\n",
      "[D loss: 0.736696] [G loss: 1.460548]\n",
      "[D loss: 1.001420] [G loss: 1.652656]\n",
      "[D loss: 1.050274] [G loss: 1.342670]\n",
      "[D loss: 0.907170] [G loss: 1.556431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.716378] [G loss: 1.313275]\n",
      "[D loss: 0.911805] [G loss: 1.260203]\n",
      "[D loss: 0.910034] [G loss: 1.440701]\n",
      "[D loss: 0.901868] [G loss: 1.301147]\n",
      "[D loss: 0.836086] [G loss: 1.334892]\n",
      "[D loss: 0.727501] [G loss: 1.326316]\n",
      "[D loss: 0.696316] [G loss: 1.343923]\n",
      "[D loss: 0.912151] [G loss: 1.484031]\n",
      "[D loss: 0.824774] [G loss: 1.573715]\n",
      "[D loss: 0.812719] [G loss: 1.521107]\n",
      "[D loss: 0.561241] [G loss: 1.807598]\n",
      "[D loss: 1.058881] [G loss: 1.301743]\n",
      "[D loss: 0.691624] [G loss: 1.474578]\n",
      "[D loss: 0.570318] [G loss: 1.629294]\n",
      "[D loss: 0.792721] [G loss: 1.296954]\n",
      "[D loss: 0.801458] [G loss: 1.525684]\n",
      "[D loss: 1.013527] [G loss: 2.027834]\n",
      "[D loss: 0.624471] [G loss: 1.753953]\n",
      "[D loss: 0.939883] [G loss: 1.544092]\n",
      "[D loss: 0.975803] [G loss: 1.399173]\n",
      "[D loss: 0.788701] [G loss: 1.519255]\n",
      "[D loss: 0.749919] [G loss: 1.642383]\n",
      "[D loss: 0.762371] [G loss: 1.646741]\n",
      "[D loss: 0.771407] [G loss: 1.397903]\n",
      "[D loss: 0.881719] [G loss: 1.418655]\n",
      "[D loss: 0.818520] [G loss: 1.561514]\n",
      "[D loss: 0.845255] [G loss: 1.566881]\n",
      "[D loss: 0.667713] [G loss: 1.480121]\n",
      "[D loss: 0.748348] [G loss: 1.544769]\n",
      "[D loss: 0.790427] [G loss: 1.584993]\n",
      "[D loss: 0.833760] [G loss: 1.523263]\n",
      "[D loss: 0.830957] [G loss: 1.518986]\n",
      "[D loss: 0.907531] [G loss: 1.500507]\n",
      "[D loss: 0.851966] [G loss: 1.548197]\n",
      "[D loss: 0.950108] [G loss: 1.541829]\n",
      "[D loss: 0.949041] [G loss: 1.681025]\n",
      "[D loss: 0.733464] [G loss: 1.729856]\n",
      "[D loss: 0.648217] [G loss: 1.714687]\n",
      "[D loss: 0.703313] [G loss: 1.785413]\n",
      "[D loss: 1.023846] [G loss: 1.363643]\n",
      "[D loss: 0.968070] [G loss: 1.277858]\n",
      "[D loss: 1.020596] [G loss: 1.252039]\n",
      "[D loss: 0.831591] [G loss: 1.251986]\n",
      "[D loss: 0.848603] [G loss: 1.428490]\n",
      "[D loss: 0.687500] [G loss: 1.487811]\n",
      "[D loss: 0.752994] [G loss: 1.731508]\n",
      "[D loss: 0.752484] [G loss: 1.520295]\n",
      "[D loss: 0.850530] [G loss: 1.494358]\n",
      "[D loss: 0.970901] [G loss: 1.631641]\n",
      "[D loss: 0.715351] [G loss: 1.689292]\n",
      "[D loss: 0.920012] [G loss: 1.484909]\n",
      "[D loss: 0.644006] [G loss: 1.709002]\n",
      "[D loss: 0.747005] [G loss: 1.463798]\n",
      "[D loss: 0.856427] [G loss: 1.573456]\n",
      "[D loss: 0.627883] [G loss: 1.662846]\n",
      "[D loss: 1.017946] [G loss: 1.482861]\n",
      "[D loss: 0.814458] [G loss: 1.506504]\n",
      "[D loss: 0.727138] [G loss: 1.645949]\n",
      "[D loss: 1.122169] [G loss: 1.496787]\n",
      "[D loss: 0.915730] [G loss: 1.480265]\n",
      "[D loss: 0.762692] [G loss: 1.301218]\n",
      "[D loss: 0.934969] [G loss: 1.603934]\n",
      "[D loss: 0.816055] [G loss: 1.482150]\n",
      "[D loss: 0.781102] [G loss: 1.538988]\n",
      "[D loss: 0.688377] [G loss: 1.531564]\n",
      "[D loss: 0.855827] [G loss: 1.423783]\n",
      "[D loss: 0.868859] [G loss: 1.362725]\n",
      "[D loss: 0.616286] [G loss: 1.804510]\n",
      "[D loss: 0.855383] [G loss: 1.364652]\n",
      "[D loss: 0.804272] [G loss: 1.538321]\n",
      "[D loss: 0.975358] [G loss: 1.613421]\n",
      "[D loss: 0.813893] [G loss: 1.593180]\n",
      "[D loss: 0.820656] [G loss: 1.471172]\n",
      "[D loss: 0.837065] [G loss: 1.364264]\n",
      "[D loss: 0.913852] [G loss: 1.479870]\n",
      "[D loss: 1.016877] [G loss: 1.583881]\n",
      "[D loss: 0.870111] [G loss: 1.428928]\n",
      "[D loss: 0.700434] [G loss: 1.559374]\n",
      "[D loss: 0.769242] [G loss: 1.386759]\n",
      "[D loss: 0.775879] [G loss: 1.535996]\n",
      "[D loss: 0.835624] [G loss: 1.345802]\n",
      "[D loss: 1.061871] [G loss: 1.408062]\n",
      "[D loss: 0.604647] [G loss: 1.525599]\n",
      "[D loss: 0.984903] [G loss: 1.363542]\n",
      "[D loss: 0.748045] [G loss: 1.483681]\n",
      "[D loss: 0.793127] [G loss: 1.370506]\n",
      "[D loss: 0.716135] [G loss: 1.527531]\n",
      "[D loss: 1.063881] [G loss: 1.384740]\n",
      "[D loss: 0.834335] [G loss: 1.402665]\n",
      "[D loss: 0.794597] [G loss: 1.655518]\n",
      "[D loss: 0.881712] [G loss: 1.677492]\n",
      "[D loss: 0.908325] [G loss: 1.454429]\n",
      "[D loss: 0.802119] [G loss: 1.485168]\n",
      "[D loss: 0.709227] [G loss: 1.352702]\n",
      "[D loss: 0.798402] [G loss: 1.343952]\n",
      "[D loss: 0.791169] [G loss: 1.312234]\n",
      "[D loss: 0.961220] [G loss: 1.417777]\n",
      "[D loss: 0.778278] [G loss: 1.427276]\n",
      "[D loss: 0.648953] [G loss: 1.413853]\n",
      "[D loss: 0.920298] [G loss: 1.521899]\n",
      "[D loss: 0.834978] [G loss: 1.392211]\n",
      "[D loss: 0.850299] [G loss: 1.390944]\n",
      "[D loss: 0.703426] [G loss: 1.392794]\n",
      "[D loss: 0.659177] [G loss: 1.570410]\n",
      "[D loss: 0.825340] [G loss: 1.649793]\n",
      "[D loss: 1.071853] [G loss: 1.581369]\n",
      "[D loss: 0.837327] [G loss: 1.181842]\n",
      "[D loss: 0.997167] [G loss: 1.198637]\n",
      "[D loss: 0.522274] [G loss: 1.505663]\n",
      "[D loss: 0.920033] [G loss: 1.425855]\n",
      "[D loss: 0.883849] [G loss: 1.375795]\n",
      "[D loss: 0.881409] [G loss: 1.562410]\n",
      "[D loss: 1.004566] [G loss: 1.453919]\n",
      "[D loss: 0.743272] [G loss: 1.518478]\n",
      "[D loss: 0.682692] [G loss: 1.524461]\n",
      "[D loss: 1.099851] [G loss: 1.402595]\n",
      "[D loss: 1.040191] [G loss: 1.327650]\n",
      "[D loss: 0.850565] [G loss: 1.359180]\n",
      "[D loss: 0.983603] [G loss: 1.324015]\n",
      "[D loss: 0.778403] [G loss: 1.401064]\n",
      "[D loss: 0.816769] [G loss: 1.261184]\n",
      "[D loss: 0.743520] [G loss: 1.282942]\n",
      "[D loss: 0.826231] [G loss: 1.181676]\n",
      "[D loss: 0.825925] [G loss: 1.142153]\n",
      "[D loss: 0.856031] [G loss: 1.377812]\n",
      "[D loss: 0.999726] [G loss: 1.525354]\n",
      "[D loss: 0.808929] [G loss: 1.500816]\n",
      "[D loss: 0.843612] [G loss: 1.705673]\n",
      "[D loss: 0.786833] [G loss: 1.695064]\n",
      "[D loss: 0.826357] [G loss: 1.627048]\n",
      "[D loss: 0.734750] [G loss: 1.545551]\n",
      "[D loss: 0.686459] [G loss: 1.568306]\n",
      "[D loss: 0.712667] [G loss: 1.466474]\n",
      "[D loss: 0.859691] [G loss: 1.552338]\n",
      "[D loss: 0.822817] [G loss: 1.282002]\n",
      "[D loss: 0.753943] [G loss: 1.523717]\n",
      "[D loss: 0.936480] [G loss: 1.732832]\n",
      "[D loss: 1.084542] [G loss: 1.487209]\n",
      "[D loss: 0.541146] [G loss: 1.493506]\n",
      "[D loss: 0.835197] [G loss: 1.423538]\n",
      "[D loss: 1.012383] [G loss: 1.173196]\n",
      "[D loss: 0.907434] [G loss: 1.490736]\n",
      "[D loss: 0.945807] [G loss: 1.412127]\n",
      "[D loss: 0.684692] [G loss: 1.658976]\n",
      "[D loss: 1.320728] [G loss: 1.164575]\n",
      "[D loss: 1.001433] [G loss: 1.141043]\n",
      "[D loss: 0.809305] [G loss: 1.369570]\n",
      "[D loss: 0.685269] [G loss: 1.611658]\n",
      "[D loss: 0.661137] [G loss: 1.439166]\n",
      "[D loss: 0.646186] [G loss: 1.787272]\n",
      "[D loss: 0.627543] [G loss: 1.527589]\n",
      "[D loss: 0.819092] [G loss: 1.450380]\n",
      "[D loss: 1.082239] [G loss: 1.253275]\n",
      "[D loss: 0.724478] [G loss: 1.623373]\n",
      "[D loss: 0.819183] [G loss: 1.391820]\n",
      "[D loss: 0.738450] [G loss: 1.687990]\n",
      "[D loss: 0.674319] [G loss: 1.829982]\n",
      "[D loss: 0.808000] [G loss: 1.333774]\n",
      "[D loss: 0.993386] [G loss: 1.801483]\n",
      "[D loss: 0.693679] [G loss: 1.534779]\n",
      "[D loss: 0.781495] [G loss: 1.383823]\n",
      "[D loss: 0.996697] [G loss: 1.331788]\n",
      "[D loss: 0.803724] [G loss: 1.306630]\n",
      "[D loss: 1.007793] [G loss: 1.361143]\n",
      "[D loss: 1.097451] [G loss: 1.400057]\n",
      "[D loss: 0.749018] [G loss: 1.578199]\n",
      "[D loss: 0.759737] [G loss: 1.706092]\n",
      "[D loss: 0.745659] [G loss: 1.522756]\n",
      "[D loss: 0.837606] [G loss: 1.575086]\n",
      "[D loss: 0.886646] [G loss: 1.372407]\n",
      "[D loss: 0.686092] [G loss: 1.571295]\n",
      "[D loss: 0.703272] [G loss: 1.543506]\n",
      "[D loss: 0.566182] [G loss: 1.709822]\n",
      "[D loss: 0.713710] [G loss: 1.493763]\n",
      "[D loss: 0.947495] [G loss: 1.346773]\n",
      "[D loss: 0.895819] [G loss: 1.427001]\n",
      "[D loss: 0.658722] [G loss: 1.542944]\n",
      "[D loss: 0.868063] [G loss: 1.497620]\n",
      "[D loss: 0.858542] [G loss: 1.478376]\n",
      "[D loss: 0.779813] [G loss: 1.571823]\n",
      "[D loss: 0.896911] [G loss: 1.434969]\n",
      "[D loss: 1.098675] [G loss: 1.372785]\n",
      "[D loss: 1.038760] [G loss: 1.440472]\n",
      "[D loss: 0.813103] [G loss: 1.501064]\n",
      "[D loss: 0.880755] [G loss: 1.483362]\n",
      "[D loss: 0.638951] [G loss: 1.635135]\n",
      "[D loss: 0.868666] [G loss: 1.437954]\n",
      "[D loss: 0.896209] [G loss: 1.218208]\n",
      "[D loss: 0.866963] [G loss: 1.512815]\n",
      "[D loss: 0.979419] [G loss: 1.509310]\n",
      "[D loss: 0.696288] [G loss: 1.688160]\n",
      "[D loss: 0.797052] [G loss: 1.494835]\n",
      "[D loss: 0.876698] [G loss: 1.478518]\n",
      "[D loss: 0.858838] [G loss: 1.296222]\n",
      "[D loss: 0.777522] [G loss: 1.386525]\n",
      "[D loss: 0.775125] [G loss: 1.416996]\n",
      "[D loss: 0.695132] [G loss: 1.440013]\n",
      "[D loss: 0.763486] [G loss: 1.465522]\n",
      "[D loss: 0.867969] [G loss: 1.501836]\n",
      "[D loss: 0.689706] [G loss: 1.305923]\n",
      "[D loss: 1.002195] [G loss: 1.534414]\n",
      "[D loss: 0.874055] [G loss: 1.232940]\n",
      "[D loss: 0.801546] [G loss: 1.566307]\n",
      "[D loss: 0.729997] [G loss: 1.665877]\n",
      "[D loss: 0.800602] [G loss: 1.447004]\n",
      "[D loss: 0.614503] [G loss: 1.691350]\n",
      "[D loss: 0.688547] [G loss: 1.602982]\n",
      "[D loss: 0.728070] [G loss: 1.383607]\n",
      "[D loss: 0.864505] [G loss: 1.435498]\n",
      "[D loss: 0.831565] [G loss: 1.468959]\n",
      "[D loss: 0.770072] [G loss: 1.420515]\n",
      "[D loss: 0.846449] [G loss: 1.689863]\n",
      "[D loss: 1.004588] [G loss: 1.495493]\n",
      "[D loss: 0.825611] [G loss: 1.385684]\n",
      "[D loss: 0.863597] [G loss: 1.591046]\n",
      "[D loss: 0.732681] [G loss: 1.466138]\n",
      "[D loss: 0.880130] [G loss: 1.473996]\n",
      "[D loss: 0.821146] [G loss: 1.306225]\n",
      "[D loss: 0.747215] [G loss: 1.322465]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.852930] [G loss: 1.290006]\n",
      "[D loss: 0.866269] [G loss: 1.378639]\n",
      "[D loss: 0.607197] [G loss: 1.743454]\n",
      "[D loss: 0.826785] [G loss: 1.511542]\n",
      "[D loss: 0.756693] [G loss: 1.672176]\n",
      "[D loss: 0.892878] [G loss: 1.304523]\n",
      "[D loss: 0.668023] [G loss: 1.541848]\n",
      "[D loss: 0.763463] [G loss: 1.650382]\n",
      "[D loss: 0.689838] [G loss: 1.659684]\n",
      "[D loss: 0.780869] [G loss: 1.613683]\n",
      "[D loss: 0.873290] [G loss: 1.841392]\n",
      "[D loss: 1.000509] [G loss: 1.541607]\n",
      "[D loss: 0.930377] [G loss: 1.322268]\n",
      "[D loss: 0.795131] [G loss: 1.454520]\n",
      "[D loss: 0.775105] [G loss: 1.316126]\n",
      "[D loss: 0.853481] [G loss: 1.645438]\n",
      "[D loss: 0.866987] [G loss: 1.468170]\n",
      "[D loss: 0.707326] [G loss: 1.508513]\n",
      "[D loss: 0.970474] [G loss: 1.604425]\n",
      "[D loss: 0.853613] [G loss: 1.474576]\n",
      "[D loss: 0.827547] [G loss: 1.230946]\n",
      "[D loss: 1.056131] [G loss: 1.307832]\n",
      "[D loss: 0.564433] [G loss: 1.576940]\n",
      "[D loss: 0.896823] [G loss: 1.638143]\n",
      "[D loss: 0.926508] [G loss: 1.776158]\n",
      "[D loss: 0.997508] [G loss: 1.692247]\n",
      "[D loss: 0.894559] [G loss: 1.301461]\n",
      "[D loss: 0.990752] [G loss: 1.482360]\n",
      "[D loss: 0.821990] [G loss: 1.117098]\n",
      "[D loss: 0.892378] [G loss: 1.388987]\n",
      "[D loss: 1.076088] [G loss: 1.425899]\n",
      "[D loss: 0.936514] [G loss: 1.423187]\n",
      "[D loss: 1.011940] [G loss: 1.471288]\n",
      "[D loss: 0.652185] [G loss: 1.358889]\n",
      "[D loss: 1.018363] [G loss: 1.396807]\n",
      "[D loss: 1.042116] [G loss: 1.440368]\n",
      "[D loss: 0.792304] [G loss: 1.489010]\n",
      "[D loss: 1.002184] [G loss: 1.344649]\n",
      "[D loss: 0.833152] [G loss: 1.340489]\n",
      "[D loss: 0.940140] [G loss: 1.489314]\n",
      "[D loss: 0.862761] [G loss: 1.289016]\n",
      "[D loss: 0.876798] [G loss: 1.480220]\n",
      "[D loss: 1.001733] [G loss: 1.452643]\n",
      "[D loss: 1.025918] [G loss: 1.346981]\n",
      "[D loss: 0.724568] [G loss: 1.406632]\n",
      "[D loss: 0.817984] [G loss: 1.473670]\n",
      "[D loss: 0.601464] [G loss: 1.595027]\n",
      "[D loss: 0.859171] [G loss: 1.516173]\n",
      "[D loss: 0.675359] [G loss: 1.303242]\n",
      "[D loss: 0.662459] [G loss: 1.330722]\n",
      "[D loss: 0.924769] [G loss: 1.316917]\n",
      "[D loss: 0.741951] [G loss: 1.475871]\n",
      "[D loss: 0.788207] [G loss: 1.647169]\n",
      "[D loss: 0.805308] [G loss: 1.385308]\n",
      "[D loss: 0.794952] [G loss: 1.740198]\n",
      "[D loss: 0.754013] [G loss: 1.486894]\n",
      "[D loss: 1.010309] [G loss: 1.386124]\n",
      "[D loss: 0.617494] [G loss: 1.584785]\n",
      "[D loss: 0.990762] [G loss: 1.267755]\n",
      "[D loss: 0.931588] [G loss: 1.567618]\n",
      "[D loss: 0.666121] [G loss: 1.522032]\n",
      "[D loss: 0.846437] [G loss: 1.494249]\n",
      "[D loss: 0.979982] [G loss: 1.311514]\n",
      "[D loss: 0.965950] [G loss: 1.434828]\n",
      "[D loss: 0.596900] [G loss: 1.486000]\n",
      "[D loss: 0.933222] [G loss: 1.214463]\n",
      "[D loss: 0.771522] [G loss: 1.498643]\n",
      "[D loss: 0.878122] [G loss: 1.547734]\n",
      "[D loss: 0.945168] [G loss: 1.515807]\n",
      "[D loss: 0.751495] [G loss: 1.404282]\n",
      "[D loss: 0.618020] [G loss: 1.627234]\n",
      "[D loss: 0.893336] [G loss: 1.586274]\n",
      "[D loss: 0.660836] [G loss: 1.405761]\n",
      "[D loss: 0.852180] [G loss: 1.558282]\n",
      "[D loss: 0.579364] [G loss: 1.665521]\n",
      "[D loss: 0.822785] [G loss: 1.540986]\n",
      "[D loss: 0.798206] [G loss: 1.403776]\n",
      "[D loss: 0.976598] [G loss: 1.326827]\n",
      "[D loss: 0.947045] [G loss: 1.369826]\n",
      "[D loss: 0.964844] [G loss: 1.334690]\n",
      "[D loss: 0.630759] [G loss: 1.443687]\n",
      "[D loss: 0.646682] [G loss: 1.429312]\n",
      "[D loss: 1.086477] [G loss: 1.447541]\n",
      "[D loss: 1.112915] [G loss: 1.259009]\n",
      "[D loss: 0.925656] [G loss: 1.246894]\n",
      "[D loss: 0.727329] [G loss: 1.372867]\n",
      "[D loss: 0.764871] [G loss: 1.484904]\n",
      "[D loss: 0.821095] [G loss: 1.389219]\n",
      "[D loss: 0.982810] [G loss: 1.303078]\n",
      "[D loss: 0.576096] [G loss: 1.636487]\n",
      "[D loss: 0.867275] [G loss: 1.538101]\n",
      "[D loss: 0.746471] [G loss: 1.373709]\n",
      "[D loss: 0.687817] [G loss: 1.430551]\n",
      "[D loss: 0.699897] [G loss: 1.507060]\n",
      "[D loss: 0.875741] [G loss: 1.487447]\n",
      "[D loss: 0.870026] [G loss: 1.604926]\n",
      "[D loss: 0.872798] [G loss: 1.523257]\n",
      "[D loss: 0.849108] [G loss: 1.640865]\n",
      "[D loss: 0.750984] [G loss: 1.496349]\n",
      "[D loss: 0.648365] [G loss: 1.735240]\n",
      "[D loss: 0.777389] [G loss: 1.457506]\n",
      "[D loss: 0.670206] [G loss: 1.413162]\n",
      "[D loss: 0.709738] [G loss: 1.424060]\n",
      "[D loss: 0.774295] [G loss: 1.635045]\n",
      "[D loss: 0.991436] [G loss: 1.384943]\n",
      "[D loss: 0.598694] [G loss: 1.468884]\n",
      "[D loss: 0.798823] [G loss: 1.490829]\n",
      "[D loss: 0.681980] [G loss: 1.586065]\n",
      "[D loss: 0.820130] [G loss: 1.386349]\n",
      "[D loss: 0.793327] [G loss: 1.532958]\n",
      "[D loss: 0.798586] [G loss: 1.630804]\n",
      "[D loss: 0.671768] [G loss: 1.658522]\n",
      "[D loss: 1.006185] [G loss: 1.472924]\n",
      "[D loss: 0.902002] [G loss: 1.519173]\n",
      "[D loss: 0.881957] [G loss: 1.389508]\n",
      "[D loss: 0.745749] [G loss: 1.534103]\n",
      "[D loss: 0.734353] [G loss: 1.606992]\n",
      "[D loss: 1.146165] [G loss: 1.473527]\n",
      "[D loss: 1.010052] [G loss: 1.405734]\n",
      "[D loss: 0.641196] [G loss: 1.437687]\n",
      "[D loss: 0.804025] [G loss: 1.411313]\n",
      "[D loss: 0.836808] [G loss: 1.250822]\n",
      "[D loss: 0.863133] [G loss: 1.458231]\n",
      "[D loss: 0.879654] [G loss: 1.565622]\n",
      "[D loss: 1.036061] [G loss: 1.533452]\n",
      "[D loss: 0.956420] [G loss: 1.370057]\n",
      "[D loss: 0.924210] [G loss: 1.434018]\n",
      "[D loss: 0.695254] [G loss: 1.321770]\n",
      "[D loss: 0.751970] [G loss: 1.460406]\n",
      "[D loss: 0.659802] [G loss: 1.404927]\n",
      "[D loss: 0.949192] [G loss: 1.391572]\n",
      "[D loss: 0.992512] [G loss: 1.500030]\n",
      "[D loss: 0.553820] [G loss: 1.601814]\n",
      "[D loss: 0.749843] [G loss: 1.392959]\n",
      "[D loss: 0.758523] [G loss: 1.366988]\n",
      "[D loss: 0.837299] [G loss: 1.435313]\n",
      "[D loss: 0.851225] [G loss: 1.599042]\n",
      "[D loss: 0.765382] [G loss: 1.489257]\n",
      "[D loss: 0.873232] [G loss: 1.690081]\n",
      "[D loss: 0.736581] [G loss: 1.681193]\n",
      "[D loss: 0.829678] [G loss: 1.594319]\n",
      "[D loss: 0.630847] [G loss: 1.550729]\n",
      "[D loss: 0.775020] [G loss: 1.633386]\n",
      "[D loss: 0.779522] [G loss: 1.392516]\n",
      "[D loss: 0.837799] [G loss: 1.509624]\n",
      "[D loss: 0.826179] [G loss: 1.393820]\n",
      "[D loss: 0.954535] [G loss: 1.274973]\n",
      "[D loss: 0.798271] [G loss: 1.539737]\n",
      "[D loss: 0.923749] [G loss: 1.367783]\n",
      "[D loss: 0.940976] [G loss: 1.381359]\n",
      "[D loss: 1.087313] [G loss: 1.506404]\n",
      "[D loss: 0.717408] [G loss: 1.727161]\n",
      "[D loss: 0.718857] [G loss: 1.490799]\n",
      "[D loss: 0.777572] [G loss: 1.669823]\n",
      "[D loss: 1.138066] [G loss: 1.654000]\n",
      "[D loss: 0.858599] [G loss: 1.519804]\n",
      "[D loss: 0.830279] [G loss: 1.315381]\n",
      "[D loss: 0.822823] [G loss: 1.486509]\n",
      "[D loss: 0.838682] [G loss: 1.321399]\n",
      "[D loss: 0.731022] [G loss: 1.324814]\n",
      "[D loss: 0.892758] [G loss: 1.454619]\n",
      "[D loss: 0.491192] [G loss: 1.588193]\n",
      "[D loss: 1.050330] [G loss: 1.355448]\n",
      "[D loss: 0.872826] [G loss: 1.427312]\n",
      "[D loss: 0.579935] [G loss: 1.520219]\n",
      "[D loss: 0.959592] [G loss: 1.632547]\n",
      "[D loss: 0.781642] [G loss: 1.613075]\n",
      "[D loss: 0.873985] [G loss: 1.582904]\n",
      "[D loss: 1.012253] [G loss: 1.611312]\n",
      "[D loss: 0.847641] [G loss: 1.530808]\n",
      "[D loss: 0.784683] [G loss: 1.298063]\n",
      "[D loss: 1.007952] [G loss: 1.329467]\n",
      "[D loss: 0.932740] [G loss: 1.228344]\n",
      "[D loss: 0.771847] [G loss: 1.478448]\n",
      "[D loss: 0.824915] [G loss: 1.493696]\n",
      "[D loss: 0.720640] [G loss: 1.654547]\n",
      "[D loss: 0.954685] [G loss: 1.498263]\n",
      "[D loss: 0.921249] [G loss: 1.187067]\n",
      "[D loss: 0.938614] [G loss: 1.341238]\n",
      "[D loss: 0.669127] [G loss: 1.430663]\n",
      "[D loss: 0.894865] [G loss: 1.492706]\n",
      "[D loss: 0.823917] [G loss: 1.393534]\n",
      "[D loss: 0.877707] [G loss: 1.472759]\n",
      "[D loss: 0.880379] [G loss: 1.384703]\n",
      "[D loss: 0.555884] [G loss: 1.632954]\n",
      "[D loss: 1.036722] [G loss: 1.432469]\n",
      "[D loss: 0.728767] [G loss: 1.483938]\n",
      "[D loss: 1.107095] [G loss: 1.257654]\n",
      "[D loss: 0.893773] [G loss: 1.471298]\n",
      "[D loss: 0.894628] [G loss: 1.552049]\n",
      "[D loss: 1.033635] [G loss: 1.599944]\n",
      "[D loss: 0.854173] [G loss: 1.706280]\n",
      "[D loss: 0.707611] [G loss: 1.391026]\n",
      "[D loss: 1.105434] [G loss: 1.314989]\n",
      "[D loss: 0.690957] [G loss: 1.593817]\n",
      "[D loss: 1.052376] [G loss: 1.307064]\n",
      "[D loss: 0.793761] [G loss: 1.594087]\n",
      "[D loss: 0.806431] [G loss: 1.468488]\n",
      "[D loss: 0.780708] [G loss: 1.267569]\n",
      "[D loss: 0.983465] [G loss: 1.509948]\n",
      "[D loss: 0.699413] [G loss: 1.497580]\n",
      "[D loss: 0.708118] [G loss: 1.671463]\n",
      "[D loss: 0.938350] [G loss: 1.410336]\n",
      "[D loss: 0.821670] [G loss: 1.370451]\n",
      "[D loss: 0.775998] [G loss: 1.616876]\n",
      "[D loss: 0.764968] [G loss: 1.365256]\n",
      "[D loss: 0.794789] [G loss: 1.430708]\n",
      "[D loss: 0.690660] [G loss: 1.332589]\n",
      "[D loss: 0.774271] [G loss: 1.378817]\n",
      "[D loss: 0.939584] [G loss: 1.342187]\n",
      "[D loss: 1.003650] [G loss: 1.271882]\n",
      "[D loss: 0.776098] [G loss: 1.548946]\n",
      "[D loss: 0.679719] [G loss: 1.520904]\n",
      "[D loss: 0.756275] [G loss: 1.532109]\n",
      "[D loss: 1.034919] [G loss: 1.484186]\n",
      "[D loss: 0.790212] [G loss: 1.451743]\n",
      "[D loss: 0.864916] [G loss: 1.547057]\n",
      "[D loss: 0.640595] [G loss: 1.561412]\n",
      "[D loss: 0.886293] [G loss: 1.549545]\n",
      "[D loss: 0.838385] [G loss: 1.566233]\n",
      "[D loss: 0.884925] [G loss: 1.650854]\n",
      "[D loss: 0.858879] [G loss: 1.547932]\n",
      "[D loss: 0.977209] [G loss: 1.261649]\n",
      "[D loss: 0.661641] [G loss: 1.470292]\n",
      "[D loss: 0.823581] [G loss: 1.592514]\n",
      "[D loss: 0.937746] [G loss: 1.479457]\n",
      "[D loss: 0.641409] [G loss: 1.503605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.771294] [G loss: 1.647125]\n",
      "[D loss: 0.783724] [G loss: 1.521568]\n",
      "[D loss: 0.810786] [G loss: 1.323254]\n",
      "[D loss: 0.797512] [G loss: 1.399617]\n",
      "[D loss: 0.735859] [G loss: 1.542472]\n",
      "[D loss: 0.764216] [G loss: 1.373118]\n",
      "[D loss: 1.172919] [G loss: 1.550780]\n",
      "[D loss: 0.783751] [G loss: 1.485601]\n",
      "[D loss: 0.950980] [G loss: 1.499852]\n",
      "[D loss: 0.926973] [G loss: 1.590118]\n",
      "[D loss: 0.886389] [G loss: 1.280005]\n",
      "[D loss: 0.666189] [G loss: 1.455599]\n",
      "[D loss: 0.850145] [G loss: 1.519776]\n",
      "[D loss: 0.935095] [G loss: 1.335933]\n",
      "[D loss: 0.722114] [G loss: 1.349005]\n",
      "[D loss: 0.902184] [G loss: 1.412183]\n",
      "[D loss: 0.949916] [G loss: 1.681218]\n",
      "[D loss: 1.104136] [G loss: 1.602150]\n",
      "[D loss: 0.835112] [G loss: 1.308683]\n",
      "[D loss: 0.901510] [G loss: 1.554691]\n",
      "[D loss: 0.991723] [G loss: 1.275141]\n",
      "[D loss: 0.943884] [G loss: 1.121173]\n",
      "[D loss: 0.882040] [G loss: 1.273033]\n",
      "[D loss: 0.834005] [G loss: 1.549387]\n",
      "[D loss: 0.810317] [G loss: 1.981249]\n",
      "[D loss: 0.684938] [G loss: 1.530817]\n",
      "[D loss: 0.759181] [G loss: 1.433243]\n",
      "[D loss: 0.788077] [G loss: 1.515222]\n",
      "[D loss: 0.801359] [G loss: 1.346064]\n",
      "[D loss: 0.889048] [G loss: 1.193847]\n",
      "[D loss: 0.929676] [G loss: 1.239069]\n",
      "[D loss: 0.907471] [G loss: 1.476486]\n",
      "[D loss: 1.033472] [G loss: 1.553869]\n",
      "[D loss: 0.829783] [G loss: 1.263438]\n",
      "[D loss: 1.138718] [G loss: 1.429622]\n",
      "[D loss: 0.820562] [G loss: 1.589185]\n",
      "[D loss: 0.660300] [G loss: 1.515026]\n",
      "[D loss: 0.789485] [G loss: 1.509228]\n",
      "[D loss: 0.888772] [G loss: 1.404744]\n",
      "[D loss: 0.635922] [G loss: 1.265691]\n",
      "[D loss: 0.924170] [G loss: 1.250535]\n",
      "[D loss: 0.798355] [G loss: 1.341439]\n",
      "[D loss: 0.948773] [G loss: 1.306170]\n",
      "[D loss: 0.727567] [G loss: 1.324576]\n",
      "[D loss: 0.726165] [G loss: 1.536478]\n",
      "[D loss: 0.761587] [G loss: 1.516016]\n",
      "[D loss: 0.884607] [G loss: 1.652902]\n",
      "[D loss: 0.813727] [G loss: 1.329258]\n",
      "[D loss: 0.860028] [G loss: 1.398193]\n",
      "[D loss: 0.882123] [G loss: 1.586009]\n",
      "[D loss: 0.964035] [G loss: 1.356151]\n",
      "[D loss: 1.154326] [G loss: 1.310925]\n",
      "[D loss: 0.721307] [G loss: 1.377673]\n",
      "[D loss: 0.795682] [G loss: 1.615142]\n",
      "[D loss: 0.699343] [G loss: 1.416609]\n",
      "[D loss: 0.867862] [G loss: 1.304143]\n",
      "[D loss: 0.896261] [G loss: 1.309843]\n",
      "[D loss: 0.661680] [G loss: 1.430500]\n",
      "[D loss: 0.886074] [G loss: 1.511194]\n",
      "[D loss: 0.778079] [G loss: 1.469297]\n",
      "[D loss: 1.008964] [G loss: 1.273108]\n",
      "[D loss: 0.802432] [G loss: 1.575169]\n",
      "[D loss: 1.084727] [G loss: 1.682648]\n",
      "[D loss: 0.779534] [G loss: 1.559479]\n",
      "[D loss: 0.726404] [G loss: 1.589598]\n",
      "[D loss: 0.817640] [G loss: 1.558187]\n",
      "[D loss: 0.932973] [G loss: 1.365310]\n",
      "[D loss: 0.673128] [G loss: 1.501790]\n",
      "[D loss: 0.727258] [G loss: 1.342621]\n",
      "[D loss: 0.769849] [G loss: 1.359787]\n",
      "[D loss: 0.788086] [G loss: 1.550660]\n",
      "[D loss: 0.894459] [G loss: 1.506217]\n",
      "[D loss: 0.786682] [G loss: 1.321616]\n",
      "[D loss: 0.755134] [G loss: 1.675855]\n",
      "[D loss: 0.886806] [G loss: 1.559786]\n",
      "[D loss: 0.737093] [G loss: 1.553581]\n",
      "[D loss: 0.787845] [G loss: 1.501326]\n",
      "[D loss: 0.988908] [G loss: 1.409231]\n",
      "[D loss: 0.855240] [G loss: 1.361162]\n",
      "[D loss: 0.837215] [G loss: 1.525411]\n",
      "[D loss: 0.938427] [G loss: 1.551634]\n",
      "[D loss: 0.795823] [G loss: 1.556075]\n",
      "[D loss: 0.750616] [G loss: 1.451360]\n",
      "[D loss: 0.800715] [G loss: 1.549950]\n",
      "[D loss: 0.968102] [G loss: 1.560418]\n",
      "[D loss: 0.838744] [G loss: 1.191579]\n",
      "[D loss: 1.085719] [G loss: 1.402022]\n",
      "[D loss: 0.807851] [G loss: 1.335450]\n",
      "[D loss: 1.016042] [G loss: 1.477605]\n",
      "[D loss: 0.833325] [G loss: 1.415339]\n",
      "[D loss: 0.906340] [G loss: 1.388840]\n",
      "[D loss: 1.178162] [G loss: 1.338481]\n",
      "[D loss: 0.782225] [G loss: 1.258099]\n",
      "[D loss: 0.806913] [G loss: 1.446162]\n",
      "[D loss: 0.889781] [G loss: 1.300842]\n",
      "[D loss: 0.706736] [G loss: 1.343981]\n",
      "[D loss: 0.605395] [G loss: 1.323978]\n",
      "[D loss: 0.759927] [G loss: 1.274733]\n",
      "[D loss: 0.819752] [G loss: 1.292190]\n",
      "[D loss: 0.473442] [G loss: 1.638324]\n",
      "[D loss: 1.009835] [G loss: 1.620574]\n",
      "[D loss: 0.757802] [G loss: 1.521189]\n",
      "[D loss: 0.764297] [G loss: 1.272102]\n",
      "[D loss: 0.704983] [G loss: 1.635872]\n",
      "[D loss: 0.746776] [G loss: 1.485430]\n",
      "[D loss: 0.871418] [G loss: 1.201026]\n",
      "[D loss: 0.961772] [G loss: 1.230289]\n",
      "[D loss: 0.809467] [G loss: 1.637708]\n",
      "[D loss: 0.884841] [G loss: 1.444676]\n",
      "[D loss: 0.728654] [G loss: 1.509002]\n",
      "[D loss: 1.070938] [G loss: 1.467297]\n",
      "[D loss: 0.858672] [G loss: 1.421360]\n",
      "[D loss: 0.773932] [G loss: 1.649575]\n",
      "[D loss: 0.925265] [G loss: 1.761120]\n",
      "[D loss: 0.924795] [G loss: 1.464409]\n",
      "[D loss: 0.885078] [G loss: 1.299770]\n",
      "[D loss: 0.804384] [G loss: 1.296932]\n",
      "[D loss: 0.934183] [G loss: 1.378006]\n",
      "[D loss: 0.694783] [G loss: 1.423459]\n",
      "[D loss: 0.836440] [G loss: 1.446849]\n",
      "[D loss: 0.601043] [G loss: 1.512216]\n",
      "[D loss: 0.753227] [G loss: 1.537977]\n",
      "[D loss: 0.746185] [G loss: 1.705750]\n",
      "[D loss: 0.836462] [G loss: 1.445721]\n",
      "[D loss: 0.772272] [G loss: 1.600302]\n",
      "[D loss: 0.826883] [G loss: 1.409149]\n",
      "[D loss: 1.062271] [G loss: 1.437526]\n",
      "[D loss: 0.723168] [G loss: 1.470211]\n",
      "[D loss: 0.918782] [G loss: 1.397940]\n",
      "[D loss: 0.940736] [G loss: 1.579257]\n",
      "[D loss: 0.829352] [G loss: 1.393956]\n",
      "[D loss: 0.859094] [G loss: 1.325052]\n",
      "[D loss: 1.042669] [G loss: 1.504299]\n",
      "[D loss: 0.839543] [G loss: 1.703257]\n",
      "[D loss: 0.766145] [G loss: 1.519338]\n",
      "[D loss: 0.779495] [G loss: 1.423676]\n",
      "[D loss: 0.739748] [G loss: 1.455200]\n",
      "[D loss: 0.981052] [G loss: 1.374573]\n",
      "[D loss: 1.132424] [G loss: 1.384864]\n",
      "[D loss: 0.617716] [G loss: 1.342412]\n",
      "[D loss: 0.821678] [G loss: 1.622306]\n",
      "[D loss: 0.858763] [G loss: 1.470097]\n",
      "[D loss: 0.962433] [G loss: 1.459165]\n",
      "[D loss: 0.753375] [G loss: 1.602916]\n",
      "[D loss: 0.848593] [G loss: 1.347072]\n",
      "[D loss: 0.802019] [G loss: 1.236943]\n",
      "[D loss: 0.996730] [G loss: 1.391533]\n",
      "[D loss: 1.074815] [G loss: 1.450284]\n",
      "[D loss: 0.916349] [G loss: 1.273252]\n",
      "[D loss: 0.843145] [G loss: 1.334186]\n",
      "[D loss: 0.605153] [G loss: 1.652270]\n",
      "[D loss: 0.821024] [G loss: 1.527393]\n",
      "[D loss: 0.814150] [G loss: 1.585844]\n",
      "[D loss: 1.005462] [G loss: 1.511719]\n",
      "[D loss: 0.873577] [G loss: 1.488145]\n",
      "[D loss: 0.914877] [G loss: 1.164048]\n",
      "[D loss: 0.688077] [G loss: 1.336678]\n",
      "[D loss: 0.682839] [G loss: 1.379940]\n",
      "[D loss: 0.960669] [G loss: 1.351568]\n",
      "[D loss: 0.728608] [G loss: 1.488690]\n",
      "[D loss: 0.769466] [G loss: 1.571240]\n",
      "[D loss: 0.863537] [G loss: 1.429806]\n",
      "[D loss: 0.948678] [G loss: 1.714958]\n",
      "[D loss: 0.749147] [G loss: 1.351932]\n",
      "[D loss: 1.181548] [G loss: 1.450890]\n",
      "[D loss: 0.759897] [G loss: 1.495009]\n",
      "[D loss: 0.752914] [G loss: 1.454978]\n",
      "[D loss: 0.754412] [G loss: 1.264965]\n",
      "[D loss: 0.714439] [G loss: 1.485108]\n",
      "[D loss: 0.878274] [G loss: 1.293895]\n",
      "[D loss: 1.029369] [G loss: 1.362277]\n",
      "[D loss: 0.748398] [G loss: 1.346455]\n",
      "[D loss: 0.895313] [G loss: 1.347861]\n",
      "[D loss: 0.795678] [G loss: 1.333766]\n",
      "[D loss: 0.813943] [G loss: 1.455334]\n",
      "[D loss: 0.872615] [G loss: 1.368348]\n",
      "[D loss: 0.642038] [G loss: 1.708885]\n",
      "[D loss: 0.788796] [G loss: 1.629034]\n",
      "[D loss: 1.235900] [G loss: 1.385311]\n",
      "[D loss: 0.762442] [G loss: 1.382083]\n",
      "[D loss: 0.901224] [G loss: 1.387441]\n",
      "[D loss: 0.653913] [G loss: 1.458326]\n",
      "[D loss: 0.826862] [G loss: 1.568544]\n",
      "[D loss: 0.870587] [G loss: 1.382923]\n",
      "[D loss: 0.792416] [G loss: 1.576418]\n",
      "[D loss: 0.797126] [G loss: 1.444720]\n",
      "[D loss: 0.763460] [G loss: 1.403271]\n",
      "[D loss: 0.860615] [G loss: 1.548369]\n",
      "[D loss: 0.991840] [G loss: 1.354293]\n",
      "[D loss: 0.887411] [G loss: 1.402087]\n",
      "[D loss: 0.966989] [G loss: 1.424907]\n",
      "[D loss: 0.765124] [G loss: 1.386454]\n",
      "[D loss: 0.596183] [G loss: 1.499638]\n",
      "[D loss: 0.968826] [G loss: 1.424764]\n",
      "[D loss: 0.939799] [G loss: 1.578875]\n",
      "[D loss: 0.706806] [G loss: 1.415256]\n",
      "[D loss: 0.935525] [G loss: 1.310118]\n",
      "[D loss: 0.905054] [G loss: 1.425570]\n",
      "[D loss: 0.823539] [G loss: 1.457889]\n",
      "[D loss: 0.952165] [G loss: 1.326568]\n",
      "[D loss: 0.970820] [G loss: 1.351155]\n",
      "[D loss: 0.708487] [G loss: 1.481682]\n",
      "[D loss: 0.865052] [G loss: 1.354051]\n",
      "[D loss: 0.822761] [G loss: 1.209354]\n",
      "[D loss: 0.882169] [G loss: 1.386491]\n",
      "[D loss: 0.877473] [G loss: 1.226816]\n",
      "[D loss: 0.785505] [G loss: 1.234812]\n",
      "[D loss: 0.712546] [G loss: 1.425815]\n",
      "[D loss: 0.898252] [G loss: 1.453070]\n",
      "[D loss: 1.024524] [G loss: 1.485184]\n",
      "[D loss: 0.812632] [G loss: 1.396772]\n",
      "[D loss: 0.728637] [G loss: 1.490594]\n",
      "[D loss: 0.790529] [G loss: 1.651922]\n",
      "[D loss: 0.740862] [G loss: 1.697827]\n",
      "[D loss: 0.878920] [G loss: 1.441449]\n",
      "[D loss: 1.004544] [G loss: 1.639559]\n",
      "[D loss: 0.796207] [G loss: 1.658287]\n",
      "[D loss: 0.884993] [G loss: 1.391253]\n",
      "[D loss: 0.810460] [G loss: 1.546341]\n",
      "[D loss: 0.937099] [G loss: 1.406547]\n",
      "[D loss: 0.802966] [G loss: 1.501019]\n",
      "[D loss: 0.755089] [G loss: 1.396880]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.637723] [G loss: 1.521261]\n",
      "[D loss: 0.656076] [G loss: 1.927275]\n",
      "[D loss: 0.894814] [G loss: 1.371421]\n",
      "[D loss: 0.732686] [G loss: 1.811588]\n",
      "[D loss: 0.894241] [G loss: 1.522236]\n",
      "[D loss: 0.768372] [G loss: 1.850236]\n",
      "[D loss: 1.067469] [G loss: 1.240230]\n",
      "[D loss: 0.648951] [G loss: 1.754119]\n",
      "[D loss: 1.051174] [G loss: 1.477597]\n",
      "[D loss: 0.779933] [G loss: 1.225244]\n",
      "[D loss: 0.762505] [G loss: 1.339061]\n",
      "[D loss: 0.872626] [G loss: 1.310141]\n",
      "[D loss: 0.743708] [G loss: 1.712334]\n",
      "[D loss: 0.562110] [G loss: 1.451575]\n",
      "[D loss: 0.882944] [G loss: 1.455543]\n",
      "[D loss: 0.773075] [G loss: 1.510403]\n",
      "[D loss: 0.846300] [G loss: 1.546618]\n",
      "[D loss: 0.896482] [G loss: 1.391903]\n",
      "[D loss: 0.851322] [G loss: 1.595334]\n",
      "[D loss: 0.863637] [G loss: 1.478491]\n",
      "[D loss: 0.702152] [G loss: 1.410686]\n",
      "[D loss: 0.792341] [G loss: 1.256962]\n",
      "[D loss: 0.862316] [G loss: 1.578467]\n",
      "[D loss: 0.800015] [G loss: 1.486811]\n",
      "[D loss: 0.659459] [G loss: 1.262219]\n",
      "[D loss: 0.812151] [G loss: 1.339606]\n",
      "[D loss: 0.658336] [G loss: 1.556709]\n",
      "[D loss: 0.630730] [G loss: 1.522974]\n",
      "[D loss: 1.128001] [G loss: 1.654377]\n",
      "[D loss: 0.660355] [G loss: 1.448503]\n",
      "[D loss: 0.831961] [G loss: 1.376258]\n",
      "[D loss: 0.854580] [G loss: 1.667241]\n",
      "[D loss: 0.648216] [G loss: 1.581434]\n",
      "[D loss: 0.711871] [G loss: 1.436541]\n",
      "[D loss: 0.834689] [G loss: 1.762936]\n",
      "[D loss: 0.813239] [G loss: 1.641837]\n",
      "[D loss: 0.759545] [G loss: 1.621126]\n",
      "[D loss: 0.718596] [G loss: 1.540552]\n",
      "[D loss: 0.754635] [G loss: 1.819987]\n",
      "[D loss: 1.044796] [G loss: 1.751130]\n",
      "[D loss: 0.832000] [G loss: 1.532638]\n",
      "[D loss: 0.698006] [G loss: 1.361269]\n",
      "[D loss: 0.946844] [G loss: 1.479714]\n",
      "[D loss: 0.908424] [G loss: 1.396519]\n",
      "[D loss: 0.995485] [G loss: 1.293927]\n",
      "[D loss: 0.905121] [G loss: 1.268091]\n",
      "[D loss: 0.840595] [G loss: 1.238635]\n",
      "[D loss: 0.869827] [G loss: 1.508819]\n",
      "[D loss: 0.853492] [G loss: 1.432341]\n",
      "[D loss: 0.823604] [G loss: 1.332806]\n",
      "[D loss: 0.759235] [G loss: 1.472701]\n",
      "[D loss: 0.911856] [G loss: 1.508933]\n",
      "[D loss: 0.956384] [G loss: 1.352586]\n",
      "[D loss: 0.895909] [G loss: 1.388613]\n",
      "[D loss: 0.829872] [G loss: 1.484808]\n",
      "[D loss: 0.677167] [G loss: 1.549780]\n",
      "[D loss: 0.985381] [G loss: 1.364370]\n",
      "[D loss: 0.662113] [G loss: 1.521266]\n",
      "[D loss: 0.849112] [G loss: 1.421188]\n",
      "[D loss: 0.766592] [G loss: 1.532063]\n",
      "[D loss: 0.867498] [G loss: 1.545359]\n",
      "[D loss: 1.283078] [G loss: 1.331635]\n",
      "[D loss: 0.790540] [G loss: 1.645666]\n",
      "[D loss: 0.930951] [G loss: 1.300045]\n",
      "[D loss: 0.768931] [G loss: 1.446719]\n",
      "[D loss: 0.872864] [G loss: 1.466143]\n",
      "[D loss: 1.063922] [G loss: 1.294521]\n",
      "[D loss: 0.627207] [G loss: 1.357237]\n",
      "[D loss: 0.989588] [G loss: 1.425189]\n",
      "[D loss: 1.037159] [G loss: 1.535051]\n",
      "[D loss: 0.893496] [G loss: 1.435470]\n",
      "[D loss: 1.008382] [G loss: 1.419273]\n",
      "[D loss: 0.880528] [G loss: 1.346284]\n",
      "[D loss: 0.771065] [G loss: 1.330822]\n",
      "[D loss: 0.687734] [G loss: 1.212047]\n",
      "[D loss: 0.647075] [G loss: 1.365969]\n",
      "[D loss: 0.860883] [G loss: 1.354197]\n",
      "[D loss: 0.945312] [G loss: 1.606155]\n",
      "[D loss: 0.957158] [G loss: 1.443962]\n",
      "[D loss: 0.981861] [G loss: 1.195206]\n",
      "[D loss: 1.063526] [G loss: 1.210891]\n",
      "[D loss: 0.818713] [G loss: 1.533913]\n",
      "[D loss: 0.861679] [G loss: 1.310587]\n",
      "[D loss: 0.917897] [G loss: 1.411652]\n",
      "[D loss: 0.926100] [G loss: 1.526030]\n",
      "[D loss: 0.665343] [G loss: 1.553312]\n",
      "[D loss: 0.812661] [G loss: 1.237233]\n",
      "[D loss: 0.792634] [G loss: 1.296426]\n",
      "[D loss: 0.872278] [G loss: 1.412394]\n",
      "[D loss: 0.844601] [G loss: 1.478821]\n",
      "[D loss: 0.997390] [G loss: 1.363838]\n",
      "[D loss: 0.853961] [G loss: 1.327705]\n",
      "[D loss: 0.740344] [G loss: 1.393617]\n",
      "[D loss: 0.642091] [G loss: 1.598002]\n",
      "[D loss: 0.804178] [G loss: 1.281533]\n",
      "[D loss: 0.763119] [G loss: 1.516850]\n",
      "[D loss: 0.775880] [G loss: 1.556511]\n",
      "[D loss: 0.906889] [G loss: 1.461440]\n",
      "[D loss: 0.666703] [G loss: 1.445863]\n",
      "[D loss: 0.710440] [G loss: 1.502411]\n",
      "[D loss: 0.857680] [G loss: 1.473008]\n",
      "[D loss: 0.715728] [G loss: 1.353221]\n",
      "[D loss: 0.680804] [G loss: 1.495748]\n",
      "[D loss: 0.859707] [G loss: 1.509907]\n",
      "[D loss: 0.612980] [G loss: 1.464210]\n",
      "[D loss: 1.094752] [G loss: 1.240826]\n",
      "[D loss: 0.760941] [G loss: 1.405326]\n",
      "[D loss: 0.932806] [G loss: 1.297271]\n",
      "[D loss: 0.765453] [G loss: 1.639276]\n",
      "[D loss: 0.983616] [G loss: 1.745744]\n",
      "[D loss: 0.990544] [G loss: 1.445112]\n",
      "[D loss: 0.875457] [G loss: 1.471823]\n",
      "[D loss: 1.074027] [G loss: 1.142080]\n",
      "[D loss: 0.757573] [G loss: 1.532839]\n",
      "[D loss: 0.894064] [G loss: 1.435746]\n",
      "[D loss: 0.836934] [G loss: 1.360179]\n",
      "[D loss: 0.665424] [G loss: 1.618270]\n",
      "[D loss: 0.671870] [G loss: 1.403934]\n",
      "[D loss: 0.792492] [G loss: 1.425526]\n",
      "[D loss: 0.794367] [G loss: 1.471514]\n",
      "[D loss: 0.932218] [G loss: 1.357683]\n",
      "[D loss: 0.933502] [G loss: 1.317868]\n",
      "[D loss: 0.719723] [G loss: 1.510053]\n",
      "[D loss: 0.751019] [G loss: 1.359792]\n",
      "[D loss: 0.961058] [G loss: 1.454440]\n",
      "[D loss: 0.706684] [G loss: 1.509107]\n",
      "[D loss: 0.876105] [G loss: 1.646759]\n",
      "[D loss: 0.762884] [G loss: 1.571229]\n",
      "[D loss: 0.898305] [G loss: 1.533234]\n",
      "[D loss: 0.726664] [G loss: 1.571572]\n",
      "[D loss: 0.575502] [G loss: 1.507156]\n",
      "[D loss: 0.755119] [G loss: 1.703256]\n",
      "[D loss: 1.057224] [G loss: 1.408484]\n",
      "[D loss: 0.856717] [G loss: 1.538167]\n",
      "[D loss: 0.966840] [G loss: 1.416247]\n",
      "[D loss: 0.773299] [G loss: 1.443101]\n",
      "[D loss: 0.784374] [G loss: 1.345766]\n",
      "[D loss: 0.833864] [G loss: 1.557333]\n",
      "[D loss: 0.940224] [G loss: 1.222646]\n",
      "[D loss: 1.015918] [G loss: 1.489204]\n",
      "[D loss: 0.807447] [G loss: 1.519288]\n",
      "[D loss: 0.793583] [G loss: 1.609922]\n",
      "[D loss: 0.805695] [G loss: 1.496844]\n",
      "[D loss: 0.859012] [G loss: 1.379295]\n",
      "[D loss: 0.725891] [G loss: 1.609526]\n",
      "[D loss: 0.804197] [G loss: 1.512562]\n",
      "[D loss: 0.650148] [G loss: 1.758193]\n",
      "[D loss: 0.842175] [G loss: 1.360714]\n",
      "[D loss: 0.806855] [G loss: 1.450700]\n",
      "[D loss: 0.926779] [G loss: 1.556584]\n",
      "[D loss: 0.864096] [G loss: 1.536723]\n",
      "[D loss: 0.738926] [G loss: 1.313899]\n",
      "[D loss: 0.928170] [G loss: 1.316280]\n",
      "[D loss: 0.791840] [G loss: 1.378204]\n",
      "[D loss: 0.811771] [G loss: 1.443048]\n",
      "[D loss: 0.949021] [G loss: 1.454232]\n",
      "[D loss: 0.815810] [G loss: 1.511316]\n",
      "[D loss: 0.979559] [G loss: 1.367093]\n",
      "[D loss: 0.871494] [G loss: 1.483022]\n",
      "[D loss: 1.008559] [G loss: 1.261011]\n",
      "[D loss: 0.829246] [G loss: 1.447259]\n",
      "[D loss: 0.926600] [G loss: 1.430071]\n",
      "[D loss: 0.585049] [G loss: 1.712976]\n",
      "[D loss: 0.728506] [G loss: 1.485282]\n",
      "[D loss: 0.674103] [G loss: 1.484700]\n",
      "[D loss: 0.938408] [G loss: 1.394543]\n",
      "[D loss: 0.732924] [G loss: 1.452035]\n",
      "[D loss: 0.693320] [G loss: 1.350161]\n",
      "[D loss: 0.831607] [G loss: 1.325049]\n",
      "[D loss: 1.043722] [G loss: 1.202942]\n",
      "[D loss: 0.595747] [G loss: 1.459440]\n",
      "[D loss: 0.847493] [G loss: 1.555334]\n",
      "[D loss: 0.659058] [G loss: 1.369236]\n",
      "[D loss: 0.732816] [G loss: 1.377522]\n",
      "[D loss: 1.106200] [G loss: 1.307328]\n",
      "[D loss: 0.887305] [G loss: 1.438116]\n",
      "[D loss: 0.782031] [G loss: 1.599110]\n",
      "[D loss: 0.723644] [G loss: 1.370340]\n",
      "[D loss: 0.710483] [G loss: 1.434036]\n",
      "[D loss: 0.700541] [G loss: 1.401783]\n",
      "[D loss: 0.855230] [G loss: 1.418257]\n",
      "[D loss: 0.899256] [G loss: 1.363580]\n",
      "[D loss: 0.825368] [G loss: 1.658718]\n",
      "[D loss: 0.944410] [G loss: 1.448373]\n",
      "[D loss: 0.951265] [G loss: 1.481675]\n",
      "[D loss: 0.847525] [G loss: 1.449515]\n",
      "[D loss: 0.932815] [G loss: 1.414464]\n",
      "[D loss: 1.000118] [G loss: 1.270558]\n",
      "[D loss: 0.931383] [G loss: 1.294261]\n",
      "[D loss: 0.953960] [G loss: 1.287305]\n",
      "[D loss: 0.654804] [G loss: 1.350808]\n",
      "[D loss: 0.772309] [G loss: 1.523380]\n",
      "[D loss: 1.100246] [G loss: 1.214938]\n",
      "[D loss: 0.969023] [G loss: 1.259682]\n",
      "[D loss: 0.738291] [G loss: 1.562795]\n",
      "[D loss: 0.788932] [G loss: 1.506541]\n",
      "[D loss: 0.847400] [G loss: 1.403499]\n",
      "[D loss: 0.718821] [G loss: 1.365847]\n",
      "[D loss: 0.912373] [G loss: 1.472341]\n",
      "[D loss: 0.704724] [G loss: 1.506118]\n",
      "[D loss: 1.037806] [G loss: 1.331265]\n",
      "[D loss: 0.903072] [G loss: 1.441585]\n",
      "[D loss: 0.703752] [G loss: 1.546224]\n",
      "[D loss: 0.740603] [G loss: 1.553744]\n",
      "[D loss: 0.725203] [G loss: 1.398848]\n",
      "[D loss: 0.701420] [G loss: 1.516131]\n",
      "[D loss: 0.760925] [G loss: 1.758781]\n",
      "[D loss: 0.609674] [G loss: 1.528809]\n",
      "[D loss: 0.863918] [G loss: 1.220688]\n",
      "[D loss: 1.137473] [G loss: 1.418184]\n",
      "[D loss: 0.696767] [G loss: 1.572568]\n",
      "[D loss: 0.847311] [G loss: 1.522573]\n",
      "[D loss: 0.861036] [G loss: 1.324589]\n",
      "[D loss: 0.602637] [G loss: 1.718572]\n",
      "[D loss: 0.821720] [G loss: 1.527778]\n",
      "[D loss: 0.691774] [G loss: 1.498086]\n",
      "[D loss: 0.797848] [G loss: 1.434613]\n",
      "[D loss: 0.779537] [G loss: 1.375367]\n",
      "[D loss: 0.648871] [G loss: 1.428819]\n",
      "[D loss: 0.930280] [G loss: 1.405792]\n",
      "[D loss: 0.735754] [G loss: 1.562046]\n",
      "[D loss: 0.815831] [G loss: 1.399047]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.847436] [G loss: 1.538997]\n",
      "[D loss: 0.814719] [G loss: 1.361144]\n",
      "[D loss: 0.889938] [G loss: 1.508295]\n",
      "[D loss: 0.894151] [G loss: 1.406914]\n",
      "[D loss: 0.850710] [G loss: 1.776522]\n",
      "[D loss: 0.930924] [G loss: 1.650856]\n",
      "[D loss: 1.078559] [G loss: 1.423495]\n",
      "[D loss: 0.701906] [G loss: 1.403568]\n",
      "[D loss: 0.780973] [G loss: 1.430867]\n",
      "[D loss: 0.686769] [G loss: 1.509624]\n",
      "[D loss: 0.805774] [G loss: 1.514825]\n",
      "[D loss: 0.799695] [G loss: 1.537121]\n",
      "[D loss: 1.077519] [G loss: 1.558517]\n",
      "[D loss: 0.702660] [G loss: 1.585132]\n",
      "[D loss: 0.783073] [G loss: 1.783022]\n",
      "[D loss: 0.990493] [G loss: 1.261961]\n",
      "[D loss: 1.124930] [G loss: 1.232509]\n",
      "[D loss: 0.942133] [G loss: 1.348761]\n",
      "[D loss: 0.746044] [G loss: 1.572604]\n",
      "[D loss: 0.747105] [G loss: 1.442067]\n",
      "[D loss: 1.076166] [G loss: 1.306338]\n",
      "[D loss: 0.930775] [G loss: 1.159614]\n",
      "[D loss: 0.973029] [G loss: 1.333591]\n",
      "[D loss: 0.739664] [G loss: 1.358209]\n",
      "[D loss: 0.885048] [G loss: 1.513647]\n",
      "[D loss: 0.967912] [G loss: 1.497653]\n",
      "[D loss: 0.909919] [G loss: 1.608653]\n",
      "[D loss: 0.763746] [G loss: 1.301184]\n",
      "[D loss: 0.875028] [G loss: 1.460497]\n",
      "[D loss: 0.788445] [G loss: 1.276690]\n",
      "[D loss: 0.854809] [G loss: 1.411139]\n",
      "[D loss: 0.792189] [G loss: 1.348581]\n",
      "[D loss: 0.974087] [G loss: 1.370730]\n",
      "[D loss: 0.879837] [G loss: 1.422876]\n",
      "[D loss: 0.762128] [G loss: 1.448298]\n",
      "[D loss: 0.775769] [G loss: 1.202078]\n",
      "[D loss: 0.981583] [G loss: 1.404979]\n",
      "[D loss: 0.963800] [G loss: 1.434581]\n",
      "[D loss: 0.999892] [G loss: 1.246084]\n",
      "[D loss: 0.741130] [G loss: 1.362733]\n",
      "[D loss: 1.023780] [G loss: 1.521619]\n",
      "[D loss: 0.841890] [G loss: 1.607293]\n",
      "[D loss: 0.875564] [G loss: 1.422363]\n",
      "[D loss: 0.688046] [G loss: 1.381010]\n",
      "[D loss: 1.078799] [G loss: 1.355113]\n",
      "[D loss: 0.666853] [G loss: 1.295294]\n",
      "[D loss: 0.677408] [G loss: 1.384010]\n",
      "[D loss: 0.829847] [G loss: 1.188861]\n",
      "[D loss: 1.085440] [G loss: 1.250002]\n",
      "[D loss: 0.820000] [G loss: 1.473364]\n",
      "[D loss: 0.793226] [G loss: 1.369786]\n",
      "[D loss: 0.918158] [G loss: 1.543131]\n",
      "[D loss: 0.808581] [G loss: 1.511452]\n",
      "[D loss: 0.812928] [G loss: 1.350347]\n",
      "[D loss: 0.879560] [G loss: 1.329981]\n",
      "[D loss: 0.757124] [G loss: 1.662174]\n",
      "[D loss: 0.781964] [G loss: 1.483793]\n",
      "[D loss: 0.937245] [G loss: 1.503043]\n",
      "[D loss: 0.843651] [G loss: 1.667653]\n",
      "[D loss: 0.935920] [G loss: 1.323009]\n",
      "[D loss: 0.826367] [G loss: 1.292934]\n",
      "[D loss: 0.983328] [G loss: 1.426497]\n",
      "[D loss: 0.794427] [G loss: 1.651455]\n",
      "[D loss: 0.785201] [G loss: 1.527205]\n",
      "[D loss: 0.920389] [G loss: 1.636392]\n",
      "[D loss: 0.895819] [G loss: 1.420341]\n",
      "[D loss: 0.978302] [G loss: 1.651761]\n",
      "[D loss: 0.870374] [G loss: 1.642500]\n",
      "[D loss: 0.889380] [G loss: 1.479432]\n",
      "[D loss: 0.938794] [G loss: 1.440607]\n",
      "[D loss: 0.901027] [G loss: 1.247528]\n",
      "[D loss: 0.776447] [G loss: 1.311726]\n",
      "[D loss: 0.794309] [G loss: 1.422721]\n",
      "[D loss: 0.717003] [G loss: 1.558681]\n",
      "[D loss: 1.067843] [G loss: 1.259435]\n",
      "[D loss: 0.738272] [G loss: 1.428777]\n",
      "[D loss: 0.982148] [G loss: 1.410304]\n",
      "[D loss: 0.770932] [G loss: 1.657093]\n",
      "[D loss: 0.912109] [G loss: 1.505122]\n",
      "[D loss: 0.793249] [G loss: 1.556272]\n",
      "[D loss: 0.670620] [G loss: 1.405136]\n",
      "[D loss: 0.948636] [G loss: 1.433342]\n",
      "[D loss: 0.741247] [G loss: 1.440758]\n",
      "[D loss: 0.660354] [G loss: 1.454863]\n",
      "[D loss: 0.900770] [G loss: 1.527294]\n",
      "[D loss: 0.730513] [G loss: 1.517974]\n",
      "[D loss: 0.903981] [G loss: 1.814612]\n",
      "[D loss: 0.775486] [G loss: 1.367881]\n",
      "[D loss: 0.836868] [G loss: 1.325760]\n",
      "[D loss: 0.757834] [G loss: 1.312720]\n",
      "[D loss: 0.836267] [G loss: 1.451646]\n",
      "[D loss: 0.754883] [G loss: 1.423351]\n",
      "[D loss: 1.078622] [G loss: 1.324522]\n",
      "[D loss: 0.958584] [G loss: 1.461812]\n",
      "[D loss: 1.250896] [G loss: 1.502993]\n",
      "[D loss: 1.037769] [G loss: 1.538992]\n",
      "[D loss: 0.825736] [G loss: 1.329653]\n",
      "[D loss: 0.748922] [G loss: 1.314314]\n",
      "[D loss: 0.653169] [G loss: 1.450052]\n",
      "[D loss: 0.934573] [G loss: 1.384176]\n",
      "[D loss: 0.606922] [G loss: 1.603412]\n",
      "[D loss: 0.791445] [G loss: 1.232575]\n",
      "[D loss: 1.064215] [G loss: 1.218539]\n",
      "[D loss: 0.862279] [G loss: 1.602877]\n",
      "[D loss: 0.864698] [G loss: 1.515345]\n",
      "[D loss: 0.762714] [G loss: 1.437013]\n",
      "[D loss: 0.669410] [G loss: 1.419186]\n",
      "[D loss: 0.822402] [G loss: 1.473341]\n",
      "[D loss: 0.926673] [G loss: 1.357394]\n",
      "[D loss: 0.960906] [G loss: 1.582094]\n",
      "[D loss: 0.828304] [G loss: 1.568282]\n",
      "[D loss: 0.767055] [G loss: 1.419252]\n",
      "[D loss: 0.618687] [G loss: 1.302042]\n",
      "[D loss: 0.928354] [G loss: 1.419969]\n",
      "[D loss: 0.938950] [G loss: 1.455159]\n",
      "[D loss: 0.811193] [G loss: 1.519960]\n",
      "[D loss: 0.701546] [G loss: 1.581365]\n",
      "[D loss: 0.884088] [G loss: 1.436075]\n",
      "[D loss: 0.912629] [G loss: 1.553246]\n",
      "[D loss: 0.749967] [G loss: 1.292518]\n",
      "[D loss: 0.623927] [G loss: 1.509250]\n",
      "[D loss: 0.663288] [G loss: 1.388175]\n",
      "[D loss: 0.702024] [G loss: 1.489960]\n",
      "[D loss: 0.699634] [G loss: 1.664999]\n",
      "[D loss: 0.776868] [G loss: 1.666822]\n",
      "[D loss: 0.995506] [G loss: 1.527898]\n",
      "[D loss: 0.869002] [G loss: 1.449963]\n",
      "[D loss: 0.612221] [G loss: 1.636559]\n",
      "[D loss: 0.800476] [G loss: 1.650003]\n",
      "[D loss: 0.862604] [G loss: 1.656128]\n",
      "[D loss: 1.045828] [G loss: 1.461120]\n",
      "[D loss: 0.835538] [G loss: 1.501518]\n",
      "[D loss: 0.606385] [G loss: 1.641201]\n",
      "[D loss: 1.046561] [G loss: 1.395939]\n",
      "[D loss: 0.883746] [G loss: 1.361235]\n",
      "[D loss: 0.869513] [G loss: 1.467383]\n",
      "[D loss: 0.857376] [G loss: 1.587571]\n",
      "[D loss: 1.169090] [G loss: 1.380748]\n",
      "[D loss: 0.652893] [G loss: 1.515222]\n",
      "[D loss: 1.049606] [G loss: 1.340504]\n",
      "[D loss: 0.898391] [G loss: 1.504551]\n",
      "[D loss: 0.974784] [G loss: 1.355636]\n",
      "[D loss: 0.927539] [G loss: 1.694582]\n",
      "[D loss: 0.724548] [G loss: 1.408166]\n",
      "[D loss: 0.947841] [G loss: 1.194582]\n",
      "[D loss: 0.987913] [G loss: 1.127233]\n",
      "[D loss: 0.763525] [G loss: 1.547211]\n",
      "[D loss: 0.678709] [G loss: 1.536150]\n",
      "[D loss: 0.913360] [G loss: 1.673840]\n",
      "[D loss: 0.768765] [G loss: 1.565108]\n",
      "[D loss: 1.088887] [G loss: 1.192170]\n",
      "[D loss: 0.664101] [G loss: 1.377376]\n",
      "[D loss: 0.955647] [G loss: 1.266279]\n",
      "[D loss: 0.859635] [G loss: 1.545943]\n",
      "[D loss: 0.954368] [G loss: 1.386509]\n",
      "[D loss: 1.053525] [G loss: 1.412496]\n",
      "[D loss: 0.987701] [G loss: 1.296767]\n",
      "[D loss: 0.764448] [G loss: 1.608374]\n",
      "[D loss: 0.852142] [G loss: 1.436549]\n",
      "[D loss: 0.957175] [G loss: 1.192728]\n",
      "[D loss: 0.852660] [G loss: 1.295583]\n",
      "[D loss: 0.858624] [G loss: 1.445166]\n",
      "[D loss: 0.952064] [G loss: 1.529195]\n",
      "[D loss: 0.783125] [G loss: 1.391560]\n",
      "[D loss: 0.771977] [G loss: 1.276602]\n",
      "[D loss: 0.758024] [G loss: 1.545272]\n",
      "[D loss: 0.905574] [G loss: 1.188553]\n",
      "[D loss: 0.876270] [G loss: 1.387719]\n",
      "[D loss: 0.715231] [G loss: 1.293465]\n",
      "[D loss: 0.993325] [G loss: 1.258006]\n",
      "[D loss: 0.717744] [G loss: 1.353394]\n",
      "[D loss: 0.676069] [G loss: 1.699545]\n",
      "[D loss: 0.901466] [G loss: 1.674232]\n",
      "[D loss: 0.803593] [G loss: 1.525832]\n",
      "[D loss: 0.755220] [G loss: 1.572471]\n",
      "[D loss: 0.794111] [G loss: 1.469644]\n",
      "[D loss: 1.089617] [G loss: 1.233484]\n",
      "[D loss: 0.876962] [G loss: 1.398128]\n",
      "[D loss: 1.103827] [G loss: 1.630391]\n",
      "[D loss: 0.603610] [G loss: 1.506443]\n",
      "[D loss: 0.843565] [G loss: 1.432365]\n",
      "[D loss: 0.796203] [G loss: 1.394091]\n",
      "[D loss: 0.833635] [G loss: 1.217090]\n",
      "[D loss: 0.727008] [G loss: 1.615953]\n",
      "[D loss: 0.675143] [G loss: 1.670041]\n",
      "[D loss: 0.660576] [G loss: 1.424608]\n",
      "[D loss: 0.700061] [G loss: 1.501969]\n",
      "[D loss: 1.129572] [G loss: 1.323903]\n",
      "[D loss: 0.896887] [G loss: 1.466640]\n",
      "[D loss: 0.788000] [G loss: 1.516771]\n",
      "[D loss: 0.675400] [G loss: 1.542426]\n",
      "[D loss: 0.980937] [G loss: 1.520198]\n",
      "[D loss: 0.988134] [G loss: 1.382149]\n",
      "[D loss: 0.753804] [G loss: 1.441230]\n",
      "[D loss: 0.667896] [G loss: 1.336427]\n",
      "[D loss: 0.585306] [G loss: 1.609537]\n",
      "[D loss: 0.756188] [G loss: 1.491666]\n",
      "[D loss: 0.792464] [G loss: 1.497788]\n",
      "[D loss: 0.729036] [G loss: 1.496045]\n",
      "[D loss: 1.014348] [G loss: 1.372602]\n",
      "[D loss: 0.729764] [G loss: 1.545410]\n",
      "[D loss: 0.984638] [G loss: 1.486296]\n",
      "[D loss: 0.709752] [G loss: 1.452253]\n",
      "[D loss: 0.670316] [G loss: 1.503778]\n",
      "[D loss: 0.959832] [G loss: 1.485291]\n",
      "[D loss: 0.813476] [G loss: 1.547805]\n",
      "[D loss: 0.698190] [G loss: 1.502548]\n",
      "[D loss: 0.978373] [G loss: 1.373241]\n",
      "[D loss: 0.946647] [G loss: 1.463936]\n",
      "[D loss: 0.669374] [G loss: 1.582703]\n",
      "[D loss: 0.880097] [G loss: 1.536481]\n",
      "[D loss: 0.775714] [G loss: 1.496185]\n",
      "[D loss: 0.821112] [G loss: 1.524270]\n",
      "[D loss: 0.952896] [G loss: 1.402744]\n",
      "[D loss: 1.018693] [G loss: 1.520602]\n",
      "[D loss: 0.949330] [G loss: 1.327208]\n",
      "[D loss: 0.840912] [G loss: 1.661854]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.868867] [G loss: 1.432809]\n",
      "[D loss: 0.700695] [G loss: 1.274576]\n",
      "[D loss: 0.952153] [G loss: 1.207590]\n",
      "[D loss: 0.826049] [G loss: 1.572353]\n",
      "[D loss: 0.679471] [G loss: 1.347022]\n",
      "[D loss: 0.816345] [G loss: 1.489512]\n",
      "[D loss: 0.827440] [G loss: 1.409199]\n",
      "[D loss: 0.620871] [G loss: 1.506206]\n",
      "[D loss: 0.842009] [G loss: 1.406811]\n",
      "[D loss: 0.808684] [G loss: 1.306327]\n",
      "[D loss: 0.860103] [G loss: 1.421799]\n",
      "[D loss: 0.879079] [G loss: 1.483490]\n",
      "[D loss: 0.904517] [G loss: 1.476397]\n",
      "[D loss: 0.985785] [G loss: 1.397280]\n",
      "[D loss: 0.781138] [G loss: 1.590132]\n",
      "[D loss: 0.746299] [G loss: 1.447537]\n",
      "[D loss: 0.759825] [G loss: 1.503264]\n",
      "[D loss: 0.746306] [G loss: 1.410115]\n",
      "[D loss: 0.704836] [G loss: 1.438756]\n",
      "[D loss: 0.790610] [G loss: 1.357702]\n",
      "[D loss: 0.861123] [G loss: 1.445849]\n",
      "[D loss: 0.915946] [G loss: 1.718637]\n",
      "[D loss: 0.787801] [G loss: 1.636902]\n",
      "[D loss: 1.119325] [G loss: 1.335616]\n",
      "[D loss: 0.635621] [G loss: 1.606189]\n",
      "[D loss: 0.926414] [G loss: 1.438732]\n",
      "[D loss: 1.136503] [G loss: 1.249193]\n",
      "[D loss: 0.801523] [G loss: 1.247882]\n",
      "[D loss: 0.852723] [G loss: 1.554795]\n",
      "[D loss: 0.477950] [G loss: 1.739654]\n",
      "[D loss: 0.826723] [G loss: 1.386171]\n",
      "[D loss: 0.933563] [G loss: 1.329268]\n",
      "[D loss: 0.702884] [G loss: 1.434878]\n",
      "[D loss: 0.740654] [G loss: 1.336030]\n",
      "[D loss: 0.940354] [G loss: 1.513777]\n",
      "[D loss: 0.801114] [G loss: 1.618327]\n",
      "[D loss: 0.694383] [G loss: 1.547637]\n",
      "[D loss: 0.628383] [G loss: 1.563974]\n",
      "[D loss: 1.029434] [G loss: 1.400067]\n",
      "[D loss: 0.847836] [G loss: 1.400768]\n",
      "[D loss: 0.545939] [G loss: 1.424878]\n",
      "[D loss: 1.087246] [G loss: 1.540793]\n",
      "[D loss: 0.742017] [G loss: 1.445140]\n",
      "[D loss: 0.840108] [G loss: 1.383839]\n",
      "[D loss: 0.961329] [G loss: 1.532847]\n",
      "[D loss: 0.848840] [G loss: 1.482300]\n",
      "[D loss: 0.737587] [G loss: 1.453909]\n",
      "[D loss: 0.801747] [G loss: 1.238700]\n",
      "[D loss: 0.855491] [G loss: 1.673649]\n",
      "[D loss: 0.877808] [G loss: 1.364925]\n",
      "[D loss: 0.880648] [G loss: 1.580084]\n",
      "[D loss: 0.773251] [G loss: 1.641329]\n",
      "[D loss: 0.966939] [G loss: 1.560771]\n",
      "[D loss: 0.721485] [G loss: 1.690950]\n",
      "[D loss: 0.959622] [G loss: 1.453268]\n",
      "[D loss: 0.631379] [G loss: 1.666480]\n",
      "[D loss: 0.640402] [G loss: 1.466094]\n",
      "[D loss: 0.770175] [G loss: 1.532397]\n",
      "[D loss: 0.827398] [G loss: 1.430037]\n",
      "[D loss: 0.696026] [G loss: 1.490487]\n",
      "[D loss: 0.939249] [G loss: 1.180665]\n",
      "[D loss: 0.708391] [G loss: 1.303037]\n",
      "[D loss: 0.667776] [G loss: 1.501828]\n",
      "[D loss: 0.731766] [G loss: 1.619862]\n",
      "[D loss: 0.973595] [G loss: 1.702139]\n",
      "[D loss: 0.842221] [G loss: 1.568207]\n",
      "[D loss: 0.741291] [G loss: 1.500048]\n",
      "[D loss: 0.871511] [G loss: 1.449794]\n",
      "[D loss: 0.754839] [G loss: 1.516703]\n",
      "[D loss: 0.722972] [G loss: 1.426244]\n",
      "[D loss: 0.922189] [G loss: 1.398560]\n",
      "[D loss: 0.948567] [G loss: 1.245100]\n",
      "[D loss: 0.775964] [G loss: 1.391048]\n",
      "[D loss: 0.777188] [G loss: 1.614291]\n",
      "[D loss: 0.941178] [G loss: 1.529987]\n",
      "[D loss: 0.747359] [G loss: 1.497182]\n",
      "[D loss: 0.990797] [G loss: 1.371073]\n",
      "[D loss: 0.802472] [G loss: 1.565612]\n",
      "[D loss: 0.951963] [G loss: 1.243590]\n",
      "[D loss: 0.814189] [G loss: 1.328875]\n",
      "[D loss: 1.073811] [G loss: 1.489198]\n",
      "[D loss: 0.586772] [G loss: 1.463170]\n",
      "[D loss: 1.023790] [G loss: 1.190994]\n",
      "[D loss: 0.743649] [G loss: 1.322454]\n",
      "[D loss: 0.759599] [G loss: 1.367707]\n",
      "[D loss: 1.013706] [G loss: 1.415861]\n",
      "[D loss: 0.841815] [G loss: 1.564607]\n",
      "[D loss: 0.763672] [G loss: 1.487255]\n",
      "[D loss: 0.980515] [G loss: 1.288465]\n",
      "[D loss: 1.004958] [G loss: 1.499659]\n",
      "[D loss: 0.815256] [G loss: 1.629345]\n",
      "[D loss: 0.797664] [G loss: 1.360040]\n",
      "[D loss: 0.727958] [G loss: 1.553709]\n",
      "[D loss: 0.602578] [G loss: 1.722882]\n",
      "[D loss: 0.954272] [G loss: 1.642764]\n",
      "[D loss: 1.040314] [G loss: 1.192745]\n",
      "[D loss: 0.822946] [G loss: 1.473752]\n",
      "[D loss: 0.743246] [G loss: 1.610536]\n",
      "[D loss: 0.845639] [G loss: 1.519534]\n",
      "[D loss: 0.953554] [G loss: 1.510842]\n",
      "[D loss: 1.042351] [G loss: 1.348193]\n",
      "[D loss: 0.760418] [G loss: 1.586282]\n",
      "[D loss: 0.894742] [G loss: 1.296700]\n",
      "[D loss: 0.950456] [G loss: 1.416169]\n",
      "[D loss: 0.670627] [G loss: 1.513106]\n",
      "[D loss: 1.083990] [G loss: 1.357705]\n",
      "[D loss: 0.952315] [G loss: 1.437453]\n",
      "[D loss: 0.871355] [G loss: 1.283863]\n",
      "[D loss: 0.889340] [G loss: 1.560714]\n",
      "[D loss: 1.009402] [G loss: 1.472182]\n",
      "[D loss: 0.741996] [G loss: 1.358827]\n",
      "[D loss: 0.896480] [G loss: 1.526195]\n",
      "[D loss: 0.780634] [G loss: 1.319295]\n",
      "[D loss: 0.722070] [G loss: 1.368112]\n",
      "[D loss: 0.839262] [G loss: 1.299109]\n",
      "[D loss: 0.829989] [G loss: 1.427756]\n",
      "[D loss: 0.760649] [G loss: 1.642614]\n",
      "[D loss: 1.065349] [G loss: 1.891452]\n",
      "[D loss: 0.740758] [G loss: 1.630923]\n",
      "[D loss: 0.921201] [G loss: 1.565171]\n",
      "[D loss: 0.994162] [G loss: 1.290416]\n",
      "[D loss: 0.850778] [G loss: 1.427482]\n",
      "[D loss: 0.817510] [G loss: 1.345931]\n",
      "[D loss: 1.040785] [G loss: 1.689748]\n",
      "[D loss: 0.999491] [G loss: 1.722995]\n",
      "[D loss: 0.890634] [G loss: 1.226155]\n",
      "[D loss: 0.983150] [G loss: 1.222430]\n",
      "[D loss: 0.838127] [G loss: 1.207124]\n",
      "[D loss: 0.935449] [G loss: 1.267557]\n",
      "[D loss: 0.792625] [G loss: 1.492384]\n",
      "[D loss: 0.749037] [G loss: 1.448442]\n",
      "[D loss: 0.965198] [G loss: 1.563305]\n",
      "[D loss: 0.794702] [G loss: 1.599338]\n",
      "[D loss: 0.749770] [G loss: 1.339253]\n",
      "[D loss: 1.063434] [G loss: 1.410857]\n",
      "[D loss: 0.931233] [G loss: 1.295419]\n",
      "[D loss: 0.983726] [G loss: 1.215293]\n",
      "[D loss: 0.796764] [G loss: 1.440475]\n",
      "[D loss: 0.840571] [G loss: 1.389823]\n",
      "[D loss: 0.800271] [G loss: 1.430118]\n",
      "[D loss: 0.823000] [G loss: 1.571425]\n",
      "[D loss: 0.757369] [G loss: 1.438355]\n",
      "[D loss: 0.676719] [G loss: 1.358578]\n",
      "[D loss: 0.869960] [G loss: 1.699375]\n",
      "[D loss: 0.736252] [G loss: 1.416807]\n",
      "[D loss: 1.010617] [G loss: 1.428834]\n",
      "[D loss: 0.797811] [G loss: 1.663593]\n",
      "[D loss: 0.812992] [G loss: 1.260964]\n",
      "[D loss: 0.849860] [G loss: 1.419626]\n",
      "[D loss: 0.892154] [G loss: 1.513022]\n",
      "[D loss: 0.889854] [G loss: 1.261078]\n",
      "[D loss: 0.747980] [G loss: 1.397376]\n",
      "[D loss: 0.887744] [G loss: 1.581287]\n",
      "[D loss: 0.665296] [G loss: 1.666647]\n",
      "[D loss: 0.962352] [G loss: 1.396038]\n",
      "[D loss: 0.894874] [G loss: 1.635727]\n",
      "[D loss: 0.921174] [G loss: 1.400183]\n",
      "[D loss: 0.943176] [G loss: 1.248146]\n",
      "[D loss: 0.840271] [G loss: 1.404156]\n",
      "[D loss: 0.862310] [G loss: 1.231024]\n",
      "[D loss: 0.826588] [G loss: 1.501508]\n",
      "[D loss: 0.718626] [G loss: 1.383962]\n",
      "[D loss: 0.831516] [G loss: 1.178699]\n",
      "[D loss: 0.905800] [G loss: 1.355029]\n",
      "[D loss: 0.755998] [G loss: 1.477678]\n",
      "[D loss: 0.851187] [G loss: 1.713961]\n",
      "[D loss: 0.639095] [G loss: 1.473325]\n",
      "[D loss: 0.837732] [G loss: 1.422317]\n",
      "[D loss: 0.879575] [G loss: 1.418174]\n",
      "[D loss: 1.203687] [G loss: 1.449299]\n",
      "[D loss: 0.801879] [G loss: 1.409325]\n",
      "[D loss: 0.995699] [G loss: 1.413229]\n",
      "[D loss: 0.981282] [G loss: 1.433284]\n",
      "[D loss: 0.793400] [G loss: 1.407824]\n",
      "[D loss: 0.859142] [G loss: 1.418344]\n",
      "[D loss: 0.855180] [G loss: 1.397652]\n",
      "[D loss: 0.736961] [G loss: 1.584173]\n",
      "[D loss: 0.884061] [G loss: 1.485769]\n",
      "[D loss: 0.843036] [G loss: 1.461446]\n",
      "[D loss: 0.989794] [G loss: 1.413238]\n",
      "[D loss: 0.787114] [G loss: 1.527048]\n",
      "[D loss: 1.069693] [G loss: 1.490865]\n",
      "[D loss: 0.796258] [G loss: 1.432055]\n",
      "[D loss: 0.880552] [G loss: 1.452547]\n",
      "[D loss: 0.909827] [G loss: 1.346370]\n",
      "[D loss: 0.624510] [G loss: 1.808171]\n",
      "[D loss: 0.783358] [G loss: 1.448134]\n",
      "[D loss: 0.762521] [G loss: 1.300981]\n",
      "[D loss: 0.914033] [G loss: 1.371785]\n",
      "[D loss: 0.823536] [G loss: 1.637978]\n",
      "[D loss: 0.972632] [G loss: 1.514429]\n",
      "[D loss: 0.817720] [G loss: 1.342185]\n",
      "[D loss: 0.715129] [G loss: 1.478015]\n",
      "[D loss: 1.072085] [G loss: 1.312537]\n",
      "[D loss: 0.912109] [G loss: 1.427093]\n",
      "[D loss: 0.906232] [G loss: 1.317412]\n",
      "[D loss: 1.031003] [G loss: 1.296658]\n",
      "[D loss: 0.883882] [G loss: 1.303192]\n",
      "[D loss: 0.890956] [G loss: 1.151915]\n",
      "[D loss: 0.698766] [G loss: 1.416152]\n",
      "[D loss: 0.827295] [G loss: 1.430551]\n",
      "[D loss: 0.710457] [G loss: 1.397182]\n",
      "[D loss: 0.670162] [G loss: 1.461949]\n",
      "[D loss: 0.640450] [G loss: 1.447917]\n",
      "[D loss: 0.666222] [G loss: 1.548225]\n",
      "[D loss: 0.772145] [G loss: 1.435648]\n",
      "[D loss: 0.938049] [G loss: 1.308697]\n",
      "[D loss: 0.810501] [G loss: 1.431806]\n",
      "[D loss: 0.796958] [G loss: 1.789404]\n",
      "[D loss: 0.877394] [G loss: 1.468716]\n",
      "[D loss: 0.810285] [G loss: 1.505509]\n",
      "[D loss: 0.642694] [G loss: 1.533455]\n",
      "[D loss: 0.819628] [G loss: 1.331565]\n",
      "[D loss: 0.744118] [G loss: 1.447865]\n",
      "[D loss: 0.937953] [G loss: 1.593704]\n",
      "[D loss: 0.778046] [G loss: 1.632756]\n",
      "[D loss: 0.580591] [G loss: 1.542143]\n",
      "[D loss: 0.759756] [G loss: 1.699451]\n",
      "[D loss: 0.599733] [G loss: 1.534460]\n",
      "[D loss: 0.856685] [G loss: 1.559526]\n",
      "[D loss: 0.976917] [G loss: 1.309836]\n",
      "[D loss: 1.076387] [G loss: 1.292579]\n",
      "[D loss: 0.693809] [G loss: 1.476082]\n",
      "[D loss: 0.887996] [G loss: 1.364564]\n",
      "[D loss: 0.949842] [G loss: 1.389537]\n",
      "[D loss: 0.856555] [G loss: 1.368449]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.825017] [G loss: 1.328083]\n",
      "[D loss: 0.755586] [G loss: 1.586114]\n",
      "[D loss: 0.812524] [G loss: 1.346651]\n",
      "[D loss: 0.975133] [G loss: 1.359609]\n",
      "[D loss: 0.936815] [G loss: 1.446029]\n",
      "[D loss: 0.761881] [G loss: 1.423152]\n",
      "[D loss: 0.837091] [G loss: 1.711090]\n",
      "[D loss: 0.789865] [G loss: 1.627658]\n",
      "[D loss: 1.012586] [G loss: 1.497662]\n",
      "[D loss: 0.802952] [G loss: 1.476505]\n",
      "[D loss: 0.731543] [G loss: 1.572808]\n",
      "[D loss: 0.651577] [G loss: 1.576078]\n",
      "[D loss: 0.719632] [G loss: 1.655639]\n",
      "[D loss: 0.986003] [G loss: 1.689251]\n",
      "[D loss: 0.651330] [G loss: 1.291333]\n",
      "[D loss: 0.992835] [G loss: 1.120795]\n",
      "[D loss: 1.079886] [G loss: 1.263882]\n",
      "[D loss: 1.003200] [G loss: 1.329312]\n",
      "[D loss: 0.688305] [G loss: 1.777398]\n",
      "[D loss: 0.870030] [G loss: 1.709403]\n",
      "[D loss: 0.897940] [G loss: 1.530213]\n",
      "[D loss: 0.666543] [G loss: 1.466111]\n",
      "[D loss: 0.943776] [G loss: 1.368786]\n",
      "[D loss: 0.805206] [G loss: 1.307697]\n",
      "[D loss: 0.895095] [G loss: 1.474866]\n",
      "[D loss: 0.970285] [G loss: 1.578998]\n",
      "[D loss: 0.809291] [G loss: 1.520161]\n",
      "[D loss: 0.867218] [G loss: 1.607632]\n",
      "[D loss: 0.740362] [G loss: 1.451919]\n",
      "[D loss: 1.001429] [G loss: 1.431857]\n",
      "[D loss: 0.832652] [G loss: 1.325505]\n",
      "[D loss: 0.855964] [G loss: 1.394753]\n",
      "[D loss: 0.693069] [G loss: 1.544943]\n",
      "[D loss: 0.953604] [G loss: 1.361934]\n",
      "[D loss: 1.008115] [G loss: 1.365000]\n",
      "[D loss: 0.888277] [G loss: 1.408404]\n",
      "[D loss: 0.831199] [G loss: 1.665325]\n",
      "[D loss: 0.817286] [G loss: 1.455066]\n",
      "[D loss: 0.901694] [G loss: 1.361688]\n",
      "[D loss: 0.824742] [G loss: 1.449252]\n",
      "[D loss: 0.904317] [G loss: 1.309091]\n",
      "[D loss: 0.741172] [G loss: 1.377655]\n",
      "[D loss: 0.805676] [G loss: 1.613602]\n",
      "[D loss: 0.862402] [G loss: 1.393218]\n",
      "[D loss: 0.792368] [G loss: 1.478370]\n",
      "[D loss: 0.895408] [G loss: 1.524204]\n",
      "[D loss: 0.882968] [G loss: 1.200124]\n",
      "[D loss: 0.909692] [G loss: 1.340725]\n",
      "[D loss: 0.794568] [G loss: 1.550143]\n",
      "[D loss: 0.836048] [G loss: 1.492280]\n",
      "[D loss: 0.967947] [G loss: 1.284044]\n",
      "[D loss: 0.882482] [G loss: 1.338566]\n",
      "[D loss: 0.969714] [G loss: 1.568367]\n",
      "[D loss: 0.888851] [G loss: 1.478054]\n",
      "[D loss: 0.726131] [G loss: 1.299174]\n",
      "[D loss: 0.867074] [G loss: 1.256636]\n",
      "[D loss: 0.895623] [G loss: 1.513949]\n",
      "[D loss: 0.881708] [G loss: 1.341045]\n",
      "[D loss: 0.833364] [G loss: 1.413292]\n",
      "[D loss: 1.148401] [G loss: 1.452421]\n",
      "[D loss: 0.637992] [G loss: 1.556109]\n",
      "[D loss: 0.958179] [G loss: 1.362571]\n",
      "[D loss: 1.151189] [G loss: 1.309813]\n",
      "[D loss: 0.829982] [G loss: 1.429356]\n",
      "[D loss: 0.864911] [G loss: 1.317431]\n",
      "[D loss: 0.800337] [G loss: 1.399595]\n",
      "[D loss: 1.151804] [G loss: 1.317229]\n",
      "[D loss: 0.895595] [G loss: 1.402453]\n",
      "[D loss: 0.793467] [G loss: 1.455316]\n",
      "[D loss: 0.777056] [G loss: 1.528255]\n",
      "[D loss: 1.047250] [G loss: 1.315008]\n",
      "[D loss: 0.924425] [G loss: 1.264551]\n",
      "[D loss: 0.946801] [G loss: 1.405717]\n",
      "[D loss: 0.775289] [G loss: 1.318168]\n",
      "[D loss: 0.940813] [G loss: 1.317402]\n",
      "[D loss: 1.081105] [G loss: 1.221939]\n",
      "[D loss: 0.830408] [G loss: 1.371626]\n",
      "[D loss: 0.764201] [G loss: 1.428809]\n",
      "[D loss: 1.076390] [G loss: 1.191586]\n",
      "[D loss: 0.941426] [G loss: 1.464923]\n",
      "[D loss: 0.829722] [G loss: 1.123669]\n",
      "[D loss: 0.793605] [G loss: 1.408023]\n",
      "[D loss: 0.886187] [G loss: 1.361044]\n",
      "[D loss: 0.623282] [G loss: 1.591876]\n",
      "[D loss: 1.100494] [G loss: 1.231759]\n",
      "[D loss: 0.734208] [G loss: 1.565530]\n",
      "[D loss: 0.762108] [G loss: 1.430075]\n",
      "[D loss: 0.730537] [G loss: 1.291785]\n",
      "[D loss: 0.701871] [G loss: 1.543225]\n",
      "[D loss: 1.075254] [G loss: 1.231542]\n",
      "[D loss: 0.770633] [G loss: 1.423726]\n",
      "[D loss: 0.749331] [G loss: 1.410505]\n",
      "[D loss: 0.855748] [G loss: 1.363344]\n",
      "[D loss: 0.574090] [G loss: 1.576703]\n",
      "[D loss: 0.830039] [G loss: 1.413055]\n",
      "[D loss: 0.853914] [G loss: 1.272505]\n",
      "[D loss: 0.724327] [G loss: 1.599246]\n",
      "[D loss: 0.602756] [G loss: 1.432127]\n",
      "[D loss: 0.847854] [G loss: 1.379818]\n",
      "[D loss: 0.780438] [G loss: 1.310699]\n",
      "[D loss: 0.813888] [G loss: 1.644840]\n",
      "[D loss: 0.569582] [G loss: 1.608309]\n",
      "[D loss: 0.726590] [G loss: 1.654675]\n",
      "[D loss: 0.884018] [G loss: 1.533037]\n",
      "[D loss: 0.941342] [G loss: 1.317468]\n",
      "[D loss: 0.864458] [G loss: 1.420875]\n",
      "[D loss: 0.851630] [G loss: 1.143196]\n",
      "[D loss: 0.857586] [G loss: 1.488039]\n",
      "[D loss: 0.845159] [G loss: 1.618919]\n",
      "[D loss: 0.974035] [G loss: 1.638386]\n",
      "[D loss: 0.767208] [G loss: 1.711432]\n",
      "[D loss: 0.755804] [G loss: 1.557103]\n",
      "[D loss: 0.942025] [G loss: 1.366901]\n",
      "[D loss: 0.837053] [G loss: 1.414041]\n",
      "[D loss: 0.952481] [G loss: 1.204878]\n",
      "[D loss: 1.027516] [G loss: 1.451730]\n",
      "[D loss: 1.050165] [G loss: 1.396455]\n",
      "[D loss: 0.842491] [G loss: 1.513304]\n",
      "[D loss: 0.762537] [G loss: 1.443833]\n",
      "[D loss: 0.852622] [G loss: 1.403499]\n",
      "[D loss: 1.127345] [G loss: 1.470105]\n",
      "[D loss: 0.909841] [G loss: 1.308590]\n",
      "[D loss: 1.052251] [G loss: 1.293742]\n",
      "[D loss: 0.744948] [G loss: 1.454907]\n",
      "[D loss: 0.762028] [G loss: 1.283384]\n",
      "[D loss: 0.750590] [G loss: 1.349687]\n",
      "[D loss: 0.748106] [G loss: 1.478485]\n",
      "[D loss: 0.548162] [G loss: 1.476590]\n",
      "[D loss: 1.112642] [G loss: 1.391422]\n",
      "[D loss: 0.806974] [G loss: 1.518844]\n",
      "[D loss: 0.805199] [G loss: 1.593550]\n",
      "[D loss: 0.866328] [G loss: 1.317530]\n",
      "[D loss: 0.932726] [G loss: 1.473946]\n",
      "[D loss: 0.815136] [G loss: 1.503812]\n",
      "[D loss: 0.833632] [G loss: 1.403162]\n",
      "[D loss: 0.857090] [G loss: 1.452731]\n",
      "[D loss: 0.851553] [G loss: 1.602802]\n",
      "[D loss: 1.056943] [G loss: 1.404639]\n",
      "[D loss: 0.860427] [G loss: 1.433371]\n",
      "[D loss: 0.822253] [G loss: 1.254488]\n",
      "[D loss: 1.022889] [G loss: 1.465872]\n",
      "[D loss: 1.063497] [G loss: 1.323008]\n",
      "[D loss: 0.775943] [G loss: 1.323948]\n",
      "[D loss: 0.872009] [G loss: 1.374188]\n",
      "[D loss: 0.780994] [G loss: 1.383012]\n",
      "[D loss: 0.863388] [G loss: 1.213893]\n",
      "[D loss: 0.854914] [G loss: 1.394166]\n",
      "[D loss: 0.857170] [G loss: 1.489026]\n",
      "[D loss: 0.721879] [G loss: 1.447969]\n",
      "[D loss: 0.866892] [G loss: 1.477699]\n",
      "[D loss: 0.931905] [G loss: 1.553154]\n",
      "[D loss: 0.801375] [G loss: 1.675364]\n",
      "[D loss: 0.758585] [G loss: 1.420036]\n",
      "[D loss: 0.961034] [G loss: 1.446030]\n",
      "[D loss: 0.937579] [G loss: 1.515515]\n",
      "[D loss: 0.991418] [G loss: 1.231166]\n",
      "[D loss: 0.960337] [G loss: 1.178382]\n",
      "[D loss: 0.671787] [G loss: 1.346497]\n",
      "[D loss: 0.859333] [G loss: 1.475252]\n",
      "[D loss: 0.898370] [G loss: 1.452985]\n",
      "[D loss: 0.895189] [G loss: 1.623279]\n",
      "[D loss: 0.817186] [G loss: 1.560525]\n",
      "[D loss: 0.953347] [G loss: 1.286366]\n",
      "[D loss: 0.952983] [G loss: 1.420603]\n",
      "[D loss: 0.911460] [G loss: 1.326564]\n",
      "[D loss: 0.649741] [G loss: 1.408563]\n",
      "[D loss: 0.657982] [G loss: 1.541118]\n",
      "[D loss: 0.804836] [G loss: 1.683233]\n",
      "[D loss: 0.894915] [G loss: 1.396069]\n",
      "[D loss: 0.794954] [G loss: 1.404719]\n",
      "[D loss: 0.803470] [G loss: 1.469683]\n",
      "[D loss: 0.835825] [G loss: 1.425007]\n",
      "[D loss: 0.883765] [G loss: 1.304166]\n",
      "[D loss: 0.740520] [G loss: 1.429307]\n",
      "[D loss: 0.807115] [G loss: 1.527523]\n",
      "[D loss: 0.769403] [G loss: 1.381417]\n",
      "[D loss: 0.874706] [G loss: 1.297540]\n",
      "[D loss: 0.998495] [G loss: 1.116835]\n",
      "[D loss: 0.649173] [G loss: 1.551400]\n",
      "[D loss: 0.647272] [G loss: 1.637734]\n",
      "[D loss: 1.142681] [G loss: 1.546856]\n",
      "[D loss: 0.823773] [G loss: 1.606546]\n",
      "[D loss: 0.823193] [G loss: 1.563145]\n",
      "[D loss: 0.755765] [G loss: 1.582438]\n",
      "[D loss: 0.886703] [G loss: 1.295545]\n",
      "[D loss: 0.697154] [G loss: 1.644493]\n",
      "[D loss: 0.889237] [G loss: 1.474776]\n",
      "[D loss: 0.779426] [G loss: 1.497469]\n",
      "[D loss: 0.898057] [G loss: 1.229226]\n",
      "[D loss: 0.845089] [G loss: 1.426699]\n",
      "[D loss: 0.783800] [G loss: 1.358544]\n",
      "[D loss: 0.796950] [G loss: 1.580243]\n",
      "[D loss: 0.797850] [G loss: 1.587580]\n",
      "[D loss: 0.744223] [G loss: 1.297993]\n",
      "[D loss: 1.172161] [G loss: 1.300287]\n",
      "[D loss: 0.766797] [G loss: 1.566474]\n",
      "[D loss: 0.647878] [G loss: 1.412068]\n",
      "[D loss: 0.961948] [G loss: 1.500465]\n",
      "[D loss: 0.910939] [G loss: 1.442439]\n",
      "[D loss: 0.793964] [G loss: 1.300702]\n",
      "[D loss: 0.948958] [G loss: 1.413993]\n",
      "[D loss: 0.796627] [G loss: 1.470875]\n",
      "[D loss: 0.938825] [G loss: 1.373982]\n",
      "[D loss: 0.786633] [G loss: 1.364885]\n",
      "[D loss: 0.760147] [G loss: 1.414015]\n",
      "[D loss: 1.053488] [G loss: 1.326273]\n",
      "[D loss: 0.868972] [G loss: 1.692826]\n",
      "[D loss: 0.969982] [G loss: 1.540105]\n",
      "[D loss: 0.714582] [G loss: 1.316790]\n",
      "[D loss: 0.696959] [G loss: 1.468305]\n",
      "[D loss: 0.923968] [G loss: 1.479812]\n",
      "[D loss: 0.627451] [G loss: 1.598141]\n",
      "[D loss: 1.067404] [G loss: 1.629175]\n",
      "[D loss: 0.956758] [G loss: 1.468996]\n",
      "[D loss: 0.887174] [G loss: 1.608799]\n",
      "[D loss: 0.848368] [G loss: 1.259964]\n",
      "[D loss: 0.754013] [G loss: 1.348788]\n",
      "[D loss: 0.912768] [G loss: 1.557954]\n",
      "[D loss: 0.850417] [G loss: 1.474433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.676579] [G loss: 1.598662]\n",
      "[D loss: 0.830229] [G loss: 1.378039]\n",
      "[D loss: 0.734857] [G loss: 1.519942]\n",
      "[D loss: 0.602315] [G loss: 1.710141]\n",
      "[D loss: 0.874344] [G loss: 1.307217]\n",
      "[D loss: 0.963368] [G loss: 1.443558]\n",
      "[D loss: 1.008874] [G loss: 1.364197]\n",
      "[D loss: 0.931306] [G loss: 1.484567]\n",
      "[D loss: 0.981221] [G loss: 1.449001]\n",
      "[D loss: 0.827110] [G loss: 1.432831]\n",
      "[D loss: 0.904557] [G loss: 1.656246]\n",
      "[D loss: 0.747332] [G loss: 1.460176]\n",
      "[D loss: 1.014400] [G loss: 1.346226]\n",
      "[D loss: 0.734468] [G loss: 1.537713]\n",
      "[D loss: 0.853477] [G loss: 1.402503]\n",
      "[D loss: 0.855843] [G loss: 1.431625]\n",
      "[D loss: 0.670154] [G loss: 1.363095]\n",
      "[D loss: 0.483991] [G loss: 1.687657]\n",
      "[D loss: 0.794553] [G loss: 1.411238]\n",
      "[D loss: 0.851246] [G loss: 1.490114]\n",
      "epoch:18, g_loss:2732.27001953125,d_loss:1573.06005859375\n",
      "[D loss: 0.786702] [G loss: 1.462483]\n",
      "[D loss: 0.613135] [G loss: 1.407010]\n",
      "[D loss: 0.705486] [G loss: 1.429560]\n",
      "[D loss: 1.003443] [G loss: 1.491048]\n",
      "[D loss: 0.891257] [G loss: 1.528697]\n",
      "[D loss: 0.629593] [G loss: 1.690191]\n",
      "[D loss: 0.658596] [G loss: 1.665909]\n",
      "[D loss: 0.904819] [G loss: 1.517275]\n",
      "[D loss: 0.826433] [G loss: 1.484897]\n",
      "[D loss: 0.806752] [G loss: 1.442717]\n",
      "[D loss: 0.844775] [G loss: 1.492093]\n",
      "[D loss: 0.722416] [G loss: 1.622115]\n",
      "[D loss: 0.912741] [G loss: 1.604380]\n",
      "[D loss: 0.851037] [G loss: 1.386986]\n",
      "[D loss: 0.814692] [G loss: 1.482240]\n",
      "[D loss: 0.917493] [G loss: 1.539361]\n",
      "[D loss: 0.930418] [G loss: 1.318286]\n",
      "[D loss: 0.710346] [G loss: 1.326152]\n",
      "[D loss: 0.832644] [G loss: 1.324130]\n",
      "[D loss: 0.992814] [G loss: 1.773294]\n",
      "[D loss: 0.829454] [G loss: 1.535533]\n",
      "[D loss: 0.988029] [G loss: 1.410227]\n",
      "[D loss: 0.802437] [G loss: 1.186421]\n",
      "[D loss: 1.031781] [G loss: 1.248635]\n",
      "[D loss: 0.886710] [G loss: 1.394606]\n",
      "[D loss: 0.711340] [G loss: 1.596360]\n",
      "[D loss: 0.852658] [G loss: 1.440674]\n",
      "[D loss: 0.958200] [G loss: 1.281933]\n",
      "[D loss: 1.089458] [G loss: 1.279644]\n",
      "[D loss: 0.816664] [G loss: 1.346230]\n",
      "[D loss: 0.681314] [G loss: 1.408008]\n",
      "[D loss: 0.835668] [G loss: 1.396934]\n",
      "[D loss: 0.832758] [G loss: 1.289389]\n",
      "[D loss: 0.796259] [G loss: 1.334238]\n",
      "[D loss: 0.993852] [G loss: 1.348228]\n",
      "[D loss: 0.895552] [G loss: 1.437523]\n",
      "[D loss: 0.866505] [G loss: 1.259946]\n",
      "[D loss: 0.791660] [G loss: 1.237989]\n",
      "[D loss: 0.729511] [G loss: 1.643374]\n",
      "[D loss: 0.801612] [G loss: 1.584232]\n",
      "[D loss: 0.888209] [G loss: 1.241820]\n",
      "[D loss: 0.995107] [G loss: 1.315838]\n",
      "[D loss: 0.811826] [G loss: 1.441506]\n",
      "[D loss: 0.502905] [G loss: 1.532612]\n",
      "[D loss: 0.953592] [G loss: 1.658263]\n",
      "[D loss: 0.816611] [G loss: 1.518288]\n",
      "[D loss: 1.006457] [G loss: 1.450784]\n",
      "[D loss: 0.622745] [G loss: 1.764870]\n",
      "[D loss: 0.839817] [G loss: 1.573711]\n",
      "[D loss: 0.937235] [G loss: 1.627123]\n",
      "[D loss: 0.783721] [G loss: 1.394375]\n",
      "[D loss: 0.901463] [G loss: 1.440661]\n",
      "[D loss: 0.761880] [G loss: 1.363376]\n",
      "[D loss: 0.955314] [G loss: 1.478504]\n",
      "[D loss: 0.677328] [G loss: 1.656385]\n",
      "[D loss: 0.714481] [G loss: 1.655131]\n",
      "[D loss: 1.038999] [G loss: 1.382792]\n",
      "[D loss: 0.678462] [G loss: 1.532148]\n",
      "[D loss: 0.590130] [G loss: 1.377767]\n",
      "[D loss: 0.879609] [G loss: 1.358699]\n",
      "[D loss: 0.621169] [G loss: 1.781412]\n",
      "[D loss: 0.674432] [G loss: 1.640562]\n",
      "[D loss: 0.983366] [G loss: 1.468231]\n",
      "[D loss: 0.637274] [G loss: 1.723240]\n",
      "[D loss: 0.763942] [G loss: 1.518852]\n",
      "[D loss: 0.603246] [G loss: 1.819219]\n",
      "[D loss: 0.699549] [G loss: 1.673625]\n",
      "[D loss: 1.034846] [G loss: 1.457907]\n",
      "[D loss: 0.564103] [G loss: 1.557112]\n",
      "[D loss: 1.090007] [G loss: 1.470861]\n",
      "[D loss: 0.874672] [G loss: 1.654719]\n",
      "[D loss: 0.629210] [G loss: 1.517734]\n",
      "[D loss: 0.649287] [G loss: 1.470871]\n",
      "[D loss: 0.716452] [G loss: 1.537579]\n",
      "[D loss: 0.624959] [G loss: 1.645027]\n",
      "[D loss: 0.998699] [G loss: 1.399875]\n",
      "[D loss: 0.881727] [G loss: 1.560513]\n",
      "[D loss: 0.761681] [G loss: 1.567714]\n",
      "[D loss: 0.657176] [G loss: 1.523823]\n",
      "[D loss: 0.687134] [G loss: 1.575542]\n",
      "[D loss: 0.655661] [G loss: 1.792362]\n",
      "[D loss: 0.880745] [G loss: 1.718681]\n",
      "[D loss: 0.986644] [G loss: 1.474561]\n",
      "[D loss: 1.069300] [G loss: 1.527963]\n",
      "[D loss: 0.550357] [G loss: 1.526608]\n",
      "[D loss: 0.886930] [G loss: 1.350997]\n",
      "[D loss: 0.810940] [G loss: 1.728174]\n",
      "[D loss: 1.038768] [G loss: 1.476137]\n",
      "[D loss: 1.028224] [G loss: 1.577932]\n",
      "[D loss: 0.806072] [G loss: 1.650736]\n",
      "[D loss: 0.653268] [G loss: 1.431971]\n",
      "[D loss: 1.020786] [G loss: 1.410838]\n",
      "[D loss: 0.822021] [G loss: 1.355410]\n",
      "[D loss: 0.630142] [G loss: 1.516143]\n",
      "[D loss: 0.860595] [G loss: 1.383349]\n",
      "[D loss: 0.982831] [G loss: 1.521810]\n",
      "[D loss: 0.725337] [G loss: 1.467583]\n",
      "[D loss: 0.725389] [G loss: 1.436908]\n",
      "[D loss: 0.990490] [G loss: 1.348616]\n",
      "[D loss: 0.813033] [G loss: 1.485483]\n",
      "[D loss: 0.843850] [G loss: 1.842694]\n",
      "[D loss: 0.977074] [G loss: 1.468589]\n",
      "[D loss: 0.910555] [G loss: 1.404866]\n",
      "[D loss: 0.901990] [G loss: 1.408831]\n",
      "[D loss: 0.771325] [G loss: 1.475317]\n",
      "[D loss: 0.850837] [G loss: 1.560559]\n",
      "[D loss: 0.841397] [G loss: 1.351446]\n",
      "[D loss: 0.945397] [G loss: 1.305612]\n",
      "[D loss: 0.683385] [G loss: 1.583743]\n",
      "[D loss: 0.872083] [G loss: 1.428563]\n",
      "[D loss: 0.761014] [G loss: 1.491245]\n",
      "[D loss: 0.713743] [G loss: 1.606073]\n",
      "[D loss: 0.768773] [G loss: 1.631631]\n",
      "[D loss: 0.672173] [G loss: 1.487354]\n",
      "[D loss: 0.625813] [G loss: 1.495338]\n",
      "[D loss: 0.833684] [G loss: 1.438438]\n",
      "[D loss: 0.919308] [G loss: 1.517943]\n",
      "[D loss: 0.807932] [G loss: 1.534943]\n",
      "[D loss: 0.618712] [G loss: 1.551405]\n",
      "[D loss: 0.945390] [G loss: 1.521856]\n",
      "[D loss: 0.804994] [G loss: 1.507781]\n",
      "[D loss: 0.869568] [G loss: 1.331359]\n",
      "[D loss: 1.043975] [G loss: 1.555200]\n",
      "[D loss: 0.620654] [G loss: 1.518772]\n",
      "[D loss: 0.951373] [G loss: 1.336860]\n",
      "[D loss: 0.881256] [G loss: 1.268670]\n",
      "[D loss: 0.848015] [G loss: 1.310071]\n",
      "[D loss: 0.765563] [G loss: 1.463235]\n",
      "[D loss: 0.805474] [G loss: 1.476071]\n",
      "[D loss: 0.602382] [G loss: 1.378994]\n",
      "[D loss: 0.819916] [G loss: 1.384902]\n",
      "[D loss: 0.808695] [G loss: 1.496723]\n",
      "[D loss: 0.817710] [G loss: 1.238700]\n",
      "[D loss: 0.949624] [G loss: 1.676465]\n",
      "[D loss: 0.780130] [G loss: 1.414486]\n",
      "[D loss: 0.721069] [G loss: 1.505851]\n",
      "[D loss: 0.984765] [G loss: 1.846800]\n",
      "[D loss: 0.929216] [G loss: 1.620409]\n",
      "[D loss: 0.853876] [G loss: 1.392373]\n",
      "[D loss: 0.769828] [G loss: 1.359018]\n",
      "[D loss: 0.885135] [G loss: 1.529248]\n",
      "[D loss: 0.752779] [G loss: 1.491314]\n",
      "[D loss: 0.802266] [G loss: 1.528477]\n",
      "[D loss: 0.955023] [G loss: 1.597642]\n",
      "[D loss: 1.008427] [G loss: 1.193066]\n",
      "[D loss: 0.844415] [G loss: 1.652968]\n",
      "[D loss: 0.570659] [G loss: 1.650013]\n",
      "[D loss: 0.680611] [G loss: 1.448589]\n",
      "[D loss: 0.665973] [G loss: 1.450740]\n",
      "[D loss: 0.650942] [G loss: 1.571681]\n",
      "[D loss: 1.029377] [G loss: 1.495049]\n",
      "[D loss: 0.748754] [G loss: 1.579773]\n",
      "[D loss: 0.871319] [G loss: 1.536727]\n",
      "[D loss: 0.841685] [G loss: 1.631471]\n",
      "[D loss: 0.962120] [G loss: 1.262470]\n",
      "[D loss: 0.778540] [G loss: 1.412584]\n",
      "[D loss: 0.648443] [G loss: 1.499894]\n",
      "[D loss: 0.839181] [G loss: 1.493309]\n",
      "[D loss: 0.917224] [G loss: 1.474669]\n",
      "[D loss: 0.920743] [G loss: 1.746935]\n",
      "[D loss: 0.853865] [G loss: 1.419132]\n",
      "[D loss: 0.920541] [G loss: 1.505574]\n",
      "[D loss: 0.739105] [G loss: 1.638929]\n",
      "[D loss: 0.883482] [G loss: 1.476894]\n",
      "[D loss: 0.603946] [G loss: 1.599877]\n",
      "[D loss: 0.886245] [G loss: 1.441741]\n",
      "[D loss: 0.841803] [G loss: 1.343859]\n",
      "[D loss: 0.805557] [G loss: 1.297392]\n",
      "[D loss: 0.904979] [G loss: 1.395261]\n",
      "[D loss: 0.868556] [G loss: 1.579793]\n",
      "[D loss: 0.896516] [G loss: 1.415452]\n",
      "[D loss: 0.715050] [G loss: 1.286311]\n",
      "[D loss: 0.788061] [G loss: 1.305123]\n",
      "[D loss: 0.849837] [G loss: 1.376001]\n",
      "[D loss: 0.815846] [G loss: 1.744812]\n",
      "[D loss: 0.757966] [G loss: 1.419962]\n",
      "[D loss: 0.893272] [G loss: 1.319179]\n",
      "[D loss: 1.107970] [G loss: 1.122236]\n",
      "[D loss: 0.827644] [G loss: 1.365774]\n",
      "[D loss: 0.942617] [G loss: 1.436355]\n",
      "[D loss: 0.640879] [G loss: 1.924818]\n",
      "[D loss: 0.797391] [G loss: 1.582473]\n",
      "[D loss: 0.815730] [G loss: 1.434737]\n",
      "[D loss: 0.918579] [G loss: 1.661029]\n",
      "[D loss: 0.659649] [G loss: 1.459933]\n",
      "[D loss: 0.755697] [G loss: 1.262750]\n",
      "[D loss: 0.797090] [G loss: 1.395818]\n",
      "[D loss: 0.712860] [G loss: 1.482433]\n",
      "[D loss: 0.954538] [G loss: 1.420203]\n",
      "[D loss: 1.143752] [G loss: 1.312602]\n",
      "[D loss: 1.039886] [G loss: 1.349452]\n",
      "[D loss: 0.762476] [G loss: 1.357407]\n",
      "[D loss: 0.729880] [G loss: 1.361453]\n",
      "[D loss: 0.728658] [G loss: 1.414798]\n",
      "[D loss: 0.781790] [G loss: 1.433473]\n",
      "[D loss: 0.971273] [G loss: 1.421225]\n",
      "[D loss: 0.856377] [G loss: 1.292511]\n",
      "[D loss: 0.658392] [G loss: 1.494137]\n",
      "[D loss: 0.842323] [G loss: 1.566905]\n",
      "[D loss: 0.958071] [G loss: 1.418004]\n",
      "[D loss: 1.048595] [G loss: 1.339856]\n",
      "[D loss: 0.752045] [G loss: 1.387728]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.858767] [G loss: 1.468004]\n",
      "[D loss: 0.819397] [G loss: 1.697531]\n",
      "[D loss: 0.795170] [G loss: 1.663321]\n",
      "[D loss: 1.019920] [G loss: 1.320774]\n",
      "[D loss: 0.717808] [G loss: 1.518376]\n",
      "[D loss: 0.806099] [G loss: 1.355048]\n",
      "[D loss: 0.814063] [G loss: 1.485282]\n",
      "[D loss: 0.671923] [G loss: 1.508025]\n",
      "[D loss: 0.901665] [G loss: 1.661785]\n",
      "[D loss: 0.918347] [G loss: 1.366900]\n",
      "[D loss: 0.686201] [G loss: 1.519134]\n",
      "[D loss: 0.841932] [G loss: 1.331387]\n",
      "[D loss: 0.731914] [G loss: 1.348658]\n",
      "[D loss: 0.627810] [G loss: 1.549562]\n",
      "[D loss: 0.782528] [G loss: 1.674873]\n",
      "[D loss: 0.739252] [G loss: 1.589478]\n",
      "[D loss: 0.858982] [G loss: 1.703174]\n",
      "[D loss: 0.948310] [G loss: 1.424436]\n",
      "[D loss: 0.710423] [G loss: 1.186635]\n",
      "[D loss: 0.957550] [G loss: 1.362544]\n",
      "[D loss: 0.892817] [G loss: 1.427236]\n",
      "[D loss: 0.882860] [G loss: 1.431291]\n",
      "[D loss: 0.870856] [G loss: 1.800465]\n",
      "[D loss: 0.701813] [G loss: 1.634493]\n",
      "[D loss: 0.884562] [G loss: 1.519816]\n",
      "[D loss: 0.828567] [G loss: 1.469519]\n",
      "[D loss: 0.756376] [G loss: 1.494072]\n",
      "[D loss: 0.925988] [G loss: 1.567704]\n",
      "[D loss: 1.004919] [G loss: 1.385860]\n",
      "[D loss: 0.817670] [G loss: 1.604074]\n",
      "[D loss: 0.786581] [G loss: 1.417898]\n",
      "[D loss: 0.841157] [G loss: 1.611124]\n",
      "[D loss: 0.835564] [G loss: 1.365777]\n",
      "[D loss: 0.605850] [G loss: 1.471617]\n",
      "[D loss: 0.855233] [G loss: 1.460238]\n",
      "[D loss: 0.962239] [G loss: 1.563621]\n",
      "[D loss: 0.997076] [G loss: 1.554225]\n",
      "[D loss: 0.685461] [G loss: 1.870970]\n",
      "[D loss: 0.815122] [G loss: 1.528953]\n",
      "[D loss: 0.665023] [G loss: 1.713396]\n",
      "[D loss: 0.835820] [G loss: 1.329988]\n",
      "[D loss: 0.859080] [G loss: 1.258355]\n",
      "[D loss: 0.896132] [G loss: 1.417489]\n",
      "[D loss: 0.930122] [G loss: 1.445543]\n",
      "[D loss: 0.936500] [G loss: 1.495769]\n",
      "[D loss: 0.945037] [G loss: 1.487876]\n",
      "[D loss: 0.820979] [G loss: 1.354267]\n",
      "[D loss: 1.004565] [G loss: 1.328005]\n",
      "[D loss: 0.801937] [G loss: 1.386724]\n",
      "[D loss: 0.771727] [G loss: 1.605697]\n",
      "[D loss: 0.907865] [G loss: 1.491317]\n",
      "[D loss: 0.747747] [G loss: 1.326444]\n",
      "[D loss: 0.856992] [G loss: 1.469245]\n",
      "[D loss: 0.767363] [G loss: 1.520149]\n",
      "[D loss: 0.781324] [G loss: 1.304818]\n",
      "[D loss: 0.795022] [G loss: 1.447080]\n",
      "[D loss: 0.892416] [G loss: 1.483611]\n",
      "[D loss: 0.741812] [G loss: 1.488976]\n",
      "[D loss: 0.966368] [G loss: 1.712424]\n",
      "[D loss: 0.895173] [G loss: 1.367367]\n",
      "[D loss: 0.755798] [G loss: 1.500840]\n",
      "[D loss: 0.913884] [G loss: 1.252946]\n",
      "[D loss: 1.019006] [G loss: 1.603701]\n",
      "[D loss: 0.831846] [G loss: 1.549687]\n",
      "[D loss: 0.978349] [G loss: 1.607623]\n",
      "[D loss: 0.777570] [G loss: 1.388839]\n",
      "[D loss: 1.037504] [G loss: 1.393468]\n",
      "[D loss: 0.737084] [G loss: 1.355569]\n",
      "[D loss: 0.838691] [G loss: 1.601206]\n",
      "[D loss: 0.770820] [G loss: 1.745867]\n",
      "[D loss: 0.828745] [G loss: 1.404469]\n",
      "[D loss: 0.905083] [G loss: 1.462863]\n",
      "[D loss: 0.869723] [G loss: 1.371052]\n",
      "[D loss: 0.900063] [G loss: 1.650425]\n",
      "[D loss: 0.829707] [G loss: 1.318736]\n",
      "[D loss: 0.924448] [G loss: 1.556013]\n",
      "[D loss: 0.878937] [G loss: 1.361036]\n",
      "[D loss: 0.838060] [G loss: 1.293645]\n",
      "[D loss: 0.716876] [G loss: 1.398368]\n",
      "[D loss: 1.000198] [G loss: 1.409960]\n",
      "[D loss: 0.694494] [G loss: 1.588525]\n",
      "[D loss: 0.728267] [G loss: 1.495039]\n",
      "[D loss: 0.798714] [G loss: 1.634911]\n",
      "[D loss: 0.780678] [G loss: 1.554300]\n",
      "[D loss: 0.745649] [G loss: 1.492762]\n",
      "[D loss: 0.821266] [G loss: 1.605432]\n",
      "[D loss: 0.805567] [G loss: 1.666198]\n",
      "[D loss: 0.714402] [G loss: 1.450742]\n",
      "[D loss: 0.915201] [G loss: 1.306759]\n",
      "[D loss: 0.731360] [G loss: 1.545720]\n",
      "[D loss: 0.710001] [G loss: 1.297675]\n",
      "[D loss: 0.812940] [G loss: 1.664526]\n",
      "[D loss: 0.763643] [G loss: 1.431103]\n",
      "[D loss: 0.901604] [G loss: 1.564778]\n",
      "[D loss: 0.791133] [G loss: 1.514316]\n",
      "[D loss: 0.739401] [G loss: 1.525208]\n",
      "[D loss: 0.937578] [G loss: 1.240829]\n",
      "[D loss: 0.778125] [G loss: 1.542743]\n",
      "[D loss: 0.890065] [G loss: 1.498079]\n",
      "[D loss: 0.527184] [G loss: 1.505020]\n",
      "[D loss: 0.900200] [G loss: 1.366027]\n",
      "[D loss: 0.878927] [G loss: 1.476850]\n",
      "[D loss: 0.857787] [G loss: 1.573036]\n",
      "[D loss: 0.948825] [G loss: 1.381919]\n",
      "[D loss: 0.713348] [G loss: 1.487664]\n",
      "[D loss: 0.724609] [G loss: 1.569022]\n",
      "[D loss: 0.861054] [G loss: 1.672639]\n",
      "[D loss: 0.871478] [G loss: 1.507330]\n",
      "[D loss: 0.748756] [G loss: 1.518469]\n",
      "[D loss: 0.786375] [G loss: 1.339024]\n",
      "[D loss: 0.779733] [G loss: 1.452228]\n",
      "[D loss: 1.015645] [G loss: 1.587042]\n",
      "[D loss: 0.727317] [G loss: 1.259757]\n",
      "[D loss: 0.923066] [G loss: 1.562500]\n",
      "[D loss: 0.701491] [G loss: 1.565089]\n",
      "[D loss: 0.928754] [G loss: 1.293535]\n",
      "[D loss: 0.708856] [G loss: 1.541415]\n",
      "[D loss: 0.898824] [G loss: 1.465223]\n",
      "[D loss: 0.728922] [G loss: 1.477307]\n",
      "[D loss: 0.908838] [G loss: 1.385160]\n",
      "[D loss: 0.774735] [G loss: 1.369520]\n",
      "[D loss: 0.894565] [G loss: 1.667944]\n",
      "[D loss: 0.879116] [G loss: 1.347988]\n",
      "[D loss: 0.823761] [G loss: 1.541035]\n",
      "[D loss: 0.911931] [G loss: 1.437051]\n",
      "[D loss: 0.789637] [G loss: 1.573802]\n",
      "[D loss: 0.839882] [G loss: 1.444679]\n",
      "[D loss: 0.827941] [G loss: 1.458333]\n",
      "[D loss: 0.594611] [G loss: 1.542943]\n",
      "[D loss: 0.848394] [G loss: 1.427176]\n",
      "[D loss: 0.935932] [G loss: 1.308145]\n",
      "[D loss: 0.848321] [G loss: 1.320117]\n",
      "[D loss: 0.678643] [G loss: 1.526362]\n",
      "[D loss: 0.863426] [G loss: 1.482699]\n",
      "[D loss: 0.855165] [G loss: 1.549149]\n",
      "[D loss: 0.838650] [G loss: 1.547361]\n",
      "[D loss: 0.759973] [G loss: 1.663571]\n",
      "[D loss: 0.828416] [G loss: 1.430853]\n",
      "[D loss: 0.898848] [G loss: 1.538110]\n",
      "[D loss: 0.861034] [G loss: 1.429444]\n",
      "[D loss: 0.902226] [G loss: 1.265153]\n",
      "[D loss: 0.802621] [G loss: 1.694615]\n",
      "[D loss: 0.700050] [G loss: 1.389724]\n",
      "[D loss: 0.871317] [G loss: 1.533974]\n",
      "[D loss: 0.911071] [G loss: 1.490375]\n",
      "[D loss: 0.820019] [G loss: 1.444645]\n",
      "[D loss: 0.915462] [G loss: 1.196708]\n",
      "[D loss: 0.695296] [G loss: 1.461373]\n",
      "[D loss: 0.984091] [G loss: 1.304744]\n",
      "[D loss: 0.811064] [G loss: 1.454689]\n",
      "[D loss: 0.841754] [G loss: 1.390692]\n",
      "[D loss: 0.796937] [G loss: 1.544107]\n",
      "[D loss: 0.767101] [G loss: 1.381892]\n",
      "[D loss: 0.828952] [G loss: 1.540276]\n",
      "[D loss: 1.044466] [G loss: 1.361513]\n",
      "[D loss: 0.987217] [G loss: 1.300267]\n",
      "[D loss: 0.816600] [G loss: 1.292075]\n",
      "[D loss: 0.735552] [G loss: 1.409075]\n",
      "[D loss: 0.818289] [G loss: 1.395848]\n",
      "[D loss: 0.919513] [G loss: 1.606236]\n",
      "[D loss: 0.679360] [G loss: 1.552152]\n",
      "[D loss: 0.881863] [G loss: 1.618680]\n",
      "[D loss: 0.777309] [G loss: 1.342397]\n",
      "[D loss: 0.886871] [G loss: 1.542605]\n",
      "[D loss: 0.877455] [G loss: 1.500424]\n",
      "[D loss: 0.716832] [G loss: 1.330118]\n",
      "[D loss: 0.759802] [G loss: 1.516033]\n",
      "[D loss: 0.964530] [G loss: 1.415849]\n",
      "[D loss: 0.710297] [G loss: 1.464453]\n",
      "[D loss: 0.703218] [G loss: 1.567843]\n",
      "[D loss: 0.870607] [G loss: 1.452289]\n",
      "[D loss: 0.625565] [G loss: 1.345751]\n",
      "[D loss: 0.732593] [G loss: 1.351396]\n",
      "[D loss: 0.824055] [G loss: 1.633003]\n",
      "[D loss: 0.820166] [G loss: 1.745351]\n",
      "[D loss: 0.750896] [G loss: 1.782127]\n",
      "[D loss: 0.644745] [G loss: 1.403491]\n",
      "[D loss: 0.880870] [G loss: 1.495413]\n",
      "[D loss: 0.820607] [G loss: 1.546913]\n",
      "[D loss: 0.843588] [G loss: 1.402645]\n",
      "[D loss: 0.866815] [G loss: 1.480944]\n",
      "[D loss: 0.821630] [G loss: 1.397902]\n",
      "[D loss: 0.735074] [G loss: 1.745842]\n",
      "[D loss: 0.732543] [G loss: 1.463175]\n",
      "[D loss: 0.935081] [G loss: 1.475994]\n",
      "[D loss: 0.791789] [G loss: 1.508914]\n",
      "[D loss: 0.696028] [G loss: 1.543141]\n",
      "[D loss: 0.955703] [G loss: 1.453734]\n",
      "[D loss: 0.603873] [G loss: 1.458442]\n",
      "[D loss: 0.650036] [G loss: 1.495541]\n",
      "[D loss: 0.774619] [G loss: 1.325845]\n",
      "[D loss: 0.788460] [G loss: 1.326859]\n",
      "[D loss: 0.845117] [G loss: 1.609938]\n",
      "[D loss: 1.042364] [G loss: 1.390303]\n",
      "[D loss: 1.021213] [G loss: 1.506871]\n",
      "[D loss: 0.910046] [G loss: 1.509420]\n",
      "[D loss: 0.959279] [G loss: 1.513616]\n",
      "[D loss: 0.708349] [G loss: 1.665747]\n",
      "[D loss: 0.794525] [G loss: 1.696473]\n",
      "[D loss: 0.687056] [G loss: 1.528491]\n",
      "[D loss: 1.006251] [G loss: 1.225638]\n",
      "[D loss: 0.915380] [G loss: 1.480581]\n",
      "[D loss: 0.707283] [G loss: 1.422898]\n",
      "[D loss: 0.758783] [G loss: 1.326183]\n",
      "[D loss: 0.875909] [G loss: 1.597448]\n",
      "[D loss: 0.831428] [G loss: 1.548322]\n",
      "[D loss: 0.720390] [G loss: 1.506202]\n",
      "[D loss: 0.930428] [G loss: 1.515122]\n",
      "[D loss: 0.932519] [G loss: 1.235922]\n",
      "[D loss: 0.760954] [G loss: 1.415132]\n",
      "[D loss: 1.061279] [G loss: 1.481937]\n",
      "[D loss: 0.600216] [G loss: 1.671352]\n",
      "[D loss: 0.692876] [G loss: 1.562881]\n",
      "[D loss: 0.665283] [G loss: 1.474293]\n",
      "[D loss: 1.097795] [G loss: 1.290035]\n",
      "[D loss: 0.886922] [G loss: 1.493905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.574392] [G loss: 1.637828]\n",
      "[D loss: 0.968890] [G loss: 1.304731]\n",
      "[D loss: 0.583569] [G loss: 1.611224]\n",
      "[D loss: 0.817725] [G loss: 1.401811]\n",
      "[D loss: 0.635853] [G loss: 1.878622]\n",
      "[D loss: 0.984178] [G loss: 1.597041]\n",
      "[D loss: 1.014167] [G loss: 1.511778]\n",
      "[D loss: 0.909144] [G loss: 1.708763]\n",
      "[D loss: 0.566030] [G loss: 1.762523]\n",
      "[D loss: 0.800038] [G loss: 1.858609]\n",
      "[D loss: 0.578459] [G loss: 1.914833]\n",
      "[D loss: 0.856007] [G loss: 1.383378]\n",
      "[D loss: 0.809257] [G loss: 1.408682]\n",
      "[D loss: 0.994329] [G loss: 1.235039]\n",
      "[D loss: 1.114043] [G loss: 1.336589]\n",
      "[D loss: 0.803827] [G loss: 1.608028]\n",
      "[D loss: 0.889900] [G loss: 1.508796]\n",
      "[D loss: 0.997807] [G loss: 1.387296]\n",
      "[D loss: 0.923106] [G loss: 1.404227]\n",
      "[D loss: 0.867195] [G loss: 1.397636]\n",
      "[D loss: 0.873521] [G loss: 1.264678]\n",
      "[D loss: 0.895128] [G loss: 1.279071]\n",
      "[D loss: 0.731186] [G loss: 1.352196]\n",
      "[D loss: 0.819900] [G loss: 1.552620]\n",
      "[D loss: 0.704174] [G loss: 1.606624]\n",
      "[D loss: 0.710812] [G loss: 1.479228]\n",
      "[D loss: 0.958314] [G loss: 1.426899]\n",
      "[D loss: 1.080998] [G loss: 1.214315]\n",
      "[D loss: 0.649307] [G loss: 1.327295]\n",
      "[D loss: 0.789801] [G loss: 1.442629]\n",
      "[D loss: 0.834897] [G loss: 1.247575]\n",
      "[D loss: 0.705713] [G loss: 1.501257]\n",
      "[D loss: 1.027158] [G loss: 1.517989]\n",
      "[D loss: 0.716568] [G loss: 1.758037]\n",
      "[D loss: 0.702506] [G loss: 1.764176]\n",
      "[D loss: 0.832621] [G loss: 1.783838]\n",
      "[D loss: 1.022675] [G loss: 1.558099]\n",
      "[D loss: 1.028095] [G loss: 1.207298]\n",
      "[D loss: 0.560517] [G loss: 1.415300]\n",
      "[D loss: 0.840553] [G loss: 1.363606]\n",
      "[D loss: 0.807249] [G loss: 1.673789]\n",
      "[D loss: 0.890104] [G loss: 1.614596]\n",
      "[D loss: 0.808714] [G loss: 1.415855]\n",
      "[D loss: 0.653615] [G loss: 1.632308]\n",
      "[D loss: 0.783388] [G loss: 1.548670]\n",
      "[D loss: 0.671478] [G loss: 1.359017]\n",
      "[D loss: 0.749812] [G loss: 1.465854]\n",
      "[D loss: 0.873763] [G loss: 1.168990]\n",
      "[D loss: 0.903487] [G loss: 1.268471]\n",
      "[D loss: 0.830669] [G loss: 1.477432]\n",
      "[D loss: 1.005300] [G loss: 1.226200]\n",
      "[D loss: 0.697796] [G loss: 1.439508]\n",
      "[D loss: 0.847870] [G loss: 1.445555]\n",
      "[D loss: 0.776882] [G loss: 1.326400]\n",
      "[D loss: 1.073172] [G loss: 1.270360]\n",
      "[D loss: 0.777417] [G loss: 1.467892]\n",
      "[D loss: 0.687862] [G loss: 1.554486]\n",
      "[D loss: 0.825350] [G loss: 1.411673]\n",
      "[D loss: 0.850701] [G loss: 1.368919]\n",
      "[D loss: 0.832913] [G loss: 1.440915]\n",
      "[D loss: 0.926261] [G loss: 1.370732]\n",
      "[D loss: 0.772950] [G loss: 1.678519]\n",
      "[D loss: 0.914590] [G loss: 1.674638]\n",
      "[D loss: 0.934390] [G loss: 1.328878]\n",
      "[D loss: 0.762501] [G loss: 1.517516]\n",
      "[D loss: 0.854695] [G loss: 1.334035]\n",
      "[D loss: 0.703915] [G loss: 1.564509]\n",
      "[D loss: 0.901972] [G loss: 1.382389]\n",
      "[D loss: 0.800273] [G loss: 1.554680]\n",
      "[D loss: 0.899042] [G loss: 1.814054]\n",
      "[D loss: 0.724234] [G loss: 1.433011]\n",
      "[D loss: 0.855257] [G loss: 1.380194]\n",
      "[D loss: 0.945718] [G loss: 1.281956]\n",
      "[D loss: 0.768502] [G loss: 1.497201]\n",
      "[D loss: 0.737388] [G loss: 1.489095]\n",
      "[D loss: 0.720462] [G loss: 1.516578]\n",
      "[D loss: 0.759557] [G loss: 1.426101]\n",
      "[D loss: 0.769155] [G loss: 1.667071]\n",
      "[D loss: 0.768382] [G loss: 1.566438]\n",
      "[D loss: 0.765639] [G loss: 1.620695]\n",
      "[D loss: 0.660050] [G loss: 1.401147]\n",
      "[D loss: 0.852108] [G loss: 1.562142]\n",
      "[D loss: 0.872780] [G loss: 1.472553]\n",
      "[D loss: 0.885131] [G loss: 1.432730]\n",
      "[D loss: 0.651438] [G loss: 1.576205]\n",
      "[D loss: 0.880382] [G loss: 1.372808]\n",
      "[D loss: 0.879845] [G loss: 1.391003]\n",
      "[D loss: 0.936736] [G loss: 1.628211]\n",
      "[D loss: 0.859159] [G loss: 1.496606]\n",
      "[D loss: 1.078153] [G loss: 1.544543]\n",
      "[D loss: 0.893117] [G loss: 1.557759]\n",
      "[D loss: 0.906348] [G loss: 1.282465]\n",
      "[D loss: 0.848970] [G loss: 1.410732]\n",
      "[D loss: 0.769565] [G loss: 1.448546]\n",
      "[D loss: 0.703022] [G loss: 1.612192]\n",
      "[D loss: 0.875890] [G loss: 1.198250]\n",
      "[D loss: 0.771647] [G loss: 1.562108]\n",
      "[D loss: 0.721537] [G loss: 1.606677]\n",
      "[D loss: 0.902710] [G loss: 1.386894]\n",
      "[D loss: 0.690712] [G loss: 1.501770]\n",
      "[D loss: 0.873685] [G loss: 1.403256]\n",
      "[D loss: 0.943433] [G loss: 1.267919]\n",
      "[D loss: 0.901059] [G loss: 1.318355]\n",
      "[D loss: 0.958499] [G loss: 1.342938]\n",
      "[D loss: 0.879876] [G loss: 1.475726]\n",
      "[D loss: 0.758489] [G loss: 1.656030]\n",
      "[D loss: 0.823166] [G loss: 1.545920]\n",
      "[D loss: 0.832694] [G loss: 1.526447]\n",
      "[D loss: 0.816947] [G loss: 1.405792]\n",
      "[D loss: 0.975472] [G loss: 1.293440]\n",
      "[D loss: 0.813194] [G loss: 1.214501]\n",
      "[D loss: 0.791898] [G loss: 1.299203]\n",
      "[D loss: 0.816044] [G loss: 1.412130]\n",
      "[D loss: 0.857206] [G loss: 1.480968]\n",
      "[D loss: 0.530102] [G loss: 1.555529]\n",
      "[D loss: 0.754399] [G loss: 1.630539]\n",
      "[D loss: 0.621855] [G loss: 1.628432]\n",
      "[D loss: 1.117510] [G loss: 1.268712]\n",
      "[D loss: 0.840368] [G loss: 1.398158]\n",
      "[D loss: 0.735030] [G loss: 1.565779]\n",
      "[D loss: 0.807377] [G loss: 1.418190]\n",
      "[D loss: 0.977066] [G loss: 1.419655]\n",
      "[D loss: 0.768324] [G loss: 1.498467]\n",
      "[D loss: 0.950592] [G loss: 1.502412]\n",
      "[D loss: 0.690560] [G loss: 1.738854]\n",
      "[D loss: 0.823011] [G loss: 1.657767]\n",
      "[D loss: 0.779066] [G loss: 1.543608]\n",
      "[D loss: 0.877937] [G loss: 1.508255]\n",
      "[D loss: 0.806205] [G loss: 1.258290]\n",
      "[D loss: 0.924451] [G loss: 1.603673]\n",
      "[D loss: 0.568048] [G loss: 1.607540]\n",
      "[D loss: 0.744196] [G loss: 1.673560]\n",
      "[D loss: 1.012422] [G loss: 1.284944]\n",
      "[D loss: 0.833692] [G loss: 1.614170]\n",
      "[D loss: 0.931790] [G loss: 1.608955]\n",
      "[D loss: 0.824256] [G loss: 1.604917]\n",
      "[D loss: 0.952863] [G loss: 1.323888]\n",
      "[D loss: 0.715339] [G loss: 1.291096]\n",
      "[D loss: 0.935522] [G loss: 1.532255]\n",
      "[D loss: 0.680630] [G loss: 1.294080]\n",
      "[D loss: 0.910669] [G loss: 1.294449]\n",
      "[D loss: 0.782051] [G loss: 1.649688]\n",
      "[D loss: 0.874510] [G loss: 1.468179]\n",
      "[D loss: 0.746795] [G loss: 1.396014]\n",
      "[D loss: 0.675453] [G loss: 1.365061]\n",
      "[D loss: 0.871204] [G loss: 1.579499]\n",
      "[D loss: 0.858912] [G loss: 1.402553]\n",
      "[D loss: 0.737914] [G loss: 1.457696]\n",
      "[D loss: 0.744650] [G loss: 1.464761]\n",
      "[D loss: 0.932740] [G loss: 1.558076]\n",
      "[D loss: 0.934996] [G loss: 1.517216]\n",
      "[D loss: 0.697040] [G loss: 1.397344]\n",
      "[D loss: 0.871108] [G loss: 1.538819]\n",
      "[D loss: 0.911304] [G loss: 1.443872]\n",
      "[D loss: 0.857192] [G loss: 1.824055]\n",
      "[D loss: 0.757154] [G loss: 1.690435]\n",
      "[D loss: 0.693620] [G loss: 1.562694]\n",
      "[D loss: 0.871935] [G loss: 1.543658]\n",
      "[D loss: 1.057886] [G loss: 1.254261]\n",
      "[D loss: 0.700811] [G loss: 1.563839]\n",
      "[D loss: 0.939609] [G loss: 1.403152]\n",
      "[D loss: 0.611868] [G loss: 1.466616]\n",
      "[D loss: 1.111483] [G loss: 1.357977]\n",
      "[D loss: 0.931121] [G loss: 1.383797]\n",
      "[D loss: 0.886768] [G loss: 1.652931]\n",
      "[D loss: 0.997314] [G loss: 1.473073]\n",
      "[D loss: 1.000984] [G loss: 1.402860]\n",
      "[D loss: 0.930405] [G loss: 1.414619]\n",
      "[D loss: 0.737740] [G loss: 1.321815]\n",
      "[D loss: 0.768782] [G loss: 1.898649]\n",
      "[D loss: 0.971380] [G loss: 1.566990]\n",
      "[D loss: 0.867843] [G loss: 1.498290]\n",
      "[D loss: 0.874661] [G loss: 1.536887]\n",
      "[D loss: 0.907569] [G loss: 1.447256]\n",
      "[D loss: 0.916264] [G loss: 1.314820]\n",
      "[D loss: 0.819037] [G loss: 1.487184]\n",
      "[D loss: 0.719564] [G loss: 1.447665]\n",
      "[D loss: 0.766202] [G loss: 1.587337]\n",
      "[D loss: 0.771220] [G loss: 1.359266]\n",
      "[D loss: 0.783297] [G loss: 1.464583]\n",
      "[D loss: 0.959936] [G loss: 1.417542]\n",
      "[D loss: 0.979521] [G loss: 1.500040]\n",
      "[D loss: 0.814876] [G loss: 1.484698]\n",
      "[D loss: 0.906071] [G loss: 1.395647]\n",
      "[D loss: 0.766694] [G loss: 1.496607]\n",
      "[D loss: 0.849103] [G loss: 1.326694]\n",
      "[D loss: 0.614463] [G loss: 1.457423]\n",
      "[D loss: 0.722990] [G loss: 1.364106]\n",
      "[D loss: 0.874199] [G loss: 1.416721]\n",
      "[D loss: 0.971946] [G loss: 1.431829]\n",
      "[D loss: 1.083031] [G loss: 1.593962]\n",
      "[D loss: 0.827524] [G loss: 1.566617]\n",
      "[D loss: 1.155606] [G loss: 1.297994]\n",
      "[D loss: 0.766237] [G loss: 1.457570]\n",
      "[D loss: 1.119865] [G loss: 1.395467]\n",
      "[D loss: 0.720337] [G loss: 1.670269]\n",
      "[D loss: 0.786482] [G loss: 1.516400]\n",
      "[D loss: 0.765493] [G loss: 1.668035]\n",
      "[D loss: 0.535902] [G loss: 1.574449]\n",
      "[D loss: 0.872865] [G loss: 1.429790]\n",
      "[D loss: 0.766195] [G loss: 1.491489]\n",
      "[D loss: 0.866002] [G loss: 1.454265]\n",
      "[D loss: 1.088838] [G loss: 1.356733]\n",
      "[D loss: 1.034070] [G loss: 1.338818]\n",
      "[D loss: 0.855624] [G loss: 1.404503]\n",
      "[D loss: 0.731931] [G loss: 1.901343]\n",
      "[D loss: 0.892755] [G loss: 1.577428]\n",
      "[D loss: 0.626691] [G loss: 1.577493]\n",
      "[D loss: 0.613731] [G loss: 1.639167]\n",
      "[D loss: 0.759705] [G loss: 1.708506]\n",
      "[D loss: 1.058837] [G loss: 1.463442]\n",
      "[D loss: 0.783871] [G loss: 1.405567]\n",
      "[D loss: 0.748927] [G loss: 1.382148]\n",
      "[D loss: 0.873462] [G loss: 1.481416]\n",
      "[D loss: 0.743355] [G loss: 1.572762]\n",
      "[D loss: 1.036664] [G loss: 1.467024]\n",
      "[D loss: 0.852128] [G loss: 1.348204]\n",
      "[D loss: 0.903606] [G loss: 1.518170]\n",
      "[D loss: 0.866807] [G loss: 1.430502]\n",
      "[D loss: 0.865741] [G loss: 1.602486]\n",
      "[D loss: 0.898201] [G loss: 1.461448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 1.123752] [G loss: 1.395829]\n",
      "[D loss: 0.958727] [G loss: 1.386695]\n",
      "[D loss: 0.751917] [G loss: 1.431921]\n",
      "[D loss: 0.773186] [G loss: 1.351195]\n",
      "[D loss: 1.086445] [G loss: 1.358125]\n",
      "[D loss: 0.737604] [G loss: 1.502218]\n",
      "[D loss: 0.781017] [G loss: 1.654093]\n",
      "[D loss: 1.013355] [G loss: 1.422131]\n",
      "[D loss: 0.731687] [G loss: 1.421504]\n",
      "[D loss: 0.780075] [G loss: 1.434328]\n",
      "[D loss: 0.752688] [G loss: 1.444830]\n",
      "[D loss: 0.777943] [G loss: 1.466033]\n",
      "[D loss: 0.848847] [G loss: 1.501678]\n",
      "[D loss: 0.786794] [G loss: 1.490198]\n",
      "[D loss: 0.838021] [G loss: 1.364989]\n",
      "[D loss: 0.999347] [G loss: 1.513138]\n",
      "[D loss: 0.787943] [G loss: 1.245482]\n",
      "[D loss: 0.685211] [G loss: 1.480591]\n",
      "[D loss: 1.137118] [G loss: 1.454754]\n",
      "[D loss: 0.905679] [G loss: 1.541725]\n",
      "[D loss: 0.845453] [G loss: 1.417159]\n",
      "[D loss: 0.781169] [G loss: 1.363282]\n",
      "[D loss: 0.728056] [G loss: 1.492419]\n",
      "[D loss: 0.881745] [G loss: 1.310145]\n",
      "[D loss: 0.700235] [G loss: 1.446674]\n",
      "[D loss: 0.643405] [G loss: 1.587204]\n",
      "[D loss: 0.790084] [G loss: 1.414674]\n",
      "[D loss: 0.814491] [G loss: 1.535275]\n",
      "[D loss: 0.935370] [G loss: 1.404512]\n",
      "[D loss: 0.798494] [G loss: 1.462998]\n",
      "[D loss: 0.895402] [G loss: 1.470761]\n",
      "[D loss: 0.717389] [G loss: 1.583634]\n",
      "[D loss: 0.743775] [G loss: 1.443356]\n",
      "[D loss: 1.035660] [G loss: 1.392957]\n",
      "[D loss: 0.648799] [G loss: 1.408376]\n",
      "[D loss: 0.769426] [G loss: 1.460665]\n",
      "[D loss: 0.657449] [G loss: 1.564212]\n",
      "[D loss: 0.814365] [G loss: 1.328095]\n",
      "[D loss: 0.977964] [G loss: 1.550846]\n",
      "[D loss: 0.735722] [G loss: 1.402618]\n",
      "[D loss: 0.815318] [G loss: 1.400627]\n",
      "[D loss: 0.809326] [G loss: 1.493431]\n",
      "[D loss: 0.805859] [G loss: 1.551152]\n",
      "[D loss: 0.951598] [G loss: 1.642034]\n",
      "[D loss: 0.883386] [G loss: 1.565732]\n",
      "[D loss: 0.772422] [G loss: 1.384987]\n",
      "[D loss: 0.821502] [G loss: 1.278797]\n",
      "[D loss: 0.766879] [G loss: 1.561457]\n",
      "[D loss: 1.032615] [G loss: 1.320244]\n",
      "[D loss: 0.710179] [G loss: 1.559073]\n",
      "[D loss: 0.766107] [G loss: 1.492226]\n",
      "[D loss: 0.894432] [G loss: 1.600256]\n",
      "[D loss: 0.775913] [G loss: 1.288375]\n",
      "[D loss: 0.793578] [G loss: 1.420031]\n",
      "[D loss: 0.749207] [G loss: 1.460089]\n",
      "[D loss: 0.839714] [G loss: 1.333727]\n",
      "[D loss: 0.802802] [G loss: 1.634795]\n",
      "[D loss: 0.656869] [G loss: 1.475860]\n",
      "[D loss: 0.936230] [G loss: 1.142575]\n",
      "[D loss: 0.890576] [G loss: 1.515608]\n",
      "[D loss: 0.789839] [G loss: 1.794775]\n",
      "[D loss: 0.624969] [G loss: 1.605676]\n",
      "[D loss: 1.179291] [G loss: 1.401927]\n",
      "[D loss: 0.685608] [G loss: 1.771879]\n",
      "[D loss: 0.728265] [G loss: 1.745573]\n",
      "[D loss: 0.823853] [G loss: 1.451364]\n",
      "[D loss: 0.826441] [G loss: 1.531639]\n",
      "[D loss: 0.848609] [G loss: 1.217001]\n",
      "[D loss: 0.947163] [G loss: 1.434691]\n",
      "[D loss: 0.900647] [G loss: 1.442605]\n",
      "[D loss: 0.694483] [G loss: 1.736128]\n",
      "[D loss: 0.901590] [G loss: 1.407131]\n",
      "[D loss: 0.816611] [G loss: 1.559784]\n",
      "[D loss: 1.061462] [G loss: 1.344664]\n",
      "[D loss: 0.839256] [G loss: 1.545176]\n",
      "[D loss: 1.057454] [G loss: 1.426594]\n",
      "[D loss: 0.983408] [G loss: 1.106116]\n",
      "[D loss: 0.916602] [G loss: 1.275005]\n",
      "[D loss: 0.936429] [G loss: 1.336970]\n",
      "[D loss: 0.872195] [G loss: 1.357252]\n",
      "[D loss: 0.927886] [G loss: 1.452568]\n",
      "[D loss: 0.895048] [G loss: 1.430349]\n",
      "[D loss: 0.932629] [G loss: 1.451633]\n",
      "[D loss: 0.705374] [G loss: 1.437594]\n",
      "[D loss: 0.604306] [G loss: 1.390687]\n",
      "[D loss: 0.779724] [G loss: 1.498821]\n",
      "[D loss: 0.873853] [G loss: 1.580654]\n",
      "[D loss: 0.986753] [G loss: 1.373408]\n",
      "[D loss: 0.829351] [G loss: 1.477120]\n",
      "[D loss: 0.680157] [G loss: 1.529219]\n",
      "[D loss: 0.702927] [G loss: 1.636771]\n",
      "[D loss: 1.034027] [G loss: 1.440382]\n",
      "[D loss: 0.742569] [G loss: 1.388469]\n",
      "[D loss: 0.776064] [G loss: 1.600484]\n",
      "[D loss: 0.622049] [G loss: 1.452432]\n",
      "[D loss: 0.954209] [G loss: 1.584692]\n",
      "[D loss: 0.503920] [G loss: 1.611270]\n",
      "[D loss: 0.910724] [G loss: 1.654692]\n",
      "[D loss: 1.051497] [G loss: 1.485382]\n",
      "[D loss: 0.834164] [G loss: 1.575957]\n",
      "[D loss: 0.787158] [G loss: 1.542311]\n",
      "[D loss: 0.909495] [G loss: 1.537845]\n",
      "[D loss: 0.836220] [G loss: 1.286921]\n",
      "[D loss: 0.846697] [G loss: 1.346692]\n",
      "[D loss: 0.746639] [G loss: 1.337614]\n",
      "[D loss: 0.717237] [G loss: 1.844099]\n",
      "[D loss: 0.781006] [G loss: 1.405687]\n",
      "[D loss: 0.777196] [G loss: 1.450451]\n",
      "[D loss: 0.726446] [G loss: 1.436912]\n",
      "[D loss: 0.886998] [G loss: 1.536449]\n",
      "[D loss: 0.925106] [G loss: 1.683714]\n",
      "[D loss: 0.966774] [G loss: 1.605110]\n",
      "[D loss: 0.975523] [G loss: 1.571980]\n",
      "[D loss: 0.676664] [G loss: 1.632722]\n",
      "[D loss: 0.728809] [G loss: 1.668883]\n",
      "[D loss: 0.772966] [G loss: 1.409180]\n",
      "[D loss: 0.830414] [G loss: 1.481989]\n",
      "[D loss: 1.015329] [G loss: 1.529665]\n",
      "[D loss: 0.780711] [G loss: 1.350161]\n",
      "[D loss: 0.852558] [G loss: 1.237768]\n",
      "[D loss: 0.908878] [G loss: 1.431376]\n",
      "[D loss: 1.087308] [G loss: 1.232283]\n",
      "[D loss: 0.964018] [G loss: 1.400560]\n",
      "[D loss: 0.641334] [G loss: 1.468885]\n",
      "[D loss: 0.656536] [G loss: 1.653725]\n",
      "[D loss: 0.783238] [G loss: 1.438291]\n",
      "[D loss: 1.004749] [G loss: 1.300315]\n",
      "[D loss: 0.756338] [G loss: 1.413084]\n",
      "[D loss: 0.862252] [G loss: 1.511419]\n",
      "[D loss: 0.810680] [G loss: 1.380491]\n",
      "[D loss: 0.919461] [G loss: 1.335806]\n",
      "[D loss: 0.871759] [G loss: 1.435622]\n",
      "[D loss: 0.708288] [G loss: 1.516349]\n",
      "[D loss: 0.863202] [G loss: 1.391528]\n",
      "[D loss: 0.900946] [G loss: 1.320163]\n",
      "[D loss: 0.915187] [G loss: 1.362553]\n",
      "[D loss: 1.020224] [G loss: 1.260285]\n",
      "[D loss: 0.862405] [G loss: 1.525561]\n",
      "[D loss: 1.130207] [G loss: 1.427805]\n",
      "[D loss: 1.022800] [G loss: 1.716060]\n",
      "[D loss: 0.821747] [G loss: 1.373641]\n",
      "[D loss: 0.734573] [G loss: 1.385913]\n",
      "[D loss: 0.753320] [G loss: 1.487062]\n",
      "[D loss: 0.751912] [G loss: 1.360099]\n",
      "[D loss: 0.809401] [G loss: 1.361555]\n",
      "[D loss: 0.898000] [G loss: 1.423980]\n",
      "[D loss: 0.598138] [G loss: 1.380591]\n",
      "[D loss: 0.868265] [G loss: 1.459609]\n",
      "[D loss: 0.927889] [G loss: 1.533789]\n",
      "[D loss: 1.031795] [G loss: 1.548322]\n",
      "[D loss: 0.811271] [G loss: 1.293836]\n",
      "[D loss: 0.746884] [G loss: 1.515583]\n",
      "[D loss: 0.788749] [G loss: 1.549898]\n",
      "[D loss: 0.718423] [G loss: 1.462632]\n",
      "[D loss: 0.855702] [G loss: 1.319542]\n",
      "[D loss: 1.023122] [G loss: 1.364905]\n",
      "[D loss: 0.812037] [G loss: 1.481053]\n",
      "[D loss: 0.798202] [G loss: 1.661240]\n",
      "[D loss: 0.811449] [G loss: 1.397234]\n",
      "[D loss: 0.749878] [G loss: 1.616060]\n",
      "[D loss: 0.914154] [G loss: 1.382117]\n",
      "[D loss: 0.650083] [G loss: 1.328018]\n",
      "[D loss: 0.932145] [G loss: 1.694401]\n",
      "[D loss: 0.824870] [G loss: 1.572134]\n",
      "[D loss: 0.995376] [G loss: 1.487016]\n",
      "[D loss: 0.935027] [G loss: 1.689615]\n",
      "[D loss: 0.930273] [G loss: 1.394113]\n",
      "[D loss: 0.714618] [G loss: 1.372225]\n",
      "[D loss: 0.682878] [G loss: 1.301982]\n",
      "[D loss: 0.710571] [G loss: 1.493891]\n",
      "[D loss: 0.931914] [G loss: 1.398535]\n",
      "[D loss: 0.729743] [G loss: 1.384523]\n",
      "[D loss: 0.958646] [G loss: 1.534995]\n",
      "[D loss: 0.655478] [G loss: 1.385453]\n",
      "[D loss: 0.821727] [G loss: 1.434305]\n",
      "[D loss: 0.919354] [G loss: 1.480793]\n",
      "[D loss: 0.806721] [G loss: 1.406386]\n",
      "[D loss: 0.809623] [G loss: 1.575310]\n",
      "[D loss: 0.802328] [G loss: 1.432247]\n",
      "[D loss: 0.642604] [G loss: 1.784898]\n",
      "[D loss: 1.216368] [G loss: 1.300119]\n",
      "[D loss: 0.773502] [G loss: 1.503623]\n",
      "[D loss: 0.830538] [G loss: 1.527768]\n",
      "[D loss: 0.944285] [G loss: 1.352932]\n",
      "[D loss: 0.947846] [G loss: 1.222471]\n",
      "[D loss: 0.965037] [G loss: 1.340315]\n",
      "[D loss: 0.873036] [G loss: 1.473897]\n",
      "[D loss: 0.881793] [G loss: 1.571937]\n",
      "[D loss: 0.781260] [G loss: 1.515419]\n",
      "[D loss: 0.805401] [G loss: 1.439204]\n",
      "[D loss: 0.564620] [G loss: 1.455236]\n",
      "[D loss: 0.942698] [G loss: 1.628744]\n",
      "[D loss: 1.031990] [G loss: 1.700509]\n",
      "[D loss: 0.799426] [G loss: 1.730148]\n",
      "[D loss: 0.819049] [G loss: 1.470547]\n",
      "[D loss: 0.949764] [G loss: 1.405843]\n",
      "[D loss: 0.794111] [G loss: 1.424623]\n",
      "[D loss: 0.845259] [G loss: 1.579945]\n",
      "[D loss: 0.981314] [G loss: 1.284875]\n",
      "[D loss: 0.692302] [G loss: 1.501785]\n",
      "[D loss: 0.876911] [G loss: 1.420111]\n",
      "[D loss: 0.970024] [G loss: 1.250292]\n",
      "[D loss: 0.949517] [G loss: 1.187892]\n",
      "[D loss: 0.710986] [G loss: 1.451594]\n",
      "[D loss: 0.656621] [G loss: 1.530651]\n",
      "[D loss: 0.633781] [G loss: 1.345777]\n",
      "[D loss: 0.753033] [G loss: 1.432358]\n",
      "[D loss: 0.665362] [G loss: 1.441297]\n",
      "[D loss: 0.728656] [G loss: 1.403382]\n",
      "[D loss: 0.778685] [G loss: 1.444525]\n",
      "[D loss: 0.661310] [G loss: 1.483066]\n",
      "[D loss: 0.924395] [G loss: 1.371615]\n",
      "[D loss: 0.848902] [G loss: 1.475906]\n",
      "[D loss: 0.794607] [G loss: 1.750599]\n",
      "[D loss: 0.852784] [G loss: 1.592459]\n",
      "[D loss: 0.547794] [G loss: 1.683074]\n",
      "[D loss: 1.097488] [G loss: 1.537817]\n",
      "[D loss: 0.827205] [G loss: 1.540205]\n",
      "[D loss: 0.625583] [G loss: 1.576867]\n",
      "[D loss: 0.790422] [G loss: 1.700842]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.842352] [G loss: 1.532542]\n",
      "[D loss: 0.981688] [G loss: 1.415988]\n",
      "[D loss: 0.639479] [G loss: 1.475753]\n",
      "[D loss: 0.701693] [G loss: 1.498455]\n",
      "[D loss: 0.823945] [G loss: 1.527585]\n",
      "[D loss: 1.048176] [G loss: 1.445728]\n",
      "[D loss: 0.880550] [G loss: 1.568294]\n",
      "[D loss: 0.801518] [G loss: 1.564647]\n",
      "[D loss: 0.978174] [G loss: 1.619057]\n",
      "[D loss: 0.614781] [G loss: 1.563598]\n",
      "[D loss: 0.822851] [G loss: 1.480951]\n",
      "[D loss: 0.783924] [G loss: 1.485951]\n",
      "[D loss: 0.746499] [G loss: 1.430109]\n",
      "[D loss: 0.853717] [G loss: 1.515701]\n",
      "[D loss: 0.665104] [G loss: 1.569149]\n",
      "[D loss: 0.644783] [G loss: 1.424720]\n",
      "[D loss: 0.795825] [G loss: 1.363459]\n",
      "[D loss: 1.030482] [G loss: 1.409411]\n",
      "[D loss: 0.765624] [G loss: 1.432265]\n",
      "[D loss: 1.048046] [G loss: 1.659045]\n",
      "[D loss: 0.955418] [G loss: 1.751703]\n",
      "[D loss: 0.992965] [G loss: 1.333256]\n",
      "[D loss: 0.773180] [G loss: 1.553160]\n",
      "[D loss: 1.094424] [G loss: 1.168504]\n",
      "[D loss: 0.800287] [G loss: 1.402775]\n",
      "[D loss: 0.967482] [G loss: 1.387821]\n",
      "[D loss: 0.755277] [G loss: 1.417653]\n",
      "[D loss: 0.717210] [G loss: 1.469068]\n",
      "[D loss: 0.885931] [G loss: 1.355478]\n",
      "[D loss: 0.698009] [G loss: 1.381693]\n",
      "[D loss: 0.829632] [G loss: 1.516682]\n",
      "[D loss: 0.958155] [G loss: 1.509826]\n",
      "[D loss: 0.775635] [G loss: 1.313015]\n",
      "[D loss: 0.777383] [G loss: 1.425694]\n",
      "[D loss: 0.904227] [G loss: 1.345922]\n",
      "[D loss: 0.769538] [G loss: 1.328666]\n",
      "[D loss: 0.728894] [G loss: 1.427286]\n",
      "[D loss: 0.869626] [G loss: 1.547325]\n",
      "[D loss: 0.880180] [G loss: 1.485614]\n",
      "[D loss: 0.831702] [G loss: 1.359039]\n",
      "[D loss: 0.665783] [G loss: 1.632003]\n",
      "[D loss: 0.906158] [G loss: 1.630145]\n",
      "[D loss: 0.704479] [G loss: 1.579419]\n",
      "[D loss: 0.844661] [G loss: 1.293252]\n",
      "[D loss: 0.610683] [G loss: 1.392770]\n",
      "[D loss: 0.896373] [G loss: 1.366284]\n",
      "[D loss: 0.823801] [G loss: 1.359907]\n",
      "[D loss: 0.686681] [G loss: 1.467875]\n",
      "[D loss: 0.723199] [G loss: 1.523335]\n",
      "[D loss: 0.923200] [G loss: 1.403969]\n",
      "[D loss: 0.839222] [G loss: 1.456133]\n",
      "[D loss: 0.823203] [G loss: 1.606499]\n",
      "[D loss: 0.854368] [G loss: 1.511852]\n",
      "[D loss: 0.776794] [G loss: 1.479924]\n",
      "[D loss: 0.927949] [G loss: 1.525858]\n",
      "[D loss: 1.088195] [G loss: 1.313419]\n",
      "[D loss: 0.803429] [G loss: 1.616147]\n",
      "[D loss: 0.613558] [G loss: 1.411583]\n",
      "[D loss: 0.876833] [G loss: 1.630939]\n",
      "[D loss: 0.797325] [G loss: 1.426106]\n",
      "[D loss: 0.802521] [G loss: 1.370492]\n",
      "[D loss: 0.871424] [G loss: 1.182756]\n",
      "[D loss: 0.824871] [G loss: 1.169393]\n",
      "[D loss: 0.796314] [G loss: 1.500399]\n",
      "[D loss: 0.569049] [G loss: 1.630405]\n",
      "[D loss: 0.670684] [G loss: 1.557627]\n",
      "[D loss: 0.770296] [G loss: 1.570989]\n",
      "[D loss: 0.771100] [G loss: 1.698502]\n",
      "[D loss: 0.564984] [G loss: 1.611191]\n",
      "[D loss: 0.815997] [G loss: 1.606550]\n",
      "[D loss: 0.887904] [G loss: 1.707587]\n",
      "[D loss: 0.681785] [G loss: 1.698435]\n",
      "[D loss: 0.774463] [G loss: 1.563154]\n",
      "[D loss: 0.841546] [G loss: 1.482113]\n",
      "[D loss: 0.655333] [G loss: 1.679029]\n",
      "[D loss: 0.785719] [G loss: 1.695125]\n",
      "[D loss: 0.827375] [G loss: 1.620212]\n",
      "[D loss: 0.869036] [G loss: 1.516554]\n",
      "[D loss: 0.727599] [G loss: 1.563248]\n",
      "[D loss: 0.826560] [G loss: 1.443157]\n",
      "[D loss: 0.674863] [G loss: 1.597793]\n",
      "[D loss: 0.798066] [G loss: 1.916580]\n",
      "[D loss: 0.724765] [G loss: 1.271903]\n",
      "[D loss: 0.628717] [G loss: 1.511778]\n",
      "[D loss: 0.720501] [G loss: 1.560624]\n",
      "[D loss: 0.829999] [G loss: 1.472760]\n",
      "[D loss: 0.940210] [G loss: 1.307112]\n",
      "[D loss: 1.015571] [G loss: 1.528644]\n",
      "[D loss: 0.738855] [G loss: 1.588345]\n",
      "[D loss: 0.882762] [G loss: 1.555117]\n",
      "[D loss: 0.809117] [G loss: 1.444479]\n",
      "[D loss: 0.870823] [G loss: 1.461571]\n",
      "[D loss: 0.789542] [G loss: 1.683184]\n",
      "[D loss: 0.929860] [G loss: 1.486046]\n",
      "[D loss: 0.976679] [G loss: 1.313824]\n",
      "[D loss: 0.692919] [G loss: 1.365875]\n",
      "[D loss: 0.808964] [G loss: 1.322253]\n",
      "[D loss: 0.891363] [G loss: 1.391969]\n",
      "[D loss: 1.041124] [G loss: 1.311609]\n",
      "[D loss: 0.843200] [G loss: 1.437559]\n",
      "[D loss: 0.923775] [G loss: 1.187285]\n",
      "[D loss: 0.693695] [G loss: 1.380011]\n",
      "[D loss: 0.545180] [G loss: 1.414203]\n",
      "[D loss: 0.848769] [G loss: 1.382308]\n",
      "[D loss: 0.740513] [G loss: 1.575111]\n",
      "[D loss: 1.075303] [G loss: 1.481054]\n",
      "[D loss: 0.727564] [G loss: 1.250489]\n",
      "[D loss: 0.857815] [G loss: 1.526634]\n",
      "[D loss: 0.597125] [G loss: 1.474011]\n",
      "[D loss: 0.682753] [G loss: 1.652037]\n",
      "[D loss: 0.662999] [G loss: 1.454598]\n",
      "[D loss: 0.890034] [G loss: 1.508833]\n",
      "[D loss: 0.944059] [G loss: 1.358558]\n",
      "[D loss: 0.718854] [G loss: 1.479538]\n",
      "[D loss: 0.866334] [G loss: 1.322546]\n",
      "[D loss: 0.934772] [G loss: 1.175019]\n",
      "[D loss: 0.778178] [G loss: 1.458951]\n",
      "[D loss: 0.585380] [G loss: 1.840684]\n",
      "[D loss: 0.693197] [G loss: 1.783090]\n",
      "[D loss: 1.054031] [G loss: 1.364835]\n",
      "[D loss: 0.880379] [G loss: 1.578282]\n",
      "[D loss: 0.653186] [G loss: 1.657942]\n",
      "[D loss: 0.800336] [G loss: 1.307866]\n",
      "[D loss: 0.962503] [G loss: 1.645375]\n",
      "[D loss: 0.778588] [G loss: 1.698042]\n",
      "[D loss: 0.829785] [G loss: 1.539625]\n",
      "[D loss: 0.977864] [G loss: 1.406897]\n",
      "[D loss: 1.044740] [G loss: 1.145461]\n",
      "[D loss: 0.835343] [G loss: 1.482485]\n",
      "[D loss: 0.984534] [G loss: 1.438109]\n",
      "[D loss: 0.792392] [G loss: 1.544695]\n",
      "[D loss: 0.796025] [G loss: 1.491319]\n",
      "[D loss: 0.575268] [G loss: 1.661882]\n",
      "[D loss: 0.777447] [G loss: 1.360759]\n",
      "[D loss: 1.085403] [G loss: 1.317454]\n",
      "[D loss: 1.043469] [G loss: 1.266139]\n",
      "[D loss: 0.537318] [G loss: 1.365745]\n",
      "[D loss: 0.675881] [G loss: 1.614454]\n",
      "[D loss: 0.935815] [G loss: 1.591270]\n",
      "[D loss: 1.085250] [G loss: 1.533678]\n",
      "[D loss: 0.970205] [G loss: 1.557271]\n",
      "[D loss: 0.845858] [G loss: 1.423208]\n",
      "[D loss: 0.735125] [G loss: 1.479943]\n",
      "[D loss: 0.850319] [G loss: 1.399860]\n",
      "[D loss: 0.826410] [G loss: 1.493210]\n",
      "[D loss: 1.046165] [G loss: 1.206367]\n",
      "[D loss: 0.777690] [G loss: 1.646275]\n",
      "[D loss: 0.774079] [G loss: 1.442651]\n",
      "[D loss: 0.882860] [G loss: 1.558335]\n",
      "[D loss: 0.925584] [G loss: 1.581559]\n",
      "[D loss: 0.974621] [G loss: 1.546660]\n",
      "[D loss: 0.819350] [G loss: 1.607781]\n",
      "[D loss: 0.854699] [G loss: 1.573551]\n",
      "[D loss: 0.758217] [G loss: 1.488854]\n",
      "[D loss: 0.793353] [G loss: 1.475352]\n",
      "[D loss: 0.957775] [G loss: 1.327005]\n",
      "[D loss: 0.719337] [G loss: 1.604585]\n",
      "[D loss: 0.669422] [G loss: 1.747606]\n",
      "[D loss: 0.789033] [G loss: 1.397101]\n",
      "[D loss: 0.873134] [G loss: 1.426784]\n",
      "[D loss: 0.827265] [G loss: 1.286444]\n",
      "[D loss: 0.854773] [G loss: 1.533441]\n",
      "[D loss: 0.863600] [G loss: 1.785472]\n",
      "[D loss: 0.797232] [G loss: 1.435182]\n",
      "[D loss: 0.641093] [G loss: 1.677829]\n",
      "[D loss: 0.698898] [G loss: 1.707367]\n",
      "[D loss: 0.717117] [G loss: 1.463531]\n",
      "[D loss: 0.748397] [G loss: 1.432526]\n",
      "[D loss: 0.796131] [G loss: 1.532420]\n",
      "[D loss: 0.780566] [G loss: 1.587402]\n",
      "[D loss: 1.045572] [G loss: 1.453700]\n",
      "[D loss: 0.937248] [G loss: 1.378747]\n",
      "[D loss: 0.611082] [G loss: 1.521411]\n",
      "[D loss: 0.753976] [G loss: 1.450728]\n",
      "[D loss: 0.918112] [G loss: 1.400016]\n",
      "[D loss: 1.034898] [G loss: 1.486777]\n",
      "[D loss: 0.769942] [G loss: 1.592584]\n",
      "[D loss: 0.770454] [G loss: 1.435271]\n",
      "[D loss: 0.954600] [G loss: 1.374965]\n",
      "[D loss: 0.989005] [G loss: 1.894688]\n",
      "[D loss: 0.818963] [G loss: 1.392127]\n",
      "[D loss: 0.990551] [G loss: 1.356927]\n",
      "[D loss: 0.612267] [G loss: 1.660963]\n",
      "[D loss: 0.770240] [G loss: 1.199110]\n",
      "[D loss: 0.797781] [G loss: 1.660592]\n",
      "[D loss: 0.931899] [G loss: 1.561707]\n",
      "[D loss: 0.995182] [G loss: 1.425779]\n",
      "[D loss: 1.008398] [G loss: 1.168993]\n",
      "[D loss: 0.781191] [G loss: 1.370087]\n",
      "[D loss: 0.734597] [G loss: 1.498455]\n",
      "[D loss: 1.020895] [G loss: 1.305770]\n",
      "[D loss: 0.898566] [G loss: 1.152901]\n",
      "[D loss: 0.794221] [G loss: 1.383764]\n",
      "[D loss: 0.709970] [G loss: 1.390435]\n",
      "[D loss: 0.850183] [G loss: 1.429692]\n",
      "[D loss: 0.938015] [G loss: 1.453567]\n",
      "[D loss: 0.922951] [G loss: 1.613845]\n",
      "[D loss: 0.539441] [G loss: 1.521120]\n",
      "[D loss: 1.045974] [G loss: 1.302653]\n",
      "[D loss: 0.843714] [G loss: 1.470720]\n",
      "[D loss: 0.771863] [G loss: 1.648421]\n",
      "[D loss: 0.785895] [G loss: 1.531709]\n",
      "[D loss: 0.774965] [G loss: 1.381718]\n",
      "[D loss: 0.888083] [G loss: 1.431409]\n",
      "[D loss: 1.016151] [G loss: 1.326891]\n",
      "[D loss: 0.718906] [G loss: 1.217411]\n",
      "[D loss: 0.924006] [G loss: 1.543365]\n",
      "[D loss: 0.651733] [G loss: 1.395228]\n",
      "[D loss: 0.802421] [G loss: 1.688483]\n",
      "[D loss: 0.615191] [G loss: 1.314538]\n",
      "[D loss: 0.851046] [G loss: 1.781280]\n",
      "[D loss: 0.729170] [G loss: 1.455941]\n",
      "[D loss: 0.829346] [G loss: 1.362672]\n",
      "[D loss: 0.721130] [G loss: 1.326146]\n",
      "[D loss: 0.753794] [G loss: 1.473604]\n",
      "[D loss: 0.833589] [G loss: 1.581972]\n",
      "[D loss: 0.562327] [G loss: 1.550365]\n",
      "[D loss: 1.192601] [G loss: 1.521404]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.933984] [G loss: 1.772506]\n",
      "[D loss: 0.962039] [G loss: 1.647728]\n",
      "[D loss: 0.867459] [G loss: 1.893338]\n",
      "[D loss: 0.796209] [G loss: 1.474015]\n",
      "[D loss: 0.585407] [G loss: 1.588852]\n",
      "[D loss: 0.653974] [G loss: 1.451442]\n",
      "[D loss: 1.029016] [G loss: 1.206619]\n",
      "[D loss: 0.896855] [G loss: 1.571954]\n",
      "[D loss: 0.848950] [G loss: 1.437761]\n",
      "[D loss: 1.075522] [G loss: 1.508601]\n",
      "[D loss: 1.025086] [G loss: 1.549032]\n",
      "[D loss: 0.740470] [G loss: 1.442593]\n",
      "[D loss: 0.820922] [G loss: 1.315231]\n",
      "[D loss: 0.943858] [G loss: 1.330164]\n",
      "[D loss: 0.669360] [G loss: 1.385441]\n",
      "[D loss: 0.816675] [G loss: 1.442065]\n",
      "[D loss: 0.896970] [G loss: 1.634795]\n",
      "[D loss: 1.084215] [G loss: 1.492256]\n",
      "[D loss: 0.903083] [G loss: 1.293913]\n",
      "[D loss: 0.740357] [G loss: 1.319174]\n",
      "[D loss: 0.819819] [G loss: 1.417719]\n",
      "[D loss: 0.743744] [G loss: 1.555716]\n",
      "[D loss: 0.885639] [G loss: 1.429099]\n",
      "[D loss: 0.688887] [G loss: 1.498436]\n",
      "[D loss: 0.849935] [G loss: 1.327292]\n",
      "[D loss: 0.895764] [G loss: 1.503344]\n",
      "[D loss: 0.997554] [G loss: 1.277728]\n",
      "[D loss: 0.894896] [G loss: 1.429852]\n",
      "[D loss: 0.848045] [G loss: 1.446095]\n",
      "[D loss: 0.808771] [G loss: 1.505459]\n",
      "[D loss: 1.173314] [G loss: 1.450964]\n",
      "[D loss: 0.754534] [G loss: 1.489980]\n",
      "[D loss: 0.979462] [G loss: 1.488637]\n",
      "[D loss: 0.813391] [G loss: 1.482160]\n",
      "[D loss: 1.128608] [G loss: 1.286593]\n",
      "[D loss: 0.998632] [G loss: 1.342728]\n",
      "[D loss: 0.858667] [G loss: 1.374770]\n",
      "[D loss: 0.763777] [G loss: 1.444273]\n",
      "[D loss: 0.876806] [G loss: 1.531407]\n",
      "[D loss: 0.789003] [G loss: 1.291057]\n",
      "[D loss: 0.838391] [G loss: 1.354007]\n",
      "[D loss: 0.674091] [G loss: 1.668628]\n",
      "[D loss: 0.717489] [G loss: 1.493720]\n",
      "[D loss: 0.891746] [G loss: 1.519088]\n",
      "[D loss: 0.649780] [G loss: 1.552862]\n",
      "[D loss: 0.860157] [G loss: 1.356837]\n",
      "[D loss: 1.158302] [G loss: 1.168027]\n",
      "[D loss: 0.680983] [G loss: 1.569992]\n",
      "[D loss: 0.854753] [G loss: 1.444442]\n",
      "[D loss: 0.836419] [G loss: 1.531918]\n",
      "[D loss: 0.817262] [G loss: 1.236846]\n",
      "[D loss: 0.970579] [G loss: 1.446588]\n",
      "[D loss: 1.044249] [G loss: 1.487363]\n",
      "[D loss: 0.769665] [G loss: 1.488661]\n",
      "[D loss: 0.924717] [G loss: 1.357229]\n",
      "[D loss: 0.856532] [G loss: 1.444950]\n",
      "[D loss: 0.795991] [G loss: 1.435547]\n",
      "[D loss: 0.911971] [G loss: 1.378492]\n",
      "[D loss: 0.805312] [G loss: 1.391243]\n",
      "[D loss: 1.017509] [G loss: 1.259462]\n",
      "[D loss: 0.775451] [G loss: 1.522598]\n",
      "[D loss: 0.691626] [G loss: 1.400808]\n",
      "[D loss: 0.772626] [G loss: 1.626135]\n",
      "[D loss: 0.822258] [G loss: 1.341800]\n",
      "[D loss: 0.818568] [G loss: 1.529279]\n",
      "[D loss: 0.890110] [G loss: 1.410159]\n",
      "[D loss: 0.904634] [G loss: 1.405774]\n",
      "[D loss: 0.606607] [G loss: 1.542172]\n",
      "[D loss: 0.988824] [G loss: 1.538671]\n",
      "[D loss: 0.683854] [G loss: 1.533489]\n",
      "[D loss: 1.019633] [G loss: 1.388373]\n",
      "[D loss: 0.713829] [G loss: 1.393817]\n",
      "[D loss: 0.949968] [G loss: 1.356873]\n",
      "[D loss: 0.799080] [G loss: 1.510366]\n",
      "[D loss: 0.962015] [G loss: 1.431772]\n",
      "[D loss: 0.720747] [G loss: 1.536656]\n",
      "[D loss: 0.795676] [G loss: 1.593118]\n",
      "[D loss: 0.754157] [G loss: 1.581018]\n",
      "[D loss: 0.831287] [G loss: 1.484823]\n",
      "[D loss: 0.986022] [G loss: 1.511865]\n",
      "[D loss: 0.768866] [G loss: 1.439689]\n",
      "[D loss: 0.807769] [G loss: 1.521927]\n",
      "[D loss: 0.954091] [G loss: 1.301168]\n",
      "[D loss: 0.769704] [G loss: 1.403217]\n",
      "[D loss: 0.874079] [G loss: 1.376860]\n",
      "[D loss: 0.969773] [G loss: 1.436448]\n",
      "[D loss: 0.910963] [G loss: 1.388707]\n",
      "[D loss: 0.966218] [G loss: 1.452646]\n",
      "[D loss: 0.911129] [G loss: 1.364219]\n",
      "[D loss: 0.801356] [G loss: 1.376938]\n",
      "[D loss: 0.915310] [G loss: 1.398226]\n",
      "[D loss: 0.809377] [G loss: 1.355965]\n",
      "[D loss: 0.706388] [G loss: 1.194849]\n",
      "[D loss: 1.063669] [G loss: 1.275655]\n",
      "[D loss: 0.904053] [G loss: 1.464592]\n",
      "[D loss: 0.691098] [G loss: 1.679521]\n",
      "[D loss: 0.838609] [G loss: 1.288030]\n",
      "[D loss: 0.823003] [G loss: 1.552428]\n",
      "[D loss: 0.924881] [G loss: 1.438219]\n",
      "[D loss: 0.703356] [G loss: 1.684277]\n",
      "[D loss: 0.811280] [G loss: 1.387339]\n",
      "[D loss: 0.877246] [G loss: 1.539202]\n",
      "[D loss: 0.651292] [G loss: 1.502877]\n",
      "[D loss: 1.022987] [G loss: 1.522278]\n",
      "[D loss: 1.068925] [G loss: 1.352731]\n",
      "[D loss: 0.832707] [G loss: 1.497145]\n",
      "[D loss: 1.149212] [G loss: 1.330369]\n",
      "[D loss: 0.655187] [G loss: 1.342415]\n",
      "[D loss: 0.911830] [G loss: 1.560016]\n",
      "[D loss: 0.718392] [G loss: 1.461953]\n",
      "[D loss: 0.934598] [G loss: 1.432151]\n",
      "[D loss: 0.913574] [G loss: 1.366289]\n",
      "[D loss: 0.834543] [G loss: 1.479685]\n",
      "[D loss: 0.919681] [G loss: 1.361788]\n",
      "[D loss: 0.782467] [G loss: 1.286802]\n",
      "[D loss: 0.632707] [G loss: 1.443505]\n",
      "[D loss: 0.636980] [G loss: 1.687361]\n",
      "[D loss: 0.936182] [G loss: 1.568823]\n",
      "[D loss: 0.785247] [G loss: 1.504830]\n",
      "[D loss: 0.934830] [G loss: 1.502076]\n",
      "[D loss: 0.976477] [G loss: 1.389489]\n",
      "[D loss: 0.632219] [G loss: 1.460711]\n",
      "[D loss: 0.569338] [G loss: 1.425105]\n",
      "[D loss: 0.557881] [G loss: 1.671290]\n",
      "[D loss: 0.919859] [G loss: 1.699755]\n",
      "[D loss: 0.732178] [G loss: 1.504497]\n",
      "[D loss: 0.864295] [G loss: 1.305087]\n",
      "[D loss: 0.789370] [G loss: 1.546148]\n",
      "[D loss: 0.890530] [G loss: 1.491307]\n",
      "[D loss: 0.682931] [G loss: 1.803219]\n",
      "[D loss: 0.785985] [G loss: 1.667208]\n",
      "[D loss: 0.788199] [G loss: 1.538083]\n",
      "[D loss: 0.852861] [G loss: 1.459612]\n",
      "[D loss: 0.958278] [G loss: 1.230916]\n",
      "[D loss: 0.821267] [G loss: 1.607800]\n",
      "[D loss: 0.904901] [G loss: 1.440362]\n",
      "[D loss: 0.785409] [G loss: 1.401492]\n",
      "[D loss: 0.792793] [G loss: 1.564870]\n",
      "[D loss: 0.707596] [G loss: 1.708646]\n",
      "[D loss: 0.761541] [G loss: 1.602127]\n",
      "[D loss: 0.798586] [G loss: 1.272017]\n",
      "[D loss: 0.716899] [G loss: 1.615903]\n",
      "[D loss: 0.920489] [G loss: 1.574932]\n",
      "[D loss: 0.980298] [G loss: 1.494122]\n",
      "[D loss: 0.858261] [G loss: 1.546242]\n",
      "[D loss: 0.646223] [G loss: 1.513273]\n",
      "[D loss: 0.837680] [G loss: 1.594416]\n",
      "[D loss: 0.718608] [G loss: 1.628893]\n",
      "[D loss: 1.129631] [G loss: 1.427328]\n",
      "[D loss: 0.872231] [G loss: 1.475251]\n",
      "[D loss: 0.746614] [G loss: 1.472192]\n",
      "[D loss: 0.749335] [G loss: 1.668824]\n",
      "[D loss: 0.814498] [G loss: 1.586172]\n",
      "[D loss: 0.786637] [G loss: 1.579543]\n",
      "[D loss: 0.772269] [G loss: 1.447531]\n",
      "[D loss: 0.723416] [G loss: 1.783414]\n",
      "[D loss: 0.742798] [G loss: 1.372257]\n",
      "[D loss: 0.949628] [G loss: 1.422994]\n",
      "[D loss: 0.783144] [G loss: 1.499616]\n",
      "[D loss: 0.632686] [G loss: 1.620132]\n",
      "[D loss: 0.970194] [G loss: 1.235307]\n",
      "[D loss: 0.750513] [G loss: 1.679736]\n",
      "[D loss: 1.030941] [G loss: 1.459350]\n",
      "[D loss: 0.879375] [G loss: 1.497720]\n",
      "[D loss: 0.967727] [G loss: 1.317502]\n",
      "[D loss: 0.759046] [G loss: 1.705113]\n",
      "[D loss: 0.716497] [G loss: 1.715151]\n",
      "[D loss: 0.816952] [G loss: 1.368180]\n",
      "[D loss: 0.648994] [G loss: 1.618744]\n",
      "[D loss: 0.754192] [G loss: 1.660198]\n",
      "[D loss: 0.736041] [G loss: 1.382391]\n",
      "[D loss: 0.693013] [G loss: 1.498447]\n",
      "[D loss: 1.159747] [G loss: 1.598549]\n",
      "[D loss: 0.655008] [G loss: 1.688025]\n",
      "[D loss: 0.946147] [G loss: 1.497838]\n",
      "[D loss: 0.911018] [G loss: 1.182855]\n",
      "[D loss: 0.713373] [G loss: 1.382260]\n",
      "[D loss: 0.775676] [G loss: 1.733224]\n",
      "[D loss: 0.883516] [G loss: 1.497431]\n",
      "[D loss: 1.134582] [G loss: 1.295970]\n",
      "[D loss: 0.580461] [G loss: 1.415871]\n",
      "[D loss: 0.790798] [G loss: 1.261660]\n",
      "[D loss: 1.031999] [G loss: 1.249015]\n",
      "[D loss: 0.839247] [G loss: 1.495618]\n",
      "[D loss: 0.981169] [G loss: 1.338269]\n",
      "[D loss: 0.811633] [G loss: 1.355016]\n",
      "[D loss: 0.730278] [G loss: 1.478835]\n",
      "[D loss: 0.955354] [G loss: 1.289006]\n",
      "[D loss: 0.865278] [G loss: 1.482476]\n",
      "[D loss: 0.913671] [G loss: 1.835137]\n",
      "[D loss: 1.012895] [G loss: 1.396223]\n",
      "[D loss: 0.730592] [G loss: 1.567141]\n",
      "[D loss: 0.824324] [G loss: 1.341191]\n",
      "[D loss: 0.807076] [G loss: 1.354127]\n",
      "[D loss: 0.760806] [G loss: 1.373650]\n",
      "[D loss: 0.992009] [G loss: 1.376940]\n",
      "[D loss: 0.810805] [G loss: 1.318369]\n",
      "[D loss: 1.032672] [G loss: 1.330869]\n",
      "[D loss: 0.965709] [G loss: 1.278550]\n",
      "[D loss: 0.981479] [G loss: 1.372690]\n",
      "[D loss: 0.807207] [G loss: 1.431898]\n",
      "[D loss: 0.739641] [G loss: 1.466508]\n",
      "[D loss: 0.890780] [G loss: 1.552164]\n",
      "[D loss: 0.596740] [G loss: 1.467904]\n",
      "[D loss: 0.670200] [G loss: 1.324940]\n",
      "[D loss: 0.995996] [G loss: 1.253475]\n",
      "[D loss: 0.976697] [G loss: 1.334369]\n",
      "[D loss: 0.760747] [G loss: 1.328288]\n",
      "[D loss: 0.986300] [G loss: 1.576996]\n",
      "[D loss: 1.030929] [G loss: 1.269344]\n",
      "[D loss: 0.766200] [G loss: 1.518800]\n",
      "[D loss: 0.750396] [G loss: 1.702826]\n",
      "[D loss: 0.829282] [G loss: 1.564464]\n",
      "[D loss: 0.842878] [G loss: 1.243191]\n",
      "[D loss: 0.984704] [G loss: 1.437226]\n",
      "[D loss: 0.874114] [G loss: 1.368686]\n",
      "[D loss: 0.800540] [G loss: 1.465876]\n",
      "[D loss: 0.983769] [G loss: 1.285168]\n",
      "[D loss: 0.921429] [G loss: 1.443262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.789070] [G loss: 1.413144]\n",
      "[D loss: 0.952546] [G loss: 1.519361]\n",
      "[D loss: 0.652634] [G loss: 1.537325]\n",
      "[D loss: 0.584235] [G loss: 1.808722]\n",
      "[D loss: 0.978442] [G loss: 1.486939]\n",
      "[D loss: 0.607416] [G loss: 1.620240]\n",
      "[D loss: 0.911570] [G loss: 1.286357]\n",
      "[D loss: 0.711313] [G loss: 1.330096]\n",
      "[D loss: 0.944660] [G loss: 1.701464]\n",
      "[D loss: 0.772133] [G loss: 1.419609]\n",
      "[D loss: 0.927335] [G loss: 1.427109]\n",
      "[D loss: 0.691415] [G loss: 1.496500]\n",
      "[D loss: 0.654349] [G loss: 1.542059]\n",
      "[D loss: 0.949777] [G loss: 1.428265]\n",
      "[D loss: 0.822739] [G loss: 1.615979]\n",
      "[D loss: 0.799885] [G loss: 1.252716]\n",
      "[D loss: 0.872539] [G loss: 1.538409]\n",
      "[D loss: 0.646021] [G loss: 1.926372]\n",
      "[D loss: 1.024676] [G loss: 1.402628]\n",
      "[D loss: 0.816368] [G loss: 1.710765]\n",
      "[D loss: 0.850749] [G loss: 1.585546]\n",
      "[D loss: 1.126454] [G loss: 1.774792]\n",
      "[D loss: 1.123184] [G loss: 1.249126]\n",
      "[D loss: 0.901154] [G loss: 1.303213]\n",
      "[D loss: 0.865750] [G loss: 1.487070]\n",
      "[D loss: 0.682697] [G loss: 1.546005]\n",
      "[D loss: 0.905641] [G loss: 1.609270]\n",
      "[D loss: 0.659727] [G loss: 1.620034]\n",
      "[D loss: 0.978220] [G loss: 1.319186]\n",
      "[D loss: 0.899515] [G loss: 1.396609]\n",
      "[D loss: 0.520243] [G loss: 1.663662]\n",
      "[D loss: 0.964485] [G loss: 1.636658]\n",
      "[D loss: 0.898891] [G loss: 1.572067]\n",
      "[D loss: 0.779331] [G loss: 1.369391]\n",
      "[D loss: 0.616335] [G loss: 1.613608]\n",
      "[D loss: 0.736287] [G loss: 1.359909]\n",
      "[D loss: 0.914333] [G loss: 1.481343]\n",
      "[D loss: 1.044844] [G loss: 1.288303]\n",
      "[D loss: 0.960145] [G loss: 1.485091]\n",
      "[D loss: 0.892305] [G loss: 1.457958]\n",
      "[D loss: 0.883602] [G loss: 1.495537]\n",
      "[D loss: 0.742132] [G loss: 1.390107]\n",
      "[D loss: 0.904814] [G loss: 1.205386]\n",
      "[D loss: 0.844231] [G loss: 1.463563]\n",
      "[D loss: 0.933937] [G loss: 1.349178]\n",
      "[D loss: 0.568909] [G loss: 1.402383]\n",
      "[D loss: 0.822337] [G loss: 1.344660]\n",
      "[D loss: 1.046185] [G loss: 1.550682]\n",
      "[D loss: 0.981698] [G loss: 1.368230]\n",
      "[D loss: 0.724049] [G loss: 1.489945]\n",
      "[D loss: 0.991259] [G loss: 1.392619]\n",
      "[D loss: 0.872115] [G loss: 1.612240]\n",
      "[D loss: 0.823355] [G loss: 1.372252]\n",
      "[D loss: 0.927184] [G loss: 1.403571]\n",
      "[D loss: 1.017697] [G loss: 1.652674]\n",
      "[D loss: 0.804941] [G loss: 1.228442]\n",
      "[D loss: 0.881195] [G loss: 1.336181]\n",
      "[D loss: 0.657436] [G loss: 1.266850]\n",
      "[D loss: 0.937362] [G loss: 1.369475]\n",
      "[D loss: 0.793730] [G loss: 1.557663]\n",
      "[D loss: 0.792840] [G loss: 1.406539]\n",
      "[D loss: 0.712867] [G loss: 1.492800]\n",
      "[D loss: 1.021646] [G loss: 1.522568]\n",
      "[D loss: 0.795973] [G loss: 1.322283]\n",
      "[D loss: 0.733731] [G loss: 1.321006]\n",
      "[D loss: 0.809953] [G loss: 1.490750]\n",
      "[D loss: 0.767628] [G loss: 1.452261]\n",
      "[D loss: 0.874561] [G loss: 1.476020]\n",
      "[D loss: 1.052873] [G loss: 1.345027]\n",
      "[D loss: 0.759757] [G loss: 1.524987]\n",
      "[D loss: 1.016113] [G loss: 1.394248]\n",
      "[D loss: 0.674716] [G loss: 1.661280]\n",
      "[D loss: 0.683669] [G loss: 1.684556]\n",
      "[D loss: 0.741891] [G loss: 1.409526]\n",
      "[D loss: 0.704760] [G loss: 1.661824]\n",
      "[D loss: 0.787048] [G loss: 1.566968]\n",
      "[D loss: 0.699037] [G loss: 1.730690]\n",
      "[D loss: 0.948450] [G loss: 1.596279]\n",
      "[D loss: 0.709359] [G loss: 1.554683]\n",
      "[D loss: 0.892027] [G loss: 1.570861]\n",
      "[D loss: 0.668510] [G loss: 1.254027]\n",
      "[D loss: 1.136069] [G loss: 1.473301]\n",
      "[D loss: 0.741964] [G loss: 1.450670]\n",
      "[D loss: 0.786260] [G loss: 1.643704]\n",
      "[D loss: 1.010831] [G loss: 1.266703]\n",
      "[D loss: 0.814990] [G loss: 1.384627]\n",
      "[D loss: 0.747975] [G loss: 1.776709]\n",
      "[D loss: 0.877556] [G loss: 1.530193]\n",
      "[D loss: 0.984566] [G loss: 1.520307]\n",
      "[D loss: 1.003726] [G loss: 1.248857]\n",
      "[D loss: 0.823931] [G loss: 1.292343]\n",
      "[D loss: 0.763321] [G loss: 1.372040]\n",
      "[D loss: 1.106925] [G loss: 1.375742]\n",
      "[D loss: 0.873226] [G loss: 1.443979]\n",
      "[D loss: 0.709779] [G loss: 1.474211]\n",
      "[D loss: 0.729605] [G loss: 1.596518]\n",
      "[D loss: 1.080657] [G loss: 1.298904]\n",
      "[D loss: 1.151533] [G loss: 1.554688]\n",
      "[D loss: 0.676723] [G loss: 1.524534]\n",
      "[D loss: 0.990147] [G loss: 1.486151]\n",
      "[D loss: 0.798357] [G loss: 1.286222]\n",
      "[D loss: 0.895223] [G loss: 1.394934]\n",
      "[D loss: 0.639273] [G loss: 1.371508]\n",
      "[D loss: 0.831054] [G loss: 1.295905]\n",
      "[D loss: 0.966052] [G loss: 1.159681]\n",
      "[D loss: 0.677562] [G loss: 1.497365]\n",
      "[D loss: 0.919326] [G loss: 1.535782]\n",
      "[D loss: 0.798643] [G loss: 1.400938]\n",
      "[D loss: 0.856622] [G loss: 1.504018]\n",
      "[D loss: 0.916327] [G loss: 1.356210]\n",
      "[D loss: 0.813021] [G loss: 1.351045]\n",
      "[D loss: 0.838370] [G loss: 1.357791]\n",
      "[D loss: 0.829975] [G loss: 1.416275]\n",
      "[D loss: 0.884581] [G loss: 1.375073]\n",
      "[D loss: 0.798742] [G loss: 1.620095]\n",
      "[D loss: 0.780459] [G loss: 1.517677]\n",
      "[D loss: 0.588867] [G loss: 1.477634]\n",
      "[D loss: 0.925324] [G loss: 1.452918]\n",
      "[D loss: 0.875780] [G loss: 1.539486]\n",
      "[D loss: 0.912383] [G loss: 1.366460]\n",
      "[D loss: 0.938067] [G loss: 1.498967]\n",
      "[D loss: 1.139419] [G loss: 1.578865]\n",
      "[D loss: 0.794259] [G loss: 1.545947]\n",
      "[D loss: 0.726370] [G loss: 1.437261]\n",
      "[D loss: 1.086178] [G loss: 1.439640]\n",
      "[D loss: 0.874815] [G loss: 1.308166]\n",
      "[D loss: 0.907304] [G loss: 1.353032]\n",
      "[D loss: 0.697494] [G loss: 1.402882]\n",
      "[D loss: 0.894985] [G loss: 1.491355]\n",
      "[D loss: 0.840492] [G loss: 1.555067]\n",
      "[D loss: 0.937979] [G loss: 1.534762]\n",
      "[D loss: 0.791125] [G loss: 1.491437]\n",
      "[D loss: 0.954967] [G loss: 1.285628]\n",
      "[D loss: 0.890005] [G loss: 1.416848]\n",
      "[D loss: 0.787657] [G loss: 1.491073]\n",
      "[D loss: 0.848526] [G loss: 1.369528]\n",
      "[D loss: 0.859365] [G loss: 1.264296]\n",
      "[D loss: 0.744813] [G loss: 1.291282]\n",
      "[D loss: 0.963941] [G loss: 1.348432]\n",
      "[D loss: 0.769679] [G loss: 1.459369]\n",
      "[D loss: 1.022018] [G loss: 1.438060]\n",
      "[D loss: 0.725778] [G loss: 1.456382]\n",
      "[D loss: 0.798790] [G loss: 1.245814]\n",
      "[D loss: 0.967462] [G loss: 1.549140]\n",
      "[D loss: 0.726353] [G loss: 1.600745]\n",
      "[D loss: 0.811783] [G loss: 1.375775]\n",
      "[D loss: 1.000752] [G loss: 1.274535]\n",
      "[D loss: 0.830240] [G loss: 1.475546]\n",
      "[D loss: 0.968387] [G loss: 1.220423]\n",
      "[D loss: 0.721314] [G loss: 1.307717]\n",
      "[D loss: 0.700488] [G loss: 1.312904]\n",
      "[D loss: 0.673729] [G loss: 1.179697]\n",
      "[D loss: 0.810687] [G loss: 1.328401]\n",
      "[D loss: 0.813023] [G loss: 1.386036]\n",
      "[D loss: 0.743887] [G loss: 1.508155]\n",
      "[D loss: 1.006046] [G loss: 1.533811]\n",
      "[D loss: 1.226502] [G loss: 1.362916]\n",
      "[D loss: 0.799404] [G loss: 1.368862]\n",
      "[D loss: 1.038771] [G loss: 1.540049]\n",
      "[D loss: 0.866863] [G loss: 1.602362]\n",
      "[D loss: 0.747174] [G loss: 1.504500]\n",
      "[D loss: 0.733447] [G loss: 1.452005]\n",
      "[D loss: 0.965868] [G loss: 1.278893]\n",
      "[D loss: 0.899105] [G loss: 1.580759]\n",
      "[D loss: 0.799725] [G loss: 1.453615]\n",
      "[D loss: 0.849452] [G loss: 1.338238]\n",
      "[D loss: 0.967500] [G loss: 1.562696]\n",
      "[D loss: 0.932097] [G loss: 1.206223]\n",
      "[D loss: 0.960594] [G loss: 1.288667]\n",
      "[D loss: 0.830678] [G loss: 1.465995]\n",
      "[D loss: 0.977336] [G loss: 1.500320]\n",
      "[D loss: 0.690894] [G loss: 1.670380]\n",
      "[D loss: 0.795460] [G loss: 1.466372]\n",
      "[D loss: 0.689170] [G loss: 1.586338]\n",
      "[D loss: 1.066715] [G loss: 1.648759]\n",
      "[D loss: 0.848404] [G loss: 1.464936]\n",
      "[D loss: 0.719811] [G loss: 1.552204]\n",
      "[D loss: 0.635117] [G loss: 1.596246]\n",
      "[D loss: 0.901620] [G loss: 1.308429]\n",
      "[D loss: 0.679720] [G loss: 1.533932]\n",
      "[D loss: 1.023404] [G loss: 1.314052]\n",
      "[D loss: 0.753357] [G loss: 1.422690]\n",
      "[D loss: 0.981359] [G loss: 1.378255]\n",
      "[D loss: 0.723904] [G loss: 1.373941]\n",
      "[D loss: 0.861957] [G loss: 1.615261]\n",
      "[D loss: 0.946095] [G loss: 1.711363]\n",
      "[D loss: 0.753757] [G loss: 1.567920]\n",
      "[D loss: 1.119053] [G loss: 1.201065]\n",
      "[D loss: 0.859505] [G loss: 1.491871]\n",
      "[D loss: 0.719342] [G loss: 1.425133]\n",
      "[D loss: 0.644821] [G loss: 1.551722]\n",
      "[D loss: 0.942175] [G loss: 1.374821]\n",
      "[D loss: 0.912480] [G loss: 1.461211]\n",
      "[D loss: 0.831131] [G loss: 1.352498]\n",
      "[D loss: 0.870828] [G loss: 1.174054]\n",
      "[D loss: 0.739728] [G loss: 1.284247]\n",
      "[D loss: 0.827201] [G loss: 1.272909]\n",
      "[D loss: 0.946321] [G loss: 1.415770]\n",
      "[D loss: 0.957044] [G loss: 1.184030]\n",
      "[D loss: 0.851862] [G loss: 1.406135]\n",
      "[D loss: 0.798630] [G loss: 1.830239]\n",
      "[D loss: 0.954028] [G loss: 1.280793]\n",
      "[D loss: 0.860456] [G loss: 1.389125]\n",
      "[D loss: 0.949561] [G loss: 1.441909]\n",
      "[D loss: 0.916306] [G loss: 1.498620]\n",
      "[D loss: 1.017321] [G loss: 1.185534]\n",
      "[D loss: 0.904816] [G loss: 1.175488]\n",
      "[D loss: 0.868312] [G loss: 1.242382]\n",
      "[D loss: 0.940397] [G loss: 1.485979]\n",
      "[D loss: 1.037088] [G loss: 1.550163]\n",
      "[D loss: 0.770719] [G loss: 1.375778]\n",
      "[D loss: 0.837166] [G loss: 1.638122]\n",
      "[D loss: 1.080670] [G loss: 1.194761]\n",
      "[D loss: 0.872411] [G loss: 1.519649]\n",
      "[D loss: 0.817609] [G loss: 1.238638]\n",
      "[D loss: 0.760499] [G loss: 1.302877]\n",
      "[D loss: 0.740310] [G loss: 1.435938]\n",
      "[D loss: 0.795658] [G loss: 1.627952]\n",
      "[D loss: 0.692333] [G loss: 1.425871]\n",
      "[D loss: 0.641636] [G loss: 1.220432]\n",
      "[D loss: 0.887253] [G loss: 1.514931]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 1.188057] [G loss: 1.500811]\n",
      "[D loss: 0.840406] [G loss: 1.586947]\n",
      "[D loss: 0.882738] [G loss: 1.578551]\n",
      "[D loss: 0.816803] [G loss: 1.395502]\n",
      "[D loss: 0.681601] [G loss: 1.519721]\n",
      "[D loss: 0.752991] [G loss: 1.659070]\n",
      "[D loss: 0.919458] [G loss: 1.490604]\n",
      "[D loss: 0.768226] [G loss: 1.396940]\n",
      "[D loss: 0.892249] [G loss: 1.593684]\n",
      "[D loss: 0.814991] [G loss: 1.424786]\n",
      "[D loss: 0.852169] [G loss: 1.297894]\n",
      "[D loss: 0.707139] [G loss: 1.481797]\n",
      "[D loss: 0.810722] [G loss: 1.734480]\n",
      "[D loss: 0.925804] [G loss: 1.375826]\n",
      "[D loss: 0.659491] [G loss: 1.592155]\n",
      "[D loss: 0.845804] [G loss: 1.443342]\n",
      "[D loss: 1.120038] [G loss: 1.494759]\n",
      "[D loss: 0.806432] [G loss: 1.536751]\n",
      "[D loss: 0.694086] [G loss: 1.416383]\n",
      "[D loss: 0.937710] [G loss: 1.462444]\n",
      "[D loss: 0.751945] [G loss: 1.508908]\n",
      "[D loss: 0.899713] [G loss: 1.502162]\n",
      "[D loss: 0.899874] [G loss: 1.404715]\n",
      "[D loss: 0.829115] [G loss: 1.196437]\n",
      "[D loss: 1.003394] [G loss: 1.097721]\n",
      "[D loss: 0.775417] [G loss: 1.471006]\n",
      "[D loss: 0.886188] [G loss: 1.565469]\n",
      "[D loss: 0.797878] [G loss: 1.564158]\n",
      "[D loss: 0.731217] [G loss: 1.524402]\n",
      "[D loss: 1.065703] [G loss: 1.420604]\n",
      "[D loss: 0.905319] [G loss: 1.324172]\n",
      "[D loss: 0.893605] [G loss: 1.343310]\n",
      "[D loss: 0.780260] [G loss: 1.384428]\n",
      "[D loss: 0.799577] [G loss: 1.497546]\n",
      "[D loss: 0.823434] [G loss: 1.725887]\n",
      "[D loss: 0.904557] [G loss: 1.584582]\n",
      "[D loss: 1.060474] [G loss: 1.458145]\n",
      "[D loss: 0.796173] [G loss: 1.421924]\n",
      "[D loss: 0.803490] [G loss: 1.486409]\n",
      "[D loss: 0.822198] [G loss: 1.360563]\n",
      "[D loss: 0.770332] [G loss: 1.379769]\n",
      "[D loss: 0.815859] [G loss: 1.286345]\n",
      "[D loss: 0.900184] [G loss: 1.552948]\n",
      "[D loss: 0.900848] [G loss: 1.544018]\n",
      "[D loss: 0.942180] [G loss: 1.401136]\n",
      "[D loss: 0.812039] [G loss: 1.303824]\n",
      "[D loss: 0.762066] [G loss: 1.332264]\n",
      "[D loss: 1.054546] [G loss: 1.198106]\n",
      "[D loss: 0.943372] [G loss: 1.251170]\n",
      "[D loss: 0.772091] [G loss: 1.362537]\n",
      "[D loss: 0.779681] [G loss: 1.362564]\n",
      "[D loss: 0.917193] [G loss: 1.660949]\n",
      "[D loss: 0.782189] [G loss: 1.453375]\n",
      "[D loss: 0.851909] [G loss: 1.509727]\n",
      "[D loss: 0.734589] [G loss: 1.382893]\n",
      "[D loss: 0.871705] [G loss: 1.325814]\n",
      "[D loss: 0.755440] [G loss: 1.415804]\n",
      "[D loss: 0.644064] [G loss: 1.472658]\n",
      "[D loss: 0.854909] [G loss: 1.299993]\n",
      "[D loss: 0.888759] [G loss: 1.734493]\n",
      "[D loss: 0.875067] [G loss: 1.228761]\n",
      "[D loss: 0.712481] [G loss: 1.621178]\n",
      "[D loss: 0.616871] [G loss: 1.510712]\n",
      "[D loss: 0.984665] [G loss: 1.404045]\n",
      "[D loss: 0.931929] [G loss: 1.416631]\n",
      "[D loss: 0.868059] [G loss: 1.443516]\n",
      "[D loss: 0.942443] [G loss: 1.475749]\n",
      "[D loss: 0.944015] [G loss: 1.352978]\n",
      "[D loss: 0.795361] [G loss: 1.420837]\n",
      "[D loss: 0.695880] [G loss: 1.383841]\n",
      "[D loss: 0.793456] [G loss: 1.465507]\n",
      "[D loss: 0.794743] [G loss: 1.555642]\n",
      "[D loss: 0.725665] [G loss: 1.443393]\n",
      "[D loss: 0.900682] [G loss: 1.436586]\n",
      "[D loss: 0.865458] [G loss: 1.528083]\n",
      "[D loss: 0.816804] [G loss: 1.577552]\n",
      "[D loss: 1.040205] [G loss: 1.434018]\n",
      "[D loss: 0.936687] [G loss: 1.439781]\n",
      "[D loss: 0.845575] [G loss: 1.492472]\n",
      "[D loss: 0.617060] [G loss: 1.518578]\n",
      "[D loss: 0.896870] [G loss: 1.475515]\n",
      "[D loss: 0.976379] [G loss: 1.586075]\n",
      "[D loss: 0.806080] [G loss: 1.525541]\n",
      "[D loss: 0.963544] [G loss: 1.527117]\n",
      "[D loss: 1.362881] [G loss: 1.438089]\n",
      "[D loss: 0.778836] [G loss: 1.268405]\n",
      "[D loss: 0.806695] [G loss: 1.370598]\n",
      "[D loss: 0.680968] [G loss: 1.363966]\n",
      "[D loss: 0.712524] [G loss: 1.324078]\n",
      "[D loss: 0.902862] [G loss: 1.441135]\n",
      "[D loss: 0.664760] [G loss: 1.459055]\n",
      "[D loss: 0.823510] [G loss: 1.471811]\n",
      "[D loss: 1.055405] [G loss: 1.445668]\n",
      "[D loss: 0.776348] [G loss: 1.497915]\n",
      "[D loss: 0.944606] [G loss: 1.403190]\n",
      "[D loss: 0.987053] [G loss: 1.309267]\n",
      "[D loss: 0.748669] [G loss: 1.443590]\n",
      "[D loss: 1.118449] [G loss: 1.159021]\n",
      "[D loss: 0.853318] [G loss: 1.475285]\n",
      "[D loss: 0.751064] [G loss: 1.508119]\n",
      "[D loss: 0.776662] [G loss: 1.334059]\n",
      "[D loss: 0.877986] [G loss: 1.516350]\n",
      "[D loss: 0.805734] [G loss: 1.642143]\n",
      "[D loss: 0.913463] [G loss: 1.315040]\n",
      "[D loss: 0.832808] [G loss: 1.385437]\n",
      "[D loss: 0.807810] [G loss: 1.500526]\n",
      "[D loss: 0.879405] [G loss: 1.499444]\n",
      "[D loss: 0.847630] [G loss: 1.279719]\n",
      "[D loss: 0.775815] [G loss: 1.381588]\n",
      "[D loss: 0.620169] [G loss: 1.455491]\n",
      "[D loss: 0.803075] [G loss: 1.213272]\n",
      "[D loss: 0.901351] [G loss: 1.284761]\n",
      "[D loss: 0.886199] [G loss: 1.445096]\n",
      "[D loss: 0.872804] [G loss: 1.349197]\n",
      "[D loss: 0.681335] [G loss: 1.691976]\n",
      "[D loss: 0.826125] [G loss: 1.363212]\n",
      "[D loss: 0.691009] [G loss: 1.548886]\n",
      "[D loss: 0.963847] [G loss: 1.384301]\n",
      "[D loss: 0.854991] [G loss: 1.448152]\n",
      "[D loss: 0.829834] [G loss: 1.518403]\n",
      "[D loss: 0.855037] [G loss: 1.458299]\n",
      "[D loss: 1.081182] [G loss: 1.310669]\n",
      "[D loss: 0.994888] [G loss: 1.254339]\n",
      "[D loss: 0.937960] [G loss: 1.330189]\n",
      "[D loss: 1.073481] [G loss: 1.310903]\n",
      "[D loss: 0.910853] [G loss: 1.481352]\n",
      "[D loss: 0.784253] [G loss: 1.313428]\n",
      "[D loss: 1.034849] [G loss: 1.210529]\n",
      "[D loss: 0.953860] [G loss: 1.275686]\n",
      "[D loss: 0.819336] [G loss: 1.503756]\n",
      "[D loss: 0.856174] [G loss: 1.313380]\n",
      "[D loss: 0.693887] [G loss: 1.255571]\n",
      "[D loss: 0.621719] [G loss: 1.430672]\n",
      "[D loss: 0.807781] [G loss: 1.393725]\n",
      "[D loss: 1.041612] [G loss: 1.457935]\n",
      "[D loss: 0.857594] [G loss: 1.310226]\n",
      "[D loss: 0.760971] [G loss: 1.627419]\n",
      "[D loss: 0.827081] [G loss: 1.460905]\n",
      "[D loss: 0.758707] [G loss: 1.559626]\n",
      "[D loss: 0.835784] [G loss: 1.573890]\n",
      "[D loss: 0.813156] [G loss: 1.478674]\n",
      "[D loss: 0.722273] [G loss: 1.531450]\n",
      "[D loss: 0.914931] [G loss: 1.315980]\n",
      "[D loss: 1.078728] [G loss: 1.299409]\n",
      "[D loss: 0.855548] [G loss: 1.247262]\n",
      "[D loss: 0.782565] [G loss: 1.605825]\n",
      "[D loss: 0.817940] [G loss: 1.510574]\n",
      "[D loss: 0.760169] [G loss: 1.409923]\n",
      "[D loss: 0.917502] [G loss: 1.344035]\n",
      "[D loss: 0.761711] [G loss: 1.355873]\n",
      "[D loss: 0.851489] [G loss: 1.338435]\n",
      "[D loss: 1.177405] [G loss: 1.268477]\n",
      "[D loss: 0.946355] [G loss: 1.489876]\n",
      "[D loss: 0.814541] [G loss: 1.402181]\n",
      "[D loss: 0.885041] [G loss: 1.472148]\n",
      "[D loss: 0.778782] [G loss: 1.571286]\n",
      "[D loss: 0.904471] [G loss: 1.285054]\n",
      "[D loss: 0.819287] [G loss: 1.364049]\n",
      "[D loss: 0.674081] [G loss: 1.499583]\n",
      "[D loss: 0.810977] [G loss: 1.461536]\n",
      "[D loss: 0.746999] [G loss: 1.416251]\n",
      "[D loss: 0.633218] [G loss: 1.454853]\n",
      "[D loss: 0.743950] [G loss: 1.707732]\n",
      "[D loss: 0.794327] [G loss: 1.739110]\n",
      "[D loss: 0.719628] [G loss: 1.729020]\n",
      "[D loss: 0.741777] [G loss: 1.557796]\n",
      "[D loss: 0.878200] [G loss: 1.576395]\n",
      "[D loss: 0.883598] [G loss: 1.491419]\n",
      "[D loss: 0.989152] [G loss: 1.509654]\n",
      "[D loss: 1.147819] [G loss: 1.370645]\n",
      "[D loss: 0.882560] [G loss: 1.400847]\n",
      "[D loss: 0.999849] [G loss: 1.255687]\n",
      "[D loss: 0.908056] [G loss: 1.373412]\n",
      "[D loss: 0.822980] [G loss: 1.676950]\n",
      "[D loss: 1.013809] [G loss: 1.518497]\n",
      "[D loss: 0.803250] [G loss: 1.336215]\n",
      "[D loss: 0.666810] [G loss: 1.464436]\n",
      "[D loss: 0.904071] [G loss: 1.224723]\n",
      "[D loss: 1.069355] [G loss: 1.337266]\n",
      "[D loss: 0.733551] [G loss: 1.312896]\n",
      "[D loss: 0.999867] [G loss: 1.527709]\n",
      "[D loss: 0.842377] [G loss: 1.442695]\n",
      "[D loss: 0.785995] [G loss: 1.341377]\n",
      "[D loss: 0.859683] [G loss: 1.466287]\n",
      "[D loss: 1.029858] [G loss: 1.455428]\n",
      "[D loss: 0.818511] [G loss: 1.399659]\n",
      "[D loss: 0.880959] [G loss: 1.435176]\n",
      "[D loss: 0.872489] [G loss: 1.485656]\n",
      "[D loss: 0.998532] [G loss: 1.370087]\n",
      "[D loss: 0.734369] [G loss: 1.458634]\n",
      "[D loss: 0.643694] [G loss: 1.635078]\n",
      "[D loss: 0.771278] [G loss: 1.469772]\n",
      "[D loss: 0.801340] [G loss: 1.307505]\n",
      "[D loss: 0.741560] [G loss: 1.659411]\n",
      "[D loss: 0.873575] [G loss: 1.424793]\n",
      "[D loss: 0.790329] [G loss: 1.473328]\n",
      "[D loss: 0.975725] [G loss: 1.453913]\n",
      "[D loss: 0.777699] [G loss: 1.355163]\n",
      "[D loss: 0.878346] [G loss: 1.450791]\n",
      "[D loss: 0.989955] [G loss: 1.446767]\n",
      "[D loss: 0.704877] [G loss: 1.466347]\n",
      "[D loss: 0.942422] [G loss: 1.303583]\n",
      "[D loss: 1.106587] [G loss: 1.339211]\n",
      "[D loss: 1.056950] [G loss: 1.379954]\n",
      "[D loss: 0.867171] [G loss: 1.399308]\n",
      "[D loss: 0.869143] [G loss: 1.626631]\n",
      "[D loss: 0.951304] [G loss: 1.192843]\n",
      "[D loss: 1.223992] [G loss: 1.158862]\n",
      "[D loss: 0.969124] [G loss: 1.353445]\n",
      "[D loss: 0.737292] [G loss: 1.421865]\n",
      "[D loss: 0.740093] [G loss: 1.338219]\n",
      "[D loss: 0.741549] [G loss: 1.518812]\n",
      "[D loss: 0.701688] [G loss: 1.387786]\n",
      "[D loss: 0.729915] [G loss: 1.412780]\n",
      "[D loss: 0.730825] [G loss: 1.396865]\n",
      "[D loss: 0.744973] [G loss: 1.455203]\n",
      "[D loss: 0.879110] [G loss: 1.421277]\n",
      "[D loss: 0.890075] [G loss: 1.322527]\n",
      "[D loss: 0.732889] [G loss: 1.336334]\n",
      "[D loss: 0.771466] [G loss: 1.302268]\n",
      "[D loss: 0.810266] [G loss: 1.432222]\n",
      "[D loss: 0.953447] [G loss: 1.326657]\n",
      "[D loss: 0.765801] [G loss: 1.369251]\n",
      "[D loss: 0.781597] [G loss: 1.483972]\n",
      "[D loss: 0.684222] [G loss: 1.723854]\n",
      "[D loss: 0.903599] [G loss: 1.755143]\n",
      "[D loss: 0.819424] [G loss: 1.565763]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.948961] [G loss: 1.327566]\n",
      "[D loss: 1.004026] [G loss: 1.299478]\n",
      "[D loss: 0.765134] [G loss: 1.327307]\n",
      "[D loss: 0.906370] [G loss: 1.310050]\n",
      "[D loss: 0.952175] [G loss: 1.581568]\n",
      "[D loss: 0.764724] [G loss: 1.698250]\n",
      "[D loss: 1.067689] [G loss: 1.299047]\n",
      "[D loss: 0.814742] [G loss: 1.404203]\n",
      "[D loss: 1.012146] [G loss: 1.250287]\n",
      "[D loss: 0.697257] [G loss: 1.414060]\n",
      "[D loss: 0.742401] [G loss: 1.283748]\n",
      "[D loss: 0.878420] [G loss: 1.659897]\n",
      "[D loss: 0.699936] [G loss: 1.473736]\n",
      "[D loss: 0.828942] [G loss: 1.249759]\n",
      "[D loss: 0.972365] [G loss: 1.156251]\n",
      "[D loss: 0.685972] [G loss: 1.588418]\n",
      "[D loss: 1.032721] [G loss: 1.478475]\n",
      "[D loss: 0.963936] [G loss: 1.666610]\n",
      "[D loss: 0.776212] [G loss: 1.550728]\n",
      "[D loss: 0.852448] [G loss: 1.329552]\n",
      "[D loss: 0.985558] [G loss: 1.338353]\n",
      "[D loss: 0.817395] [G loss: 1.489650]\n",
      "[D loss: 0.751278] [G loss: 1.666844]\n",
      "[D loss: 0.760226] [G loss: 1.397842]\n",
      "[D loss: 0.725363] [G loss: 1.434718]\n",
      "[D loss: 0.928143] [G loss: 1.209418]\n",
      "[D loss: 0.830322] [G loss: 1.457998]\n",
      "[D loss: 0.943224] [G loss: 1.574602]\n",
      "[D loss: 0.810325] [G loss: 1.353095]\n",
      "[D loss: 0.750033] [G loss: 1.520620]\n",
      "[D loss: 0.752038] [G loss: 1.372809]\n",
      "[D loss: 1.145649] [G loss: 1.352612]\n",
      "[D loss: 0.808321] [G loss: 1.568335]\n",
      "[D loss: 0.682504] [G loss: 1.437671]\n",
      "[D loss: 0.724702] [G loss: 1.542769]\n",
      "[D loss: 0.992093] [G loss: 1.407259]\n",
      "[D loss: 0.895741] [G loss: 1.638118]\n",
      "[D loss: 0.896040] [G loss: 1.436711]\n",
      "[D loss: 0.872216] [G loss: 1.508301]\n",
      "[D loss: 0.659692] [G loss: 1.538106]\n",
      "[D loss: 0.891940] [G loss: 1.433714]\n",
      "[D loss: 0.829975] [G loss: 1.656064]\n",
      "[D loss: 1.173612] [G loss: 1.514471]\n",
      "[D loss: 0.818120] [G loss: 1.583781]\n",
      "[D loss: 0.691246] [G loss: 1.348696]\n",
      "[D loss: 0.971888] [G loss: 1.192709]\n",
      "[D loss: 0.731971] [G loss: 1.459728]\n",
      "[D loss: 0.751243] [G loss: 1.396731]\n",
      "[D loss: 0.875644] [G loss: 1.330010]\n",
      "[D loss: 0.803185] [G loss: 1.668244]\n",
      "[D loss: 0.909854] [G loss: 1.461256]\n",
      "[D loss: 0.683238] [G loss: 1.432576]\n",
      "[D loss: 1.262736] [G loss: 1.459920]\n",
      "[D loss: 0.811953] [G loss: 1.513080]\n",
      "[D loss: 0.674717] [G loss: 1.422426]\n",
      "[D loss: 0.799846] [G loss: 1.508503]\n",
      "[D loss: 0.892123] [G loss: 1.452004]\n",
      "[D loss: 1.144321] [G loss: 1.521788]\n",
      "[D loss: 0.683671] [G loss: 1.409851]\n",
      "[D loss: 0.738068] [G loss: 1.509187]\n",
      "[D loss: 0.969458] [G loss: 1.378423]\n",
      "[D loss: 0.742992] [G loss: 1.663376]\n",
      "[D loss: 0.809043] [G loss: 1.219820]\n",
      "[D loss: 0.741198] [G loss: 1.598031]\n",
      "[D loss: 0.874964] [G loss: 1.514277]\n",
      "[D loss: 0.647361] [G loss: 1.354734]\n",
      "[D loss: 1.070728] [G loss: 1.472272]\n",
      "[D loss: 0.917870] [G loss: 1.534120]\n",
      "[D loss: 1.030119] [G loss: 1.410910]\n",
      "[D loss: 0.780235] [G loss: 1.278747]\n",
      "[D loss: 0.786022] [G loss: 1.350148]\n",
      "[D loss: 0.606583] [G loss: 1.448986]\n",
      "[D loss: 0.532994] [G loss: 1.570514]\n",
      "[D loss: 1.013131] [G loss: 1.367647]\n",
      "[D loss: 0.854804] [G loss: 1.651080]\n",
      "[D loss: 0.727055] [G loss: 1.420578]\n",
      "[D loss: 1.002381] [G loss: 1.317538]\n",
      "[D loss: 1.049532] [G loss: 1.428749]\n",
      "[D loss: 0.779785] [G loss: 1.478793]\n",
      "[D loss: 0.618202] [G loss: 1.578473]\n",
      "[D loss: 1.222818] [G loss: 1.251721]\n",
      "[D loss: 0.620882] [G loss: 1.424922]\n",
      "[D loss: 1.122773] [G loss: 1.422954]\n",
      "[D loss: 0.867222] [G loss: 1.247699]\n",
      "[D loss: 1.078060] [G loss: 1.370713]\n",
      "[D loss: 0.887503] [G loss: 1.401029]\n",
      "[D loss: 0.841446] [G loss: 1.649535]\n",
      "[D loss: 0.716482] [G loss: 1.409577]\n",
      "[D loss: 1.021727] [G loss: 1.148291]\n",
      "[D loss: 0.787643] [G loss: 1.494282]\n",
      "[D loss: 0.754605] [G loss: 1.430446]\n",
      "[D loss: 0.867937] [G loss: 1.382725]\n",
      "[D loss: 0.953274] [G loss: 1.204829]\n",
      "[D loss: 0.906994] [G loss: 1.681649]\n",
      "[D loss: 0.688733] [G loss: 1.554769]\n",
      "[D loss: 0.785709] [G loss: 1.530131]\n",
      "[D loss: 0.747195] [G loss: 1.397108]\n",
      "[D loss: 0.948630] [G loss: 1.447601]\n",
      "[D loss: 1.145573] [G loss: 1.323400]\n",
      "[D loss: 0.684719] [G loss: 1.435784]\n",
      "[D loss: 0.691174] [G loss: 1.535111]\n",
      "[D loss: 0.772014] [G loss: 1.349289]\n",
      "[D loss: 0.864954] [G loss: 1.342539]\n",
      "[D loss: 0.891459] [G loss: 1.490088]\n",
      "[D loss: 0.797825] [G loss: 1.542684]\n",
      "[D loss: 0.926741] [G loss: 1.493360]\n",
      "[D loss: 0.713434] [G loss: 1.224069]\n",
      "[D loss: 0.659612] [G loss: 1.544153]\n",
      "[D loss: 0.562208] [G loss: 1.483478]\n",
      "[D loss: 0.904424] [G loss: 1.541912]\n",
      "[D loss: 0.785269] [G loss: 1.720416]\n",
      "[D loss: 1.043022] [G loss: 1.427932]\n",
      "[D loss: 0.756932] [G loss: 1.606112]\n",
      "[D loss: 0.856080] [G loss: 1.407162]\n",
      "[D loss: 0.970658] [G loss: 1.324543]\n",
      "[D loss: 0.695842] [G loss: 1.585000]\n",
      "[D loss: 0.859805] [G loss: 1.645891]\n",
      "[D loss: 0.750381] [G loss: 1.405827]\n",
      "[D loss: 0.848813] [G loss: 1.353814]\n",
      "[D loss: 0.911783] [G loss: 1.473875]\n",
      "[D loss: 0.941142] [G loss: 1.433999]\n",
      "[D loss: 0.572944] [G loss: 1.712292]\n",
      "[D loss: 0.795093] [G loss: 1.531830]\n",
      "[D loss: 0.965443] [G loss: 1.454179]\n",
      "[D loss: 0.969183] [G loss: 1.431724]\n",
      "[D loss: 0.890246] [G loss: 1.675689]\n",
      "[D loss: 0.803971] [G loss: 1.598666]\n",
      "[D loss: 0.719017] [G loss: 1.478850]\n",
      "[D loss: 0.835710] [G loss: 1.463590]\n",
      "[D loss: 0.854887] [G loss: 1.443627]\n",
      "[D loss: 0.931792] [G loss: 1.416325]\n",
      "epoch:19, g_loss:2746.748779296875,d_loss:1569.2252197265625\n",
      "[D loss: 0.696795] [G loss: 1.564242]\n",
      "[D loss: 0.831006] [G loss: 1.491367]\n",
      "[D loss: 0.768125] [G loss: 1.499194]\n",
      "[D loss: 1.013670] [G loss: 1.695245]\n",
      "[D loss: 0.933070] [G loss: 1.293752]\n",
      "[D loss: 0.883861] [G loss: 1.319052]\n",
      "[D loss: 0.963821] [G loss: 1.394899]\n",
      "[D loss: 0.817285] [G loss: 1.510160]\n",
      "[D loss: 0.771819] [G loss: 1.400445]\n",
      "[D loss: 0.759050] [G loss: 1.356495]\n",
      "[D loss: 0.742482] [G loss: 1.736948]\n",
      "[D loss: 0.714246] [G loss: 1.508602]\n",
      "[D loss: 0.843885] [G loss: 1.475095]\n",
      "[D loss: 0.709256] [G loss: 1.423443]\n",
      "[D loss: 0.869223] [G loss: 1.367005]\n",
      "[D loss: 0.694790] [G loss: 1.479604]\n",
      "[D loss: 1.013881] [G loss: 1.465752]\n",
      "[D loss: 0.897217] [G loss: 1.493456]\n",
      "[D loss: 0.674683] [G loss: 1.498202]\n",
      "[D loss: 1.178756] [G loss: 2.073841]\n",
      "[D loss: 1.003388] [G loss: 1.476161]\n",
      "[D loss: 0.736025] [G loss: 1.412999]\n",
      "[D loss: 0.731630] [G loss: 1.420823]\n",
      "[D loss: 0.831392] [G loss: 1.229092]\n",
      "[D loss: 0.851572] [G loss: 1.379704]\n",
      "[D loss: 0.713548] [G loss: 1.392973]\n",
      "[D loss: 0.918148] [G loss: 1.351495]\n",
      "[D loss: 0.793218] [G loss: 1.392095]\n",
      "[D loss: 0.706307] [G loss: 1.550355]\n",
      "[D loss: 0.917988] [G loss: 1.356610]\n",
      "[D loss: 0.897256] [G loss: 1.368888]\n",
      "[D loss: 0.716070] [G loss: 1.350784]\n",
      "[D loss: 0.909870] [G loss: 1.582025]\n",
      "[D loss: 0.799332] [G loss: 1.551248]\n",
      "[D loss: 0.773650] [G loss: 1.425762]\n",
      "[D loss: 1.120754] [G loss: 1.489841]\n",
      "[D loss: 0.793545] [G loss: 1.353160]\n",
      "[D loss: 0.897759] [G loss: 1.403284]\n",
      "[D loss: 0.642397] [G loss: 1.366760]\n",
      "[D loss: 0.996093] [G loss: 1.429438]\n",
      "[D loss: 0.771491] [G loss: 1.651800]\n",
      "[D loss: 0.693784] [G loss: 1.433227]\n",
      "[D loss: 1.110902] [G loss: 1.442841]\n",
      "[D loss: 0.951292] [G loss: 1.468414]\n",
      "[D loss: 0.820726] [G loss: 1.478145]\n",
      "[D loss: 0.826236] [G loss: 1.328417]\n",
      "[D loss: 0.856515] [G loss: 1.220599]\n",
      "[D loss: 0.846554] [G loss: 1.368294]\n",
      "[D loss: 0.776666] [G loss: 1.359727]\n",
      "[D loss: 0.683955] [G loss: 1.399637]\n",
      "[D loss: 0.995966] [G loss: 1.362046]\n",
      "[D loss: 0.898444] [G loss: 1.513748]\n",
      "[D loss: 0.971018] [G loss: 1.359877]\n",
      "[D loss: 0.926192] [G loss: 1.222394]\n",
      "[D loss: 0.734821] [G loss: 1.399060]\n",
      "[D loss: 0.846766] [G loss: 1.650996]\n",
      "[D loss: 0.843437] [G loss: 1.385502]\n",
      "[D loss: 0.891104] [G loss: 1.434305]\n",
      "[D loss: 0.752866] [G loss: 1.427534]\n",
      "[D loss: 0.766516] [G loss: 1.536890]\n",
      "[D loss: 0.723527] [G loss: 1.534451]\n",
      "[D loss: 0.881818] [G loss: 1.363054]\n",
      "[D loss: 0.806765] [G loss: 1.228625]\n",
      "[D loss: 0.843002] [G loss: 1.453363]\n",
      "[D loss: 0.767003] [G loss: 1.521660]\n",
      "[D loss: 0.873432] [G loss: 1.418244]\n",
      "[D loss: 0.837409] [G loss: 1.603040]\n",
      "[D loss: 0.740543] [G loss: 1.549892]\n",
      "[D loss: 0.802266] [G loss: 1.416642]\n",
      "[D loss: 0.633553] [G loss: 1.641892]\n",
      "[D loss: 0.746206] [G loss: 1.498394]\n",
      "[D loss: 0.962405] [G loss: 1.689540]\n",
      "[D loss: 0.676356] [G loss: 1.466616]\n",
      "[D loss: 0.639309] [G loss: 1.333440]\n",
      "[D loss: 0.866282] [G loss: 1.369312]\n",
      "[D loss: 1.124145] [G loss: 1.217121]\n",
      "[D loss: 0.845551] [G loss: 1.417377]\n",
      "[D loss: 0.934486] [G loss: 1.326183]\n",
      "[D loss: 0.681548] [G loss: 1.394876]\n",
      "[D loss: 0.879237] [G loss: 1.521317]\n",
      "[D loss: 0.891769] [G loss: 1.601092]\n",
      "[D loss: 0.813991] [G loss: 1.613443]\n",
      "[D loss: 0.674161] [G loss: 1.517328]\n",
      "[D loss: 0.573592] [G loss: 1.583379]\n",
      "[D loss: 0.823757] [G loss: 1.411638]\n",
      "[D loss: 0.962022] [G loss: 1.630298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.627187] [G loss: 1.541527]\n",
      "[D loss: 0.845448] [G loss: 1.516114]\n",
      "[D loss: 0.754764] [G loss: 1.463436]\n",
      "[D loss: 0.854351] [G loss: 1.503102]\n",
      "[D loss: 0.804574] [G loss: 1.497392]\n",
      "[D loss: 0.873560] [G loss: 1.454984]\n",
      "[D loss: 0.974764] [G loss: 1.365524]\n",
      "[D loss: 0.791770] [G loss: 1.620761]\n",
      "[D loss: 0.744725] [G loss: 1.584881]\n",
      "[D loss: 0.868006] [G loss: 1.542029]\n",
      "[D loss: 0.802959] [G loss: 1.496531]\n",
      "[D loss: 0.899156] [G loss: 1.510246]\n",
      "[D loss: 0.855330] [G loss: 1.479461]\n",
      "[D loss: 0.875324] [G loss: 1.498430]\n",
      "[D loss: 0.796104] [G loss: 1.215440]\n",
      "[D loss: 1.020276] [G loss: 1.368260]\n",
      "[D loss: 1.081480] [G loss: 1.518270]\n",
      "[D loss: 0.653780] [G loss: 1.395631]\n",
      "[D loss: 0.598083] [G loss: 1.429749]\n",
      "[D loss: 0.725429] [G loss: 1.756372]\n",
      "[D loss: 0.705966] [G loss: 1.632815]\n",
      "[D loss: 0.692569] [G loss: 1.809154]\n",
      "[D loss: 0.905149] [G loss: 1.591779]\n",
      "[D loss: 0.745229] [G loss: 1.510042]\n",
      "[D loss: 0.693805] [G loss: 1.441389]\n",
      "[D loss: 0.811004] [G loss: 1.613872]\n",
      "[D loss: 0.635501] [G loss: 1.715292]\n",
      "[D loss: 0.742176] [G loss: 1.849818]\n",
      "[D loss: 0.980473] [G loss: 1.708814]\n",
      "[D loss: 0.624118] [G loss: 1.670371]\n",
      "[D loss: 0.747211] [G loss: 1.511049]\n",
      "[D loss: 0.940707] [G loss: 1.327492]\n",
      "[D loss: 0.941951] [G loss: 1.429373]\n",
      "[D loss: 0.749655] [G loss: 1.632270]\n",
      "[D loss: 1.037075] [G loss: 1.392658]\n",
      "[D loss: 0.754913] [G loss: 1.521278]\n",
      "[D loss: 0.924853] [G loss: 1.594512]\n",
      "[D loss: 0.864489] [G loss: 1.258479]\n",
      "[D loss: 0.738430] [G loss: 1.416171]\n",
      "[D loss: 0.681877] [G loss: 1.574359]\n",
      "[D loss: 0.883483] [G loss: 1.394948]\n",
      "[D loss: 0.725494] [G loss: 1.634768]\n",
      "[D loss: 0.758670] [G loss: 1.435262]\n",
      "[D loss: 0.707772] [G loss: 1.484993]\n",
      "[D loss: 1.034509] [G loss: 1.648733]\n",
      "[D loss: 0.693512] [G loss: 1.443107]\n",
      "[D loss: 0.627057] [G loss: 1.773165]\n",
      "[D loss: 0.708718] [G loss: 1.504807]\n",
      "[D loss: 0.793105] [G loss: 1.481743]\n",
      "[D loss: 0.777872] [G loss: 1.551480]\n",
      "[D loss: 0.801802] [G loss: 1.508849]\n",
      "[D loss: 0.664218] [G loss: 1.392512]\n",
      "[D loss: 0.640663] [G loss: 1.612897]\n",
      "[D loss: 0.685893] [G loss: 1.596427]\n",
      "[D loss: 0.703419] [G loss: 1.430615]\n",
      "[D loss: 0.836154] [G loss: 1.812206]\n",
      "[D loss: 0.878388] [G loss: 1.494133]\n",
      "[D loss: 1.012441] [G loss: 1.443189]\n",
      "[D loss: 0.826679] [G loss: 1.568166]\n",
      "[D loss: 1.075880] [G loss: 1.273520]\n",
      "[D loss: 0.979009] [G loss: 1.482184]\n",
      "[D loss: 0.877886] [G loss: 1.520238]\n",
      "[D loss: 1.055707] [G loss: 1.484701]\n",
      "[D loss: 0.991320] [G loss: 1.524508]\n",
      "[D loss: 0.861490] [G loss: 1.446063]\n",
      "[D loss: 0.783623] [G loss: 1.454188]\n",
      "[D loss: 0.825396] [G loss: 1.370751]\n",
      "[D loss: 0.910769] [G loss: 1.330824]\n",
      "[D loss: 0.831251] [G loss: 1.533429]\n",
      "[D loss: 0.847912] [G loss: 1.394753]\n",
      "[D loss: 0.768394] [G loss: 1.538172]\n",
      "[D loss: 1.001102] [G loss: 1.472628]\n",
      "[D loss: 0.774057] [G loss: 1.528947]\n",
      "[D loss: 0.721065] [G loss: 1.395812]\n",
      "[D loss: 0.942325] [G loss: 1.522633]\n",
      "[D loss: 0.923223] [G loss: 1.457394]\n",
      "[D loss: 0.831849] [G loss: 1.557304]\n",
      "[D loss: 0.603636] [G loss: 1.355186]\n",
      "[D loss: 0.803300] [G loss: 1.460057]\n",
      "[D loss: 0.844673] [G loss: 1.584876]\n",
      "[D loss: 0.972961] [G loss: 1.555515]\n",
      "[D loss: 0.940030] [G loss: 1.379068]\n",
      "[D loss: 0.940419] [G loss: 1.454716]\n",
      "[D loss: 0.756518] [G loss: 1.352397]\n",
      "[D loss: 0.869233] [G loss: 1.494566]\n",
      "[D loss: 0.827601] [G loss: 1.564608]\n",
      "[D loss: 0.944260] [G loss: 1.537458]\n",
      "[D loss: 1.023838] [G loss: 1.360110]\n",
      "[D loss: 1.068111] [G loss: 1.397995]\n",
      "[D loss: 0.813232] [G loss: 1.550350]\n",
      "[D loss: 0.901886] [G loss: 1.796894]\n",
      "[D loss: 0.911664] [G loss: 1.560338]\n",
      "[D loss: 0.852798] [G loss: 1.220033]\n",
      "[D loss: 0.695996] [G loss: 1.369893]\n",
      "[D loss: 0.737911] [G loss: 1.506952]\n",
      "[D loss: 0.891005] [G loss: 1.540116]\n",
      "[D loss: 0.920825] [G loss: 1.345660]\n",
      "[D loss: 0.675843] [G loss: 1.334732]\n",
      "[D loss: 0.951940] [G loss: 1.342152]\n",
      "[D loss: 0.656447] [G loss: 1.511872]\n",
      "[D loss: 0.975255] [G loss: 1.148463]\n",
      "[D loss: 0.728338] [G loss: 1.233389]\n",
      "[D loss: 0.815052] [G loss: 1.461980]\n",
      "[D loss: 0.670718] [G loss: 1.578731]\n",
      "[D loss: 0.727791] [G loss: 1.649145]\n",
      "[D loss: 0.712769] [G loss: 1.459048]\n",
      "[D loss: 0.988695] [G loss: 1.507601]\n",
      "[D loss: 0.915946] [G loss: 1.430612]\n",
      "[D loss: 0.910867] [G loss: 1.413845]\n",
      "[D loss: 0.896579] [G loss: 1.436898]\n",
      "[D loss: 0.761689] [G loss: 1.456833]\n",
      "[D loss: 0.814064] [G loss: 1.553980]\n",
      "[D loss: 0.740815] [G loss: 1.557620]\n",
      "[D loss: 0.775784] [G loss: 1.394849]\n",
      "[D loss: 0.628148] [G loss: 1.414848]\n",
      "[D loss: 0.777520] [G loss: 1.484843]\n",
      "[D loss: 0.760791] [G loss: 1.711265]\n",
      "[D loss: 1.098146] [G loss: 1.374159]\n",
      "[D loss: 0.953551] [G loss: 1.425890]\n",
      "[D loss: 0.770247] [G loss: 1.674762]\n",
      "[D loss: 1.072528] [G loss: 1.528956]\n",
      "[D loss: 0.859983] [G loss: 1.498214]\n",
      "[D loss: 0.770229] [G loss: 1.451482]\n",
      "[D loss: 0.519141] [G loss: 1.656664]\n",
      "[D loss: 0.881799] [G loss: 1.484092]\n",
      "[D loss: 0.973149] [G loss: 1.424477]\n",
      "[D loss: 1.049008] [G loss: 1.306258]\n",
      "[D loss: 1.018868] [G loss: 1.478316]\n",
      "[D loss: 0.830289] [G loss: 1.460948]\n",
      "[D loss: 0.698256] [G loss: 1.369622]\n",
      "[D loss: 0.970514] [G loss: 1.497723]\n",
      "[D loss: 0.846821] [G loss: 1.766575]\n",
      "[D loss: 0.997847] [G loss: 1.731871]\n",
      "[D loss: 0.643311] [G loss: 1.594697]\n",
      "[D loss: 0.792847] [G loss: 1.467424]\n",
      "[D loss: 0.901984] [G loss: 1.324352]\n",
      "[D loss: 1.004971] [G loss: 1.265496]\n",
      "[D loss: 0.970991] [G loss: 1.357551]\n",
      "[D loss: 1.162143] [G loss: 1.323654]\n",
      "[D loss: 0.843012] [G loss: 1.493123]\n",
      "[D loss: 0.703413] [G loss: 1.305640]\n",
      "[D loss: 0.607212] [G loss: 1.262268]\n",
      "[D loss: 0.570511] [G loss: 1.478685]\n",
      "[D loss: 0.897806] [G loss: 1.548863]\n",
      "[D loss: 1.013953] [G loss: 1.312100]\n",
      "[D loss: 0.841032] [G loss: 1.565396]\n",
      "[D loss: 0.771656] [G loss: 1.514547]\n",
      "[D loss: 0.906279] [G loss: 1.597889]\n",
      "[D loss: 0.791006] [G loss: 1.480031]\n",
      "[D loss: 0.944375] [G loss: 1.229398]\n",
      "[D loss: 0.684410] [G loss: 1.545936]\n",
      "[D loss: 0.951223] [G loss: 1.269094]\n",
      "[D loss: 0.697630] [G loss: 1.443321]\n",
      "[D loss: 0.777375] [G loss: 1.575708]\n",
      "[D loss: 0.981723] [G loss: 1.428988]\n",
      "[D loss: 0.685983] [G loss: 1.461269]\n",
      "[D loss: 0.869211] [G loss: 1.323964]\n",
      "[D loss: 0.677437] [G loss: 1.433110]\n",
      "[D loss: 1.008160] [G loss: 1.507825]\n",
      "[D loss: 0.728873] [G loss: 1.612030]\n",
      "[D loss: 0.622303] [G loss: 1.469302]\n",
      "[D loss: 0.844442] [G loss: 1.343737]\n",
      "[D loss: 0.730432] [G loss: 1.642752]\n",
      "[D loss: 0.746052] [G loss: 1.436450]\n",
      "[D loss: 0.846299] [G loss: 1.734368]\n",
      "[D loss: 0.690747] [G loss: 1.531528]\n",
      "[D loss: 0.765823] [G loss: 1.573539]\n",
      "[D loss: 0.947177] [G loss: 1.371981]\n",
      "[D loss: 1.100323] [G loss: 1.174015]\n",
      "[D loss: 1.064398] [G loss: 1.484339]\n",
      "[D loss: 0.756949] [G loss: 1.526296]\n",
      "[D loss: 1.021812] [G loss: 1.306112]\n",
      "[D loss: 0.738716] [G loss: 1.714505]\n",
      "[D loss: 0.932113] [G loss: 1.527978]\n",
      "[D loss: 0.791185] [G loss: 1.367050]\n",
      "[D loss: 0.796300] [G loss: 1.436075]\n",
      "[D loss: 0.735592] [G loss: 1.404286]\n",
      "[D loss: 1.070041] [G loss: 1.344715]\n",
      "[D loss: 0.688383] [G loss: 1.573482]\n",
      "[D loss: 0.938457] [G loss: 1.421286]\n",
      "[D loss: 0.729471] [G loss: 1.673032]\n",
      "[D loss: 0.742165] [G loss: 1.612759]\n",
      "[D loss: 0.818133] [G loss: 1.412252]\n",
      "[D loss: 1.130189] [G loss: 1.388123]\n",
      "[D loss: 0.814413] [G loss: 1.457926]\n",
      "[D loss: 0.886679] [G loss: 1.427738]\n",
      "[D loss: 0.878393] [G loss: 1.326323]\n",
      "[D loss: 0.714558] [G loss: 1.495300]\n",
      "[D loss: 0.878856] [G loss: 1.359036]\n",
      "[D loss: 0.765679] [G loss: 1.422018]\n",
      "[D loss: 0.808398] [G loss: 1.479012]\n",
      "[D loss: 0.959189] [G loss: 1.242749]\n",
      "[D loss: 0.830273] [G loss: 1.359602]\n",
      "[D loss: 0.803970] [G loss: 1.427628]\n",
      "[D loss: 0.670800] [G loss: 1.318663]\n",
      "[D loss: 0.925303] [G loss: 1.274606]\n",
      "[D loss: 0.770012] [G loss: 1.346194]\n",
      "[D loss: 0.994187] [G loss: 1.352622]\n",
      "[D loss: 0.824481] [G loss: 1.361967]\n",
      "[D loss: 0.946835] [G loss: 1.424370]\n",
      "[D loss: 1.038521] [G loss: 1.340235]\n",
      "[D loss: 0.786056] [G loss: 1.403631]\n",
      "[D loss: 1.042228] [G loss: 1.208756]\n",
      "[D loss: 0.850383] [G loss: 1.568097]\n",
      "[D loss: 0.873927] [G loss: 1.348995]\n",
      "[D loss: 0.596873] [G loss: 1.541528]\n",
      "[D loss: 0.699849] [G loss: 1.312102]\n",
      "[D loss: 1.024534] [G loss: 1.287042]\n",
      "[D loss: 0.761558] [G loss: 1.651715]\n",
      "[D loss: 0.745883] [G loss: 1.578169]\n",
      "[D loss: 0.903423] [G loss: 1.473538]\n",
      "[D loss: 0.871506] [G loss: 1.323532]\n",
      "[D loss: 0.843454] [G loss: 1.603323]\n",
      "[D loss: 0.811106] [G loss: 1.476465]\n",
      "[D loss: 0.807373] [G loss: 1.222121]\n",
      "[D loss: 0.861180] [G loss: 1.578979]\n",
      "[D loss: 0.556572] [G loss: 1.312934]\n",
      "[D loss: 0.915844] [G loss: 1.507600]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.749344] [G loss: 1.721717]\n",
      "[D loss: 0.743683] [G loss: 1.756621]\n",
      "[D loss: 0.796981] [G loss: 1.528637]\n",
      "[D loss: 0.892743] [G loss: 1.499116]\n",
      "[D loss: 0.869561] [G loss: 1.294304]\n",
      "[D loss: 1.004232] [G loss: 1.284880]\n",
      "[D loss: 0.764134] [G loss: 1.602767]\n",
      "[D loss: 0.671523] [G loss: 1.516276]\n",
      "[D loss: 0.797919] [G loss: 1.560005]\n",
      "[D loss: 0.810830] [G loss: 1.345212]\n",
      "[D loss: 1.068019] [G loss: 1.239752]\n",
      "[D loss: 0.713971] [G loss: 1.407871]\n",
      "[D loss: 0.821321] [G loss: 1.524647]\n",
      "[D loss: 0.961719] [G loss: 1.373384]\n",
      "[D loss: 0.881603] [G loss: 1.456967]\n",
      "[D loss: 0.757254] [G loss: 1.446636]\n",
      "[D loss: 0.705290] [G loss: 1.760909]\n",
      "[D loss: 0.662769] [G loss: 1.622446]\n",
      "[D loss: 0.851373] [G loss: 1.579826]\n",
      "[D loss: 0.861804] [G loss: 1.363749]\n",
      "[D loss: 0.763622] [G loss: 1.449923]\n",
      "[D loss: 0.853785] [G loss: 1.523433]\n",
      "[D loss: 1.148869] [G loss: 1.346835]\n",
      "[D loss: 0.763313] [G loss: 1.384294]\n",
      "[D loss: 1.053761] [G loss: 1.250524]\n",
      "[D loss: 0.881621] [G loss: 1.411455]\n",
      "[D loss: 0.787867] [G loss: 1.600585]\n",
      "[D loss: 0.997683] [G loss: 1.609937]\n",
      "[D loss: 0.753843] [G loss: 1.309484]\n",
      "[D loss: 1.073925] [G loss: 1.330948]\n",
      "[D loss: 0.774471] [G loss: 1.520263]\n",
      "[D loss: 0.783675] [G loss: 1.397619]\n",
      "[D loss: 0.927484] [G loss: 1.234583]\n",
      "[D loss: 0.774212] [G loss: 1.347450]\n",
      "[D loss: 0.922801] [G loss: 1.508000]\n",
      "[D loss: 0.943503] [G loss: 1.609979]\n",
      "[D loss: 0.928172] [G loss: 1.639679]\n",
      "[D loss: 0.744393] [G loss: 1.404934]\n",
      "[D loss: 0.769231] [G loss: 1.483707]\n",
      "[D loss: 0.928161] [G loss: 1.296747]\n",
      "[D loss: 0.824372] [G loss: 1.410875]\n",
      "[D loss: 0.841517] [G loss: 1.543490]\n",
      "[D loss: 1.087607] [G loss: 1.485510]\n",
      "[D loss: 0.757069] [G loss: 1.441103]\n",
      "[D loss: 0.823645] [G loss: 1.415734]\n",
      "[D loss: 0.744353] [G loss: 1.388329]\n",
      "[D loss: 0.741359] [G loss: 1.550079]\n",
      "[D loss: 0.636369] [G loss: 1.588387]\n",
      "[D loss: 0.706536] [G loss: 1.493480]\n",
      "[D loss: 0.691092] [G loss: 1.358009]\n",
      "[D loss: 0.839497] [G loss: 1.398022]\n",
      "[D loss: 0.867193] [G loss: 1.478642]\n",
      "[D loss: 0.686451] [G loss: 1.478045]\n",
      "[D loss: 1.182945] [G loss: 1.427823]\n",
      "[D loss: 0.988923] [G loss: 1.467757]\n",
      "[D loss: 0.778171] [G loss: 1.481310]\n",
      "[D loss: 0.770111] [G loss: 1.553864]\n",
      "[D loss: 0.697621] [G loss: 1.531131]\n",
      "[D loss: 0.912214] [G loss: 1.448030]\n",
      "[D loss: 0.860464] [G loss: 1.402609]\n",
      "[D loss: 0.723181] [G loss: 1.572100]\n",
      "[D loss: 0.946105] [G loss: 1.428951]\n",
      "[D loss: 0.925964] [G loss: 1.804006]\n",
      "[D loss: 0.817617] [G loss: 1.526147]\n",
      "[D loss: 0.771934] [G loss: 1.506637]\n",
      "[D loss: 1.002667] [G loss: 1.430530]\n",
      "[D loss: 0.829299] [G loss: 1.278474]\n",
      "[D loss: 0.920418] [G loss: 1.468530]\n",
      "[D loss: 0.917561] [G loss: 1.626017]\n",
      "[D loss: 0.788395] [G loss: 1.452342]\n",
      "[D loss: 0.800134] [G loss: 1.518832]\n",
      "[D loss: 0.858273] [G loss: 1.426042]\n",
      "[D loss: 1.194051] [G loss: 1.245316]\n",
      "[D loss: 0.976883] [G loss: 1.599495]\n",
      "[D loss: 0.764157] [G loss: 1.501978]\n",
      "[D loss: 0.782505] [G loss: 1.340939]\n",
      "[D loss: 0.875890] [G loss: 1.421663]\n",
      "[D loss: 0.764526] [G loss: 1.499120]\n",
      "[D loss: 0.908976] [G loss: 1.628786]\n",
      "[D loss: 1.250044] [G loss: 1.211215]\n",
      "[D loss: 0.882246] [G loss: 1.439582]\n",
      "[D loss: 0.670218] [G loss: 1.458120]\n",
      "[D loss: 0.734499] [G loss: 1.463616]\n",
      "[D loss: 1.042970] [G loss: 1.562485]\n",
      "[D loss: 0.939371] [G loss: 1.174971]\n",
      "[D loss: 0.651549] [G loss: 1.662347]\n",
      "[D loss: 0.967616] [G loss: 1.386817]\n",
      "[D loss: 1.021533] [G loss: 1.549491]\n",
      "[D loss: 0.825384] [G loss: 1.641227]\n",
      "[D loss: 1.127613] [G loss: 1.385267]\n",
      "[D loss: 0.527107] [G loss: 1.398928]\n",
      "[D loss: 1.325078] [G loss: 1.263361]\n",
      "[D loss: 0.763269] [G loss: 1.230857]\n",
      "[D loss: 0.775792] [G loss: 1.334722]\n",
      "[D loss: 0.806873] [G loss: 1.425591]\n",
      "[D loss: 0.749335] [G loss: 1.268894]\n",
      "[D loss: 0.840315] [G loss: 1.291154]\n",
      "[D loss: 0.952048] [G loss: 1.352296]\n",
      "[D loss: 0.828103] [G loss: 1.423712]\n",
      "[D loss: 0.838203] [G loss: 1.236239]\n",
      "[D loss: 0.730901] [G loss: 1.740870]\n",
      "[D loss: 0.848289] [G loss: 1.423891]\n",
      "[D loss: 0.680606] [G loss: 1.631436]\n",
      "[D loss: 0.688377] [G loss: 1.837519]\n",
      "[D loss: 1.028842] [G loss: 1.291161]\n",
      "[D loss: 0.912497] [G loss: 1.425410]\n",
      "[D loss: 0.749872] [G loss: 1.622256]\n",
      "[D loss: 0.808054] [G loss: 1.740887]\n",
      "[D loss: 0.918652] [G loss: 1.433745]\n",
      "[D loss: 0.881356] [G loss: 1.422034]\n",
      "[D loss: 0.931082] [G loss: 1.513644]\n",
      "[D loss: 1.036087] [G loss: 1.293208]\n",
      "[D loss: 0.743477] [G loss: 1.536057]\n",
      "[D loss: 0.790648] [G loss: 1.533554]\n",
      "[D loss: 0.948111] [G loss: 1.327262]\n",
      "[D loss: 0.927179] [G loss: 1.564257]\n",
      "[D loss: 0.766069] [G loss: 1.510335]\n",
      "[D loss: 0.751585] [G loss: 1.330664]\n",
      "[D loss: 0.913050] [G loss: 1.305096]\n",
      "[D loss: 0.774359] [G loss: 1.754200]\n",
      "[D loss: 0.871607] [G loss: 1.574692]\n",
      "[D loss: 1.088523] [G loss: 1.529901]\n",
      "[D loss: 0.933985] [G loss: 1.334550]\n",
      "[D loss: 0.911855] [G loss: 1.234428]\n",
      "[D loss: 0.820845] [G loss: 1.352896]\n",
      "[D loss: 0.811062] [G loss: 1.402672]\n",
      "[D loss: 0.764624] [G loss: 1.541270]\n",
      "[D loss: 0.780012] [G loss: 1.548458]\n",
      "[D loss: 0.780094] [G loss: 1.479904]\n",
      "[D loss: 0.811254] [G loss: 1.267463]\n",
      "[D loss: 0.767893] [G loss: 1.619393]\n",
      "[D loss: 0.885125] [G loss: 1.551123]\n",
      "[D loss: 0.869720] [G loss: 1.323886]\n",
      "[D loss: 0.707707] [G loss: 1.421559]\n",
      "[D loss: 0.685017] [G loss: 1.619628]\n",
      "[D loss: 0.828926] [G loss: 1.555650]\n",
      "[D loss: 0.858316] [G loss: 1.354089]\n",
      "[D loss: 0.888156] [G loss: 1.382912]\n",
      "[D loss: 0.858385] [G loss: 1.366058]\n",
      "[D loss: 0.682359] [G loss: 1.491852]\n",
      "[D loss: 0.802570] [G loss: 1.362954]\n",
      "[D loss: 0.978046] [G loss: 1.497876]\n",
      "[D loss: 0.975305] [G loss: 1.444946]\n",
      "[D loss: 0.744052] [G loss: 1.510617]\n",
      "[D loss: 0.684672] [G loss: 1.313639]\n",
      "[D loss: 0.919597] [G loss: 1.402298]\n",
      "[D loss: 0.889698] [G loss: 1.427783]\n",
      "[D loss: 0.749447] [G loss: 1.414745]\n",
      "[D loss: 0.827958] [G loss: 1.306905]\n",
      "[D loss: 0.670548] [G loss: 1.573794]\n",
      "[D loss: 0.806791] [G loss: 1.405375]\n",
      "[D loss: 1.101032] [G loss: 1.429717]\n",
      "[D loss: 0.913439] [G loss: 1.449738]\n",
      "[D loss: 0.880731] [G loss: 1.496675]\n",
      "[D loss: 0.777566] [G loss: 1.455699]\n",
      "[D loss: 0.522190] [G loss: 1.535256]\n",
      "[D loss: 0.867945] [G loss: 1.300728]\n",
      "[D loss: 0.928072] [G loss: 1.532320]\n",
      "[D loss: 0.893430] [G loss: 1.743588]\n",
      "[D loss: 0.767889] [G loss: 1.620570]\n",
      "[D loss: 0.719047] [G loss: 1.555498]\n",
      "[D loss: 0.840235] [G loss: 1.440264]\n",
      "[D loss: 0.874370] [G loss: 1.571539]\n",
      "[D loss: 0.809118] [G loss: 1.436484]\n",
      "[D loss: 0.797487] [G loss: 1.602530]\n",
      "[D loss: 0.901962] [G loss: 1.273999]\n",
      "[D loss: 0.931490] [G loss: 1.260190]\n",
      "[D loss: 0.853081] [G loss: 1.270599]\n",
      "[D loss: 0.736659] [G loss: 1.620219]\n",
      "[D loss: 0.752485] [G loss: 1.508963]\n",
      "[D loss: 0.927383] [G loss: 1.440972]\n",
      "[D loss: 0.941747] [G loss: 1.539766]\n",
      "[D loss: 0.895700] [G loss: 1.661103]\n",
      "[D loss: 0.782991] [G loss: 1.411891]\n",
      "[D loss: 0.580425] [G loss: 1.511345]\n",
      "[D loss: 0.825058] [G loss: 1.495181]\n",
      "[D loss: 0.821764] [G loss: 1.319949]\n",
      "[D loss: 0.947272] [G loss: 1.296698]\n",
      "[D loss: 0.808328] [G loss: 1.689143]\n",
      "[D loss: 0.775149] [G loss: 1.577216]\n",
      "[D loss: 1.007129] [G loss: 1.366146]\n",
      "[D loss: 0.693426] [G loss: 1.360623]\n",
      "[D loss: 0.683860] [G loss: 1.436223]\n",
      "[D loss: 0.793005] [G loss: 1.473503]\n",
      "[D loss: 0.851697] [G loss: 1.343927]\n",
      "[D loss: 0.912547] [G loss: 1.352434]\n",
      "[D loss: 0.948958] [G loss: 1.492692]\n",
      "[D loss: 0.895425] [G loss: 1.588082]\n",
      "[D loss: 0.918061] [G loss: 1.582927]\n",
      "[D loss: 0.768931] [G loss: 1.462706]\n",
      "[D loss: 0.798696] [G loss: 1.671482]\n",
      "[D loss: 0.708543] [G loss: 1.491635]\n",
      "[D loss: 0.763014] [G loss: 1.494580]\n",
      "[D loss: 0.877012] [G loss: 1.597999]\n",
      "[D loss: 0.655722] [G loss: 1.493326]\n",
      "[D loss: 0.745148] [G loss: 1.546676]\n",
      "[D loss: 0.975217] [G loss: 1.374590]\n",
      "[D loss: 0.952494] [G loss: 1.274679]\n",
      "[D loss: 0.671281] [G loss: 1.531265]\n",
      "[D loss: 0.902546] [G loss: 1.242660]\n",
      "[D loss: 0.767362] [G loss: 1.567826]\n",
      "[D loss: 0.909913] [G loss: 1.379845]\n",
      "[D loss: 1.016029] [G loss: 1.402335]\n",
      "[D loss: 0.849774] [G loss: 1.272825]\n",
      "[D loss: 0.919983] [G loss: 1.290202]\n",
      "[D loss: 0.721612] [G loss: 1.523277]\n",
      "[D loss: 0.888546] [G loss: 1.271247]\n",
      "[D loss: 0.923937] [G loss: 1.486566]\n",
      "[D loss: 0.853574] [G loss: 1.428689]\n",
      "[D loss: 0.768903] [G loss: 1.762264]\n",
      "[D loss: 0.984460] [G loss: 1.243264]\n",
      "[D loss: 0.768629] [G loss: 1.607602]\n",
      "[D loss: 0.725463] [G loss: 1.403145]\n",
      "[D loss: 0.934396] [G loss: 1.334937]\n",
      "[D loss: 0.962926] [G loss: 1.366452]\n",
      "[D loss: 0.963751] [G loss: 1.360646]\n",
      "[D loss: 1.074179] [G loss: 1.569802]\n",
      "[D loss: 0.869116] [G loss: 1.504594]\n",
      "[D loss: 1.000124] [G loss: 1.411943]\n",
      "[D loss: 0.800127] [G loss: 1.414333]\n",
      "[D loss: 0.726774] [G loss: 1.438543]\n",
      "[D loss: 0.775651] [G loss: 1.330243]\n",
      "[D loss: 0.857676] [G loss: 1.245372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.781670] [G loss: 1.309804]\n",
      "[D loss: 0.997188] [G loss: 1.365386]\n",
      "[D loss: 0.984530] [G loss: 1.478007]\n",
      "[D loss: 0.871862] [G loss: 1.513057]\n",
      "[D loss: 1.056759] [G loss: 1.506256]\n",
      "[D loss: 0.972461] [G loss: 1.287673]\n",
      "[D loss: 0.729259] [G loss: 1.234947]\n",
      "[D loss: 0.832277] [G loss: 1.264298]\n",
      "[D loss: 0.703987] [G loss: 1.385992]\n",
      "[D loss: 0.932207] [G loss: 1.424000]\n",
      "[D loss: 0.875664] [G loss: 1.591587]\n",
      "[D loss: 0.935357] [G loss: 1.385888]\n",
      "[D loss: 0.845798] [G loss: 1.273991]\n",
      "[D loss: 0.799120] [G loss: 1.462386]\n",
      "[D loss: 0.814923] [G loss: 1.687446]\n",
      "[D loss: 1.090005] [G loss: 1.488263]\n",
      "[D loss: 0.772796] [G loss: 1.648895]\n",
      "[D loss: 0.749209] [G loss: 1.541083]\n",
      "[D loss: 0.713381] [G loss: 1.477964]\n",
      "[D loss: 0.926703] [G loss: 1.349506]\n",
      "[D loss: 0.833577] [G loss: 1.412638]\n",
      "[D loss: 0.741353] [G loss: 1.448771]\n",
      "[D loss: 0.900623] [G loss: 1.441006]\n",
      "[D loss: 0.736349] [G loss: 1.454083]\n",
      "[D loss: 0.715995] [G loss: 1.303403]\n",
      "[D loss: 0.794592] [G loss: 1.263291]\n",
      "[D loss: 0.557415] [G loss: 1.485559]\n",
      "[D loss: 0.752705] [G loss: 1.608328]\n",
      "[D loss: 0.989101] [G loss: 1.413312]\n",
      "[D loss: 0.874309] [G loss: 1.402205]\n",
      "[D loss: 0.878168] [G loss: 1.476381]\n",
      "[D loss: 0.840335] [G loss: 1.520753]\n",
      "[D loss: 0.685839] [G loss: 1.404734]\n",
      "[D loss: 0.655093] [G loss: 1.593284]\n",
      "[D loss: 0.921330] [G loss: 1.433671]\n",
      "[D loss: 0.892687] [G loss: 1.378005]\n",
      "[D loss: 0.736179] [G loss: 1.517370]\n",
      "[D loss: 0.718733] [G loss: 1.389568]\n",
      "[D loss: 0.813391] [G loss: 1.518692]\n",
      "[D loss: 0.824157] [G loss: 1.596902]\n",
      "[D loss: 0.785863] [G loss: 1.353496]\n",
      "[D loss: 1.236189] [G loss: 1.407167]\n",
      "[D loss: 1.084877] [G loss: 1.828912]\n",
      "[D loss: 0.876327] [G loss: 1.298519]\n",
      "[D loss: 0.942811] [G loss: 1.661957]\n",
      "[D loss: 0.939075] [G loss: 1.490798]\n",
      "[D loss: 1.072054] [G loss: 1.203627]\n",
      "[D loss: 0.887444] [G loss: 1.520558]\n",
      "[D loss: 0.815165] [G loss: 1.520846]\n",
      "[D loss: 0.741754] [G loss: 1.374657]\n",
      "[D loss: 0.960080] [G loss: 1.489531]\n",
      "[D loss: 0.946383] [G loss: 1.228205]\n",
      "[D loss: 0.835117] [G loss: 1.200497]\n",
      "[D loss: 0.789122] [G loss: 1.290070]\n",
      "[D loss: 0.853414] [G loss: 1.368629]\n",
      "[D loss: 1.114907] [G loss: 1.429607]\n",
      "[D loss: 1.164261] [G loss: 1.318743]\n",
      "[D loss: 0.679112] [G loss: 1.576208]\n",
      "[D loss: 0.822100] [G loss: 1.384812]\n",
      "[D loss: 0.762430] [G loss: 1.604642]\n",
      "[D loss: 0.969309] [G loss: 1.207922]\n",
      "[D loss: 0.893512] [G loss: 1.408033]\n",
      "[D loss: 0.861864] [G loss: 1.496319]\n",
      "[D loss: 0.877418] [G loss: 1.375459]\n",
      "[D loss: 0.794841] [G loss: 1.585030]\n",
      "[D loss: 0.753446] [G loss: 1.331380]\n",
      "[D loss: 0.723584] [G loss: 1.400826]\n",
      "[D loss: 0.805327] [G loss: 1.523591]\n",
      "[D loss: 0.734933] [G loss: 1.546400]\n",
      "[D loss: 0.886839] [G loss: 1.351857]\n",
      "[D loss: 0.763942] [G loss: 1.701421]\n",
      "[D loss: 0.947067] [G loss: 1.617783]\n",
      "[D loss: 0.874335] [G loss: 1.554462]\n",
      "[D loss: 0.810898] [G loss: 1.355506]\n",
      "[D loss: 0.875516] [G loss: 1.320549]\n",
      "[D loss: 0.851196] [G loss: 1.305180]\n",
      "[D loss: 0.722743] [G loss: 1.385242]\n",
      "[D loss: 0.862475] [G loss: 1.560193]\n",
      "[D loss: 0.886925] [G loss: 1.610764]\n",
      "[D loss: 0.768326] [G loss: 1.415256]\n",
      "[D loss: 0.998348] [G loss: 1.283626]\n",
      "[D loss: 0.635203] [G loss: 1.457686]\n",
      "[D loss: 0.803318] [G loss: 1.309548]\n",
      "[D loss: 0.958535] [G loss: 1.435293]\n",
      "[D loss: 0.887268] [G loss: 1.459714]\n",
      "[D loss: 0.875972] [G loss: 1.291919]\n",
      "[D loss: 0.639539] [G loss: 1.548375]\n",
      "[D loss: 0.853379] [G loss: 1.629094]\n",
      "[D loss: 0.917383] [G loss: 1.367691]\n",
      "[D loss: 1.002586] [G loss: 1.722841]\n",
      "[D loss: 0.832505] [G loss: 1.462447]\n",
      "[D loss: 0.827258] [G loss: 1.455710]\n",
      "[D loss: 0.906801] [G loss: 1.315943]\n",
      "[D loss: 0.910368] [G loss: 1.270430]\n",
      "[D loss: 0.951586] [G loss: 1.168977]\n",
      "[D loss: 1.173109] [G loss: 1.224805]\n",
      "[D loss: 0.878375] [G loss: 1.358891]\n",
      "[D loss: 0.789190] [G loss: 1.655186]\n",
      "[D loss: 0.795902] [G loss: 1.518090]\n",
      "[D loss: 0.957126] [G loss: 1.274885]\n",
      "[D loss: 0.965144] [G loss: 1.430419]\n",
      "[D loss: 1.037562] [G loss: 1.294199]\n",
      "[D loss: 0.764739] [G loss: 1.239415]\n",
      "[D loss: 0.811985] [G loss: 1.583062]\n",
      "[D loss: 0.623896] [G loss: 1.592884]\n",
      "[D loss: 0.954406] [G loss: 1.463528]\n",
      "[D loss: 0.767941] [G loss: 1.590790]\n",
      "[D loss: 0.731095] [G loss: 1.443906]\n",
      "[D loss: 0.702767] [G loss: 1.459586]\n",
      "[D loss: 1.099588] [G loss: 1.329040]\n",
      "[D loss: 0.930504] [G loss: 1.619097]\n",
      "[D loss: 0.885657] [G loss: 1.436806]\n",
      "[D loss: 0.929054] [G loss: 1.516338]\n",
      "[D loss: 0.774318] [G loss: 1.657230]\n",
      "[D loss: 0.769998] [G loss: 1.514454]\n",
      "[D loss: 0.732788] [G loss: 1.532552]\n",
      "[D loss: 0.932552] [G loss: 1.394051]\n",
      "[D loss: 0.617419] [G loss: 1.450781]\n",
      "[D loss: 1.023036] [G loss: 1.316175]\n",
      "[D loss: 1.069714] [G loss: 1.357178]\n",
      "[D loss: 0.859878] [G loss: 1.512439]\n",
      "[D loss: 1.156238] [G loss: 1.504307]\n",
      "[D loss: 0.903800] [G loss: 1.272676]\n",
      "[D loss: 0.641135] [G loss: 1.289968]\n",
      "[D loss: 0.804187] [G loss: 1.482739]\n",
      "[D loss: 0.762157] [G loss: 1.464894]\n",
      "[D loss: 1.009729] [G loss: 1.503436]\n",
      "[D loss: 0.973600] [G loss: 1.407335]\n",
      "[D loss: 0.667332] [G loss: 1.732986]\n",
      "[D loss: 0.751456] [G loss: 1.602260]\n",
      "[D loss: 0.952103] [G loss: 1.356669]\n",
      "[D loss: 0.858914] [G loss: 1.390046]\n",
      "[D loss: 0.878880] [G loss: 1.466994]\n",
      "[D loss: 0.764361] [G loss: 1.343065]\n",
      "[D loss: 0.910727] [G loss: 1.182521]\n",
      "[D loss: 0.788568] [G loss: 1.515314]\n",
      "[D loss: 0.809523] [G loss: 1.205720]\n",
      "[D loss: 0.850470] [G loss: 1.636321]\n",
      "[D loss: 1.022225] [G loss: 1.455502]\n",
      "[D loss: 0.846405] [G loss: 1.332069]\n",
      "[D loss: 0.799889] [G loss: 1.337878]\n",
      "[D loss: 0.944479] [G loss: 1.245120]\n",
      "[D loss: 0.779963] [G loss: 1.582863]\n",
      "[D loss: 0.907478] [G loss: 1.484167]\n",
      "[D loss: 0.680471] [G loss: 1.463218]\n",
      "[D loss: 0.785377] [G loss: 1.429955]\n",
      "[D loss: 0.845377] [G loss: 1.457529]\n",
      "[D loss: 0.691832] [G loss: 1.498260]\n",
      "[D loss: 0.798508] [G loss: 1.462612]\n",
      "[D loss: 0.843956] [G loss: 1.342682]\n",
      "[D loss: 0.698732] [G loss: 1.554151]\n",
      "[D loss: 0.832185] [G loss: 1.471607]\n",
      "[D loss: 0.682205] [G loss: 1.582951]\n",
      "[D loss: 0.824447] [G loss: 1.448456]\n",
      "[D loss: 1.082681] [G loss: 1.417591]\n",
      "[D loss: 0.660557] [G loss: 1.473825]\n",
      "[D loss: 0.761128] [G loss: 1.659847]\n",
      "[D loss: 0.855604] [G loss: 1.547278]\n",
      "[D loss: 0.811841] [G loss: 1.385341]\n",
      "[D loss: 0.735287] [G loss: 1.344273]\n",
      "[D loss: 0.961038] [G loss: 1.372979]\n",
      "[D loss: 0.794358] [G loss: 1.448095]\n",
      "[D loss: 0.960208] [G loss: 1.581525]\n",
      "[D loss: 0.680983] [G loss: 1.524734]\n",
      "[D loss: 0.693879] [G loss: 1.754040]\n",
      "[D loss: 0.687058] [G loss: 1.495576]\n",
      "[D loss: 0.823956] [G loss: 1.480629]\n",
      "[D loss: 0.838701] [G loss: 1.567926]\n",
      "[D loss: 0.681000] [G loss: 1.682538]\n",
      "[D loss: 0.801631] [G loss: 1.637695]\n",
      "[D loss: 0.882186] [G loss: 1.336483]\n",
      "[D loss: 1.012994] [G loss: 1.422108]\n",
      "[D loss: 0.712025] [G loss: 1.288601]\n",
      "[D loss: 0.832096] [G loss: 1.600318]\n",
      "[D loss: 0.740691] [G loss: 1.488190]\n",
      "[D loss: 0.917371] [G loss: 1.371218]\n",
      "[D loss: 0.747675] [G loss: 1.360190]\n",
      "[D loss: 0.742644] [G loss: 1.516508]\n",
      "[D loss: 0.865825] [G loss: 1.483639]\n",
      "[D loss: 0.843785] [G loss: 1.536146]\n",
      "[D loss: 0.834774] [G loss: 1.671965]\n",
      "[D loss: 1.048914] [G loss: 1.541227]\n",
      "[D loss: 0.741061] [G loss: 1.655746]\n",
      "[D loss: 0.719597] [G loss: 1.424970]\n",
      "[D loss: 0.735396] [G loss: 1.540150]\n",
      "[D loss: 1.077080] [G loss: 1.499115]\n",
      "[D loss: 0.833515] [G loss: 1.319299]\n",
      "[D loss: 0.838093] [G loss: 1.704710]\n",
      "[D loss: 0.875625] [G loss: 1.630231]\n",
      "[D loss: 0.841120] [G loss: 1.677921]\n",
      "[D loss: 0.751065] [G loss: 1.519043]\n",
      "[D loss: 0.973537] [G loss: 1.225899]\n",
      "[D loss: 0.731867] [G loss: 1.407713]\n",
      "[D loss: 1.035238] [G loss: 1.379654]\n",
      "[D loss: 0.682988] [G loss: 1.506474]\n",
      "[D loss: 1.093059] [G loss: 1.300732]\n",
      "[D loss: 0.727020] [G loss: 1.399752]\n",
      "[D loss: 1.011245] [G loss: 1.281523]\n",
      "[D loss: 0.765648] [G loss: 1.495442]\n",
      "[D loss: 0.813833] [G loss: 1.616102]\n",
      "[D loss: 0.736679] [G loss: 1.728710]\n",
      "[D loss: 0.887435] [G loss: 1.522882]\n",
      "[D loss: 0.816000] [G loss: 1.331754]\n",
      "[D loss: 0.733521] [G loss: 1.646212]\n",
      "[D loss: 0.718334] [G loss: 1.511980]\n",
      "[D loss: 0.794856] [G loss: 1.632140]\n",
      "[D loss: 0.743852] [G loss: 1.469792]\n",
      "[D loss: 0.715697] [G loss: 1.493733]\n",
      "[D loss: 0.590621] [G loss: 1.729163]\n",
      "[D loss: 0.686238] [G loss: 1.409428]\n",
      "[D loss: 0.759386] [G loss: 1.464257]\n",
      "[D loss: 0.752505] [G loss: 1.472739]\n",
      "[D loss: 0.646310] [G loss: 1.739625]\n",
      "[D loss: 0.791448] [G loss: 1.657651]\n",
      "[D loss: 0.870168] [G loss: 1.483693]\n",
      "[D loss: 0.965759] [G loss: 1.420676]\n",
      "[D loss: 0.918439] [G loss: 1.470895]\n",
      "[D loss: 1.144552] [G loss: 1.122072]\n",
      "[D loss: 0.866319] [G loss: 1.599788]\n",
      "[D loss: 0.803936] [G loss: 1.515544]\n",
      "[D loss: 0.842065] [G loss: 1.633141]\n",
      "[D loss: 0.893358] [G loss: 1.243869]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.783791] [G loss: 1.380218]\n",
      "[D loss: 1.032837] [G loss: 1.245058]\n",
      "[D loss: 0.868364] [G loss: 1.391100]\n",
      "[D loss: 0.864789] [G loss: 1.389369]\n",
      "[D loss: 0.681309] [G loss: 1.772177]\n",
      "[D loss: 0.728222] [G loss: 1.880280]\n",
      "[D loss: 0.854295] [G loss: 1.399715]\n",
      "[D loss: 0.742992] [G loss: 1.355759]\n",
      "[D loss: 0.794700] [G loss: 1.489229]\n",
      "[D loss: 0.759326] [G loss: 1.280133]\n",
      "[D loss: 0.665055] [G loss: 1.505507]\n",
      "[D loss: 0.734963] [G loss: 1.466280]\n",
      "[D loss: 0.848220] [G loss: 1.627623]\n",
      "[D loss: 0.796036] [G loss: 1.692316]\n",
      "[D loss: 0.783397] [G loss: 1.492293]\n",
      "[D loss: 0.928728] [G loss: 1.381150]\n",
      "[D loss: 0.624804] [G loss: 1.426774]\n",
      "[D loss: 0.825479] [G loss: 1.487461]\n",
      "[D loss: 0.634011] [G loss: 1.618967]\n",
      "[D loss: 0.694782] [G loss: 1.569520]\n",
      "[D loss: 0.881174] [G loss: 1.603498]\n",
      "[D loss: 0.958062] [G loss: 1.478494]\n",
      "[D loss: 0.660950] [G loss: 1.484152]\n",
      "[D loss: 1.002103] [G loss: 1.680848]\n",
      "[D loss: 0.885435] [G loss: 1.799528]\n",
      "[D loss: 0.893870] [G loss: 1.565237]\n",
      "[D loss: 0.840564] [G loss: 1.597970]\n",
      "[D loss: 0.968682] [G loss: 1.343976]\n",
      "[D loss: 0.972366] [G loss: 1.517458]\n",
      "[D loss: 1.054828] [G loss: 1.146386]\n",
      "[D loss: 0.913056] [G loss: 1.437342]\n",
      "[D loss: 0.903485] [G loss: 1.472901]\n",
      "[D loss: 0.809945] [G loss: 1.430835]\n",
      "[D loss: 0.743299] [G loss: 1.533606]\n",
      "[D loss: 0.692653] [G loss: 1.348717]\n",
      "[D loss: 0.883192] [G loss: 1.187660]\n",
      "[D loss: 0.864320] [G loss: 1.228983]\n",
      "[D loss: 0.638838] [G loss: 1.646586]\n",
      "[D loss: 0.735350] [G loss: 1.445976]\n",
      "[D loss: 0.780545] [G loss: 1.693783]\n",
      "[D loss: 0.750265] [G loss: 1.633488]\n",
      "[D loss: 0.810736] [G loss: 1.344660]\n",
      "[D loss: 0.693665] [G loss: 1.510581]\n",
      "[D loss: 0.674785] [G loss: 1.658839]\n",
      "[D loss: 0.648023] [G loss: 1.555574]\n",
      "[D loss: 0.800883] [G loss: 1.552812]\n",
      "[D loss: 0.913464] [G loss: 1.369118]\n",
      "[D loss: 0.998015] [G loss: 1.468193]\n",
      "[D loss: 0.880571] [G loss: 1.330843]\n",
      "[D loss: 0.765332] [G loss: 1.594773]\n",
      "[D loss: 0.919159] [G loss: 1.412395]\n",
      "[D loss: 0.853853] [G loss: 1.498533]\n",
      "[D loss: 0.945213] [G loss: 1.511620]\n",
      "[D loss: 0.665033] [G loss: 1.420940]\n",
      "[D loss: 0.957280] [G loss: 1.486806]\n",
      "[D loss: 0.803961] [G loss: 1.370447]\n",
      "[D loss: 0.819396] [G loss: 1.595421]\n",
      "[D loss: 0.709539] [G loss: 1.361219]\n",
      "[D loss: 0.857001] [G loss: 1.702278]\n",
      "[D loss: 0.670549] [G loss: 1.375944]\n",
      "[D loss: 0.823301] [G loss: 1.646978]\n",
      "[D loss: 0.775649] [G loss: 1.493035]\n",
      "[D loss: 0.891948] [G loss: 1.540460]\n",
      "[D loss: 0.805824] [G loss: 1.571669]\n",
      "[D loss: 0.711239] [G loss: 1.651311]\n",
      "[D loss: 0.862839] [G loss: 1.690436]\n",
      "[D loss: 0.764820] [G loss: 1.550776]\n",
      "[D loss: 0.946214] [G loss: 1.314176]\n",
      "[D loss: 0.921556] [G loss: 1.214376]\n",
      "[D loss: 0.756811] [G loss: 1.401843]\n",
      "[D loss: 0.775065] [G loss: 1.557233]\n",
      "[D loss: 1.008133] [G loss: 1.394794]\n",
      "[D loss: 1.137454] [G loss: 1.598677]\n",
      "[D loss: 0.990379] [G loss: 1.497881]\n",
      "[D loss: 0.870627] [G loss: 1.850948]\n",
      "[D loss: 0.802039] [G loss: 1.541180]\n",
      "[D loss: 0.712679] [G loss: 1.416781]\n",
      "[D loss: 0.631472] [G loss: 1.633942]\n",
      "[D loss: 0.790950] [G loss: 1.536298]\n",
      "[D loss: 0.613893] [G loss: 1.507094]\n",
      "[D loss: 0.890105] [G loss: 1.476756]\n",
      "[D loss: 0.847634] [G loss: 1.415466]\n",
      "[D loss: 0.958272] [G loss: 1.517083]\n",
      "[D loss: 0.974557] [G loss: 1.442176]\n",
      "[D loss: 0.829118] [G loss: 1.404742]\n",
      "[D loss: 0.790916] [G loss: 1.673809]\n",
      "[D loss: 0.836775] [G loss: 1.472470]\n",
      "[D loss: 0.918154] [G loss: 1.391394]\n",
      "[D loss: 0.793870] [G loss: 1.231125]\n",
      "[D loss: 0.977713] [G loss: 1.048752]\n",
      "[D loss: 0.807657] [G loss: 1.522573]\n",
      "[D loss: 0.734136] [G loss: 1.497826]\n",
      "[D loss: 0.797465] [G loss: 1.254733]\n",
      "[D loss: 0.620486] [G loss: 1.303198]\n",
      "[D loss: 0.930947] [G loss: 1.719157]\n",
      "[D loss: 0.686450] [G loss: 1.751025]\n",
      "[D loss: 0.625817] [G loss: 1.671766]\n",
      "[D loss: 0.763488] [G loss: 1.655962]\n",
      "[D loss: 0.849252] [G loss: 1.622654]\n",
      "[D loss: 0.949258] [G loss: 1.519356]\n",
      "[D loss: 0.839091] [G loss: 1.493709]\n",
      "[D loss: 0.798461] [G loss: 1.434054]\n",
      "[D loss: 0.600728] [G loss: 1.663337]\n",
      "[D loss: 0.951567] [G loss: 1.238792]\n",
      "[D loss: 0.815530] [G loss: 1.358549]\n",
      "[D loss: 0.863951] [G loss: 1.672557]\n",
      "[D loss: 0.848615] [G loss: 1.607105]\n",
      "[D loss: 0.948469] [G loss: 1.486947]\n",
      "[D loss: 0.840302] [G loss: 1.619196]\n",
      "[D loss: 0.864869] [G loss: 1.482846]\n",
      "[D loss: 0.758527] [G loss: 1.528216]\n",
      "[D loss: 0.864514] [G loss: 1.419900]\n",
      "[D loss: 0.815109] [G loss: 1.436280]\n",
      "[D loss: 0.885515] [G loss: 1.347364]\n",
      "[D loss: 0.832670] [G loss: 1.294262]\n",
      "[D loss: 0.774711] [G loss: 1.604708]\n",
      "[D loss: 0.788870] [G loss: 1.602486]\n",
      "[D loss: 0.534679] [G loss: 1.543085]\n",
      "[D loss: 0.949011] [G loss: 1.443555]\n",
      "[D loss: 0.906225] [G loss: 1.650519]\n",
      "[D loss: 0.717845] [G loss: 1.640954]\n",
      "[D loss: 0.841643] [G loss: 1.682810]\n",
      "[D loss: 0.777056] [G loss: 1.487398]\n",
      "[D loss: 0.765782] [G loss: 1.392222]\n",
      "[D loss: 0.861481] [G loss: 1.639197]\n",
      "[D loss: 0.653741] [G loss: 1.637534]\n",
      "[D loss: 0.749398] [G loss: 1.553067]\n",
      "[D loss: 0.786251] [G loss: 1.489235]\n",
      "[D loss: 1.107347] [G loss: 1.336937]\n",
      "[D loss: 0.790793] [G loss: 1.333955]\n",
      "[D loss: 0.760341] [G loss: 1.382840]\n",
      "[D loss: 0.635371] [G loss: 1.531948]\n",
      "[D loss: 0.621947] [G loss: 1.605707]\n",
      "[D loss: 0.814560] [G loss: 1.692148]\n",
      "[D loss: 0.775390] [G loss: 1.544078]\n",
      "[D loss: 0.779786] [G loss: 1.656931]\n",
      "[D loss: 0.822720] [G loss: 1.696265]\n",
      "[D loss: 0.743491] [G loss: 1.756353]\n",
      "[D loss: 0.813036] [G loss: 1.343266]\n",
      "[D loss: 1.149180] [G loss: 1.423090]\n",
      "[D loss: 0.600842] [G loss: 1.616766]\n",
      "[D loss: 0.497269] [G loss: 1.443300]\n",
      "[D loss: 0.829175] [G loss: 1.562160]\n",
      "[D loss: 0.713449] [G loss: 1.556698]\n",
      "[D loss: 0.889397] [G loss: 1.440838]\n",
      "[D loss: 0.631447] [G loss: 1.661928]\n",
      "[D loss: 0.859912] [G loss: 1.520471]\n",
      "[D loss: 0.778934] [G loss: 1.478506]\n",
      "[D loss: 1.065252] [G loss: 1.359025]\n",
      "[D loss: 0.814147] [G loss: 1.277011]\n",
      "[D loss: 0.774947] [G loss: 1.485306]\n",
      "[D loss: 0.712132] [G loss: 1.711255]\n",
      "[D loss: 0.900372] [G loss: 1.645330]\n",
      "[D loss: 0.627198] [G loss: 1.739133]\n",
      "[D loss: 1.087983] [G loss: 1.591813]\n",
      "[D loss: 0.967201] [G loss: 1.561410]\n",
      "[D loss: 0.798858] [G loss: 1.665998]\n",
      "[D loss: 0.777169] [G loss: 1.505760]\n",
      "[D loss: 0.573011] [G loss: 1.485365]\n",
      "[D loss: 0.987304] [G loss: 1.357523]\n",
      "[D loss: 0.961405] [G loss: 1.270869]\n",
      "[D loss: 0.673924] [G loss: 1.703754]\n",
      "[D loss: 0.835680] [G loss: 1.368456]\n",
      "[D loss: 0.906849] [G loss: 1.664485]\n",
      "[D loss: 0.546871] [G loss: 1.852826]\n",
      "[D loss: 0.688686] [G loss: 1.488166]\n",
      "[D loss: 0.822025] [G loss: 1.465571]\n",
      "[D loss: 0.636346] [G loss: 1.530951]\n",
      "[D loss: 0.916143] [G loss: 1.442186]\n",
      "[D loss: 0.806098] [G loss: 1.543331]\n",
      "[D loss: 0.812170] [G loss: 1.577628]\n",
      "[D loss: 0.746428] [G loss: 1.707407]\n",
      "[D loss: 0.714007] [G loss: 1.672151]\n",
      "[D loss: 0.919881] [G loss: 1.466409]\n",
      "[D loss: 0.898604] [G loss: 1.449744]\n",
      "[D loss: 0.945282] [G loss: 1.457339]\n",
      "[D loss: 1.112363] [G loss: 1.384779]\n",
      "[D loss: 0.729428] [G loss: 1.471847]\n",
      "[D loss: 0.729499] [G loss: 1.320473]\n",
      "[D loss: 0.697037] [G loss: 1.473712]\n",
      "[D loss: 1.188030] [G loss: 1.448793]\n",
      "[D loss: 0.734727] [G loss: 1.615319]\n",
      "[D loss: 1.067374] [G loss: 1.483306]\n",
      "[D loss: 0.746471] [G loss: 1.502643]\n",
      "[D loss: 0.868624] [G loss: 1.702322]\n",
      "[D loss: 0.926740] [G loss: 1.361007]\n",
      "[D loss: 0.889649] [G loss: 1.333080]\n",
      "[D loss: 0.741105] [G loss: 1.333624]\n",
      "[D loss: 0.996597] [G loss: 1.464562]\n",
      "[D loss: 0.948318] [G loss: 1.365349]\n",
      "[D loss: 0.881287] [G loss: 1.545493]\n",
      "[D loss: 0.751395] [G loss: 1.545531]\n",
      "[D loss: 0.668501] [G loss: 1.512946]\n",
      "[D loss: 0.750515] [G loss: 1.363364]\n",
      "[D loss: 0.879588] [G loss: 1.408621]\n",
      "[D loss: 0.770288] [G loss: 1.196876]\n",
      "[D loss: 1.003724] [G loss: 1.713580]\n",
      "[D loss: 0.824875] [G loss: 1.519836]\n",
      "[D loss: 0.876591] [G loss: 1.551655]\n",
      "[D loss: 0.928241] [G loss: 1.471655]\n",
      "[D loss: 0.635244] [G loss: 1.569154]\n",
      "[D loss: 0.807392] [G loss: 1.484004]\n",
      "[D loss: 1.077203] [G loss: 1.335020]\n",
      "[D loss: 0.736680] [G loss: 1.551594]\n",
      "[D loss: 0.942174] [G loss: 1.194742]\n",
      "[D loss: 0.858880] [G loss: 1.667076]\n",
      "[D loss: 0.667177] [G loss: 1.433907]\n",
      "[D loss: 0.834165] [G loss: 1.382214]\n",
      "[D loss: 1.029729] [G loss: 1.423404]\n",
      "[D loss: 0.676530] [G loss: 1.575003]\n",
      "[D loss: 0.712618] [G loss: 1.373163]\n",
      "[D loss: 0.659915] [G loss: 1.511288]\n",
      "[D loss: 0.909102] [G loss: 1.529858]\n",
      "[D loss: 1.024516] [G loss: 1.486527]\n",
      "[D loss: 0.802355] [G loss: 1.822269]\n",
      "[D loss: 0.995233] [G loss: 1.374944]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.874036] [G loss: 1.289240]\n",
      "[D loss: 0.868709] [G loss: 1.478552]\n",
      "[D loss: 0.944267] [G loss: 1.495003]\n",
      "[D loss: 0.871691] [G loss: 1.443710]\n",
      "[D loss: 0.836039] [G loss: 1.429796]\n",
      "[D loss: 0.594510] [G loss: 1.491264]\n",
      "[D loss: 0.756139] [G loss: 1.468074]\n",
      "[D loss: 0.619946] [G loss: 1.542650]\n",
      "[D loss: 0.959591] [G loss: 1.422892]\n",
      "[D loss: 0.854668] [G loss: 1.542307]\n",
      "[D loss: 0.907253] [G loss: 1.262273]\n",
      "[D loss: 1.088239] [G loss: 1.389142]\n",
      "[D loss: 0.910586] [G loss: 1.494876]\n",
      "[D loss: 0.890986] [G loss: 1.387625]\n",
      "[D loss: 0.586742] [G loss: 1.598194]\n",
      "[D loss: 0.863852] [G loss: 1.349269]\n",
      "[D loss: 1.100667] [G loss: 1.225563]\n",
      "[D loss: 0.723872] [G loss: 1.241567]\n",
      "[D loss: 0.867732] [G loss: 1.457038]\n",
      "[D loss: 0.663306] [G loss: 1.592426]\n",
      "[D loss: 0.643006] [G loss: 1.677545]\n",
      "[D loss: 0.670819] [G loss: 1.475553]\n",
      "[D loss: 0.753030] [G loss: 1.498120]\n",
      "[D loss: 0.780639] [G loss: 1.602193]\n",
      "[D loss: 0.866002] [G loss: 1.512087]\n",
      "[D loss: 0.745480] [G loss: 1.445033]\n",
      "[D loss: 0.736777] [G loss: 1.606347]\n",
      "[D loss: 0.727901] [G loss: 1.411288]\n",
      "[D loss: 0.734815] [G loss: 1.570417]\n",
      "[D loss: 0.759546] [G loss: 1.518483]\n",
      "[D loss: 0.764014] [G loss: 1.691960]\n",
      "[D loss: 0.809249] [G loss: 1.865322]\n",
      "[D loss: 0.733263] [G loss: 1.633307]\n",
      "[D loss: 0.986466] [G loss: 1.401546]\n",
      "[D loss: 0.611589] [G loss: 1.575031]\n",
      "[D loss: 0.818466] [G loss: 1.390834]\n",
      "[D loss: 0.775857] [G loss: 1.514731]\n",
      "[D loss: 0.926303] [G loss: 1.325364]\n",
      "[D loss: 0.775701] [G loss: 1.415502]\n",
      "[D loss: 0.841008] [G loss: 1.504581]\n",
      "[D loss: 0.995128] [G loss: 1.461978]\n",
      "[D loss: 0.786340] [G loss: 1.410762]\n",
      "[D loss: 1.078869] [G loss: 1.566788]\n",
      "[D loss: 0.840534] [G loss: 1.490555]\n",
      "[D loss: 0.873381] [G loss: 1.397352]\n",
      "[D loss: 0.925321] [G loss: 1.534506]\n",
      "[D loss: 0.681439] [G loss: 1.558695]\n",
      "[D loss: 0.746133] [G loss: 1.294806]\n",
      "[D loss: 1.023679] [G loss: 1.280247]\n",
      "[D loss: 0.716271] [G loss: 1.762844]\n",
      "[D loss: 0.725166] [G loss: 1.466013]\n",
      "[D loss: 1.081805] [G loss: 1.215987]\n",
      "[D loss: 0.738055] [G loss: 1.416577]\n",
      "[D loss: 0.690257] [G loss: 1.553189]\n",
      "[D loss: 0.790943] [G loss: 1.590461]\n",
      "[D loss: 0.734514] [G loss: 1.430807]\n",
      "[D loss: 0.874184] [G loss: 1.518498]\n",
      "[D loss: 0.944795] [G loss: 1.382131]\n",
      "[D loss: 0.919175] [G loss: 1.045384]\n",
      "[D loss: 0.925448] [G loss: 1.549944]\n",
      "[D loss: 0.946331] [G loss: 1.594428]\n",
      "[D loss: 0.989956] [G loss: 1.361376]\n",
      "[D loss: 0.716166] [G loss: 1.435686]\n",
      "[D loss: 0.934042] [G loss: 1.295537]\n",
      "[D loss: 0.824646] [G loss: 1.396955]\n",
      "[D loss: 1.095274] [G loss: 1.367259]\n",
      "[D loss: 0.862279] [G loss: 1.447803]\n",
      "[D loss: 0.674970] [G loss: 1.482057]\n",
      "[D loss: 0.804386] [G loss: 1.478666]\n",
      "[D loss: 1.150138] [G loss: 1.471181]\n",
      "[D loss: 0.756805] [G loss: 1.385967]\n",
      "[D loss: 0.800450] [G loss: 1.449710]\n",
      "[D loss: 0.652978] [G loss: 1.415763]\n",
      "[D loss: 0.970647] [G loss: 1.338329]\n",
      "[D loss: 0.661095] [G loss: 1.365893]\n",
      "[D loss: 0.762676] [G loss: 1.371476]\n",
      "[D loss: 0.922427] [G loss: 1.458838]\n",
      "[D loss: 0.982307] [G loss: 1.301126]\n",
      "[D loss: 0.721227] [G loss: 1.582922]\n",
      "[D loss: 0.870042] [G loss: 1.405385]\n",
      "[D loss: 0.831180] [G loss: 1.411880]\n",
      "[D loss: 0.847444] [G loss: 1.297819]\n",
      "[D loss: 0.886758] [G loss: 1.434744]\n",
      "[D loss: 0.898177] [G loss: 1.378553]\n",
      "[D loss: 0.765987] [G loss: 1.554761]\n",
      "[D loss: 1.015493] [G loss: 1.144326]\n",
      "[D loss: 0.926968] [G loss: 1.392862]\n",
      "[D loss: 0.658811] [G loss: 1.464582]\n",
      "[D loss: 0.774115] [G loss: 1.490195]\n",
      "[D loss: 0.983206] [G loss: 1.262512]\n",
      "[D loss: 0.805128] [G loss: 1.467489]\n",
      "[D loss: 0.772322] [G loss: 1.486379]\n",
      "[D loss: 0.665926] [G loss: 1.618809]\n",
      "[D loss: 0.707572] [G loss: 1.380232]\n",
      "[D loss: 0.719595] [G loss: 1.485301]\n",
      "[D loss: 0.837075] [G loss: 1.367159]\n",
      "[D loss: 0.918424] [G loss: 1.362223]\n",
      "[D loss: 0.817564] [G loss: 1.355680]\n",
      "[D loss: 1.046386] [G loss: 1.513588]\n",
      "[D loss: 1.169322] [G loss: 1.351262]\n",
      "[D loss: 0.741409] [G loss: 1.436084]\n",
      "[D loss: 0.759499] [G loss: 1.535344]\n",
      "[D loss: 0.995791] [G loss: 1.541286]\n",
      "[D loss: 0.820021] [G loss: 1.482175]\n",
      "[D loss: 0.761660] [G loss: 1.519088]\n",
      "[D loss: 0.804753] [G loss: 1.431145]\n",
      "[D loss: 0.793302] [G loss: 1.594245]\n",
      "[D loss: 0.779838] [G loss: 1.465564]\n",
      "[D loss: 0.682120] [G loss: 1.609490]\n",
      "[D loss: 0.949268] [G loss: 1.315607]\n",
      "[D loss: 0.605427] [G loss: 1.447320]\n",
      "[D loss: 0.955353] [G loss: 1.648231]\n",
      "[D loss: 0.801877] [G loss: 1.630630]\n",
      "[D loss: 1.054380] [G loss: 1.393879]\n",
      "[D loss: 0.969074] [G loss: 1.422565]\n",
      "[D loss: 1.009339] [G loss: 1.328352]\n",
      "[D loss: 0.968955] [G loss: 1.273121]\n",
      "[D loss: 0.788581] [G loss: 1.585447]\n",
      "[D loss: 0.845587] [G loss: 1.444742]\n",
      "[D loss: 0.978006] [G loss: 1.313332]\n",
      "[D loss: 0.771663] [G loss: 1.494575]\n",
      "[D loss: 0.879272] [G loss: 1.324417]\n",
      "[D loss: 0.783165] [G loss: 1.385145]\n",
      "[D loss: 0.849744] [G loss: 1.360900]\n",
      "[D loss: 0.910325] [G loss: 1.576958]\n",
      "[D loss: 0.887388] [G loss: 1.390815]\n",
      "[D loss: 0.792540] [G loss: 1.494658]\n",
      "[D loss: 0.854801] [G loss: 1.298923]\n",
      "[D loss: 0.926687] [G loss: 1.429042]\n",
      "[D loss: 0.829642] [G loss: 1.558944]\n",
      "[D loss: 0.661637] [G loss: 1.339473]\n",
      "[D loss: 0.871702] [G loss: 1.285141]\n",
      "[D loss: 0.673920] [G loss: 1.326675]\n",
      "[D loss: 1.090188] [G loss: 1.456565]\n",
      "[D loss: 0.864047] [G loss: 1.432775]\n",
      "[D loss: 0.873775] [G loss: 1.304028]\n",
      "[D loss: 0.855027] [G loss: 1.589806]\n",
      "[D loss: 0.857127] [G loss: 1.446048]\n",
      "[D loss: 0.723435] [G loss: 1.394504]\n",
      "[D loss: 1.142562] [G loss: 1.410372]\n",
      "[D loss: 0.711512] [G loss: 1.606136]\n",
      "[D loss: 0.879792] [G loss: 1.350583]\n",
      "[D loss: 0.747316] [G loss: 1.358125]\n",
      "[D loss: 0.705955] [G loss: 1.465156]\n",
      "[D loss: 0.871665] [G loss: 1.570343]\n",
      "[D loss: 0.801931] [G loss: 1.573325]\n",
      "[D loss: 0.801940] [G loss: 1.512711]\n",
      "[D loss: 0.768922] [G loss: 1.418032]\n",
      "[D loss: 0.794545] [G loss: 1.674976]\n",
      "[D loss: 1.116698] [G loss: 1.397510]\n",
      "[D loss: 0.932760] [G loss: 1.410075]\n",
      "[D loss: 0.735641] [G loss: 1.454982]\n",
      "[D loss: 0.736264] [G loss: 1.477099]\n",
      "[D loss: 0.914917] [G loss: 1.494018]\n",
      "[D loss: 0.806162] [G loss: 1.595048]\n",
      "[D loss: 0.875342] [G loss: 1.273950]\n",
      "[D loss: 0.658642] [G loss: 1.454919]\n",
      "[D loss: 1.118658] [G loss: 1.277371]\n",
      "[D loss: 0.881351] [G loss: 1.479922]\n",
      "[D loss: 0.845069] [G loss: 1.427993]\n",
      "[D loss: 0.903092] [G loss: 1.650175]\n",
      "[D loss: 0.672293] [G loss: 1.541604]\n",
      "[D loss: 0.710454] [G loss: 1.321708]\n",
      "[D loss: 1.138380] [G loss: 1.504767]\n",
      "[D loss: 0.965299] [G loss: 1.324736]\n",
      "[D loss: 0.850652] [G loss: 1.621732]\n",
      "[D loss: 0.543841] [G loss: 1.521708]\n",
      "[D loss: 0.883956] [G loss: 1.602240]\n",
      "[D loss: 0.816739] [G loss: 1.335726]\n",
      "[D loss: 0.817131] [G loss: 1.460780]\n",
      "[D loss: 0.908968] [G loss: 1.567726]\n",
      "[D loss: 1.052972] [G loss: 1.348850]\n",
      "[D loss: 0.745480] [G loss: 1.361625]\n",
      "[D loss: 0.891934] [G loss: 1.357698]\n",
      "[D loss: 0.955874] [G loss: 1.622048]\n",
      "[D loss: 0.781652] [G loss: 1.542094]\n",
      "[D loss: 0.760167] [G loss: 1.557803]\n",
      "[D loss: 1.000309] [G loss: 1.673894]\n",
      "[D loss: 0.794077] [G loss: 1.421451]\n",
      "[D loss: 0.769587] [G loss: 1.410584]\n",
      "[D loss: 0.719787] [G loss: 1.463877]\n",
      "[D loss: 0.583883] [G loss: 1.553753]\n",
      "[D loss: 0.899049] [G loss: 1.506842]\n",
      "[D loss: 0.721278] [G loss: 1.706279]\n",
      "[D loss: 0.888617] [G loss: 1.441563]\n",
      "[D loss: 0.573045] [G loss: 1.511508]\n",
      "[D loss: 0.726620] [G loss: 1.320500]\n",
      "[D loss: 0.822835] [G loss: 1.507554]\n",
      "[D loss: 0.842303] [G loss: 1.549647]\n",
      "[D loss: 0.862252] [G loss: 1.349878]\n",
      "[D loss: 0.840579] [G loss: 1.251281]\n",
      "[D loss: 0.658249] [G loss: 1.619497]\n",
      "[D loss: 0.865349] [G loss: 1.300638]\n",
      "[D loss: 0.853746] [G loss: 1.713761]\n",
      "[D loss: 0.850194] [G loss: 1.559779]\n",
      "[D loss: 0.806370] [G loss: 1.482214]\n",
      "[D loss: 0.951894] [G loss: 1.588321]\n",
      "[D loss: 0.717210] [G loss: 1.531443]\n",
      "[D loss: 0.735198] [G loss: 1.265752]\n",
      "[D loss: 0.857231] [G loss: 1.557393]\n",
      "[D loss: 1.001806] [G loss: 1.453789]\n",
      "[D loss: 0.770615] [G loss: 1.526721]\n",
      "[D loss: 0.656262] [G loss: 1.772980]\n",
      "[D loss: 0.959330] [G loss: 1.768219]\n",
      "[D loss: 0.741805] [G loss: 1.753692]\n",
      "[D loss: 0.826351] [G loss: 1.217682]\n",
      "[D loss: 0.634531] [G loss: 1.632977]\n",
      "[D loss: 0.838258] [G loss: 1.625378]\n",
      "[D loss: 0.933825] [G loss: 1.718585]\n",
      "[D loss: 0.739429] [G loss: 1.466813]\n",
      "[D loss: 0.866439] [G loss: 1.575811]\n",
      "[D loss: 0.959492] [G loss: 1.520296]\n",
      "[D loss: 0.820581] [G loss: 1.506621]\n",
      "[D loss: 1.082366] [G loss: 1.394720]\n",
      "[D loss: 0.812347] [G loss: 1.588199]\n",
      "[D loss: 0.567847] [G loss: 1.652647]\n",
      "[D loss: 0.730390] [G loss: 1.550808]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.703009] [G loss: 1.575052]\n",
      "[D loss: 0.903865] [G loss: 1.526128]\n",
      "[D loss: 0.780556] [G loss: 1.375117]\n",
      "[D loss: 0.706581] [G loss: 1.342430]\n",
      "[D loss: 0.659126] [G loss: 1.481650]\n",
      "[D loss: 0.718758] [G loss: 1.582165]\n",
      "[D loss: 0.873798] [G loss: 1.764009]\n",
      "[D loss: 0.900485] [G loss: 1.617661]\n",
      "[D loss: 0.909588] [G loss: 1.638303]\n",
      "[D loss: 1.125015] [G loss: 1.240899]\n",
      "[D loss: 0.739324] [G loss: 1.323563]\n",
      "[D loss: 1.009740] [G loss: 1.537728]\n",
      "[D loss: 0.827790] [G loss: 1.374907]\n",
      "[D loss: 1.042254] [G loss: 1.611800]\n",
      "[D loss: 0.886169] [G loss: 1.229633]\n",
      "[D loss: 1.035734] [G loss: 1.407950]\n",
      "[D loss: 0.824241] [G loss: 1.417754]\n",
      "[D loss: 0.816131] [G loss: 1.452808]\n",
      "[D loss: 0.974008] [G loss: 1.586407]\n",
      "[D loss: 0.871551] [G loss: 1.498817]\n",
      "[D loss: 0.888447] [G loss: 1.642612]\n",
      "[D loss: 0.737463] [G loss: 1.514969]\n",
      "[D loss: 0.647022] [G loss: 1.319035]\n",
      "[D loss: 0.873684] [G loss: 1.291119]\n",
      "[D loss: 0.812407] [G loss: 1.390264]\n",
      "[D loss: 0.854004] [G loss: 1.344795]\n",
      "[D loss: 0.782087] [G loss: 1.415766]\n",
      "[D loss: 0.966647] [G loss: 1.607810]\n",
      "[D loss: 0.629090] [G loss: 1.420434]\n",
      "[D loss: 0.781190] [G loss: 1.581912]\n",
      "[D loss: 0.798557] [G loss: 1.397927]\n",
      "[D loss: 0.788557] [G loss: 1.573078]\n",
      "[D loss: 0.923159] [G loss: 1.536995]\n",
      "[D loss: 0.649401] [G loss: 1.700560]\n",
      "[D loss: 0.679216] [G loss: 1.603596]\n",
      "[D loss: 0.863436] [G loss: 1.302567]\n",
      "[D loss: 0.826191] [G loss: 1.445234]\n",
      "[D loss: 0.870411] [G loss: 1.419286]\n",
      "[D loss: 0.818786] [G loss: 1.508535]\n",
      "[D loss: 0.858752] [G loss: 1.379488]\n",
      "[D loss: 0.793170] [G loss: 1.351227]\n",
      "[D loss: 1.105974] [G loss: 1.298479]\n",
      "[D loss: 0.742641] [G loss: 1.484893]\n",
      "[D loss: 0.929876] [G loss: 1.321663]\n",
      "[D loss: 0.929398] [G loss: 1.490093]\n",
      "[D loss: 0.585041] [G loss: 1.325843]\n",
      "[D loss: 0.894276] [G loss: 1.371151]\n",
      "[D loss: 0.984083] [G loss: 1.500204]\n",
      "[D loss: 0.851283] [G loss: 1.500406]\n",
      "[D loss: 0.856563] [G loss: 1.397169]\n",
      "[D loss: 0.845778] [G loss: 1.546911]\n",
      "[D loss: 0.824905] [G loss: 1.430021]\n",
      "[D loss: 0.960288] [G loss: 1.353968]\n",
      "[D loss: 0.834111] [G loss: 1.417664]\n",
      "[D loss: 0.682492] [G loss: 1.558684]\n",
      "[D loss: 1.194912] [G loss: 1.242409]\n",
      "[D loss: 0.819209] [G loss: 1.299411]\n",
      "[D loss: 0.763516] [G loss: 1.417864]\n",
      "[D loss: 0.668450] [G loss: 1.668398]\n",
      "[D loss: 0.795662] [G loss: 1.427509]\n",
      "[D loss: 0.623462] [G loss: 1.573033]\n",
      "[D loss: 0.814970] [G loss: 1.524940]\n",
      "[D loss: 0.740529] [G loss: 1.631284]\n",
      "[D loss: 0.815053] [G loss: 1.420163]\n",
      "[D loss: 0.764397] [G loss: 1.335840]\n",
      "[D loss: 0.904872] [G loss: 1.531638]\n",
      "[D loss: 0.969356] [G loss: 1.336658]\n",
      "[D loss: 0.862165] [G loss: 1.440835]\n",
      "[D loss: 0.601468] [G loss: 1.479910]\n",
      "[D loss: 0.847479] [G loss: 1.461838]\n",
      "[D loss: 0.774686] [G loss: 1.605109]\n",
      "[D loss: 0.734360] [G loss: 1.574145]\n",
      "[D loss: 0.860777] [G loss: 1.605669]\n",
      "[D loss: 1.122033] [G loss: 1.662037]\n",
      "[D loss: 0.877373] [G loss: 1.524745]\n",
      "[D loss: 0.808267] [G loss: 1.393069]\n",
      "[D loss: 0.694795] [G loss: 1.643186]\n",
      "[D loss: 1.012416] [G loss: 1.318274]\n",
      "[D loss: 0.691101] [G loss: 1.433280]\n",
      "[D loss: 0.911137] [G loss: 1.209687]\n",
      "[D loss: 0.601637] [G loss: 1.326212]\n",
      "[D loss: 0.761252] [G loss: 1.596152]\n",
      "[D loss: 0.900955] [G loss: 1.363949]\n",
      "[D loss: 0.727300] [G loss: 1.695124]\n",
      "[D loss: 0.779018] [G loss: 1.443289]\n",
      "[D loss: 0.832310] [G loss: 1.624147]\n",
      "[D loss: 0.678210] [G loss: 1.484997]\n",
      "[D loss: 0.770392] [G loss: 1.616849]\n",
      "[D loss: 0.898101] [G loss: 1.450228]\n",
      "[D loss: 0.856666] [G loss: 1.419866]\n",
      "[D loss: 0.704923] [G loss: 1.564673]\n",
      "[D loss: 0.947547] [G loss: 1.635453]\n",
      "[D loss: 0.801464] [G loss: 1.534038]\n",
      "[D loss: 0.748087] [G loss: 1.747272]\n",
      "[D loss: 0.881112] [G loss: 1.369940]\n",
      "[D loss: 0.833217] [G loss: 1.350397]\n",
      "[D loss: 0.769776] [G loss: 1.545581]\n",
      "[D loss: 0.627083] [G loss: 1.506176]\n",
      "[D loss: 0.940263] [G loss: 1.310484]\n",
      "[D loss: 0.812849] [G loss: 1.458389]\n",
      "[D loss: 0.707580] [G loss: 1.588212]\n",
      "[D loss: 0.913173] [G loss: 1.405978]\n",
      "[D loss: 0.899337] [G loss: 1.357143]\n",
      "[D loss: 0.871668] [G loss: 1.513227]\n",
      "[D loss: 1.209255] [G loss: 1.384964]\n",
      "[D loss: 0.892887] [G loss: 1.341069]\n",
      "[D loss: 0.800666] [G loss: 1.496881]\n",
      "[D loss: 0.755093] [G loss: 1.476558]\n",
      "[D loss: 0.843567] [G loss: 1.482862]\n",
      "[D loss: 0.789505] [G loss: 1.504585]\n",
      "[D loss: 1.044975] [G loss: 1.320784]\n",
      "[D loss: 1.079619] [G loss: 1.307907]\n",
      "[D loss: 0.713046] [G loss: 1.404174]\n",
      "[D loss: 0.544989] [G loss: 1.539786]\n",
      "[D loss: 0.855209] [G loss: 1.561838]\n",
      "[D loss: 0.666350] [G loss: 1.562981]\n",
      "[D loss: 0.887536] [G loss: 1.341596]\n",
      "[D loss: 0.844563] [G loss: 1.499895]\n",
      "[D loss: 0.662169] [G loss: 1.560334]\n",
      "[D loss: 0.863181] [G loss: 1.337210]\n",
      "[D loss: 0.818973] [G loss: 1.402649]\n",
      "[D loss: 0.995299] [G loss: 1.222685]\n",
      "[D loss: 0.723404] [G loss: 1.316043]\n",
      "[D loss: 0.991849] [G loss: 1.384396]\n",
      "[D loss: 0.903236] [G loss: 1.349020]\n",
      "[D loss: 0.894308] [G loss: 1.356408]\n",
      "[D loss: 1.013347] [G loss: 1.571067]\n",
      "[D loss: 0.615300] [G loss: 1.505856]\n",
      "[D loss: 0.808842] [G loss: 1.651355]\n",
      "[D loss: 0.955206] [G loss: 1.455986]\n",
      "[D loss: 0.692536] [G loss: 1.405738]\n",
      "[D loss: 0.685588] [G loss: 1.409409]\n",
      "[D loss: 0.600510] [G loss: 1.463190]\n",
      "[D loss: 0.666057] [G loss: 1.332165]\n",
      "[D loss: 0.712401] [G loss: 1.552699]\n",
      "[D loss: 0.749150] [G loss: 1.279929]\n",
      "[D loss: 0.778930] [G loss: 1.573720]\n",
      "[D loss: 0.907342] [G loss: 1.741797]\n",
      "[D loss: 0.676909] [G loss: 1.811211]\n",
      "[D loss: 0.931014] [G loss: 1.517027]\n",
      "[D loss: 0.581461] [G loss: 1.391332]\n",
      "[D loss: 0.747678] [G loss: 1.514073]\n",
      "[D loss: 1.000460] [G loss: 1.400600]\n",
      "[D loss: 0.729131] [G loss: 1.368118]\n",
      "[D loss: 0.707913] [G loss: 1.558918]\n",
      "[D loss: 0.672175] [G loss: 1.441631]\n",
      "[D loss: 0.885723] [G loss: 1.543461]\n",
      "[D loss: 0.713190] [G loss: 1.439916]\n",
      "[D loss: 0.954691] [G loss: 1.412903]\n",
      "[D loss: 0.852760] [G loss: 1.402983]\n",
      "[D loss: 0.868296] [G loss: 1.431389]\n",
      "[D loss: 0.749553] [G loss: 1.459941]\n",
      "[D loss: 0.737861] [G loss: 1.564492]\n",
      "[D loss: 0.862036] [G loss: 1.415561]\n",
      "[D loss: 0.792271] [G loss: 1.610567]\n",
      "[D loss: 1.065524] [G loss: 1.406065]\n",
      "[D loss: 0.878318] [G loss: 1.542587]\n",
      "[D loss: 0.692944] [G loss: 1.524169]\n",
      "[D loss: 0.874769] [G loss: 1.413916]\n",
      "[D loss: 0.852474] [G loss: 1.408127]\n",
      "[D loss: 0.747073] [G loss: 1.417328]\n",
      "[D loss: 0.988322] [G loss: 1.274215]\n",
      "[D loss: 0.839521] [G loss: 1.427908]\n",
      "[D loss: 0.836257] [G loss: 1.390484]\n",
      "[D loss: 0.684462] [G loss: 1.583983]\n",
      "[D loss: 0.671169] [G loss: 1.585968]\n",
      "[D loss: 0.665614] [G loss: 1.381911]\n",
      "[D loss: 0.782147] [G loss: 1.529708]\n",
      "[D loss: 1.198171] [G loss: 1.374839]\n",
      "[D loss: 0.947705] [G loss: 1.348467]\n",
      "[D loss: 0.792090] [G loss: 1.619493]\n",
      "[D loss: 1.070584] [G loss: 1.375732]\n",
      "[D loss: 0.809908] [G loss: 1.533556]\n",
      "[D loss: 0.984245] [G loss: 1.362610]\n",
      "[D loss: 0.664346] [G loss: 1.319877]\n",
      "[D loss: 0.901716] [G loss: 1.571097]\n",
      "[D loss: 0.644831] [G loss: 1.643897]\n",
      "[D loss: 0.627399] [G loss: 1.435348]\n",
      "[D loss: 0.749588] [G loss: 1.359807]\n",
      "[D loss: 0.591483] [G loss: 1.690378]\n",
      "[D loss: 0.831871] [G loss: 1.571700]\n",
      "[D loss: 0.682183] [G loss: 1.464770]\n",
      "[D loss: 1.009595] [G loss: 1.576232]\n",
      "[D loss: 0.868869] [G loss: 1.724822]\n",
      "[D loss: 0.699672] [G loss: 1.634289]\n",
      "[D loss: 0.974932] [G loss: 1.413818]\n",
      "[D loss: 0.984520] [G loss: 1.427116]\n",
      "[D loss: 0.783656] [G loss: 1.656447]\n",
      "[D loss: 0.983773] [G loss: 1.383899]\n",
      "[D loss: 0.625097] [G loss: 1.605582]\n",
      "[D loss: 0.784770] [G loss: 1.530468]\n",
      "[D loss: 0.763263] [G loss: 1.531065]\n",
      "[D loss: 0.707113] [G loss: 1.580566]\n",
      "[D loss: 0.691669] [G loss: 1.525331]\n",
      "[D loss: 0.634535] [G loss: 1.462532]\n",
      "[D loss: 0.854500] [G loss: 1.273020]\n",
      "[D loss: 0.738157] [G loss: 1.308113]\n",
      "[D loss: 0.907324] [G loss: 1.430490]\n",
      "[D loss: 0.815582] [G loss: 1.455499]\n",
      "[D loss: 0.737536] [G loss: 1.556175]\n",
      "[D loss: 0.803477] [G loss: 1.481794]\n",
      "[D loss: 1.005916] [G loss: 1.399775]\n",
      "[D loss: 0.761565] [G loss: 1.453343]\n",
      "[D loss: 0.909194] [G loss: 1.363728]\n",
      "[D loss: 0.839077] [G loss: 1.683660]\n",
      "[D loss: 0.844004] [G loss: 1.376742]\n",
      "[D loss: 0.857061] [G loss: 1.289171]\n",
      "[D loss: 0.735852] [G loss: 1.233632]\n",
      "[D loss: 0.898204] [G loss: 1.336106]\n",
      "[D loss: 0.775767] [G loss: 1.451850]\n",
      "[D loss: 0.575555] [G loss: 1.292439]\n",
      "[D loss: 1.085723] [G loss: 1.509122]\n",
      "[D loss: 0.937322] [G loss: 1.455203]\n",
      "[D loss: 0.809384] [G loss: 1.427330]\n",
      "[D loss: 0.744344] [G loss: 1.608302]\n",
      "[D loss: 0.880076] [G loss: 1.549597]\n",
      "[D loss: 0.816202] [G loss: 1.537886]\n",
      "[D loss: 0.948780] [G loss: 1.534138]\n",
      "[D loss: 0.904873] [G loss: 1.483168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.809815] [G loss: 1.502659]\n",
      "[D loss: 0.545490] [G loss: 1.455767]\n",
      "[D loss: 0.809927] [G loss: 1.487325]\n",
      "[D loss: 0.771582] [G loss: 1.486719]\n",
      "[D loss: 0.756904] [G loss: 1.368843]\n",
      "[D loss: 0.778777] [G loss: 1.425544]\n",
      "[D loss: 1.046881] [G loss: 1.500309]\n",
      "[D loss: 0.797706] [G loss: 1.589436]\n",
      "[D loss: 0.820969] [G loss: 1.661465]\n",
      "[D loss: 1.000189] [G loss: 1.539147]\n",
      "[D loss: 0.716475] [G loss: 1.543975]\n",
      "[D loss: 0.726581] [G loss: 1.608324]\n",
      "[D loss: 0.809897] [G loss: 1.317783]\n",
      "[D loss: 0.684458] [G loss: 1.392242]\n",
      "[D loss: 0.664925] [G loss: 1.748765]\n",
      "[D loss: 0.844471] [G loss: 1.583441]\n",
      "[D loss: 0.815143] [G loss: 1.389791]\n",
      "[D loss: 0.717743] [G loss: 1.676905]\n",
      "[D loss: 0.702547] [G loss: 1.515205]\n",
      "[D loss: 1.016383] [G loss: 1.352847]\n",
      "[D loss: 0.778806] [G loss: 1.387667]\n",
      "[D loss: 0.682026] [G loss: 1.649974]\n",
      "[D loss: 0.950736] [G loss: 1.584795]\n",
      "[D loss: 0.917974] [G loss: 1.348497]\n",
      "[D loss: 0.586456] [G loss: 1.566743]\n",
      "[D loss: 0.964865] [G loss: 1.613200]\n",
      "[D loss: 0.743399] [G loss: 1.461181]\n",
      "[D loss: 0.575649] [G loss: 1.704147]\n",
      "[D loss: 0.815735] [G loss: 1.449353]\n",
      "[D loss: 0.918216] [G loss: 1.494455]\n",
      "[D loss: 0.833331] [G loss: 1.584048]\n",
      "[D loss: 0.750467] [G loss: 1.537615]\n",
      "[D loss: 0.884886] [G loss: 1.538608]\n",
      "[D loss: 0.663868] [G loss: 1.912009]\n",
      "[D loss: 0.819921] [G loss: 1.772884]\n",
      "[D loss: 0.825057] [G loss: 1.469255]\n",
      "[D loss: 0.843650] [G loss: 1.556816]\n",
      "[D loss: 0.756136] [G loss: 1.380857]\n",
      "[D loss: 0.748421] [G loss: 1.533825]\n",
      "[D loss: 0.823809] [G loss: 1.768491]\n",
      "[D loss: 0.600128] [G loss: 1.697978]\n",
      "[D loss: 0.772103] [G loss: 1.667231]\n",
      "[D loss: 1.057965] [G loss: 1.718367]\n",
      "[D loss: 0.779303] [G loss: 1.580695]\n",
      "[D loss: 0.754993] [G loss: 1.339454]\n",
      "[D loss: 1.087926] [G loss: 1.594792]\n",
      "[D loss: 0.660177] [G loss: 1.712962]\n",
      "[D loss: 0.780920] [G loss: 1.348448]\n",
      "[D loss: 0.557590] [G loss: 1.685799]\n",
      "[D loss: 0.824680] [G loss: 1.565162]\n",
      "[D loss: 0.798057] [G loss: 1.500054]\n",
      "[D loss: 0.896072] [G loss: 1.377827]\n",
      "[D loss: 0.829048] [G loss: 1.543745]\n",
      "[D loss: 0.710786] [G loss: 1.541071]\n",
      "[D loss: 0.769780] [G loss: 1.515246]\n",
      "[D loss: 0.741244] [G loss: 1.436687]\n",
      "[D loss: 1.071469] [G loss: 1.730443]\n",
      "[D loss: 0.866202] [G loss: 1.337213]\n",
      "[D loss: 0.784507] [G loss: 1.969047]\n",
      "[D loss: 0.815927] [G loss: 1.714506]\n",
      "[D loss: 0.843835] [G loss: 1.558559]\n",
      "[D loss: 0.915330] [G loss: 1.339670]\n",
      "[D loss: 0.786142] [G loss: 1.599990]\n",
      "[D loss: 1.047616] [G loss: 1.287286]\n",
      "[D loss: 0.883450] [G loss: 1.401064]\n",
      "[D loss: 0.758538] [G loss: 1.721149]\n",
      "[D loss: 0.790739] [G loss: 1.523189]\n",
      "[D loss: 0.736694] [G loss: 1.490024]\n",
      "[D loss: 1.139184] [G loss: 1.372430]\n",
      "[D loss: 0.736027] [G loss: 1.112275]\n",
      "[D loss: 0.844722] [G loss: 1.295437]\n",
      "[D loss: 0.833127] [G loss: 1.384593]\n",
      "[D loss: 0.870267] [G loss: 1.551615]\n",
      "[D loss: 0.962991] [G loss: 1.452069]\n",
      "[D loss: 0.719969] [G loss: 1.641051]\n",
      "[D loss: 1.100804] [G loss: 1.309672]\n",
      "[D loss: 0.687115] [G loss: 1.428195]\n",
      "[D loss: 0.547406] [G loss: 1.484838]\n",
      "[D loss: 1.020833] [G loss: 1.420159]\n",
      "[D loss: 0.833604] [G loss: 1.429543]\n",
      "[D loss: 0.897286] [G loss: 1.595372]\n",
      "[D loss: 0.719410] [G loss: 1.918565]\n",
      "[D loss: 0.688119] [G loss: 1.666784]\n",
      "[D loss: 0.735187] [G loss: 1.462698]\n",
      "[D loss: 0.776334] [G loss: 1.551445]\n",
      "[D loss: 0.802873] [G loss: 1.389427]\n",
      "[D loss: 1.006501] [G loss: 1.439537]\n",
      "[D loss: 0.816645] [G loss: 1.736261]\n",
      "[D loss: 0.857201] [G loss: 1.784311]\n",
      "[D loss: 0.655674] [G loss: 1.603542]\n",
      "[D loss: 0.782330] [G loss: 1.332258]\n",
      "[D loss: 1.058199] [G loss: 1.336001]\n",
      "[D loss: 1.022456] [G loss: 1.389022]\n",
      "[D loss: 0.761429] [G loss: 1.369738]\n",
      "[D loss: 0.722764] [G loss: 1.361238]\n",
      "[D loss: 0.933608] [G loss: 1.356806]\n",
      "[D loss: 0.956896] [G loss: 1.450639]\n",
      "[D loss: 0.840800] [G loss: 1.469546]\n",
      "[D loss: 0.950848] [G loss: 1.477311]\n",
      "[D loss: 1.100881] [G loss: 1.290105]\n",
      "[D loss: 0.844401] [G loss: 1.336689]\n",
      "[D loss: 0.745107] [G loss: 1.475020]\n",
      "[D loss: 0.532345] [G loss: 1.630945]\n",
      "[D loss: 0.645465] [G loss: 1.544943]\n",
      "[D loss: 0.773190] [G loss: 1.646006]\n",
      "[D loss: 0.927654] [G loss: 1.300877]\n",
      "[D loss: 0.887961] [G loss: 1.675781]\n",
      "[D loss: 0.889595] [G loss: 1.539735]\n",
      "[D loss: 1.008710] [G loss: 1.285689]\n",
      "[D loss: 0.819450] [G loss: 1.343350]\n",
      "[D loss: 0.720636] [G loss: 1.528838]\n",
      "[D loss: 0.965675] [G loss: 1.712697]\n",
      "[D loss: 0.708689] [G loss: 1.379286]\n",
      "[D loss: 0.737593] [G loss: 1.695824]\n",
      "[D loss: 0.804324] [G loss: 1.523747]\n",
      "[D loss: 0.685277] [G loss: 1.665131]\n",
      "[D loss: 0.852332] [G loss: 1.425726]\n",
      "[D loss: 0.736313] [G loss: 1.554387]\n",
      "[D loss: 1.051038] [G loss: 1.472376]\n",
      "[D loss: 0.720521] [G loss: 1.542335]\n",
      "[D loss: 0.754307] [G loss: 1.304023]\n",
      "[D loss: 0.938670] [G loss: 1.386453]\n",
      "[D loss: 0.788248] [G loss: 1.605498]\n",
      "[D loss: 0.586475] [G loss: 1.456214]\n",
      "[D loss: 0.728940] [G loss: 1.478120]\n",
      "[D loss: 1.016193] [G loss: 1.444613]\n",
      "[D loss: 1.033360] [G loss: 1.370709]\n",
      "[D loss: 0.821853] [G loss: 1.344476]\n",
      "[D loss: 0.806564] [G loss: 1.386025]\n",
      "[D loss: 0.883805] [G loss: 1.425055]\n",
      "[D loss: 0.904000] [G loss: 1.502863]\n",
      "[D loss: 1.038433] [G loss: 1.358712]\n",
      "[D loss: 0.783986] [G loss: 1.785258]\n",
      "[D loss: 0.809305] [G loss: 1.774902]\n",
      "[D loss: 0.719258] [G loss: 1.445292]\n",
      "[D loss: 0.765390] [G loss: 1.532445]\n",
      "[D loss: 0.760558] [G loss: 1.397572]\n",
      "[D loss: 0.745817] [G loss: 1.442405]\n",
      "[D loss: 0.816512] [G loss: 1.678565]\n",
      "[D loss: 0.992489] [G loss: 1.534676]\n",
      "[D loss: 0.722328] [G loss: 1.438694]\n",
      "[D loss: 0.954794] [G loss: 1.287603]\n",
      "[D loss: 0.721058] [G loss: 1.497637]\n",
      "[D loss: 1.010236] [G loss: 1.288576]\n",
      "[D loss: 0.761992] [G loss: 1.430306]\n",
      "[D loss: 0.757286] [G loss: 1.428766]\n",
      "[D loss: 0.995009] [G loss: 1.427896]\n",
      "[D loss: 0.641095] [G loss: 1.546099]\n",
      "[D loss: 0.886252] [G loss: 1.265462]\n",
      "[D loss: 1.071484] [G loss: 1.427496]\n",
      "[D loss: 0.635748] [G loss: 1.624194]\n",
      "[D loss: 0.817595] [G loss: 1.802569]\n",
      "[D loss: 0.599672] [G loss: 1.457993]\n",
      "[D loss: 0.788587] [G loss: 1.405697]\n",
      "[D loss: 0.868662] [G loss: 1.555619]\n",
      "[D loss: 0.770416] [G loss: 1.587443]\n",
      "[D loss: 0.681917] [G loss: 1.478489]\n",
      "[D loss: 0.878031] [G loss: 1.556000]\n",
      "[D loss: 0.847970] [G loss: 1.587606]\n",
      "[D loss: 0.707061] [G loss: 1.496204]\n",
      "[D loss: 0.592605] [G loss: 1.507486]\n",
      "[D loss: 0.590033] [G loss: 1.742753]\n",
      "[D loss: 0.777607] [G loss: 1.199132]\n",
      "[D loss: 0.687686] [G loss: 1.545216]\n",
      "[D loss: 0.882463] [G loss: 1.260965]\n",
      "[D loss: 0.702665] [G loss: 1.441500]\n",
      "[D loss: 0.930712] [G loss: 1.464747]\n",
      "[D loss: 0.985151] [G loss: 1.371373]\n",
      "[D loss: 0.752068] [G loss: 1.565226]\n",
      "[D loss: 0.775319] [G loss: 1.735273]\n",
      "[D loss: 1.031592] [G loss: 1.489486]\n",
      "[D loss: 0.822773] [G loss: 1.515265]\n",
      "[D loss: 0.789356] [G loss: 1.337860]\n",
      "[D loss: 0.839972] [G loss: 1.361514]\n",
      "[D loss: 0.745144] [G loss: 1.431964]\n",
      "[D loss: 0.799677] [G loss: 1.570509]\n",
      "[D loss: 0.799539] [G loss: 1.323595]\n",
      "[D loss: 0.958391] [G loss: 1.375480]\n",
      "[D loss: 0.537810] [G loss: 1.662318]\n",
      "[D loss: 1.064927] [G loss: 1.464703]\n",
      "[D loss: 1.006715] [G loss: 1.255750]\n",
      "[D loss: 0.646352] [G loss: 1.394482]\n",
      "[D loss: 0.670364] [G loss: 1.710183]\n",
      "[D loss: 0.576093] [G loss: 1.438141]\n",
      "[D loss: 0.913594] [G loss: 1.696549]\n",
      "[D loss: 0.757006] [G loss: 1.556389]\n",
      "[D loss: 0.901042] [G loss: 1.536173]\n",
      "[D loss: 0.747771] [G loss: 1.285756]\n",
      "[D loss: 0.799570] [G loss: 1.535669]\n",
      "[D loss: 0.774321] [G loss: 1.369079]\n",
      "[D loss: 1.082149] [G loss: 1.465678]\n",
      "[D loss: 0.857630] [G loss: 1.734152]\n",
      "[D loss: 0.927750] [G loss: 1.437502]\n",
      "[D loss: 0.815042] [G loss: 1.620255]\n",
      "[D loss: 0.822587] [G loss: 1.522506]\n",
      "[D loss: 0.667015] [G loss: 1.624304]\n",
      "[D loss: 0.896183] [G loss: 1.391371]\n",
      "[D loss: 0.866115] [G loss: 1.520824]\n",
      "[D loss: 0.899773] [G loss: 1.527279]\n",
      "[D loss: 0.669965] [G loss: 1.569627]\n",
      "[D loss: 0.838440] [G loss: 1.436790]\n",
      "[D loss: 0.866498] [G loss: 1.470859]\n",
      "[D loss: 0.909701] [G loss: 1.410377]\n",
      "[D loss: 0.677703] [G loss: 1.466949]\n",
      "[D loss: 0.720621] [G loss: 1.430731]\n",
      "[D loss: 0.643153] [G loss: 1.487108]\n",
      "[D loss: 0.987058] [G loss: 1.347288]\n",
      "[D loss: 0.971209] [G loss: 1.525713]\n",
      "[D loss: 0.930780] [G loss: 1.579062]\n",
      "[D loss: 0.992442] [G loss: 1.491011]\n",
      "[D loss: 0.666976] [G loss: 1.680292]\n",
      "[D loss: 0.762973] [G loss: 1.550839]\n",
      "[D loss: 0.884851] [G loss: 1.491008]\n",
      "[D loss: 0.924741] [G loss: 1.468386]\n",
      "[D loss: 0.709763] [G loss: 1.453035]\n",
      "[D loss: 0.718617] [G loss: 1.679119]\n",
      "[D loss: 0.977210] [G loss: 1.566703]\n",
      "[D loss: 1.000728] [G loss: 1.526637]\n",
      "[D loss: 0.817792] [G loss: 1.385189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 1.003102] [G loss: 1.271435]\n",
      "[D loss: 0.761988] [G loss: 1.231106]\n",
      "[D loss: 0.761173] [G loss: 1.240430]\n",
      "[D loss: 0.795628] [G loss: 1.412004]\n",
      "[D loss: 0.994762] [G loss: 1.616379]\n",
      "[D loss: 1.124455] [G loss: 1.313943]\n",
      "[D loss: 0.696761] [G loss: 1.452135]\n",
      "[D loss: 0.614091] [G loss: 1.509021]\n",
      "[D loss: 0.796980] [G loss: 1.580463]\n",
      "[D loss: 0.718979] [G loss: 1.567792]\n",
      "[D loss: 0.793866] [G loss: 1.487664]\n",
      "[D loss: 0.947194] [G loss: 1.452679]\n",
      "[D loss: 0.804459] [G loss: 1.440256]\n",
      "[D loss: 0.591990] [G loss: 1.416389]\n",
      "[D loss: 0.854329] [G loss: 1.485846]\n",
      "[D loss: 0.815689] [G loss: 1.362370]\n",
      "[D loss: 0.746857] [G loss: 1.458553]\n",
      "[D loss: 0.888612] [G loss: 1.413411]\n",
      "[D loss: 0.867972] [G loss: 1.815552]\n",
      "[D loss: 0.984971] [G loss: 1.689798]\n",
      "[D loss: 0.868367] [G loss: 1.626531]\n",
      "[D loss: 0.745941] [G loss: 1.276085]\n",
      "[D loss: 0.952259] [G loss: 1.387099]\n",
      "[D loss: 0.806277] [G loss: 1.571183]\n",
      "[D loss: 0.906574] [G loss: 1.344087]\n",
      "[D loss: 0.754843] [G loss: 1.508711]\n",
      "[D loss: 0.719040] [G loss: 1.414786]\n",
      "[D loss: 0.841917] [G loss: 1.605407]\n",
      "[D loss: 0.852019] [G loss: 1.445727]\n",
      "[D loss: 0.869212] [G loss: 1.374804]\n",
      "[D loss: 0.906092] [G loss: 1.564847]\n",
      "[D loss: 0.790823] [G loss: 1.532382]\n",
      "[D loss: 0.955046] [G loss: 1.340852]\n",
      "[D loss: 0.812606] [G loss: 1.562541]\n",
      "[D loss: 0.972906] [G loss: 1.354458]\n",
      "[D loss: 0.691158] [G loss: 1.392789]\n",
      "[D loss: 0.950267] [G loss: 1.680833]\n",
      "[D loss: 0.992223] [G loss: 1.254065]\n",
      "[D loss: 1.006329] [G loss: 1.412419]\n",
      "[D loss: 0.917994] [G loss: 1.394665]\n",
      "[D loss: 0.962677] [G loss: 1.219939]\n",
      "[D loss: 0.520580] [G loss: 1.513588]\n",
      "[D loss: 0.754278] [G loss: 1.409572]\n",
      "[D loss: 0.834841] [G loss: 1.505331]\n",
      "[D loss: 0.767286] [G loss: 1.515056]\n",
      "[D loss: 0.850322] [G loss: 1.453992]\n",
      "[D loss: 0.648811] [G loss: 1.511335]\n",
      "[D loss: 0.965631] [G loss: 1.428708]\n",
      "[D loss: 0.745304] [G loss: 1.493988]\n",
      "[D loss: 0.883506] [G loss: 1.352406]\n",
      "[D loss: 0.680723] [G loss: 1.383045]\n",
      "[D loss: 0.909570] [G loss: 1.522414]\n",
      "[D loss: 0.525649] [G loss: 1.403887]\n",
      "[D loss: 0.759555] [G loss: 1.652454]\n",
      "[D loss: 0.741828] [G loss: 1.488435]\n",
      "[D loss: 0.835675] [G loss: 1.388107]\n",
      "[D loss: 0.922778] [G loss: 1.484868]\n",
      "[D loss: 0.856193] [G loss: 1.362269]\n",
      "[D loss: 0.766503] [G loss: 1.420573]\n",
      "[D loss: 0.808589] [G loss: 1.354394]\n",
      "[D loss: 0.767300] [G loss: 1.591950]\n",
      "[D loss: 0.617385] [G loss: 1.599674]\n",
      "[D loss: 0.783907] [G loss: 1.613404]\n",
      "[D loss: 0.995386] [G loss: 1.459514]\n",
      "[D loss: 0.905038] [G loss: 1.406772]\n",
      "[D loss: 0.868005] [G loss: 1.514456]\n",
      "[D loss: 1.045263] [G loss: 1.322463]\n",
      "[D loss: 0.934972] [G loss: 1.440911]\n",
      "[D loss: 0.928879] [G loss: 1.471800]\n",
      "[D loss: 0.774795] [G loss: 1.416144]\n",
      "[D loss: 0.867586] [G loss: 1.390045]\n",
      "[D loss: 0.728214] [G loss: 1.583402]\n",
      "[D loss: 0.854993] [G loss: 1.395855]\n",
      "[D loss: 0.680567] [G loss: 1.458123]\n",
      "[D loss: 0.704672] [G loss: 1.396790]\n",
      "[D loss: 0.984932] [G loss: 1.838695]\n",
      "[D loss: 0.874697] [G loss: 1.454681]\n",
      "[D loss: 0.756924] [G loss: 1.468203]\n",
      "[D loss: 1.061891] [G loss: 1.315807]\n",
      "[D loss: 0.669885] [G loss: 1.485020]\n",
      "[D loss: 0.799267] [G loss: 1.511109]\n",
      "[D loss: 0.827048] [G loss: 1.554978]\n",
      "[D loss: 0.755104] [G loss: 1.726889]\n",
      "[D loss: 0.805253] [G loss: 1.335751]\n",
      "[D loss: 0.870670] [G loss: 1.433600]\n",
      "[D loss: 0.704211] [G loss: 1.411281]\n",
      "[D loss: 0.724091] [G loss: 1.436495]\n",
      "[D loss: 0.988399] [G loss: 1.417084]\n",
      "[D loss: 0.798280] [G loss: 1.393512]\n",
      "[D loss: 0.850949] [G loss: 1.730319]\n",
      "[D loss: 0.845947] [G loss: 1.426126]\n",
      "[D loss: 0.968873] [G loss: 1.408236]\n",
      "[D loss: 0.694362] [G loss: 1.670724]\n",
      "[D loss: 0.712963] [G loss: 1.520610]\n",
      "[D loss: 0.994592] [G loss: 1.586331]\n",
      "[D loss: 0.783635] [G loss: 1.457458]\n",
      "[D loss: 0.846191] [G loss: 1.575455]\n",
      "[D loss: 1.048025] [G loss: 1.219656]\n",
      "[D loss: 0.846744] [G loss: 1.588467]\n",
      "[D loss: 0.784190] [G loss: 1.565964]\n",
      "[D loss: 0.797211] [G loss: 1.325901]\n",
      "[D loss: 0.964834] [G loss: 1.177594]\n",
      "[D loss: 1.049220] [G loss: 1.324772]\n",
      "[D loss: 0.647461] [G loss: 1.580276]\n",
      "[D loss: 0.728422] [G loss: 1.359323]\n",
      "[D loss: 1.028789] [G loss: 1.338966]\n",
      "[D loss: 0.811072] [G loss: 1.358011]\n",
      "[D loss: 0.854504] [G loss: 1.372118]\n",
      "[D loss: 0.741170] [G loss: 1.557333]\n",
      "[D loss: 1.076163] [G loss: 1.693196]\n",
      "[D loss: 0.684009] [G loss: 1.294339]\n",
      "[D loss: 0.860717] [G loss: 1.376542]\n",
      "[D loss: 0.880899] [G loss: 1.206888]\n",
      "[D loss: 0.828657] [G loss: 1.514138]\n",
      "[D loss: 0.690819] [G loss: 1.688119]\n",
      "[D loss: 0.854336] [G loss: 1.408721]\n",
      "[D loss: 0.606333] [G loss: 1.431149]\n",
      "[D loss: 0.789394] [G loss: 1.615677]\n",
      "[D loss: 0.701049] [G loss: 1.471132]\n",
      "[D loss: 1.002143] [G loss: 1.336962]\n",
      "[D loss: 0.850223] [G loss: 1.247741]\n",
      "[D loss: 0.767842] [G loss: 1.253825]\n",
      "[D loss: 1.032438] [G loss: 1.326091]\n",
      "[D loss: 0.932453] [G loss: 1.215632]\n",
      "[D loss: 0.783319] [G loss: 1.433376]\n",
      "[D loss: 0.973813] [G loss: 1.355989]\n",
      "[D loss: 0.708715] [G loss: 1.417388]\n",
      "[D loss: 0.955570] [G loss: 1.309759]\n",
      "[D loss: 1.076848] [G loss: 1.400127]\n",
      "[D loss: 0.739706] [G loss: 1.514237]\n",
      "[D loss: 0.791864] [G loss: 1.499186]\n",
      "[D loss: 0.878633] [G loss: 1.405044]\n",
      "[D loss: 0.853641] [G loss: 1.348320]\n",
      "[D loss: 0.942780] [G loss: 1.293923]\n",
      "[D loss: 1.097315] [G loss: 1.277711]\n",
      "[D loss: 0.728933] [G loss: 1.286783]\n",
      "[D loss: 0.936572] [G loss: 1.345271]\n",
      "[D loss: 0.967227] [G loss: 1.462552]\n",
      "[D loss: 1.000794] [G loss: 1.527040]\n",
      "[D loss: 0.798722] [G loss: 1.401204]\n",
      "[D loss: 0.724938] [G loss: 1.256107]\n",
      "[D loss: 0.664743] [G loss: 1.321145]\n",
      "[D loss: 0.871626] [G loss: 1.346596]\n",
      "[D loss: 0.950276] [G loss: 1.338938]\n",
      "[D loss: 0.878193] [G loss: 1.444072]\n",
      "[D loss: 0.889042] [G loss: 1.428779]\n",
      "[D loss: 0.758734] [G loss: 1.543416]\n",
      "[D loss: 0.839119] [G loss: 1.574807]\n",
      "[D loss: 0.793742] [G loss: 1.344727]\n",
      "[D loss: 0.690888] [G loss: 1.523128]\n",
      "[D loss: 0.683003] [G loss: 1.571962]\n",
      "[D loss: 0.891339] [G loss: 1.430354]\n",
      "[D loss: 0.753896] [G loss: 1.237943]\n",
      "[D loss: 0.846832] [G loss: 1.545957]\n",
      "[D loss: 0.612921] [G loss: 1.629060]\n",
      "[D loss: 0.705665] [G loss: 1.383373]\n",
      "[D loss: 0.741118] [G loss: 1.502487]\n",
      "[D loss: 0.740172] [G loss: 1.416622]\n",
      "[D loss: 0.846611] [G loss: 1.470831]\n",
      "[D loss: 0.772987] [G loss: 1.509337]\n",
      "[D loss: 0.783828] [G loss: 1.551041]\n",
      "[D loss: 0.957894] [G loss: 1.841309]\n",
      "[D loss: 0.735219] [G loss: 1.548979]\n",
      "[D loss: 0.856879] [G loss: 1.490883]\n",
      "[D loss: 0.985571] [G loss: 1.456021]\n",
      "[D loss: 0.658921] [G loss: 1.453663]\n",
      "[D loss: 0.817982] [G loss: 1.516572]\n",
      "[D loss: 0.890524] [G loss: 1.417295]\n",
      "[D loss: 0.677840] [G loss: 1.454308]\n",
      "[D loss: 0.648573] [G loss: 1.403002]\n",
      "[D loss: 0.814905] [G loss: 1.606726]\n",
      "[D loss: 0.852164] [G loss: 1.531047]\n",
      "[D loss: 1.058484] [G loss: 1.370909]\n",
      "[D loss: 0.763539] [G loss: 1.397651]\n",
      "[D loss: 0.547584] [G loss: 1.498049]\n",
      "[D loss: 0.895429] [G loss: 1.520422]\n",
      "[D loss: 0.776332] [G loss: 1.562422]\n",
      "[D loss: 0.834512] [G loss: 1.466917]\n",
      "[D loss: 0.786987] [G loss: 1.339749]\n",
      "[D loss: 0.886179] [G loss: 1.564846]\n",
      "[D loss: 1.024068] [G loss: 1.339068]\n",
      "[D loss: 0.802549] [G loss: 1.574154]\n",
      "[D loss: 0.625603] [G loss: 1.547148]\n",
      "[D loss: 0.963283] [G loss: 1.615621]\n",
      "[D loss: 0.758077] [G loss: 1.453947]\n",
      "[D loss: 0.684961] [G loss: 1.672834]\n",
      "[D loss: 0.854108] [G loss: 1.688333]\n",
      "[D loss: 0.748173] [G loss: 1.393447]\n",
      "[D loss: 0.868958] [G loss: 1.380046]\n",
      "[D loss: 1.099684] [G loss: 1.295433]\n",
      "[D loss: 0.694931] [G loss: 1.535209]\n",
      "[D loss: 0.780762] [G loss: 1.294990]\n",
      "[D loss: 0.633441] [G loss: 1.558190]\n",
      "[D loss: 0.823769] [G loss: 1.496786]\n",
      "[D loss: 0.773226] [G loss: 1.660091]\n",
      "[D loss: 0.780080] [G loss: 1.608412]\n",
      "[D loss: 1.148657] [G loss: 1.330907]\n",
      "[D loss: 0.836419] [G loss: 1.491498]\n",
      "[D loss: 0.735393] [G loss: 1.558610]\n",
      "[D loss: 0.955699] [G loss: 1.473470]\n",
      "[D loss: 0.679561] [G loss: 1.353904]\n",
      "[D loss: 0.920179] [G loss: 1.513649]\n",
      "[D loss: 0.905697] [G loss: 1.346359]\n",
      "[D loss: 0.925248] [G loss: 1.505676]\n",
      "[D loss: 0.883310] [G loss: 1.491890]\n",
      "[D loss: 0.871667] [G loss: 1.462976]\n",
      "[D loss: 0.748768] [G loss: 1.417383]\n",
      "[D loss: 0.847694] [G loss: 1.238423]\n",
      "[D loss: 0.832645] [G loss: 1.362698]\n",
      "[D loss: 0.771565] [G loss: 1.700014]\n",
      "[D loss: 0.719112] [G loss: 1.649030]\n",
      "[D loss: 0.832730] [G loss: 1.566656]\n",
      "[D loss: 0.437408] [G loss: 1.613831]\n",
      "[D loss: 0.808660] [G loss: 1.611090]\n",
      "[D loss: 0.612772] [G loss: 1.587393]\n",
      "[D loss: 0.884939] [G loss: 1.432213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D loss: 0.921828] [G loss: 1.485921]\n",
      "[D loss: 0.752081] [G loss: 1.397077]\n",
      "[D loss: 0.562890] [G loss: 1.483294]\n",
      "[D loss: 0.841833] [G loss: 1.405160]\n",
      "[D loss: 0.661876] [G loss: 1.500688]\n",
      "[D loss: 0.793272] [G loss: 1.277819]\n",
      "[D loss: 1.099249] [G loss: 1.405992]\n",
      "[D loss: 0.821283] [G loss: 1.369401]\n",
      "[D loss: 1.071697] [G loss: 1.516780]\n",
      "[D loss: 1.012444] [G loss: 1.467178]\n",
      "[D loss: 1.070923] [G loss: 1.574413]\n",
      "[D loss: 0.733868] [G loss: 1.553588]\n",
      "[D loss: 0.851477] [G loss: 1.383924]\n",
      "[D loss: 0.904410] [G loss: 1.330988]\n",
      "[D loss: 0.751352] [G loss: 1.534686]\n",
      "[D loss: 0.817088] [G loss: 1.256067]\n",
      "[D loss: 0.987490] [G loss: 1.482011]\n",
      "[D loss: 0.791845] [G loss: 1.338148]\n",
      "[D loss: 0.962094] [G loss: 1.495779]\n",
      "[D loss: 0.781825] [G loss: 1.467132]\n",
      "[D loss: 0.828866] [G loss: 1.437512]\n",
      "[D loss: 0.755314] [G loss: 1.682248]\n",
      "[D loss: 0.893806] [G loss: 1.270350]\n",
      "[D loss: 0.743218] [G loss: 1.419915]\n",
      "[D loss: 0.831322] [G loss: 1.389137]\n",
      "[D loss: 1.122896] [G loss: 1.262034]\n",
      "[D loss: 0.974534] [G loss: 1.467551]\n",
      "[D loss: 0.683176] [G loss: 1.568548]\n",
      "[D loss: 1.043009] [G loss: 1.206446]\n",
      "[D loss: 0.606012] [G loss: 1.399295]\n",
      "[D loss: 0.536113] [G loss: 1.661151]\n",
      "[D loss: 0.808867] [G loss: 1.474657]\n",
      "[D loss: 0.837488] [G loss: 1.538680]\n",
      "[D loss: 1.083106] [G loss: 1.417614]\n",
      "[D loss: 0.776913] [G loss: 1.564261]\n",
      "[D loss: 0.835172] [G loss: 1.395401]\n",
      "[D loss: 0.716510] [G loss: 1.501232]\n",
      "[D loss: 1.083608] [G loss: 1.282332]\n",
      "[D loss: 1.033252] [G loss: 1.533010]\n",
      "epoch:20, g_loss:2759.35498046875,d_loss:1559.114013671875\n"
     ]
    }
   ],
   "source": [
    "# epoch numbers\n",
    "for epoch in range(num_epoch):\n",
    "    total_g_loss = 0.0\n",
    "    total_d_loss = 0.0\n",
    "    # only need image,no use label\n",
    "    for i,(imgs,_) in enumerate(train_loader):\n",
    "        # true image label:1\n",
    "        valid = Variable(Tensor(imgs.size(0),1).fill_(1.0),requires_grad=False)\n",
    "        # fake image label:0\n",
    "        fake = Variable(Tensor(imgs.size(0),1).fill_(0.0),requires_grad=False)\n",
    "        \n",
    "        # real_image\n",
    "        real_image = Variable(imgs.type(Tensor))\n",
    "        \n",
    "        # gen_image\n",
    "        # train generator\n",
    "        g_optimizer.zero_grad()\n",
    "        # generator random noises\n",
    "        z = Variable(Tensor(\n",
    "            np.random.normal(0,1,(imgs.shape[0],latent_dim))\n",
    "        ))\n",
    "        # generator fake image\n",
    "        gen_image = G(z)\n",
    "        # generaor loss:make discriminator error\n",
    "        g_loss = loss_fn(D(gen_image),valid)\n",
    "        # gradient descent,update args\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        # train discriminator\n",
    "        d_optimizer.zero_grad()\n",
    "        # real image loss\n",
    "        real_loss = loss_fn(D(real_image),valid)\n",
    "        # fake image loss\n",
    "        fake_loss = loss_fn(D(gen_image.detach()),fake)\n",
    "        # the mean of real iamge and fake image\n",
    "        d_loss = real_loss+fake_loss\n",
    "        # gradient descent and update args\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "#         print(\n",
    "#             \"[D loss: %f] [G loss: %f]\"\n",
    "#             % (d_loss.item(), g_loss.item())\n",
    "#         )\n",
    "        total_d_loss += d_loss\n",
    "        total_g_loss += g_loss\n",
    "    print(\"epoch:{}, g_loss:{},d_loss:{}\".format(epoch+1,total_g_loss.item(),total_d_loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478e3412",
   "metadata": {},
   "source": [
    "## 测试图像生成效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2d01a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Variable(Tensor(\n",
    "    np.random.normal(0,1,(imgs.shape[0],latent_dim))\n",
    "))\n",
    "# generator fake image\n",
    "gen_image = G(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e376ef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "toPIL = torchvision.transforms.ToPILImage()\n",
    "pic = toPIL(gen_image[0])\n",
    "pic.save('result.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbfb18b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
